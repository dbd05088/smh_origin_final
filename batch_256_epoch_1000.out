Remove the tensorboard dir
[INFO] main.py:222 > Set the device (cuda)
[INFO] main.py:267 > Using train-transforms Compose(
    RandomHorizontalFlip(p=0.5)
    RandomCrop(size=(32, 32), padding=4)
    ToTensor()
    Normalize(mean=(0.4914, 0.482158, 0.4465231), std=(0.247032, 0.243485, 0.2615877))
)
[INFO] augment.py:18 > cifar10: autoaugmentation is applied
[INFO] main.py:290 > Using train-transforms [AutoAugment CIFAR10 Policy]
[INFO] main.py:300 > [1] Select a CIL method (rm)
[INFO] method_manager.py:48 > CIL Scenario: 
n_tasks: 5
n_init_cls: 2
n_cls_a_task: 2
total cls: 10
[INFO] main.py:306 > [2] Incrementally training 5 tasks

##################################################
# Task 0 iteration
##################################################

[INFO] main.py:316 > [2-1] Prepare a datalist for the current task
total : 1000  current step :  0
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.51s. Batch: 7.87s. S_Loss: 2.4182. T_Loss: 2.6348. Mask: 0.0000. :   0%|          | 0/50 [00:07<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.51s. Batch: 7.87s. S_Loss: 2.4182. T_Loss: 2.6348. Mask: 0.0000. :   2%|▏         | 1/50 [00:07<06:25,  7.88s/it]Train Iter:   2/1000. LR: 0.0000. Data: 0.26s. Batch: 4.04s. S_Loss: 2.4395. T_Loss: 2.5916. Mask: 0.0039. :   2%|▏         | 1/50 [00:08<06:25,  7.88s/it]Train Iter:   2/1000. LR: 0.0000. Data: 0.26s. Batch: 4.04s. S_Loss: 2.4395. T_Loss: 2.5916. Mask: 0.0039. :   4%|▍         | 2/50 [00:08<02:41,  3.37s/it]Train Iter:   3/1000. LR: 0.0000. Data: 0.17s. Batch: 2.75s. S_Loss: 2.4254. T_Loss: 2.5865. Mask: 0.0052. :   4%|▍         | 2/50 [00:08<02:41,  3.37s/it]Train Iter:   3/1000. LR: 0.0000. Data: 0.17s. Batch: 2.75s. S_Loss: 2.4254. T_Loss: 2.5865. Mask: 0.0052. :   6%|▌         | 3/50 [00:08<01:29,  1.90s/it]total : 1000  current step :  1
total : 1000  current step :  2
total : 1000  current step :  3
Train Iter:   4/1000. LR: 0.0000. Data: 0.26s. Batch: 2.26s. S_Loss: 2.4162. T_Loss: 2.5642. Mask: 0.0039. :   6%|▌         | 3/50 [00:09<01:29,  1.90s/it]Train Iter:   4/1000. LR: 0.0000. Data: 0.26s. Batch: 2.26s. S_Loss: 2.4162. T_Loss: 2.5642. Mask: 0.0039. :   8%|▊         | 4/50 [00:09<01:07,  1.46s/it]Train Iter:   5/1000. LR: 0.0000. Data: 0.21s. Batch: 1.84s. S_Loss: 2.4183. T_Loss: 2.5739. Mask: 0.0031. :   8%|▊         | 4/50 [00:09<01:07,  1.46s/it]Train Iter:   5/1000. LR: 0.0000. Data: 0.21s. Batch: 1.84s. S_Loss: 2.4183. T_Loss: 2.5739. Mask: 0.0031. :  10%|█         | 5/50 [00:09<00:44,  1.00it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.19s. Batch: 1.57s. S_Loss: 2.4121. T_Loss: 2.5727. Mask: 0.0026. :  10%|█         | 5/50 [00:09<00:44,  1.00it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.19s. Batch: 1.57s. S_Loss: 2.4121. T_Loss: 2.5727. Mask: 0.0026. :  12%|█▏        | 6/50 [00:09<00:32,  1.35it/s]total : 1000  current step :  4
total : 1000  current step :  5
total : 1000  current step :  6
Train Iter:   7/1000. LR: 0.0000. Data: 0.24s. Batch: 1.45s. S_Loss: 2.4063. T_Loss: 2.5528. Mask: 0.0022. :  12%|█▏        | 6/50 [00:10<00:32,  1.35it/s]Train Iter:   7/1000. LR: 0.0000. Data: 0.24s. Batch: 1.45s. S_Loss: 2.4063. T_Loss: 2.5528. Mask: 0.0022. :  14%|█▍        | 7/50 [00:10<00:31,  1.38it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.22s. Batch: 1.30s. S_Loss: 2.4106. T_Loss: 2.5461. Mask: 0.0024. :  14%|█▍        | 7/50 [00:10<00:31,  1.38it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.22s. Batch: 1.30s. S_Loss: 2.4106. T_Loss: 2.5461. Mask: 0.0024. :  16%|█▌        | 8/50 [00:10<00:24,  1.74it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.21s. Batch: 1.18s. S_Loss: 2.4116. T_Loss: 2.5324. Mask: 0.0026. :  16%|█▌        | 8/50 [00:10<00:24,  1.74it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.21s. Batch: 1.18s. S_Loss: 2.4116. T_Loss: 2.5324. Mask: 0.0026. :  18%|█▊        | 9/50 [00:10<00:19,  2.11it/s]total : 1000  current step :  7
total : 1000  current step :  8
total : 1000  current step :  9
Train Iter:  10/1000. LR: 0.0000. Data: 0.25s. Batch: 1.14s. S_Loss: 2.4120. T_Loss: 2.5254. Mask: 0.0023. :  18%|█▊        | 9/50 [00:11<00:19,  2.11it/s]Train Iter:  10/1000. LR: 0.0000. Data: 0.25s. Batch: 1.14s. S_Loss: 2.4120. T_Loss: 2.5254. Mask: 0.0023. :  20%|██        | 10/50 [00:11<00:22,  1.78it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.23s. Batch: 1.06s. S_Loss: 2.4088. T_Loss: 2.5113. Mask: 0.0025. :  20%|██        | 10/50 [00:11<00:22,  1.78it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.23s. Batch: 1.06s. S_Loss: 2.4088. T_Loss: 2.5113. Mask: 0.0025. :  22%|██▏       | 11/50 [00:11<00:18,  2.07it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.21s. Batch: 0.99s. S_Loss: 2.4121. T_Loss: 2.4951. Mask: 0.0026. :  22%|██▏       | 11/50 [00:11<00:18,  2.07it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.21s. Batch: 0.99s. S_Loss: 2.4121. T_Loss: 2.4951. Mask: 0.0026. :  24%|██▍       | 12/50 [00:11<00:14,  2.61it/s]total : 1000  current step :  10
total : 1000  current step :  11
total : 1000  current step :  12
Train Iter:  13/1000. LR: 0.0000. Data: 0.24s. Batch: 0.97s. S_Loss: 2.4115. T_Loss: 2.4788. Mask: 0.0024. :  24%|██▍       | 12/50 [00:12<00:14,  2.61it/s]Train Iter:  13/1000. LR: 0.0000. Data: 0.24s. Batch: 0.97s. S_Loss: 2.4115. T_Loss: 2.4788. Mask: 0.0024. :  26%|██▌       | 13/50 [00:12<00:18,  1.97it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.22s. Batch: 0.92s. S_Loss: 2.4164. T_Loss: 2.4543. Mask: 0.0025. :  26%|██▌       | 13/50 [00:12<00:18,  1.97it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.22s. Batch: 0.92s. S_Loss: 2.4164. T_Loss: 2.4543. Mask: 0.0025. :  28%|██▊       | 14/50 [00:12<00:15,  2.38it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.21s. Batch: 0.87s. S_Loss: 2.4203. T_Loss: 2.4334. Mask: 0.0029. :  28%|██▊       | 14/50 [00:13<00:15,  2.38it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.21s. Batch: 0.87s. S_Loss: 2.4203. T_Loss: 2.4334. Mask: 0.0029. :  30%|███       | 15/50 [00:13<00:12,  2.73it/s]total : 1000  current step :  13
total : 1000  current step :  14
total : 1000  current step :  15
Train Iter:  16/1000. LR: 0.0000. Data: 0.23s. Batch: 0.86s. S_Loss: 2.4264. T_Loss: 2.4133. Mask: 0.0027. :  30%|███       | 15/50 [00:13<00:12,  2.73it/s]Train Iter:  16/1000. LR: 0.0000. Data: 0.23s. Batch: 0.86s. S_Loss: 2.4264. T_Loss: 2.4133. Mask: 0.0027. :  32%|███▏      | 16/50 [00:13<00:16,  2.10it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.22s. Batch: 0.83s. S_Loss: 2.4297. T_Loss: 2.3917. Mask: 0.0025. :  32%|███▏      | 16/50 [00:14<00:16,  2.10it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.22s. Batch: 0.83s. S_Loss: 2.4297. T_Loss: 2.3917. Mask: 0.0025. :  34%|███▍      | 17/50 [00:14<00:13,  2.37it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.21s. Batch: 0.80s. S_Loss: 2.4332. T_Loss: 2.3731. Mask: 0.0026. :  34%|███▍      | 17/50 [00:14<00:13,  2.37it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.21s. Batch: 0.80s. S_Loss: 2.4332. T_Loss: 2.3731. Mask: 0.0026. :  36%|███▌      | 18/50 [00:14<00:12,  2.63it/s]total : 1000  current step :  16
total : 1000  current step :  17
total : 1000  current step :  18
Train Iter:  19/1000. LR: 0.0000. Data: 0.23s. Batch: 0.79s. S_Loss: 2.4402. T_Loss: 2.3449. Mask: 0.0025. :  36%|███▌      | 18/50 [00:15<00:12,  2.63it/s]Train Iter:  19/1000. LR: 0.0000. Data: 0.23s. Batch: 0.79s. S_Loss: 2.4402. T_Loss: 2.3449. Mask: 0.0025. :  38%|███▊      | 19/50 [00:15<00:14,  2.10it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.22s. Batch: 0.77s. S_Loss: 2.4419. T_Loss: 2.3220. Mask: 0.0031. :  38%|███▊      | 19/50 [00:15<00:14,  2.10it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.22s. Batch: 0.77s. S_Loss: 2.4419. T_Loss: 2.3220. Mask: 0.0031. :  40%|████      | 20/50 [00:15<00:12,  2.35it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.22s. Batch: 0.75s. S_Loss: 2.4476. T_Loss: 2.2984. Mask: 0.0033. :  40%|████      | 20/50 [00:15<00:12,  2.35it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.22s. Batch: 0.75s. S_Loss: 2.4476. T_Loss: 2.2984. Mask: 0.0033. :  42%|████▏     | 21/50 [00:15<00:10,  2.69it/s]total : 1000  current step :  19
total : 1000  current step :  20
total : 1000  current step :  21
Train Iter:  22/1000. LR: 0.0000. Data: 0.23s. Batch: 0.75s. S_Loss: 2.4531. T_Loss: 2.2736. Mask: 0.0057. :  42%|████▏     | 21/50 [00:16<00:10,  2.69it/s]Train Iter:  22/1000. LR: 0.0000. Data: 0.23s. Batch: 0.75s. S_Loss: 2.4531. T_Loss: 2.2736. Mask: 0.0057. :  44%|████▍     | 22/50 [00:16<00:13,  2.06it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.23s. Batch: 0.73s. S_Loss: 2.4563. T_Loss: 2.2479. Mask: 0.0095. :  44%|████▍     | 22/50 [00:16<00:13,  2.06it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.23s. Batch: 0.73s. S_Loss: 2.4563. T_Loss: 2.2479. Mask: 0.0095. :  46%|████▌     | 23/50 [00:16<00:11,  2.34it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.22s. Batch: 0.71s. S_Loss: 2.4601. T_Loss: 2.2251. Mask: 0.0155. :  46%|████▌     | 23/50 [00:17<00:11,  2.34it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.22s. Batch: 0.71s. S_Loss: 2.4601. T_Loss: 2.2251. Mask: 0.0155. :  48%|████▊     | 24/50 [00:17<00:10,  2.57it/s]total : 1000  current step :  22
total : 1000  current step :  23
total : 1000  current step :  24
Train Iter:  25/1000. LR: 0.0000. Data: 0.23s. Batch: 0.71s. S_Loss: 2.4622. T_Loss: 2.2051. Mask: 0.0238. :  48%|████▊     | 24/50 [00:17<00:10,  2.57it/s]Train Iter:  25/1000. LR: 0.0000. Data: 0.23s. Batch: 0.71s. S_Loss: 2.4622. T_Loss: 2.2051. Mask: 0.0238. :  50%|█████     | 25/50 [00:17<00:12,  2.06it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.23s. Batch: 0.69s. S_Loss: 2.4648. T_Loss: 2.1851. Mask: 0.0335. :  50%|█████     | 25/50 [00:18<00:12,  2.06it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.23s. Batch: 0.69s. S_Loss: 2.4648. T_Loss: 2.1851. Mask: 0.0335. :  52%|█████▏    | 26/50 [00:18<00:10,  2.33it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.22s. Batch: 0.68s. S_Loss: 2.4670. T_Loss: 2.1695. Mask: 0.0460. :  52%|█████▏    | 26/50 [00:18<00:10,  2.33it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.22s. Batch: 0.68s. S_Loss: 2.4670. T_Loss: 2.1695. Mask: 0.0460. :  54%|█████▍    | 27/50 [00:18<00:08,  2.62it/s]total : 1000  current step :  25
total : 1000  current step :  26
total : 1000  current step :  27
Train Iter:  28/1000. LR: 0.0000. Data: 0.24s. Batch: 0.68s. S_Loss: 2.4692. T_Loss: 2.1520. Mask: 0.0594. :  54%|█████▍    | 27/50 [00:19<00:08,  2.62it/s]Train Iter:  28/1000. LR: 0.0000. Data: 0.24s. Batch: 0.68s. S_Loss: 2.4692. T_Loss: 2.1520. Mask: 0.0594. :  56%|█████▌    | 28/50 [00:19<00:11,  1.92it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.24s. Batch: 0.67s. S_Loss: 2.4713. T_Loss: 2.1372. Mask: 0.0733. :  56%|█████▌    | 28/50 [00:19<00:11,  1.92it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.24s. Batch: 0.67s. S_Loss: 2.4713. T_Loss: 2.1372. Mask: 0.0733. :  58%|█████▊    | 29/50 [00:19<00:09,  2.14it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.24s. Batch: 0.66s. S_Loss: 2.4743. T_Loss: 2.1239. Mask: 0.0901. :  58%|█████▊    | 29/50 [00:19<00:09,  2.14it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.24s. Batch: 0.66s. S_Loss: 2.4743. T_Loss: 2.1239. Mask: 0.0901. :  60%|██████    | 30/50 [00:19<00:08,  2.23it/s]total : 1000  current step :  28
total : 1000  current step :  29
total : 1000  current step :  30
Train Iter:  31/1000. LR: 0.0000. Data: 0.25s. Batch: 0.67s. S_Loss: 2.4775. T_Loss: 2.1115. Mask: 0.1070. :  60%|██████    | 30/50 [00:20<00:08,  2.23it/s]Train Iter:  31/1000. LR: 0.0000. Data: 0.25s. Batch: 0.67s. S_Loss: 2.4775. T_Loss: 2.1115. Mask: 0.1070. :  62%|██████▏   | 31/50 [00:20<00:10,  1.75it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.24s. Batch: 0.65s. S_Loss: 2.4796. T_Loss: 2.1016. Mask: 0.1249. :  62%|██████▏   | 31/50 [00:20<00:10,  1.75it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.24s. Batch: 0.65s. S_Loss: 2.4796. T_Loss: 2.1016. Mask: 0.1249. :  64%|██████▍   | 32/50 [00:20<00:08,  2.14it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.24s. Batch: 0.64s. S_Loss: 2.4801. T_Loss: 2.0902. Mask: 0.1407. :  64%|██████▍   | 32/50 [00:21<00:08,  2.14it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.24s. Batch: 0.64s. S_Loss: 2.4801. T_Loss: 2.0902. Mask: 0.1407. :  66%|██████▌   | 33/50 [00:21<00:07,  2.35it/s]total : 1000  current step :  31
total : 1000  current step :  32
total : 1000  current step :  33
Train Iter:  34/1000. LR: 0.0000. Data: 0.25s. Batch: 0.65s. S_Loss: 2.4807. T_Loss: 2.0806. Mask: 0.1576. :  66%|██████▌   | 33/50 [00:22<00:07,  2.35it/s]Train Iter:  34/1000. LR: 0.0000. Data: 0.25s. Batch: 0.65s. S_Loss: 2.4807. T_Loss: 2.0806. Mask: 0.1576. :  68%|██████▊   | 34/50 [00:22<00:08,  1.85it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.25s. Batch: 0.64s. S_Loss: 2.4817. T_Loss: 2.0697. Mask: 0.1720. :  68%|██████▊   | 34/50 [00:22<00:08,  1.85it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.25s. Batch: 0.64s. S_Loss: 2.4817. T_Loss: 2.0697. Mask: 0.1720. :  70%|███████   | 35/50 [00:22<00:06,  2.18it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.24s. Batch: 0.63s. S_Loss: 2.4827. T_Loss: 2.0597. Mask: 0.1880. :  70%|███████   | 35/50 [00:22<00:06,  2.18it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.24s. Batch: 0.63s. S_Loss: 2.4827. T_Loss: 2.0597. Mask: 0.1880. :  72%|███████▏  | 36/50 [00:22<00:05,  2.49it/s]total : 1000  current step :  34
total : 1000  current step :  35
total : 1000  current step :  36
Train Iter:  37/1000. LR: 0.0000. Data: 0.25s. Batch: 0.63s. S_Loss: 2.4841. T_Loss: 2.0488. Mask: 0.2024. :  72%|███████▏  | 36/50 [00:23<00:05,  2.49it/s]Train Iter:  37/1000. LR: 0.0000. Data: 0.25s. Batch: 0.63s. S_Loss: 2.4841. T_Loss: 2.0488. Mask: 0.2024. :  74%|███████▍  | 37/50 [00:23<00:06,  1.92it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.25s. Batch: 0.63s. S_Loss: 2.4852. T_Loss: 2.0388. Mask: 0.2172. :  74%|███████▍  | 37/50 [00:23<00:06,  1.92it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.25s. Batch: 0.63s. S_Loss: 2.4852. T_Loss: 2.0388. Mask: 0.2172. :  76%|███████▌  | 38/50 [00:23<00:05,  2.10it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.25s. Batch: 0.62s. S_Loss: 2.4865. T_Loss: 2.0278. Mask: 0.2300. :  76%|███████▌  | 38/50 [00:24<00:05,  2.10it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.25s. Batch: 0.62s. S_Loss: 2.4865. T_Loss: 2.0278. Mask: 0.2300. :  78%|███████▊  | 39/50 [00:24<00:04,  2.35it/s]total : 1000  current step :  37
total : 1000  current step :  38
total : 1000  current step :  39
Train Iter:  40/1000. LR: 0.0000. Data: 0.26s. Batch: 0.63s. S_Loss: 2.4879. T_Loss: 2.0167. Mask: 0.2421. :  78%|███████▊  | 39/50 [00:25<00:04,  2.35it/s]Train Iter:  40/1000. LR: 0.0000. Data: 0.26s. Batch: 0.63s. S_Loss: 2.4879. T_Loss: 2.0167. Mask: 0.2421. :  80%|████████  | 40/50 [00:25<00:06,  1.63it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.27s. Batch: 0.63s. S_Loss: 2.4888. T_Loss: 2.0069. Mask: 0.2536. :  80%|████████  | 40/50 [00:25<00:06,  1.63it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.27s. Batch: 0.63s. S_Loss: 2.4888. T_Loss: 2.0069. Mask: 0.2536. :  82%|████████▏ | 41/50 [00:25<00:05,  1.65it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.27s. Batch: 0.62s. S_Loss: 2.4886. T_Loss: 1.9974. Mask: 0.2642. :  82%|████████▏ | 41/50 [00:26<00:05,  1.65it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.27s. Batch: 0.62s. S_Loss: 2.4886. T_Loss: 1.9974. Mask: 0.2642. :  84%|████████▍ | 42/50 [00:26<00:04,  1.80it/s]total : 1000  current step :  40
total : 1000  current step :  41
total : 1000  current step :  42
Train Iter:  43/1000. LR: 0.0000. Data: 0.28s. Batch: 0.63s. S_Loss: 2.4889. T_Loss: 1.9885. Mask: 0.2756. :  84%|████████▍ | 42/50 [00:26<00:04,  1.80it/s]Train Iter:  43/1000. LR: 0.0000. Data: 0.28s. Batch: 0.63s. S_Loss: 2.4889. T_Loss: 1.9885. Mask: 0.2756. :  86%|████████▌ | 43/50 [00:26<00:04,  1.62it/s]Train Iter:  44/1000. LR: 0.0000. Data: 0.27s. Batch: 0.62s. S_Loss: 2.4895. T_Loss: 1.9778. Mask: 0.2845. :  86%|████████▌ | 43/50 [00:27<00:04,  1.62it/s]Train Iter:  44/1000. LR: 0.0000. Data: 0.27s. Batch: 0.62s. S_Loss: 2.4895. T_Loss: 1.9778. Mask: 0.2845. :  88%|████████▊ | 44/50 [00:27<00:03,  1.82it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.27s. Batch: 0.61s. S_Loss: 2.4900. T_Loss: 1.9685. Mask: 0.2933. :  88%|████████▊ | 44/50 [00:27<00:03,  1.82it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.27s. Batch: 0.61s. S_Loss: 2.4900. T_Loss: 1.9685. Mask: 0.2933. :  90%|█████████ | 45/50 [00:27<00:02,  2.08it/s]total : 1000  current step :  43
total : 1000  current step :  44
total : 1000  current step :  45
Train Iter:  46/1000. LR: 0.0000. Data: 0.28s. Batch: 0.62s. S_Loss: 2.4905. T_Loss: 1.9586. Mask: 0.3015. :  90%|█████████ | 45/50 [00:28<00:02,  2.08it/s]Train Iter:  46/1000. LR: 0.0000. Data: 0.28s. Batch: 0.62s. S_Loss: 2.4905. T_Loss: 1.9586. Mask: 0.3015. :  92%|█████████▏| 46/50 [00:28<00:02,  1.73it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.27s. Batch: 0.62s. S_Loss: 2.4907. T_Loss: 1.9504. Mask: 0.3108. :  92%|█████████▏| 46/50 [00:29<00:02,  1.73it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.27s. Batch: 0.62s. S_Loss: 2.4907. T_Loss: 1.9504. Mask: 0.3108. :  94%|█████████▍| 47/50 [00:29<00:01,  1.75it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.27s. Batch: 0.61s. S_Loss: 2.4913. T_Loss: 1.9427. Mask: 0.3194. :  94%|█████████▍| 47/50 [00:29<00:01,  1.75it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.27s. Batch: 0.61s. S_Loss: 2.4913. T_Loss: 1.9427. Mask: 0.3194. :  96%|█████████▌| 48/50 [00:29<00:00,  2.13it/s]total : 1000  current step :  46
total : 1000  current step :  47
total : 1000  current step :  48
Train Iter:  49/1000. LR: 0.0000. Data: 0.27s. Batch: 0.61s. S_Loss: 2.4919. T_Loss: 1.9353. Mask: 0.3268. :  96%|█████████▌| 48/50 [00:30<00:00,  2.13it/s]Train Iter:  49/1000. LR: 0.0000. Data: 0.27s. Batch: 0.61s. S_Loss: 2.4919. T_Loss: 1.9353. Mask: 0.3268. :  98%|█████████▊| 49/50 [00:30<00:00,  1.80it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.27s. Batch: 0.61s. S_Loss: 2.4926. T_Loss: 1.9296. Mask: 0.3352. :  98%|█████████▊| 49/50 [00:30<00:00,  1.80it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.27s. Batch: 0.61s. S_Loss: 2.4926. T_Loss: 1.9296. Mask: 0.3352. : 100%|██████████| 50/50 [00:30<00:00,  2.02it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.27s. Batch: 0.61s. S_Loss: 2.4926. T_Loss: 1.9296. Mask: 0.3352. : 100%|██████████| 50/50 [00:30<00:00,  1.65it/s]
total : 1000  current step :  49
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 4.4789. top1: 0.00. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 4.4789. top1: 0.00. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.07it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 4.3353. top1: 0.00. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.07it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 4.3353. top1: 0.00. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 4.3158. top1: 0.00. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 4.3158. top1: 0.00. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 4.4239. top1: 0.00. top5: 99.90. :  38%|███▊      | 3/8 [00:01<00:01,  3.56it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 4.4239. top1: 0.00. top5: 99.90. :  50%|█████     | 4/8 [00:01<00:01,  3.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 4.8005. top1: 0.00. top5: 99.92. :  50%|█████     | 4/8 [00:01<00:01,  3.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 4.8005. top1: 0.00. top5: 99.92. :  62%|██████▎   | 5/8 [00:01<00:00,  4.09it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 5.1094. top1: 0.00. top5: 99.87. :  62%|██████▎   | 5/8 [00:01<00:00,  4.09it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 5.1094. top1: 0.00. top5: 99.87. :  75%|███████▌  | 6/8 [00:01<00:00,  4.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 5.3312. top1: 0.00. top5: 99.89. :  75%|███████▌  | 6/8 [00:01<00:00,  4.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 5.3312. top1: 0.00. top5: 99.89. :  88%|████████▊ | 7/8 [00:01<00:00,  4.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 5.4316. top1: 0.00. top5: 99.90. :  88%|████████▊ | 7/8 [00:02<00:00,  4.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 5.4316. top1: 0.00. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  4.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 5.4316. top1: 0.00. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  3.77it/s]
total : 1000  current step :  50
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.01s. Batch: 0.23s. S_Loss: 2.4983. T_Loss: 1.5815. Mask: 0.7617. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.01s. Batch: 0.23s. S_Loss: 2.4983. T_Loss: 1.5815. Mask: 0.7617. :   2%|▏         | 1/50 [00:00<00:11,  4.25it/s]total : 1000  current step :  51
Train Iter:  52/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.5187. T_Loss: 1.5831. Mask: 0.7656. :   2%|▏         | 1/50 [00:01<00:11,  4.25it/s]Train Iter:  52/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.5187. T_Loss: 1.5831. Mask: 0.7656. :   4%|▍         | 2/50 [00:01<00:27,  1.74it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.21s. Batch: 0.49s. S_Loss: 2.5220. T_Loss: 1.5947. Mask: 0.7656. :   4%|▍         | 2/50 [00:01<00:27,  1.74it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.21s. Batch: 0.49s. S_Loss: 2.5220. T_Loss: 1.5947. Mask: 0.7656. :   6%|▌         | 3/50 [00:01<00:23,  1.97it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.16s. Batch: 0.42s. S_Loss: 2.5218. T_Loss: 1.5930. Mask: 0.7686. :   6%|▌         | 3/50 [00:01<00:23,  1.97it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.16s. Batch: 0.42s. S_Loss: 2.5218. T_Loss: 1.5930. Mask: 0.7686. :   8%|▊         | 4/50 [00:01<00:18,  2.55it/s]total : 1000  current step :  52
total : 1000  current step :  53
total : 1000  current step :  54
Train Iter:  55/1000. LR: 0.0000. Data: 0.26s. Batch: 0.50s. S_Loss: 2.5325. T_Loss: 1.5997. Mask: 0.7633. :   8%|▊         | 4/50 [00:02<00:18,  2.55it/s]Train Iter:  55/1000. LR: 0.0000. Data: 0.26s. Batch: 0.50s. S_Loss: 2.5325. T_Loss: 1.5997. Mask: 0.7633. :  10%|█         | 5/50 [00:02<00:24,  1.83it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.22s. Batch: 0.46s. S_Loss: 2.5362. T_Loss: 1.6066. Mask: 0.7682. :  10%|█         | 5/50 [00:02<00:24,  1.83it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.22s. Batch: 0.46s. S_Loss: 2.5362. T_Loss: 1.6066. Mask: 0.7682. :  12%|█▏        | 6/50 [00:02<00:19,  2.28it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.19s. Batch: 0.43s. S_Loss: 2.5332. T_Loss: 1.6151. Mask: 0.7757. :  12%|█▏        | 6/50 [00:02<00:19,  2.28it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.19s. Batch: 0.43s. S_Loss: 2.5332. T_Loss: 1.6151. Mask: 0.7757. :  14%|█▍        | 7/50 [00:02<00:16,  2.66it/s]total : 1000  current step :  55
total : 1000  current step :  56
total : 1000  current step :  57
Train Iter:  58/1000. LR: 0.0000. Data: 0.25s. Batch: 0.48s. S_Loss: 2.5362. T_Loss: 1.6268. Mask: 0.7856. :  14%|█▍        | 7/50 [00:03<00:16,  2.66it/s]Train Iter:  58/1000. LR: 0.0000. Data: 0.25s. Batch: 0.48s. S_Loss: 2.5362. T_Loss: 1.6268. Mask: 0.7856. :  16%|█▌        | 8/50 [00:03<00:22,  1.89it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.24s. Batch: 0.46s. S_Loss: 2.5368. T_Loss: 1.6222. Mask: 0.7847. :  16%|█▌        | 8/50 [00:04<00:22,  1.89it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.24s. Batch: 0.46s. S_Loss: 2.5368. T_Loss: 1.6222. Mask: 0.7847. :  18%|█▊        | 9/50 [00:04<00:18,  2.20it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.22s. Batch: 0.46s. S_Loss: 2.5352. T_Loss: 1.6235. Mask: 0.7855. :  18%|█▊        | 9/50 [00:04<00:18,  2.20it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.22s. Batch: 0.46s. S_Loss: 2.5352. T_Loss: 1.6235. Mask: 0.7855. :  20%|██        | 10/50 [00:04<00:18,  2.19it/s]total : 1000  current step :  58
total : 1000  current step :  59
total : 1000  current step :  60
Train Iter:  61/1000. LR: 0.0000. Data: 0.27s. Batch: 0.50s. S_Loss: 2.5384. T_Loss: 1.6284. Mask: 0.7908. :  20%|██        | 10/50 [00:05<00:18,  2.19it/s]Train Iter:  61/1000. LR: 0.0000. Data: 0.27s. Batch: 0.50s. S_Loss: 2.5384. T_Loss: 1.6284. Mask: 0.7908. :  22%|██▏       | 11/50 [00:05<00:23,  1.68it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.26s. Batch: 0.49s. S_Loss: 2.5365. T_Loss: 1.6290. Mask: 0.7930. :  22%|██▏       | 11/50 [00:05<00:23,  1.68it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.26s. Batch: 0.49s. S_Loss: 2.5365. T_Loss: 1.6290. Mask: 0.7930. :  24%|██▍       | 12/50 [00:05<00:19,  1.91it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.24s. Batch: 0.47s. S_Loss: 2.5390. T_Loss: 1.6311. Mask: 0.7930. :  24%|██▍       | 12/50 [00:06<00:19,  1.91it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.24s. Batch: 0.47s. S_Loss: 2.5390. T_Loss: 1.6311. Mask: 0.7930. :  26%|██▌       | 13/50 [00:06<00:16,  2.24it/s]total : 1000  current step :  61
total : 1000  current step :  62
total : 1000  current step :  63
Train Iter:  64/1000. LR: 0.0000. Data: 0.26s. Batch: 0.49s. S_Loss: 2.5381. T_Loss: 1.6376. Mask: 0.7994. :  26%|██▌       | 13/50 [00:06<00:16,  2.24it/s]Train Iter:  64/1000. LR: 0.0000. Data: 0.26s. Batch: 0.49s. S_Loss: 2.5381. T_Loss: 1.6376. Mask: 0.7994. :  28%|██▊       | 14/50 [00:06<00:19,  1.83it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5390. T_Loss: 1.6420. Mask: 0.8060. :  28%|██▊       | 14/50 [00:07<00:19,  1.83it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5390. T_Loss: 1.6420. Mask: 0.8060. :  30%|███       | 15/50 [00:07<00:15,  2.30it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5393. T_Loss: 1.6446. Mask: 0.8076. :  30%|███       | 15/50 [00:07<00:15,  2.30it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5393. T_Loss: 1.6446. Mask: 0.8076. :  32%|███▏      | 16/50 [00:07<00:15,  2.21it/s]total : 1000  current step :  64
total : 1000  current step :  65
total : 1000  current step :  66
Train Iter:  67/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5366. T_Loss: 1.6506. Mask: 0.8086. :  32%|███▏      | 16/50 [00:08<00:15,  2.21it/s]Train Iter:  67/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5366. T_Loss: 1.6506. Mask: 0.8086. :  34%|███▍      | 17/50 [00:08<00:18,  1.78it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5360. T_Loss: 1.6504. Mask: 0.8071. :  34%|███▍      | 17/50 [00:08<00:18,  1.78it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5360. T_Loss: 1.6504. Mask: 0.8071. :  36%|███▌      | 18/50 [00:08<00:14,  2.16it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5356. T_Loss: 1.6547. Mask: 0.8082. :  36%|███▌      | 18/50 [00:08<00:14,  2.16it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5356. T_Loss: 1.6547. Mask: 0.8082. :  38%|███▊      | 19/50 [00:08<00:12,  2.52it/s]total : 1000  current step :  67
total : 1000  current step :  68
total : 1000  current step :  69
Train Iter:  70/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5355. T_Loss: 1.6544. Mask: 0.8063. :  38%|███▊      | 19/50 [00:09<00:12,  2.52it/s]Train Iter:  70/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5355. T_Loss: 1.6544. Mask: 0.8063. :  40%|████      | 20/50 [00:09<00:15,  1.89it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5363. T_Loss: 1.6570. Mask: 0.8060. :  40%|████      | 20/50 [00:09<00:15,  1.89it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5363. T_Loss: 1.6570. Mask: 0.8060. :  42%|████▏     | 21/50 [00:09<00:13,  2.22it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5356. T_Loss: 1.6607. Mask: 0.8089. :  42%|████▏     | 21/50 [00:10<00:13,  2.22it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5356. T_Loss: 1.6607. Mask: 0.8089. :  44%|████▍     | 22/50 [00:10<00:11,  2.48it/s]total : 1000  current step :  70
total : 1000  current step :  71
total : 1000  current step :  72
Train Iter:  73/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5330. T_Loss: 1.6634. Mask: 0.8086. :  44%|████▍     | 22/50 [00:11<00:11,  2.48it/s]Train Iter:  73/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5330. T_Loss: 1.6634. Mask: 0.8086. :  46%|████▌     | 23/50 [00:11<00:14,  1.86it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5327. T_Loss: 1.6674. Mask: 0.8102. :  46%|████▌     | 23/50 [00:11<00:14,  1.86it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5327. T_Loss: 1.6674. Mask: 0.8102. :  48%|████▊     | 24/50 [00:11<00:13,  2.00it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5304. T_Loss: 1.6692. Mask: 0.8102. :  48%|████▊     | 24/50 [00:11<00:13,  2.00it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5304. T_Loss: 1.6692. Mask: 0.8102. :  50%|█████     | 25/50 [00:11<00:09,  2.51it/s]total : 1000  current step :  73
total : 1000  current step :  74
total : 1000  current step :  75
Train Iter:  76/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5300. T_Loss: 1.6715. Mask: 0.8114. :  50%|█████     | 25/50 [00:12<00:09,  2.51it/s]Train Iter:  76/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5300. T_Loss: 1.6715. Mask: 0.8114. :  52%|█████▏    | 26/50 [00:12<00:12,  1.85it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5304. T_Loss: 1.6770. Mask: 0.8131. :  52%|█████▏    | 26/50 [00:12<00:12,  1.85it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5304. T_Loss: 1.6770. Mask: 0.8131. :  54%|█████▍    | 27/50 [00:12<00:10,  2.24it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5306. T_Loss: 1.6777. Mask: 0.8140. :  54%|█████▍    | 27/50 [00:13<00:10,  2.24it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5306. T_Loss: 1.6777. Mask: 0.8140. :  56%|█████▌    | 28/50 [00:13<00:08,  2.61it/s]total : 1000  current step :  76
total : 1000  current step :  77
total : 1000  current step :  78
Train Iter:  79/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5296. T_Loss: 1.6802. Mask: 0.8152. :  56%|█████▌    | 28/50 [00:13<00:08,  2.61it/s]Train Iter:  79/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5296. T_Loss: 1.6802. Mask: 0.8152. :  58%|█████▊    | 29/50 [00:13<00:10,  1.92it/s]total : 1000  current step :  79
Train Iter:  80/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5290. T_Loss: 1.6822. Mask: 0.8154. :  58%|█████▊    | 29/50 [00:14<00:10,  1.92it/s]Train Iter:  80/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5290. T_Loss: 1.6822. Mask: 0.8154. :  60%|██████    | 30/50 [00:14<00:10,  1.93it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5289. T_Loss: 1.6852. Mask: 0.8177. :  60%|██████    | 30/50 [00:15<00:10,  1.93it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5289. T_Loss: 1.6852. Mask: 0.8177. :  62%|██████▏   | 31/50 [00:15<00:11,  1.65it/s]total : 1000  current step :  80
total : 1000  current step :  81
Train Iter:  82/1000. LR: 0.0000. Data: 0.28s. Batch: 0.50s. S_Loss: 2.5290. T_Loss: 1.6870. Mask: 0.8185. :  62%|██████▏   | 31/50 [00:15<00:11,  1.65it/s]Train Iter:  82/1000. LR: 0.0000. Data: 0.28s. Batch: 0.50s. S_Loss: 2.5290. T_Loss: 1.6870. Mask: 0.8185. :  64%|██████▍   | 32/50 [00:15<00:11,  1.57it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5289. T_Loss: 1.6908. Mask: 0.8202. :  64%|██████▍   | 32/50 [00:16<00:11,  1.57it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5289. T_Loss: 1.6908. Mask: 0.8202. :  66%|██████▌   | 33/50 [00:16<00:08,  1.90it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5294. T_Loss: 1.6924. Mask: 0.8212. :  66%|██████▌   | 33/50 [00:16<00:08,  1.90it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5294. T_Loss: 1.6924. Mask: 0.8212. :  68%|██████▊   | 34/50 [00:16<00:07,  2.24it/s]total : 1000  current step :  82
total : 1000  current step :  83
total : 1000  current step :  84
Train Iter:  85/1000. LR: 0.0000. Data: 0.28s. Batch: 0.49s. S_Loss: 2.5301. T_Loss: 1.6959. Mask: 0.8225. :  68%|██████▊   | 34/50 [00:17<00:07,  2.24it/s]Train Iter:  85/1000. LR: 0.0000. Data: 0.28s. Batch: 0.49s. S_Loss: 2.5301. T_Loss: 1.6959. Mask: 0.8225. :  70%|███████   | 35/50 [00:17<00:08,  1.84it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5315. T_Loss: 1.6978. Mask: 0.8228. :  70%|███████   | 35/50 [00:17<00:08,  1.84it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5315. T_Loss: 1.6978. Mask: 0.8228. :  72%|███████▏  | 36/50 [00:17<00:06,  2.26it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5308. T_Loss: 1.7024. Mask: 0.8251. :  72%|███████▏  | 36/50 [00:17<00:06,  2.26it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5308. T_Loss: 1.7024. Mask: 0.8251. :  74%|███████▍  | 37/50 [00:17<00:05,  2.44it/s]total : 1000  current step :  85
total : 1000  current step :  86
total : 1000  current step :  87
Train Iter:  88/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5295. T_Loss: 1.7060. Mask: 0.8246. :  74%|███████▍  | 37/50 [00:18<00:05,  2.44it/s]Train Iter:  88/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5295. T_Loss: 1.7060. Mask: 0.8246. :  76%|███████▌  | 38/50 [00:18<00:06,  1.75it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5292. T_Loss: 1.7091. Mask: 0.8254. :  76%|███████▌  | 38/50 [00:18<00:06,  1.75it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5292. T_Loss: 1.7091. Mask: 0.8254. :  78%|███████▊  | 39/50 [00:18<00:04,  2.24it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5292. T_Loss: 1.7116. Mask: 0.8266. :  78%|███████▊  | 39/50 [00:19<00:04,  2.24it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5292. T_Loss: 1.7116. Mask: 0.8266. :  80%|████████  | 40/50 [00:19<00:03,  2.78it/s]total : 1000  current step :  88
total : 1000  current step :  89
total : 1000  current step :  90
Train Iter:  91/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5292. T_Loss: 1.7145. Mask: 0.8279. :  80%|████████  | 40/50 [00:19<00:03,  2.78it/s]Train Iter:  91/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5292. T_Loss: 1.7145. Mask: 0.8279. :  82%|████████▏ | 41/50 [00:19<00:04,  2.01it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5291. T_Loss: 1.7159. Mask: 0.8283. :  82%|████████▏ | 41/50 [00:20<00:04,  2.01it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5291. T_Loss: 1.7159. Mask: 0.8283. :  84%|████████▍ | 42/50 [00:20<00:03,  2.41it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5297. T_Loss: 1.7185. Mask: 0.8287. :  84%|████████▍ | 42/50 [00:20<00:03,  2.41it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5297. T_Loss: 1.7185. Mask: 0.8287. :  86%|████████▌ | 43/50 [00:20<00:02,  2.89it/s]total : 1000  current step :  91
total : 1000  current step :  92
total : 1000  current step :  93
Train Iter:  94/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5301. T_Loss: 1.7216. Mask: 0.8296. :  86%|████████▌ | 43/50 [00:20<00:02,  2.89it/s]Train Iter:  94/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5301. T_Loss: 1.7216. Mask: 0.8296. :  88%|████████▊ | 44/50 [00:20<00:02,  2.12it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5300. T_Loss: 1.7237. Mask: 0.8298. :  88%|████████▊ | 44/50 [00:21<00:02,  2.12it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5300. T_Loss: 1.7237. Mask: 0.8298. :  90%|█████████ | 45/50 [00:21<00:02,  2.36it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5296. T_Loss: 1.7261. Mask: 0.8307. :  90%|█████████ | 45/50 [00:21<00:02,  2.36it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5296. T_Loss: 1.7261. Mask: 0.8307. :  92%|█████████▏| 46/50 [00:21<00:01,  2.76it/s]total : 1000  current step :  94
total : 1000  current step :  95
total : 1000  current step :  96
Train Iter:  97/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5298. T_Loss: 1.7288. Mask: 0.8319. :  92%|█████████▏| 46/50 [00:22<00:01,  2.76it/s]Train Iter:  97/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5298. T_Loss: 1.7288. Mask: 0.8319. :  94%|█████████▍| 47/50 [00:22<00:01,  2.20it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5303. T_Loss: 1.7328. Mask: 0.8328. :  94%|█████████▍| 47/50 [00:22<00:01,  2.20it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5303. T_Loss: 1.7328. Mask: 0.8328. :  96%|█████████▌| 48/50 [00:22<00:00,  2.43it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5305. T_Loss: 1.7357. Mask: 0.8337. :  96%|█████████▌| 48/50 [00:22<00:00,  2.43it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5305. T_Loss: 1.7357. Mask: 0.8337. :  98%|█████████▊| 49/50 [00:22<00:00,  2.86it/s]total : 1000  current step :  97
total : 1000  current step :  98
total : 1000  current step :  99
Train Iter: 100/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5309. T_Loss: 1.7378. Mask: 0.8352. :  98%|█████████▊| 49/50 [00:23<00:00,  2.86it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5309. T_Loss: 1.7378. Mask: 0.8352. : 100%|██████████| 50/50 [00:23<00:00,  2.07it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5309. T_Loss: 1.7378. Mask: 0.8352. : 100%|██████████| 50/50 [00:23<00:00,  2.13it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 2.4648. top1: 0.00. top5: 99.22. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 2.4648. top1: 0.00. top5: 99.22. :  12%|█▎        | 1/8 [00:00<00:03,  2.33it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.4353. top1: 0.00. top5: 99.41. :  12%|█▎        | 1/8 [00:00<00:03,  2.33it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.4353. top1: 0.00. top5: 99.41. :  25%|██▌       | 2/8 [00:00<00:01,  3.40it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.4321. top1: 0.00. top5: 99.61. :  25%|██▌       | 2/8 [00:00<00:01,  3.40it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.4321. top1: 0.00. top5: 99.61. :  38%|███▊      | 3/8 [00:00<00:01,  3.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.26s. Loss: 2.4713. top1: 0.00. top5: 99.32. :  38%|███▊      | 3/8 [00:01<00:01,  3.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.26s. Loss: 2.4713. top1: 0.00. top5: 99.32. :  50%|█████     | 4/8 [00:01<00:00,  4.26it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 2.6375. top1: 0.00. top5: 98.98. :  50%|█████     | 4/8 [00:01<00:00,  4.26it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 2.6375. top1: 0.00. top5: 98.98. :  62%|██████▎   | 5/8 [00:01<00:00,  4.47it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 2.7697. top1: 0.00. top5: 98.76. :  62%|██████▎   | 5/8 [00:01<00:00,  4.47it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 2.7697. top1: 0.00. top5: 98.76. :  75%|███████▌  | 6/8 [00:01<00:00,  4.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 2.8680. top1: 0.00. top5: 98.72. :  75%|███████▌  | 6/8 [00:01<00:00,  4.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 2.8680. top1: 0.00. top5: 98.72. :  88%|████████▊ | 7/8 [00:01<00:00,  4.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 2.9155. top1: 0.00. top5: 98.35. :  88%|████████▊ | 7/8 [00:01<00:00,  4.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 2.9155. top1: 0.00. top5: 98.35. : 100%|██████████| 8/8 [00:01<00:00,  4.90it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 2.9155. top1: 0.00. top5: 98.35. : 100%|██████████| 8/8 [00:01<00:00,  4.15it/s]
total : 1000  current step :  100
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.17s. S_Loss: 2.4889. T_Loss: 1.8718. Mask: 0.8906. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.17s. S_Loss: 2.4889. T_Loss: 1.8718. Mask: 0.8906. :   2%|▏         | 1/50 [00:00<00:08,  5.74it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.32s. S_Loss: 2.4939. T_Loss: 1.8841. Mask: 0.8984. :   2%|▏         | 1/50 [00:00<00:08,  5.74it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.32s. S_Loss: 2.4939. T_Loss: 1.8841. Mask: 0.8984. :   4%|▍         | 2/50 [00:00<00:16,  2.91it/s]total : 1000  current step :  101
total : 1000  current step :  102
Train Iter: 103/1000. LR: 0.0000. Data: 0.20s. Batch: 0.47s. S_Loss: 2.4992. T_Loss: 1.9011. Mask: 0.9036. :   4%|▍         | 2/50 [00:01<00:16,  2.91it/s]Train Iter: 103/1000. LR: 0.0000. Data: 0.20s. Batch: 0.47s. S_Loss: 2.4992. T_Loss: 1.9011. Mask: 0.9036. :   6%|▌         | 3/50 [00:01<00:25,  1.84it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.17s. Batch: 0.43s. S_Loss: 2.5049. T_Loss: 1.9116. Mask: 0.9072. :   6%|▌         | 3/50 [00:01<00:25,  1.84it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.17s. Batch: 0.43s. S_Loss: 2.5049. T_Loss: 1.9116. Mask: 0.9072. :   8%|▊         | 4/50 [00:01<00:20,  2.24it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.14s. Batch: 0.39s. S_Loss: 2.5142. T_Loss: 1.9125. Mask: 0.9031. :   8%|▊         | 4/50 [00:01<00:20,  2.24it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.14s. Batch: 0.39s. S_Loss: 2.5142. T_Loss: 1.9125. Mask: 0.9031. :  10%|█         | 5/50 [00:01<00:16,  2.67it/s]total : 1000  current step :  103
total : 1000  current step :  104
total : 1000  current step :  105
Train Iter: 106/1000. LR: 0.0000. Data: 0.23s. Batch: 0.47s. S_Loss: 2.5119. T_Loss: 1.9174. Mask: 0.8997. :  10%|█         | 5/50 [00:02<00:16,  2.67it/s]Train Iter: 106/1000. LR: 0.0000. Data: 0.23s. Batch: 0.47s. S_Loss: 2.5119. T_Loss: 1.9174. Mask: 0.8997. :  12%|█▏        | 6/50 [00:02<00:23,  1.87it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.22s. Batch: 0.45s. S_Loss: 2.5109. T_Loss: 1.9087. Mask: 0.9007. :  12%|█▏        | 6/50 [00:03<00:23,  1.87it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.22s. Batch: 0.45s. S_Loss: 2.5109. T_Loss: 1.9087. Mask: 0.9007. :  14%|█▍        | 7/50 [00:03<00:20,  2.13it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.19s. Batch: 0.42s. S_Loss: 2.5144. T_Loss: 1.9099. Mask: 0.8989. :  14%|█▍        | 7/50 [00:03<00:20,  2.13it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.19s. Batch: 0.42s. S_Loss: 2.5144. T_Loss: 1.9099. Mask: 0.8989. :  16%|█▌        | 8/50 [00:03<00:16,  2.51it/s]total : 1000  current step :  106
total : 1000  current step :  107
total : 1000  current step :  108
Train Iter: 109/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5181. T_Loss: 1.9162. Mask: 0.9002. :  16%|█▌        | 8/50 [00:04<00:16,  2.51it/s]Train Iter: 109/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5181. T_Loss: 1.9162. Mask: 0.9002. :  18%|█▊        | 9/50 [00:04<00:22,  1.82it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.23s. Batch: 0.45s. S_Loss: 2.5157. T_Loss: 1.9148. Mask: 0.9031. :  18%|█▊        | 9/50 [00:04<00:22,  1.82it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.23s. Batch: 0.45s. S_Loss: 2.5157. T_Loss: 1.9148. Mask: 0.9031. :  20%|██        | 10/50 [00:04<00:18,  2.15it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.21s. Batch: 0.44s. S_Loss: 2.5183. T_Loss: 1.9183. Mask: 0.9016. :  20%|██        | 10/50 [00:04<00:18,  2.15it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.21s. Batch: 0.44s. S_Loss: 2.5183. T_Loss: 1.9183. Mask: 0.9016. :  22%|██▏       | 11/50 [00:04<00:15,  2.49it/s]total : 1000  current step :  109
total : 1000  current step :  110
total : 1000  current step :  111
Train Iter: 112/1000. LR: 0.0000. Data: 0.24s. Batch: 0.47s. S_Loss: 2.5218. T_Loss: 1.9265. Mask: 0.8991. :  22%|██▏       | 11/50 [00:05<00:15,  2.49it/s]Train Iter: 112/1000. LR: 0.0000. Data: 0.24s. Batch: 0.47s. S_Loss: 2.5218. T_Loss: 1.9265. Mask: 0.8991. :  24%|██▍       | 12/50 [00:05<00:19,  1.91it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.23s. Batch: 0.45s. S_Loss: 2.5252. T_Loss: 1.9317. Mask: 0.8978. :  24%|██▍       | 12/50 [00:05<00:19,  1.91it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.23s. Batch: 0.45s. S_Loss: 2.5252. T_Loss: 1.9317. Mask: 0.8978. :  26%|██▌       | 13/50 [00:05<00:16,  2.30it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.22s. Batch: 0.44s. S_Loss: 2.5259. T_Loss: 1.9317. Mask: 0.8982. :  26%|██▌       | 13/50 [00:06<00:16,  2.30it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.22s. Batch: 0.44s. S_Loss: 2.5259. T_Loss: 1.9317. Mask: 0.8982. :  28%|██▊       | 14/50 [00:06<00:14,  2.46it/s]total : 1000  current step :  112
total : 1000  current step :  113
total : 1000  current step :  114
Train Iter: 115/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5270. T_Loss: 1.9350. Mask: 0.8974. :  28%|██▊       | 14/50 [00:07<00:14,  2.46it/s]Train Iter: 115/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5270. T_Loss: 1.9350. Mask: 0.8974. :  30%|███       | 15/50 [00:07<00:18,  1.84it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5267. T_Loss: 1.9407. Mask: 0.8982. :  30%|███       | 15/50 [00:07<00:18,  1.84it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5267. T_Loss: 1.9407. Mask: 0.8982. :  32%|███▏      | 16/50 [00:07<00:17,  1.90it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.23s. Batch: 0.45s. S_Loss: 2.5269. T_Loss: 1.9407. Mask: 0.9007. :  32%|███▏      | 16/50 [00:07<00:17,  1.90it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.23s. Batch: 0.45s. S_Loss: 2.5269. T_Loss: 1.9407. Mask: 0.9007. :  34%|███▍      | 17/50 [00:07<00:14,  2.28it/s]total : 1000  current step :  115
total : 1000  current step :  116
total : 1000  current step :  117
Train Iter: 118/1000. LR: 0.0000. Data: 0.25s. Batch: 0.48s. S_Loss: 2.5267. T_Loss: 1.9394. Mask: 0.8991. :  34%|███▍      | 17/50 [00:08<00:14,  2.28it/s]Train Iter: 118/1000. LR: 0.0000. Data: 0.25s. Batch: 0.48s. S_Loss: 2.5267. T_Loss: 1.9394. Mask: 0.8991. :  36%|███▌      | 18/50 [00:08<00:18,  1.77it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5268. T_Loss: 1.9401. Mask: 0.8988. :  36%|███▌      | 18/50 [00:08<00:18,  1.77it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5268. T_Loss: 1.9401. Mask: 0.8988. :  38%|███▊      | 19/50 [00:08<00:15,  1.99it/s]total : 1000  current step :  118
total : 1000  current step :  119
Train Iter: 120/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5250. T_Loss: 1.9429. Mask: 0.8996. :  38%|███▊      | 19/50 [00:09<00:15,  1.99it/s]Train Iter: 120/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5250. T_Loss: 1.9429. Mask: 0.8996. :  40%|████      | 20/50 [00:09<00:14,  2.14it/s]total : 1000  current step :  120
Train Iter: 121/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5251. T_Loss: 1.9491. Mask: 0.8999. :  40%|████      | 20/50 [00:10<00:14,  2.14it/s]Train Iter: 121/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5251. T_Loss: 1.9491. Mask: 0.8999. :  42%|████▏     | 21/50 [00:10<00:17,  1.67it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5238. T_Loss: 1.9551. Mask: 0.8995. :  42%|████▏     | 21/50 [00:10<00:17,  1.67it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5238. T_Loss: 1.9551. Mask: 0.8995. :  44%|████▍     | 22/50 [00:10<00:13,  2.04it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5239. T_Loss: 1.9598. Mask: 0.8976. :  44%|████▍     | 22/50 [00:10<00:13,  2.04it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5239. T_Loss: 1.9598. Mask: 0.8976. :  46%|████▌     | 23/50 [00:10<00:12,  2.23it/s]total : 1000  current step :  121
total : 1000  current step :  122
total : 1000  current step :  123
Train Iter: 124/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5241. T_Loss: 1.9622. Mask: 0.8978. :  46%|████▌     | 23/50 [00:11<00:12,  2.23it/s]Train Iter: 124/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5241. T_Loss: 1.9622. Mask: 0.8978. :  48%|████▊     | 24/50 [00:11<00:14,  1.82it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5243. T_Loss: 1.9617. Mask: 0.8962. :  48%|████▊     | 24/50 [00:11<00:14,  1.82it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5243. T_Loss: 1.9617. Mask: 0.8962. :  50%|█████     | 25/50 [00:11<00:11,  2.19it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5238. T_Loss: 1.9621. Mask: 0.8968. :  50%|█████     | 25/50 [00:12<00:11,  2.19it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5238. T_Loss: 1.9621. Mask: 0.8968. :  52%|█████▏    | 26/50 [00:12<00:09,  2.55it/s]total : 1000  current step :  124
total : 1000  current step :  125
total : 1000  current step :  126
Train Iter: 127/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5244. T_Loss: 1.9608. Mask: 0.8954. :  52%|█████▏    | 26/50 [00:12<00:09,  2.55it/s]Train Iter: 127/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5244. T_Loss: 1.9608. Mask: 0.8954. :  54%|█████▍    | 27/50 [00:12<00:11,  1.98it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5234. T_Loss: 1.9613. Mask: 0.8948. :  54%|█████▍    | 27/50 [00:13<00:11,  1.98it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5234. T_Loss: 1.9613. Mask: 0.8948. :  56%|█████▌    | 28/50 [00:13<00:09,  2.37it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5248. T_Loss: 1.9646. Mask: 0.8963. :  56%|█████▌    | 28/50 [00:13<00:09,  2.37it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5248. T_Loss: 1.9646. Mask: 0.8963. :  58%|█████▊    | 29/50 [00:13<00:07,  2.73it/s]total : 1000  current step :  127
total : 1000  current step :  128
total : 1000  current step :  129
Train Iter: 130/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5249. T_Loss: 1.9682. Mask: 0.8964. :  58%|█████▊    | 29/50 [00:14<00:07,  2.73it/s]Train Iter: 130/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5249. T_Loss: 1.9682. Mask: 0.8964. :  60%|██████    | 30/50 [00:14<00:10,  1.91it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5261. T_Loss: 1.9687. Mask: 0.8959. :  60%|██████    | 30/50 [00:14<00:10,  1.91it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5261. T_Loss: 1.9687. Mask: 0.8959. :  62%|██████▏   | 31/50 [00:14<00:08,  2.27it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5262. T_Loss: 1.9727. Mask: 0.8958. :  62%|██████▏   | 31/50 [00:14<00:08,  2.27it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5262. T_Loss: 1.9727. Mask: 0.8958. :  64%|██████▍   | 32/50 [00:14<00:06,  2.61it/s]total : 1000  current step :  130
total : 1000  current step :  131
total : 1000  current step :  132
Train Iter: 133/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5259. T_Loss: 1.9721. Mask: 0.8956. :  64%|██████▍   | 32/50 [00:15<00:06,  2.61it/s]Train Iter: 133/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5259. T_Loss: 1.9721. Mask: 0.8956. :  66%|██████▌   | 33/50 [00:15<00:09,  1.86it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5267. T_Loss: 1.9759. Mask: 0.8957. :  66%|██████▌   | 33/50 [00:15<00:09,  1.86it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5267. T_Loss: 1.9759. Mask: 0.8957. :  68%|██████▊   | 34/50 [00:15<00:07,  2.16it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5258. T_Loss: 1.9788. Mask: 0.8969. :  68%|██████▊   | 34/50 [00:16<00:07,  2.16it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5258. T_Loss: 1.9788. Mask: 0.8969. :  70%|███████   | 35/50 [00:16<00:05,  2.61it/s]total : 1000  current step :  133
total : 1000  current step :  134
total : 1000  current step :  135
Train Iter: 136/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5269. T_Loss: 1.9787. Mask: 0.8963. :  70%|███████   | 35/50 [00:16<00:05,  2.61it/s]Train Iter: 136/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5269. T_Loss: 1.9787. Mask: 0.8963. :  72%|███████▏  | 36/50 [00:16<00:06,  2.03it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5275. T_Loss: 1.9816. Mask: 0.8962. :  72%|███████▏  | 36/50 [00:17<00:06,  2.03it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5275. T_Loss: 1.9816. Mask: 0.8962. :  74%|███████▍  | 37/50 [00:17<00:06,  1.97it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5288. T_Loss: 1.9877. Mask: 0.8963. :  74%|███████▍  | 37/50 [00:17<00:06,  1.97it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5288. T_Loss: 1.9877. Mask: 0.8963. :  76%|███████▌  | 38/50 [00:17<00:05,  2.38it/s]total : 1000  current step :  136
total : 1000  current step :  137
total : 1000  current step :  138
Train Iter: 139/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5288. T_Loss: 1.9921. Mask: 0.8970. :  76%|███████▌  | 38/50 [00:18<00:05,  2.38it/s]Train Iter: 139/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5288. T_Loss: 1.9921. Mask: 0.8970. :  78%|███████▊  | 39/50 [00:18<00:06,  1.78it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5299. T_Loss: 1.9943. Mask: 0.8974. :  78%|███████▊  | 39/50 [00:18<00:06,  1.78it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5299. T_Loss: 1.9943. Mask: 0.8974. :  80%|████████  | 40/50 [00:18<00:04,  2.04it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5293. T_Loss: 1.9985. Mask: 0.8979. :  80%|████████  | 40/50 [00:19<00:04,  2.04it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5293. T_Loss: 1.9985. Mask: 0.8979. :  82%|████████▏ | 41/50 [00:19<00:03,  2.26it/s]total : 1000  current step :  139
total : 1000  current step :  140
total : 1000  current step :  141
Train Iter: 142/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5290. T_Loss: 2.0026. Mask: 0.8972. :  82%|████████▏ | 41/50 [00:19<00:03,  2.26it/s]Train Iter: 142/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5290. T_Loss: 2.0026. Mask: 0.8972. :  84%|████████▍ | 42/50 [00:19<00:04,  1.83it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5281. T_Loss: 2.0054. Mask: 0.8967. :  84%|████████▍ | 42/50 [00:20<00:04,  1.83it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5281. T_Loss: 2.0054. Mask: 0.8967. :  86%|████████▌ | 43/50 [00:20<00:03,  2.12it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5284. T_Loss: 2.0086. Mask: 0.8972. :  86%|████████▌ | 43/50 [00:20<00:03,  2.12it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5284. T_Loss: 2.0086. Mask: 0.8972. :  88%|████████▊ | 44/50 [00:20<00:02,  2.24it/s]total : 1000  current step :  142
total : 1000  current step :  143
total : 1000  current step :  144
Train Iter: 145/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5287. T_Loss: 2.0134. Mask: 0.8969. :  88%|████████▊ | 44/50 [00:21<00:02,  2.24it/s]Train Iter: 145/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5287. T_Loss: 2.0134. Mask: 0.8969. :  90%|█████████ | 45/50 [00:21<00:02,  1.82it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5280. T_Loss: 2.0173. Mask: 0.8968. :  90%|█████████ | 45/50 [00:21<00:02,  1.82it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5280. T_Loss: 2.0173. Mask: 0.8968. :  92%|█████████▏| 46/50 [00:21<00:01,  2.08it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5279. T_Loss: 2.0226. Mask: 0.8976. :  92%|█████████▏| 46/50 [00:21<00:01,  2.08it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.5279. T_Loss: 2.0226. Mask: 0.8976. :  94%|█████████▍| 47/50 [00:21<00:01,  2.44it/s]total : 1000  current step :  145
total : 1000  current step :  146
total : 1000  current step :  147
Train Iter: 148/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5275. T_Loss: 2.0238. Mask: 0.8979. :  94%|█████████▍| 47/50 [00:22<00:01,  2.44it/s]Train Iter: 148/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5275. T_Loss: 2.0238. Mask: 0.8979. :  96%|█████████▌| 48/50 [00:22<00:01,  1.93it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5275. T_Loss: 2.0252. Mask: 0.8972. :  96%|█████████▌| 48/50 [00:23<00:01,  1.93it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5275. T_Loss: 2.0252. Mask: 0.8972. :  98%|█████████▊| 49/50 [00:23<00:00,  2.27it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5268. T_Loss: 2.0298. Mask: 0.8978. :  98%|█████████▊| 49/50 [00:23<00:00,  2.27it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5268. T_Loss: 2.0298. Mask: 0.8978. : 100%|██████████| 50/50 [00:23<00:00,  2.55it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.25s. Batch: 0.46s. S_Loss: 2.5268. T_Loss: 2.0298. Mask: 0.8978. : 100%|██████████| 50/50 [00:23<00:00,  2.14it/s]
total : 1000  current step :  148
total : 1000  current step :  149
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 2.1195. top1: 0.78. top5: 96.88. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 2.1195. top1: 0.78. top5: 96.88. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.1274. top1: 0.98. top5: 98.05. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.1274. top1: 0.98. top5: 98.05. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.1288. top1: 0.91. top5: 98.18. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.1288. top1: 0.91. top5: 98.18. :  38%|███▊      | 3/8 [00:00<00:01,  3.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.1451. top1: 0.88. top5: 97.85. :  38%|███▊      | 3/8 [00:01<00:01,  3.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.1451. top1: 0.88. top5: 97.85. :  50%|█████     | 4/8 [00:01<00:01,  3.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 2.2374. top1: 0.70. top5: 96.56. :  50%|█████     | 4/8 [00:01<00:01,  3.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 2.2374. top1: 0.70. top5: 96.56. :  62%|██████▎   | 5/8 [00:01<00:00,  4.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 2.3061. top1: 0.59. top5: 95.96. :  62%|██████▎   | 5/8 [00:01<00:00,  4.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 2.3061. top1: 0.59. top5: 95.96. :  75%|███████▌  | 6/8 [00:01<00:00,  3.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 2.3589. top1: 0.50. top5: 95.31. :  75%|███████▌  | 6/8 [00:01<00:00,  3.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 2.3589. top1: 0.50. top5: 95.31. :  88%|████████▊ | 7/8 [00:01<00:00,  4.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 2.3872. top1: 0.45. top5: 94.55. :  88%|████████▊ | 7/8 [00:02<00:00,  4.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 2.3872. top1: 0.45. top5: 94.55. : 100%|██████████| 8/8 [00:02<00:00,  4.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 2.3872. top1: 0.45. top5: 94.55. : 100%|██████████| 8/8 [00:02<00:00,  3.74it/s]
total : 1000  current step :  150
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 0.60s. Batch: 0.86s. S_Loss: 2.5399. T_Loss: 2.2382. Mask: 0.8984. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 0.60s. Batch: 0.86s. S_Loss: 2.5399. T_Loss: 2.2382. Mask: 0.8984. :   2%|▏         | 1/50 [00:00<00:42,  1.16it/s]Train Iter: 152/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.5552. T_Loss: 2.2582. Mask: 0.9062. :   2%|▏         | 1/50 [00:01<00:42,  1.16it/s]Train Iter: 152/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.5552. T_Loss: 2.2582. Mask: 0.9062. :   4%|▍         | 2/50 [00:01<00:22,  2.11it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.22s. Batch: 0.42s. S_Loss: 2.5460. T_Loss: 2.2056. Mask: 0.9128. :   4%|▍         | 2/50 [00:01<00:22,  2.11it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.22s. Batch: 0.42s. S_Loss: 2.5460. T_Loss: 2.2056. Mask: 0.9128. :   6%|▌         | 3/50 [00:01<00:16,  2.86it/s]total : 1000  current step :  151
total : 1000  current step :  152
total : 1000  current step :  153
Train Iter: 154/1000. LR: 0.0000. Data: 0.30s. Batch: 0.49s. S_Loss: 2.5340. T_Loss: 2.2108. Mask: 0.9189. :   6%|▌         | 3/50 [00:01<00:16,  2.86it/s]Train Iter: 154/1000. LR: 0.0000. Data: 0.30s. Batch: 0.49s. S_Loss: 2.5340. T_Loss: 2.2108. Mask: 0.9189. :   8%|▊         | 4/50 [00:01<00:22,  2.08it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.26s. Batch: 0.44s. S_Loss: 2.5320. T_Loss: 2.2026. Mask: 0.9148. :   8%|▊         | 4/50 [00:02<00:22,  2.08it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.26s. Batch: 0.44s. S_Loss: 2.5320. T_Loss: 2.2026. Mask: 0.9148. :  10%|█         | 5/50 [00:02<00:18,  2.49it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.22s. Batch: 0.41s. S_Loss: 2.5274. T_Loss: 2.2124. Mask: 0.9154. :  10%|█         | 5/50 [00:02<00:18,  2.49it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.22s. Batch: 0.41s. S_Loss: 2.5274. T_Loss: 2.2124. Mask: 0.9154. :  12%|█▏        | 6/50 [00:02<00:15,  2.89it/s]total : 1000  current step :  154
total : 1000  current step :  155
total : 1000  current step :  156
Train Iter: 157/1000. LR: 0.0000. Data: 0.27s. Batch: 0.46s. S_Loss: 2.5330. T_Loss: 2.1985. Mask: 0.9102. :  12%|█▏        | 6/50 [00:03<00:15,  2.89it/s]Train Iter: 157/1000. LR: 0.0000. Data: 0.27s. Batch: 0.46s. S_Loss: 2.5330. T_Loss: 2.1985. Mask: 0.9102. :  14%|█▍        | 7/50 [00:03<00:20,  2.05it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.24s. Batch: 0.45s. S_Loss: 2.5317. T_Loss: 2.2145. Mask: 0.9111. :  14%|█▍        | 7/50 [00:03<00:20,  2.05it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.24s. Batch: 0.45s. S_Loss: 2.5317. T_Loss: 2.2145. Mask: 0.9111. :  16%|█▌        | 8/50 [00:03<00:18,  2.24it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.22s. Batch: 0.42s. S_Loss: 2.5301. T_Loss: 2.2101. Mask: 0.9110. :  16%|█▌        | 8/50 [00:03<00:18,  2.24it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.22s. Batch: 0.42s. S_Loss: 2.5301. T_Loss: 2.2101. Mask: 0.9110. :  18%|█▊        | 9/50 [00:03<00:14,  2.77it/s]total : 1000  current step :  157
total : 1000  current step :  158
total : 1000  current step :  159
Train Iter: 160/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.5293. T_Loss: 2.2057. Mask: 0.9074. :  18%|█▊        | 9/50 [00:04<00:14,  2.77it/s]Train Iter: 160/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.5293. T_Loss: 2.2057. Mask: 0.9074. :  20%|██        | 10/50 [00:04<00:22,  1.77it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.5331. T_Loss: 2.2088. Mask: 0.9034. :  20%|██        | 10/50 [00:05<00:22,  1.77it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.5331. T_Loss: 2.2088. Mask: 0.9034. :  22%|██▏       | 11/50 [00:05<00:21,  1.82it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.28s. Batch: 0.47s. S_Loss: 2.5335. T_Loss: 2.2122. Mask: 0.9053. :  22%|██▏       | 11/50 [00:05<00:21,  1.82it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.28s. Batch: 0.47s. S_Loss: 2.5335. T_Loss: 2.2122. Mask: 0.9053. :  24%|██▍       | 12/50 [00:05<00:18,  2.03it/s]total : 1000  current step :  160
total : 1000  current step :  161
total : 1000  current step :  162
Train Iter: 163/1000. LR: 0.0000. Data: 0.30s. Batch: 0.49s. S_Loss: 2.5325. T_Loss: 2.2200. Mask: 0.9069. :  24%|██▍       | 12/50 [00:06<00:18,  2.03it/s]Train Iter: 163/1000. LR: 0.0000. Data: 0.30s. Batch: 0.49s. S_Loss: 2.5325. T_Loss: 2.2200. Mask: 0.9069. :  26%|██▌       | 13/50 [00:06<00:20,  1.76it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.28s. Batch: 0.47s. S_Loss: 2.5341. T_Loss: 2.2260. Mask: 0.9043. :  26%|██▌       | 13/50 [00:06<00:20,  1.76it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.28s. Batch: 0.47s. S_Loss: 2.5341. T_Loss: 2.2260. Mask: 0.9043. :  28%|██▊       | 14/50 [00:06<00:16,  2.15it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5342. T_Loss: 2.2291. Mask: 0.9042. :  28%|██▊       | 14/50 [00:07<00:16,  2.15it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5342. T_Loss: 2.2291. Mask: 0.9042. :  30%|███       | 15/50 [00:07<00:16,  2.17it/s]total : 1000  current step :  163
total : 1000  current step :  164
total : 1000  current step :  165
Train Iter: 166/1000. LR: 0.0000. Data: 0.29s. Batch: 0.49s. S_Loss: 2.5326. T_Loss: 2.2335. Mask: 0.9033. :  30%|███       | 15/50 [00:07<00:16,  2.17it/s]Train Iter: 166/1000. LR: 0.0000. Data: 0.29s. Batch: 0.49s. S_Loss: 2.5326. T_Loss: 2.2335. Mask: 0.9033. :  32%|███▏      | 16/50 [00:07<00:18,  1.81it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.28s. Batch: 0.47s. S_Loss: 2.5326. T_Loss: 2.2385. Mask: 0.9035. :  32%|███▏      | 16/50 [00:08<00:18,  1.81it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.28s. Batch: 0.47s. S_Loss: 2.5326. T_Loss: 2.2385. Mask: 0.9035. :  34%|███▍      | 17/50 [00:08<00:15,  2.16it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5302. T_Loss: 2.2354. Mask: 0.9017. :  34%|███▍      | 17/50 [00:08<00:15,  2.16it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5302. T_Loss: 2.2354. Mask: 0.9017. :  36%|███▌      | 18/50 [00:08<00:13,  2.34it/s]total : 1000  current step :  166
total : 1000  current step :  167
total : 1000  current step :  168
Train Iter: 169/1000. LR: 0.0000. Data: 0.30s. Batch: 0.49s. S_Loss: 2.5285. T_Loss: 2.2396. Mask: 0.9001. :  36%|███▌      | 18/50 [00:09<00:13,  2.34it/s]Train Iter: 169/1000. LR: 0.0000. Data: 0.30s. Batch: 0.49s. S_Loss: 2.5285. T_Loss: 2.2396. Mask: 0.9001. :  38%|███▊      | 19/50 [00:09<00:17,  1.77it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.29s. Batch: 0.48s. S_Loss: 2.5285. T_Loss: 2.2476. Mask: 0.9012. :  38%|███▊      | 19/50 [00:09<00:17,  1.77it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.29s. Batch: 0.48s. S_Loss: 2.5285. T_Loss: 2.2476. Mask: 0.9012. :  40%|████      | 20/50 [00:09<00:14,  2.04it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5271. T_Loss: 2.2605. Mask: 0.9023. :  40%|████      | 20/50 [00:09<00:14,  2.04it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5271. T_Loss: 2.2605. Mask: 0.9023. :  42%|████▏     | 21/50 [00:09<00:12,  2.33it/s]total : 1000  current step :  169
total : 1000  current step :  170
total : 1000  current step :  171
Train Iter: 172/1000. LR: 0.0000. Data: 0.29s. Batch: 0.50s. S_Loss: 2.5257. T_Loss: 2.2635. Mask: 0.8988. :  42%|████▏     | 21/50 [00:10<00:12,  2.33it/s]Train Iter: 172/1000. LR: 0.0000. Data: 0.29s. Batch: 0.50s. S_Loss: 2.5257. T_Loss: 2.2635. Mask: 0.8988. :  44%|████▍     | 22/50 [00:10<00:17,  1.62it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.28s. Batch: 0.49s. S_Loss: 2.5270. T_Loss: 2.2684. Mask: 0.9001. :  44%|████▍     | 22/50 [00:11<00:17,  1.62it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.28s. Batch: 0.49s. S_Loss: 2.5270. T_Loss: 2.2684. Mask: 0.9001. :  46%|████▌     | 23/50 [00:11<00:13,  1.97it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5259. T_Loss: 2.2753. Mask: 0.9017. :  46%|████▌     | 23/50 [00:11<00:13,  1.97it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5259. T_Loss: 2.2753. Mask: 0.9017. :  48%|████▊     | 24/50 [00:11<00:11,  2.36it/s]total : 1000  current step :  172
total : 1000  current step :  173
total : 1000  current step :  174
Train Iter: 175/1000. LR: 0.0000. Data: 0.28s. Batch: 0.49s. S_Loss: 2.5257. T_Loss: 2.2760. Mask: 0.9014. :  48%|████▊     | 24/50 [00:12<00:11,  2.36it/s]Train Iter: 175/1000. LR: 0.0000. Data: 0.28s. Batch: 0.49s. S_Loss: 2.5257. T_Loss: 2.2760. Mask: 0.9014. :  50%|█████     | 25/50 [00:12<00:12,  1.97it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5247. T_Loss: 2.2807. Mask: 0.9014. :  50%|█████     | 25/50 [00:12<00:12,  1.97it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5247. T_Loss: 2.2807. Mask: 0.9014. :  52%|█████▏    | 26/50 [00:12<00:10,  2.27it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5248. T_Loss: 2.2811. Mask: 0.9005. :  52%|█████▏    | 26/50 [00:12<00:10,  2.27it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5248. T_Loss: 2.2811. Mask: 0.9005. :  54%|█████▍    | 27/50 [00:12<00:08,  2.57it/s]total : 1000  current step :  175
total : 1000  current step :  176
total : 1000  current step :  177
Train Iter: 178/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.5264. T_Loss: 2.2899. Mask: 0.9012. :  54%|█████▍    | 27/50 [00:13<00:08,  2.57it/s]Train Iter: 178/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.5264. T_Loss: 2.2899. Mask: 0.9012. :  56%|█████▌    | 28/50 [00:13<00:11,  1.92it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5257. T_Loss: 2.2924. Mask: 0.9013. :  56%|█████▌    | 28/50 [00:13<00:11,  1.92it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5257. T_Loss: 2.2924. Mask: 0.9013. :  58%|█████▊    | 29/50 [00:13<00:09,  2.15it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5269. T_Loss: 2.2918. Mask: 0.9014. :  58%|█████▊    | 29/50 [00:14<00:09,  2.15it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5269. T_Loss: 2.2918. Mask: 0.9014. :  60%|██████    | 30/50 [00:14<00:07,  2.62it/s]total : 1000  current step :  178
total : 1000  current step :  179
total : 1000  current step :  180
Train Iter: 181/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5257. T_Loss: 2.2953. Mask: 0.9032. :  60%|██████    | 30/50 [00:14<00:07,  2.62it/s]Train Iter: 181/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5257. T_Loss: 2.2953. Mask: 0.9032. :  62%|██████▏   | 31/50 [00:14<00:09,  2.00it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5259. T_Loss: 2.2976. Mask: 0.9043. :  62%|██████▏   | 31/50 [00:15<00:09,  2.00it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5259. T_Loss: 2.2976. Mask: 0.9043. :  64%|██████▍   | 32/50 [00:15<00:07,  2.36it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5256. T_Loss: 2.2993. Mask: 0.9045. :  64%|██████▍   | 32/50 [00:15<00:07,  2.36it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5256. T_Loss: 2.2993. Mask: 0.9045. :  66%|██████▌   | 33/50 [00:15<00:07,  2.32it/s]total : 1000  current step :  181
total : 1000  current step :  182
total : 1000  current step :  183
Train Iter: 184/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5250. T_Loss: 2.3039. Mask: 0.9037. :  66%|██████▌   | 33/50 [00:16<00:07,  2.32it/s]Train Iter: 184/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5250. T_Loss: 2.3039. Mask: 0.9037. :  68%|██████▊   | 34/50 [00:16<00:08,  1.78it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5250. T_Loss: 2.3054. Mask: 0.9038. :  68%|██████▊   | 34/50 [00:16<00:08,  1.78it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5250. T_Loss: 2.3054. Mask: 0.9038. :  70%|███████   | 35/50 [00:16<00:06,  2.17it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5251. T_Loss: 2.3110. Mask: 0.9043. :  70%|███████   | 35/50 [00:16<00:06,  2.17it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5251. T_Loss: 2.3110. Mask: 0.9043. :  72%|███████▏  | 36/50 [00:16<00:05,  2.49it/s]total : 1000  current step :  184
total : 1000  current step :  185
total : 1000  current step :  186
Train Iter: 187/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5260. T_Loss: 2.3136. Mask: 0.9048. :  72%|███████▏  | 36/50 [00:17<00:05,  2.49it/s]Train Iter: 187/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5260. T_Loss: 2.3136. Mask: 0.9048. :  74%|███████▍  | 37/50 [00:17<00:06,  1.90it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5258. T_Loss: 2.3193. Mask: 0.9050. :  74%|███████▍  | 37/50 [00:17<00:06,  1.90it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5258. T_Loss: 2.3193. Mask: 0.9050. :  76%|███████▌  | 38/50 [00:17<00:05,  2.26it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.26s. Batch: 0.46s. S_Loss: 2.5255. T_Loss: 2.3243. Mask: 0.9057. :  76%|███████▌  | 38/50 [00:18<00:05,  2.26it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.26s. Batch: 0.46s. S_Loss: 2.5255. T_Loss: 2.3243. Mask: 0.9057. :  78%|███████▊  | 39/50 [00:18<00:04,  2.70it/s]total : 1000  current step :  187
total : 1000  current step :  188
total : 1000  current step :  189
Train Iter: 190/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5256. T_Loss: 2.3256. Mask: 0.9061. :  78%|███████▊  | 39/50 [00:19<00:04,  2.70it/s]Train Iter: 190/1000. LR: 0.0000. Data: 0.27s. Batch: 0.47s. S_Loss: 2.5256. T_Loss: 2.3256. Mask: 0.9061. :  80%|████████  | 40/50 [00:19<00:05,  1.89it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5262. T_Loss: 2.3316. Mask: 0.9062. :  80%|████████  | 40/50 [00:19<00:05,  1.89it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5262. T_Loss: 2.3316. Mask: 0.9062. :  82%|████████▏ | 41/50 [00:19<00:04,  2.06it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5267. T_Loss: 2.3355. Mask: 0.9066. :  82%|████████▏ | 41/50 [00:19<00:04,  2.06it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.26s. Batch: 0.47s. S_Loss: 2.5267. T_Loss: 2.3355. Mask: 0.9066. :  84%|████████▍ | 42/50 [00:19<00:03,  2.30it/s]total : 1000  current step :  190
total : 1000  current step :  191
total : 1000  current step :  192
Train Iter: 193/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5271. T_Loss: 2.3395. Mask: 0.9072. :  84%|████████▍ | 42/50 [00:20<00:03,  2.30it/s]Train Iter: 193/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.5271. T_Loss: 2.3395. Mask: 0.9072. :  86%|████████▌ | 43/50 [00:20<00:04,  1.68it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5264. T_Loss: 2.3440. Mask: 0.9078. :  86%|████████▌ | 43/50 [00:21<00:04,  1.68it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5264. T_Loss: 2.3440. Mask: 0.9078. :  88%|████████▊ | 44/50 [00:21<00:03,  1.90it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5264. T_Loss: 2.3556. Mask: 0.9074. :  88%|████████▊ | 44/50 [00:21<00:03,  1.90it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5264. T_Loss: 2.3556. Mask: 0.9074. :  90%|█████████ | 45/50 [00:21<00:02,  2.11it/s]total : 1000  current step :  193
total : 1000  current step :  194
total : 1000  current step :  195
Train Iter: 196/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5259. T_Loss: 2.3613. Mask: 0.9075. :  90%|█████████ | 45/50 [00:22<00:02,  2.11it/s]Train Iter: 196/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5259. T_Loss: 2.3613. Mask: 0.9075. :  92%|█████████▏| 46/50 [00:22<00:02,  1.74it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5267. T_Loss: 2.3668. Mask: 0.9072. :  92%|█████████▏| 46/50 [00:22<00:02,  1.74it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5267. T_Loss: 2.3668. Mask: 0.9072. :  94%|█████████▍| 47/50 [00:22<00:01,  1.92it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5267. T_Loss: 2.3718. Mask: 0.9067. :  94%|█████████▍| 47/50 [00:22<00:01,  1.92it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5267. T_Loss: 2.3718. Mask: 0.9067. :  96%|█████████▌| 48/50 [00:22<00:00,  2.22it/s]total : 1000  current step :  196
total : 1000  current step :  197
total : 1000  current step :  198
Train Iter: 199/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5268. T_Loss: 2.3796. Mask: 0.9071. :  96%|█████████▌| 48/50 [00:23<00:00,  2.22it/s]Train Iter: 199/1000. LR: 0.0000. Data: 0.26s. Batch: 0.48s. S_Loss: 2.5268. T_Loss: 2.3796. Mask: 0.9071. :  98%|█████████▊| 49/50 [00:23<00:00,  1.82it/s]total : 1000  current step :  199
Train Iter: 200/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5263. T_Loss: 2.3843. Mask: 0.9070. :  98%|█████████▊| 49/50 [00:24<00:00,  1.82it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5263. T_Loss: 2.3843. Mask: 0.9070. : 100%|██████████| 50/50 [00:24<00:00,  1.64it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.27s. Batch: 0.49s. S_Loss: 2.5263. T_Loss: 2.3843. Mask: 0.9070. : 100%|██████████| 50/50 [00:24<00:00,  2.04it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 2.0980. top1: 6.64. top5: 96.48. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 2.0980. top1: 6.64. top5: 96.48. :  12%|█▎        | 1/8 [00:00<00:03,  1.81it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.1149. top1: 5.66. top5: 96.88. :  12%|█▎        | 1/8 [00:00<00:03,  1.81it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.1149. top1: 5.66. top5: 96.88. :  25%|██▌       | 2/8 [00:00<00:02,  2.79it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.1165. top1: 4.95. top5: 97.01. :  25%|██▌       | 2/8 [00:01<00:02,  2.79it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.1165. top1: 4.95. top5: 97.01. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.1260. top1: 5.08. top5: 96.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.1260. top1: 5.08. top5: 96.00. :  50%|█████     | 4/8 [00:01<00:01,  3.23it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.1877. top1: 4.06. top5: 93.05. :  50%|█████     | 4/8 [00:01<00:01,  3.23it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.1877. top1: 4.06. top5: 93.05. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.2308. top1: 3.39. top5: 91.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.2308. top1: 3.39. top5: 91.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.90it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2651. top1: 2.90. top5: 90.57. :  75%|███████▌  | 6/8 [00:02<00:00,  3.90it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2651. top1: 2.90. top5: 90.57. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.2849. top1: 2.60. top5: 89.45. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.2849. top1: 2.60. top5: 89.45. : 100%|██████████| 8/8 [00:02<00:00,  4.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.2849. top1: 2.60. top5: 89.45. : 100%|██████████| 8/8 [00:02<00:00,  3.38it/s]
total : 1000  current step :  200
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.01s. Batch: 0.32s. S_Loss: 2.5051. T_Loss: 2.7981. Mask: 0.9102. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.01s. Batch: 0.32s. S_Loss: 2.5051. T_Loss: 2.7981. Mask: 0.9102. :   2%|▏         | 1/50 [00:00<00:15,  3.06it/s]total : 1000  current step :  201
Train Iter: 202/1000. LR: 0.0000. Data: 0.35s. Batch: 0.65s. S_Loss: 2.5260. T_Loss: 2.6961. Mask: 0.9141. :   2%|▏         | 1/50 [00:01<00:15,  3.06it/s]Train Iter: 202/1000. LR: 0.0000. Data: 0.35s. Batch: 0.65s. S_Loss: 2.5260. T_Loss: 2.6961. Mask: 0.9141. :   4%|▍         | 2/50 [00:01<00:33,  1.42it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.24s. Batch: 0.55s. S_Loss: 2.5351. T_Loss: 2.6323. Mask: 0.9089. :   4%|▍         | 2/50 [00:01<00:33,  1.42it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.24s. Batch: 0.55s. S_Loss: 2.5351. T_Loss: 2.6323. Mask: 0.9089. :   6%|▌         | 3/50 [00:01<00:25,  1.82it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.18s. Batch: 0.50s. S_Loss: 2.5329. T_Loss: 2.6208. Mask: 0.9131. :   6%|▌         | 3/50 [00:02<00:25,  1.82it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.18s. Batch: 0.50s. S_Loss: 2.5329. T_Loss: 2.6208. Mask: 0.9131. :   8%|▊         | 4/50 [00:02<00:21,  2.11it/s]total : 1000  current step :  202
total : 1000  current step :  203
total : 1000  current step :  204
Train Iter: 205/1000. LR: 0.0000. Data: 0.29s. Batch: 0.59s. S_Loss: 2.5293. T_Loss: 2.6299. Mask: 0.9109. :   8%|▊         | 4/50 [00:02<00:21,  2.11it/s]Train Iter: 205/1000. LR: 0.0000. Data: 0.29s. Batch: 0.59s. S_Loss: 2.5293. T_Loss: 2.6299. Mask: 0.9109. :  10%|█         | 5/50 [00:02<00:29,  1.55it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.25s. Batch: 0.57s. S_Loss: 2.5272. T_Loss: 2.6276. Mask: 0.9128. :  10%|█         | 5/50 [00:03<00:29,  1.55it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.25s. Batch: 0.57s. S_Loss: 2.5272. T_Loss: 2.6276. Mask: 0.9128. :  12%|█▏        | 6/50 [00:03<00:25,  1.73it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.22s. Batch: 0.55s. S_Loss: 2.5304. T_Loss: 2.6254. Mask: 0.9102. :  12%|█▏        | 6/50 [00:03<00:25,  1.73it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.22s. Batch: 0.55s. S_Loss: 2.5304. T_Loss: 2.6254. Mask: 0.9102. :  14%|█▍        | 7/50 [00:03<00:23,  1.85it/s]total : 1000  current step :  205
total : 1000  current step :  206
total : 1000  current step :  207
Train Iter: 208/1000. LR: 0.0000. Data: 0.28s. Batch: 0.61s. S_Loss: 2.5297. T_Loss: 2.6172. Mask: 0.9092. :  14%|█▍        | 7/50 [00:04<00:23,  1.85it/s]Train Iter: 208/1000. LR: 0.0000. Data: 0.28s. Batch: 0.61s. S_Loss: 2.5297. T_Loss: 2.6172. Mask: 0.9092. :  16%|█▌        | 8/50 [00:04<00:28,  1.47it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.25s. Batch: 0.57s. S_Loss: 2.5296. T_Loss: 2.6138. Mask: 0.9106. :  16%|█▌        | 8/50 [00:05<00:28,  1.47it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.25s. Batch: 0.57s. S_Loss: 2.5296. T_Loss: 2.6138. Mask: 0.9106. :  18%|█▊        | 9/50 [00:05<00:23,  1.75it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.22s. Batch: 0.55s. S_Loss: 2.5262. T_Loss: 2.6271. Mask: 0.9117. :  18%|█▊        | 9/50 [00:05<00:23,  1.75it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.22s. Batch: 0.55s. S_Loss: 2.5262. T_Loss: 2.6271. Mask: 0.9117. :  20%|██        | 10/50 [00:05<00:19,  2.01it/s]total : 1000  current step :  208
total : 1000  current step :  209
total : 1000  current step :  210
Train Iter: 211/1000. LR: 0.0000. Data: 0.27s. Batch: 0.58s. S_Loss: 2.5273. T_Loss: 2.6132. Mask: 0.9126. :  20%|██        | 10/50 [00:06<00:19,  2.01it/s]Train Iter: 211/1000. LR: 0.0000. Data: 0.27s. Batch: 0.58s. S_Loss: 2.5273. T_Loss: 2.6132. Mask: 0.9126. :  22%|██▏       | 11/50 [00:06<00:24,  1.59it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.26s. Batch: 0.56s. S_Loss: 2.5264. T_Loss: 2.6191. Mask: 0.9144. :  22%|██▏       | 11/50 [00:06<00:24,  1.59it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.26s. Batch: 0.56s. S_Loss: 2.5264. T_Loss: 2.6191. Mask: 0.9144. :  24%|██▍       | 12/50 [00:06<00:20,  1.87it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.24s. Batch: 0.53s. S_Loss: 2.5248. T_Loss: 2.6183. Mask: 0.9147. :  24%|██▍       | 12/50 [00:06<00:20,  1.87it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.24s. Batch: 0.53s. S_Loss: 2.5248. T_Loss: 2.6183. Mask: 0.9147. :  26%|██▌       | 13/50 [00:06<00:15,  2.31it/s]total : 1000  current step :  211
total : 1000  current step :  212
total : 1000  current step :  213
Train Iter: 214/1000. LR: 0.0000. Data: 0.27s. Batch: 0.57s. S_Loss: 2.5250. T_Loss: 2.6128. Mask: 0.9160. :  26%|██▌       | 13/50 [00:08<00:15,  2.31it/s]Train Iter: 214/1000. LR: 0.0000. Data: 0.27s. Batch: 0.57s. S_Loss: 2.5250. T_Loss: 2.6128. Mask: 0.9160. :  28%|██▊       | 14/50 [00:08<00:22,  1.59it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.26s. Batch: 0.54s. S_Loss: 2.5263. T_Loss: 2.6026. Mask: 0.9138. :  28%|██▊       | 14/50 [00:08<00:22,  1.59it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.26s. Batch: 0.54s. S_Loss: 2.5263. T_Loss: 2.6026. Mask: 0.9138. :  30%|███       | 15/50 [00:08<00:16,  2.06it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.24s. Batch: 0.52s. S_Loss: 2.5222. T_Loss: 2.6044. Mask: 0.9121. :  30%|███       | 15/50 [00:08<00:16,  2.06it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.24s. Batch: 0.52s. S_Loss: 2.5222. T_Loss: 2.6044. Mask: 0.9121. :  32%|███▏      | 16/50 [00:08<00:13,  2.50it/s]total : 1000  current step :  214
total : 1000  current step :  215
total : 1000  current step :  216
Train Iter: 217/1000. LR: 0.0000. Data: 0.26s. Batch: 0.54s. S_Loss: 2.5235. T_Loss: 2.6037. Mask: 0.9122. :  32%|███▏      | 16/50 [00:09<00:13,  2.50it/s]Train Iter: 217/1000. LR: 0.0000. Data: 0.26s. Batch: 0.54s. S_Loss: 2.5235. T_Loss: 2.6037. Mask: 0.9122. :  34%|███▍      | 17/50 [00:09<00:16,  1.97it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.25s. Batch: 0.53s. S_Loss: 2.5242. T_Loss: 2.6036. Mask: 0.9130. :  34%|███▍      | 17/50 [00:09<00:16,  1.97it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.25s. Batch: 0.53s. S_Loss: 2.5242. T_Loss: 2.6036. Mask: 0.9130. :  36%|███▌      | 18/50 [00:09<00:14,  2.15it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.24s. Batch: 0.51s. S_Loss: 2.5234. T_Loss: 2.6102. Mask: 0.9130. :  36%|███▌      | 18/50 [00:09<00:14,  2.15it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.24s. Batch: 0.51s. S_Loss: 2.5234. T_Loss: 2.6102. Mask: 0.9130. :  38%|███▊      | 19/50 [00:09<00:12,  2.49it/s]total : 1000  current step :  217
total : 1000  current step :  218
total : 1000  current step :  219
Train Iter: 220/1000. LR: 0.0000. Data: 0.26s. Batch: 0.53s. S_Loss: 2.5248. T_Loss: 2.6185. Mask: 0.9133. :  38%|███▊      | 19/50 [00:10<00:12,  2.49it/s]Train Iter: 220/1000. LR: 0.0000. Data: 0.26s. Batch: 0.53s. S_Loss: 2.5248. T_Loss: 2.6185. Mask: 0.9133. :  40%|████      | 20/50 [00:10<00:16,  1.79it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.25s. Batch: 0.52s. S_Loss: 2.5243. T_Loss: 2.6189. Mask: 0.9146. :  40%|████      | 20/50 [00:10<00:16,  1.79it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.25s. Batch: 0.52s. S_Loss: 2.5243. T_Loss: 2.6189. Mask: 0.9146. :  42%|████▏     | 21/50 [00:10<00:13,  2.09it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.24s. Batch: 0.51s. S_Loss: 2.5262. T_Loss: 2.6269. Mask: 0.9158. :  42%|████▏     | 21/50 [00:11<00:13,  2.09it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.24s. Batch: 0.51s. S_Loss: 2.5262. T_Loss: 2.6269. Mask: 0.9158. :  44%|████▍     | 22/50 [00:11<00:12,  2.28it/s]total : 1000  current step :  220
total : 1000  current step :  221
total : 1000  current step :  222
Train Iter: 223/1000. LR: 0.0000. Data: 0.27s. Batch: 0.54s. S_Loss: 2.5255. T_Loss: 2.6303. Mask: 0.9156. :  44%|████▍     | 22/50 [00:12<00:12,  2.28it/s]Train Iter: 223/1000. LR: 0.0000. Data: 0.27s. Batch: 0.54s. S_Loss: 2.5255. T_Loss: 2.6303. Mask: 0.9156. :  46%|████▌     | 23/50 [00:12<00:16,  1.61it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5235. T_Loss: 2.6301. Mask: 0.9141. :  46%|████▌     | 23/50 [00:12<00:16,  1.61it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5235. T_Loss: 2.6301. Mask: 0.9141. :  48%|████▊     | 24/50 [00:12<00:12,  2.05it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5239. T_Loss: 2.6430. Mask: 0.9153. :  48%|████▊     | 24/50 [00:12<00:12,  2.05it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5239. T_Loss: 2.6430. Mask: 0.9153. :  50%|█████     | 25/50 [00:12<00:10,  2.40it/s]total : 1000  current step :  223
total : 1000  current step :  224
total : 1000  current step :  225
Train Iter: 226/1000. LR: 0.0000. Data: 0.27s. Batch: 0.53s. S_Loss: 2.5237. T_Loss: 2.6455. Mask: 0.9144. :  50%|█████     | 25/50 [00:13<00:10,  2.40it/s]Train Iter: 226/1000. LR: 0.0000. Data: 0.27s. Batch: 0.53s. S_Loss: 2.5237. T_Loss: 2.6455. Mask: 0.9144. :  52%|█████▏    | 26/50 [00:13<00:14,  1.69it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5238. T_Loss: 2.6551. Mask: 0.9146. :  52%|█████▏    | 26/50 [00:14<00:14,  1.69it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5238. T_Loss: 2.6551. Mask: 0.9146. :  54%|█████▍    | 27/50 [00:14<00:10,  2.11it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5233. T_Loss: 2.6602. Mask: 0.9145. :  54%|█████▍    | 27/50 [00:14<00:10,  2.11it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5233. T_Loss: 2.6602. Mask: 0.9145. :  56%|█████▌    | 28/50 [00:14<00:10,  2.15it/s]total : 1000  current step :  226
total : 1000  current step :  227
total : 1000  current step :  228
Train Iter: 229/1000. LR: 0.0000. Data: 0.27s. Batch: 0.53s. S_Loss: 2.5231. T_Loss: 2.6640. Mask: 0.9141. :  56%|█████▌    | 28/50 [00:15<00:10,  2.15it/s]Train Iter: 229/1000. LR: 0.0000. Data: 0.27s. Batch: 0.53s. S_Loss: 2.5231. T_Loss: 2.6640. Mask: 0.9141. :  58%|█████▊    | 29/50 [00:15<00:13,  1.61it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5238. T_Loss: 2.6643. Mask: 0.9135. :  58%|█████▊    | 29/50 [00:15<00:13,  1.61it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5238. T_Loss: 2.6643. Mask: 0.9135. :  60%|██████    | 30/50 [00:15<00:10,  1.94it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5240. T_Loss: 2.6703. Mask: 0.9144. :  60%|██████    | 30/50 [00:15<00:10,  1.94it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5240. T_Loss: 2.6703. Mask: 0.9144. :  62%|██████▏   | 31/50 [00:15<00:07,  2.45it/s]total : 1000  current step :  229
total : 1000  current step :  230
total : 1000  current step :  231
Train Iter: 232/1000. LR: 0.0000. Data: 0.26s. Batch: 0.53s. S_Loss: 2.5258. T_Loss: 2.6726. Mask: 0.9144. :  62%|██████▏   | 31/50 [00:16<00:07,  2.45it/s]Train Iter: 232/1000. LR: 0.0000. Data: 0.26s. Batch: 0.53s. S_Loss: 2.5258. T_Loss: 2.6726. Mask: 0.9144. :  64%|██████▍   | 32/50 [00:16<00:10,  1.69it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5256. T_Loss: 2.6769. Mask: 0.9147. :  64%|██████▍   | 32/50 [00:17<00:10,  1.69it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5256. T_Loss: 2.6769. Mask: 0.9147. :  66%|██████▌   | 33/50 [00:17<00:08,  1.96it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5269. T_Loss: 2.6832. Mask: 0.9150. :  66%|██████▌   | 33/50 [00:17<00:08,  1.96it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5269. T_Loss: 2.6832. Mask: 0.9150. :  68%|██████▊   | 34/50 [00:17<00:07,  2.20it/s]total : 1000  current step :  232
total : 1000  current step :  233
total : 1000  current step :  234
Train Iter: 235/1000. LR: 0.0000. Data: 0.26s. Batch: 0.53s. S_Loss: 2.5271. T_Loss: 2.6828. Mask: 0.9148. :  68%|██████▊   | 34/50 [00:18<00:07,  2.20it/s]Train Iter: 235/1000. LR: 0.0000. Data: 0.26s. Batch: 0.53s. S_Loss: 2.5271. T_Loss: 2.6828. Mask: 0.9148. :  70%|███████   | 35/50 [00:18<00:09,  1.60it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5266. T_Loss: 2.6865. Mask: 0.9147. :  70%|███████   | 35/50 [00:18<00:09,  1.60it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5266. T_Loss: 2.6865. Mask: 0.9147. :  72%|███████▏  | 36/50 [00:18<00:07,  1.87it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5269. T_Loss: 2.6861. Mask: 0.9144. :  72%|███████▏  | 36/50 [00:19<00:07,  1.87it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5269. T_Loss: 2.6861. Mask: 0.9144. :  74%|███████▍  | 37/50 [00:19<00:05,  2.25it/s]total : 1000  current step :  235
total : 1000  current step :  236
total : 1000  current step :  237
Train Iter: 238/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5268. T_Loss: 2.6839. Mask: 0.9140. :  74%|███████▍  | 37/50 [00:19<00:05,  2.25it/s]Train Iter: 238/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5268. T_Loss: 2.6839. Mask: 0.9140. :  76%|███████▌  | 38/50 [00:19<00:06,  1.84it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.25s. Batch: 0.52s. S_Loss: 2.5267. T_Loss: 2.6887. Mask: 0.9144. :  76%|███████▌  | 38/50 [00:20<00:06,  1.84it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.25s. Batch: 0.52s. S_Loss: 2.5267. T_Loss: 2.6887. Mask: 0.9144. :  78%|███████▊  | 39/50 [00:20<00:05,  2.09it/s]total : 1000  current step :  238
total : 1000  current step :  239
Train Iter: 240/1000. LR: 0.0000. Data: 0.25s. Batch: 0.52s. S_Loss: 2.5277. T_Loss: 2.6886. Mask: 0.9148. :  78%|███████▊  | 39/50 [00:20<00:05,  2.09it/s]Train Iter: 240/1000. LR: 0.0000. Data: 0.25s. Batch: 0.52s. S_Loss: 2.5277. T_Loss: 2.6886. Mask: 0.9148. :  80%|████████  | 40/50 [00:20<00:04,  2.12it/s]total : 1000  current step :  240
Train Iter: 241/1000. LR: 0.0000. Data: 0.27s. Batch: 0.53s. S_Loss: 2.5276. T_Loss: 2.6910. Mask: 0.9143. :  80%|████████  | 40/50 [00:21<00:04,  2.12it/s]Train Iter: 241/1000. LR: 0.0000. Data: 0.27s. Batch: 0.53s. S_Loss: 2.5276. T_Loss: 2.6910. Mask: 0.9143. :  82%|████████▏ | 41/50 [00:21<00:05,  1.62it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5274. T_Loss: 2.6918. Mask: 0.9142. :  82%|████████▏ | 41/50 [00:21<00:05,  1.62it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5274. T_Loss: 2.6918. Mask: 0.9142. :  84%|████████▍ | 42/50 [00:21<00:04,  1.94it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.26s. Batch: 0.51s. S_Loss: 2.5273. T_Loss: 2.6959. Mask: 0.9140. :  84%|████████▍ | 42/50 [00:22<00:04,  1.94it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.26s. Batch: 0.51s. S_Loss: 2.5273. T_Loss: 2.6959. Mask: 0.9140. :  86%|████████▌ | 43/50 [00:22<00:03,  2.26it/s]total : 1000  current step :  241
total : 1000  current step :  242
total : 1000  current step :  243
Train Iter: 244/1000. LR: 0.0000. Data: 0.27s. Batch: 0.52s. S_Loss: 2.5272. T_Loss: 2.6971. Mask: 0.9142. :  86%|████████▌ | 43/50 [00:23<00:03,  2.26it/s]Train Iter: 244/1000. LR: 0.0000. Data: 0.27s. Batch: 0.52s. S_Loss: 2.5272. T_Loss: 2.6971. Mask: 0.9142. :  88%|████████▊ | 44/50 [00:23<00:03,  1.76it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5271. T_Loss: 2.7011. Mask: 0.9141. :  88%|████████▊ | 44/50 [00:23<00:03,  1.76it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5271. T_Loss: 2.7011. Mask: 0.9141. :  90%|█████████ | 45/50 [00:23<00:02,  2.04it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5280. T_Loss: 2.7026. Mask: 0.9141. :  90%|█████████ | 45/50 [00:23<00:02,  2.04it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.25s. Batch: 0.51s. S_Loss: 2.5280. T_Loss: 2.7026. Mask: 0.9141. :  92%|█████████▏| 46/50 [00:23<00:01,  2.33it/s]total : 1000  current step :  244
total : 1000  current step :  245
total : 1000  current step :  246
Train Iter: 247/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5271. T_Loss: 2.7072. Mask: 0.9145. :  92%|█████████▏| 46/50 [00:24<00:01,  2.33it/s]Train Iter: 247/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5271. T_Loss: 2.7072. Mask: 0.9145. :  94%|█████████▍| 47/50 [00:24<00:01,  1.72it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.26s. Batch: 0.51s. S_Loss: 2.5273. T_Loss: 2.7101. Mask: 0.9152. :  94%|█████████▍| 47/50 [00:24<00:01,  1.72it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.26s. Batch: 0.51s. S_Loss: 2.5273. T_Loss: 2.7101. Mask: 0.9152. :  96%|█████████▌| 48/50 [00:24<00:00,  2.12it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5276. T_Loss: 2.7144. Mask: 0.9153. :  96%|█████████▌| 48/50 [00:25<00:00,  2.12it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5276. T_Loss: 2.7144. Mask: 0.9153. :  98%|█████████▊| 49/50 [00:25<00:00,  2.01it/s]total : 1000  current step :  247
total : 1000  current step :  248
total : 1000  current step :  249
Train Iter: 250/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5274. T_Loss: 2.7205. Mask: 0.9152. :  98%|█████████▊| 49/50 [00:26<00:00,  2.01it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5274. T_Loss: 2.7205. Mask: 0.9152. : 100%|██████████| 50/50 [00:26<00:00,  1.59it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.26s. Batch: 0.52s. S_Loss: 2.5274. T_Loss: 2.7205. Mask: 0.9152. : 100%|██████████| 50/50 [00:26<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 2.1353. top1: 10.16. top5: 91.02. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 2.1353. top1: 10.16. top5: 91.02. :  12%|█▎        | 1/8 [00:00<00:03,  1.95it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.1530. top1: 8.20. top5: 90.82. :  12%|█▎        | 1/8 [00:00<00:03,  1.95it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.1530. top1: 8.20. top5: 90.82. :  25%|██▌       | 2/8 [00:00<00:02,  2.82it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.1538. top1: 7.16. top5: 92.06. :  25%|██▌       | 2/8 [00:00<00:02,  2.82it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.1538. top1: 7.16. top5: 92.06. :  38%|███▊      | 3/8 [00:00<00:01,  3.33it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.1603. top1: 7.81. top5: 91.80. :  38%|███▊      | 3/8 [00:01<00:01,  3.33it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.1603. top1: 7.81. top5: 91.80. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2066. top1: 6.25. top5: 86.48. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2066. top1: 6.25. top5: 86.48. :  62%|██████▎   | 5/8 [00:01<00:00,  3.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2374. top1: 5.21. top5: 84.38. :  62%|██████▎   | 5/8 [00:01<00:00,  3.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2374. top1: 5.21. top5: 84.38. :  75%|███████▌  | 6/8 [00:01<00:00,  3.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.2626. top1: 4.46. top5: 81.53. :  75%|███████▌  | 6/8 [00:01<00:00,  3.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.2626. top1: 4.46. top5: 81.53. :  88%|████████▊ | 7/8 [00:01<00:00,  4.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 2.2776. top1: 4.00. top5: 80.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 2.2776. top1: 4.00. top5: 80.15. : 100%|██████████| 8/8 [00:02<00:00,  4.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 2.2776. top1: 4.00. top5: 80.15. : 100%|██████████| 8/8 [00:02<00:00,  3.64it/s]
total : 1000  current step :  250
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.22s. S_Loss: 2.5082. T_Loss: 3.0681. Mask: 0.9062. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.22s. S_Loss: 2.5082. T_Loss: 3.0681. Mask: 0.9062. :   2%|▏         | 1/50 [00:00<00:10,  4.46it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.22s. S_Loss: 2.5256. T_Loss: 2.9833. Mask: 0.9062. :   2%|▏         | 1/50 [00:00<00:10,  4.46it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.22s. S_Loss: 2.5256. T_Loss: 2.9833. Mask: 0.9062. :   4%|▍         | 2/50 [00:00<00:10,  4.56it/s]total : 1000  current step :  251
total : 1000  current step :  252
Train Iter: 253/1000. LR: 0.0000. Data: 0.23s. Batch: 0.46s. S_Loss: 2.5368. T_Loss: 2.9396. Mask: 0.9102. :   4%|▍         | 2/50 [00:01<00:10,  4.56it/s]Train Iter: 253/1000. LR: 0.0000. Data: 0.23s. Batch: 0.46s. S_Loss: 2.5368. T_Loss: 2.9396. Mask: 0.9102. :   6%|▌         | 3/50 [00:01<00:25,  1.82it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.17s. Batch: 0.40s. S_Loss: 2.5395. T_Loss: 2.9138. Mask: 0.9131. :   6%|▌         | 3/50 [00:01<00:25,  1.82it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.17s. Batch: 0.40s. S_Loss: 2.5395. T_Loss: 2.9138. Mask: 0.9131. :   8%|▊         | 4/50 [00:01<00:19,  2.37it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.14s. Batch: 0.37s. S_Loss: 2.5343. T_Loss: 2.9156. Mask: 0.9125. :   8%|▊         | 4/50 [00:01<00:19,  2.37it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.14s. Batch: 0.37s. S_Loss: 2.5343. T_Loss: 2.9156. Mask: 0.9125. :  10%|█         | 5/50 [00:01<00:15,  2.83it/s]total : 1000  current step :  253
total : 1000  current step :  254
total : 1000  current step :  255
Train Iter: 256/1000. LR: 0.0000. Data: 0.21s. Batch: 0.45s. S_Loss: 2.5375. T_Loss: 2.9223. Mask: 0.9173. :  10%|█         | 5/50 [00:02<00:15,  2.83it/s]Train Iter: 256/1000. LR: 0.0000. Data: 0.21s. Batch: 0.45s. S_Loss: 2.5375. T_Loss: 2.9223. Mask: 0.9173. :  12%|█▏        | 6/50 [00:02<00:23,  1.86it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.18s. Batch: 0.43s. S_Loss: 2.5383. T_Loss: 2.9590. Mask: 0.9185. :  12%|█▏        | 6/50 [00:03<00:23,  1.86it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.18s. Batch: 0.43s. S_Loss: 2.5383. T_Loss: 2.9590. Mask: 0.9185. :  14%|█▍        | 7/50 [00:03<00:19,  2.18it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.16s. Batch: 0.40s. S_Loss: 2.5415. T_Loss: 2.9861. Mask: 0.9272. :  14%|█▍        | 7/50 [00:03<00:19,  2.18it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.16s. Batch: 0.40s. S_Loss: 2.5415. T_Loss: 2.9861. Mask: 0.9272. :  16%|█▌        | 8/50 [00:03<00:15,  2.76it/s]total : 1000  current step :  256
total : 1000  current step :  257
total : 1000  current step :  258
Train Iter: 259/1000. LR: 0.0000. Data: 0.20s. Batch: 0.45s. S_Loss: 2.5431. T_Loss: 2.9917. Mask: 0.9266. :  16%|█▌        | 8/50 [00:04<00:15,  2.76it/s]Train Iter: 259/1000. LR: 0.0000. Data: 0.20s. Batch: 0.45s. S_Loss: 2.5431. T_Loss: 2.9917. Mask: 0.9266. :  18%|█▊        | 9/50 [00:04<00:21,  1.88it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.18s. Batch: 0.44s. S_Loss: 2.5388. T_Loss: 3.0043. Mask: 0.9254. :  18%|█▊        | 9/50 [00:04<00:21,  1.88it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.18s. Batch: 0.44s. S_Loss: 2.5388. T_Loss: 3.0043. Mask: 0.9254. :  20%|██        | 10/50 [00:04<00:19,  2.10it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.17s. Batch: 0.42s. S_Loss: 2.5376. T_Loss: 2.9969. Mask: 0.9258. :  20%|██        | 10/50 [00:04<00:19,  2.10it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.17s. Batch: 0.42s. S_Loss: 2.5376. T_Loss: 2.9969. Mask: 0.9258. :  22%|██▏       | 11/50 [00:04<00:15,  2.58it/s]total : 1000  current step :  259
total : 1000  current step :  260
total : 1000  current step :  261
Train Iter: 262/1000. LR: 0.0000. Data: 0.20s. Batch: 0.45s. S_Loss: 2.5376. T_Loss: 2.9791. Mask: 0.9225. :  22%|██▏       | 11/50 [00:05<00:15,  2.58it/s]Train Iter: 262/1000. LR: 0.0000. Data: 0.20s. Batch: 0.45s. S_Loss: 2.5376. T_Loss: 2.9791. Mask: 0.9225. :  24%|██▍       | 12/50 [00:05<00:19,  1.91it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.19s. Batch: 0.46s. S_Loss: 2.5370. T_Loss: 2.9616. Mask: 0.9213. :  24%|██▍       | 12/50 [00:05<00:19,  1.91it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.19s. Batch: 0.46s. S_Loss: 2.5370. T_Loss: 2.9616. Mask: 0.9213. :  26%|██▌       | 13/50 [00:05<00:19,  1.93it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.17s. Batch: 0.44s. S_Loss: 2.5346. T_Loss: 2.9716. Mask: 0.9202. :  26%|██▌       | 13/50 [00:06<00:19,  1.93it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.17s. Batch: 0.44s. S_Loss: 2.5346. T_Loss: 2.9716. Mask: 0.9202. :  28%|██▊       | 14/50 [00:06<00:15,  2.25it/s]total : 1000  current step :  262
total : 1000  current step :  263
total : 1000  current step :  264
Train Iter: 265/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5329. T_Loss: 2.9681. Mask: 0.9201. :  28%|██▊       | 14/50 [00:07<00:15,  2.25it/s]Train Iter: 265/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5329. T_Loss: 2.9681. Mask: 0.9201. :  30%|███       | 15/50 [00:07<00:20,  1.71it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5317. T_Loss: 2.9669. Mask: 0.9187. :  30%|███       | 15/50 [00:07<00:20,  1.71it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5317. T_Loss: 2.9669. Mask: 0.9187. :  32%|███▏      | 16/50 [00:07<00:17,  1.93it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.18s. Batch: 0.46s. S_Loss: 2.5322. T_Loss: 2.9750. Mask: 0.9189. :  32%|███▏      | 16/50 [00:07<00:17,  1.93it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.18s. Batch: 0.46s. S_Loss: 2.5322. T_Loss: 2.9750. Mask: 0.9189. :  34%|███▍      | 17/50 [00:07<00:15,  2.19it/s]total : 1000  current step :  265
total : 1000  current step :  266
total : 1000  current step :  267
Train Iter: 268/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5306. T_Loss: 2.9716. Mask: 0.9188. :  34%|███▍      | 17/50 [00:08<00:15,  2.19it/s]Train Iter: 268/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5306. T_Loss: 2.9716. Mask: 0.9188. :  36%|███▌      | 18/50 [00:08<00:18,  1.71it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5293. T_Loss: 2.9732. Mask: 0.9192. :  36%|███▌      | 18/50 [00:08<00:18,  1.71it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5293. T_Loss: 2.9732. Mask: 0.9192. :  38%|███▊      | 19/50 [00:08<00:15,  2.04it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.18s. Batch: 0.47s. S_Loss: 2.5281. T_Loss: 2.9812. Mask: 0.9199. :  38%|███▊      | 19/50 [00:09<00:15,  2.04it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.18s. Batch: 0.47s. S_Loss: 2.5281. T_Loss: 2.9812. Mask: 0.9199. :  40%|████      | 20/50 [00:09<00:13,  2.22it/s]total : 1000  current step :  268
total : 1000  current step :  269
total : 1000  current step :  270
Train Iter: 271/1000. LR: 0.0000. Data: 0.19s. Batch: 0.48s. S_Loss: 2.5288. T_Loss: 2.9777. Mask: 0.9209. :  40%|████      | 20/50 [00:10<00:13,  2.22it/s]Train Iter: 271/1000. LR: 0.0000. Data: 0.19s. Batch: 0.48s. S_Loss: 2.5288. T_Loss: 2.9777. Mask: 0.9209. :  42%|████▏     | 21/50 [00:10<00:15,  1.83it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.18s. Batch: 0.47s. S_Loss: 2.5298. T_Loss: 2.9887. Mask: 0.9213. :  42%|████▏     | 21/50 [00:10<00:15,  1.83it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.18s. Batch: 0.47s. S_Loss: 2.5298. T_Loss: 2.9887. Mask: 0.9213. :  44%|████▍     | 22/50 [00:10<00:13,  2.04it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.18s. Batch: 0.47s. S_Loss: 2.5299. T_Loss: 2.9932. Mask: 0.9214. :  44%|████▍     | 22/50 [00:10<00:13,  2.04it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.18s. Batch: 0.47s. S_Loss: 2.5299. T_Loss: 2.9932. Mask: 0.9214. :  46%|████▌     | 23/50 [00:10<00:11,  2.34it/s]total : 1000  current step :  271
total : 1000  current step :  272
total : 1000  current step :  273
Train Iter: 274/1000. LR: 0.0000. Data: 0.19s. Batch: 0.48s. S_Loss: 2.5283. T_Loss: 3.0016. Mask: 0.9207. :  46%|████▌     | 23/50 [00:11<00:11,  2.34it/s]Train Iter: 274/1000. LR: 0.0000. Data: 0.19s. Batch: 0.48s. S_Loss: 2.5283. T_Loss: 3.0016. Mask: 0.9207. :  48%|████▊     | 24/50 [00:11<00:13,  1.95it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5290. T_Loss: 3.0048. Mask: 0.9211. :  48%|████▊     | 24/50 [00:11<00:13,  1.95it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5290. T_Loss: 3.0048. Mask: 0.9211. :  50%|█████     | 25/50 [00:11<00:11,  2.19it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.18s. Batch: 0.46s. S_Loss: 2.5283. T_Loss: 3.0123. Mask: 0.9225. :  50%|█████     | 25/50 [00:12<00:11,  2.19it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.18s. Batch: 0.46s. S_Loss: 2.5283. T_Loss: 3.0123. Mask: 0.9225. :  52%|█████▏    | 26/50 [00:12<00:09,  2.47it/s]total : 1000  current step :  274
total : 1000  current step :  275
total : 1000  current step :  276
Train Iter: 277/1000. LR: 0.0000. Data: 0.19s. Batch: 0.48s. S_Loss: 2.5285. T_Loss: 3.0165. Mask: 0.9225. :  52%|█████▏    | 26/50 [00:12<00:09,  2.47it/s]Train Iter: 277/1000. LR: 0.0000. Data: 0.19s. Batch: 0.48s. S_Loss: 2.5285. T_Loss: 3.0165. Mask: 0.9225. :  54%|█████▍    | 27/50 [00:12<00:12,  1.80it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5284. T_Loss: 3.0217. Mask: 0.9222. :  54%|█████▍    | 27/50 [00:13<00:12,  1.80it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5284. T_Loss: 3.0217. Mask: 0.9222. :  56%|█████▌    | 28/50 [00:13<00:10,  2.14it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.18s. Batch: 0.47s. S_Loss: 2.5270. T_Loss: 3.0302. Mask: 0.9228. :  56%|█████▌    | 28/50 [00:13<00:10,  2.14it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.18s. Batch: 0.47s. S_Loss: 2.5270. T_Loss: 3.0302. Mask: 0.9228. :  58%|█████▊    | 29/50 [00:13<00:09,  2.29it/s]total : 1000  current step :  277
total : 1000  current step :  278
total : 1000  current step :  279
Train Iter: 280/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5262. T_Loss: 3.0281. Mask: 0.9228. :  58%|█████▊    | 29/50 [00:14<00:09,  2.29it/s]Train Iter: 280/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5262. T_Loss: 3.0281. Mask: 0.9228. :  60%|██████    | 30/50 [00:14<00:11,  1.68it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5266. T_Loss: 3.0404. Mask: 0.9234. :  60%|██████    | 30/50 [00:15<00:11,  1.68it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5266. T_Loss: 3.0404. Mask: 0.9234. :  62%|██████▏   | 31/50 [00:15<00:10,  1.83it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.19s. Batch: 0.48s. S_Loss: 2.5263. T_Loss: 3.0517. Mask: 0.9235. :  62%|██████▏   | 31/50 [00:15<00:10,  1.83it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.19s. Batch: 0.48s. S_Loss: 2.5263. T_Loss: 3.0517. Mask: 0.9235. :  64%|██████▍   | 32/50 [00:15<00:08,  2.16it/s]total : 1000  current step :  280
total : 1000  current step :  281
total : 1000  current step :  282
Train Iter: 283/1000. LR: 0.0000. Data: 0.21s. Batch: 0.49s. S_Loss: 2.5266. T_Loss: 3.0638. Mask: 0.9237. :  64%|██████▍   | 32/50 [00:16<00:08,  2.16it/s]Train Iter: 283/1000. LR: 0.0000. Data: 0.21s. Batch: 0.49s. S_Loss: 2.5266. T_Loss: 3.0638. Mask: 0.9237. :  66%|██████▌   | 33/50 [00:16<00:10,  1.69it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5260. T_Loss: 3.0695. Mask: 0.9239. :  66%|██████▌   | 33/50 [00:16<00:10,  1.69it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5260. T_Loss: 3.0695. Mask: 0.9239. :  68%|██████▊   | 34/50 [00:16<00:07,  2.07it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5258. T_Loss: 3.0756. Mask: 0.9230. :  68%|██████▊   | 34/50 [00:16<00:07,  2.07it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5258. T_Loss: 3.0756. Mask: 0.9230. :  70%|███████   | 35/50 [00:16<00:05,  2.54it/s]total : 1000  current step :  283
total : 1000  current step :  284
total : 1000  current step :  285
Train Iter: 286/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5258. T_Loss: 3.0847. Mask: 0.9234. :  70%|███████   | 35/50 [00:17<00:05,  2.54it/s]Train Iter: 286/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5258. T_Loss: 3.0847. Mask: 0.9234. :  72%|███████▏  | 36/50 [00:17<00:07,  1.91it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5253. T_Loss: 3.0853. Mask: 0.9232. :  72%|███████▏  | 36/50 [00:17<00:07,  1.91it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5253. T_Loss: 3.0853. Mask: 0.9232. :  74%|███████▍  | 37/50 [00:17<00:06,  2.10it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5255. T_Loss: 3.0894. Mask: 0.9230. :  74%|███████▍  | 37/50 [00:17<00:06,  2.10it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5255. T_Loss: 3.0894. Mask: 0.9230. :  76%|███████▌  | 38/50 [00:17<00:04,  2.58it/s]total : 1000  current step :  286
total : 1000  current step :  287
total : 1000  current step :  288
Train Iter: 289/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5256. T_Loss: 3.0921. Mask: 0.9232. :  76%|███████▌  | 38/50 [00:18<00:04,  2.58it/s]Train Iter: 289/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5256. T_Loss: 3.0921. Mask: 0.9232. :  78%|███████▊  | 39/50 [00:18<00:05,  1.89it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.20s. Batch: 0.47s. S_Loss: 2.5264. T_Loss: 3.0916. Mask: 0.9228. :  78%|███████▊  | 39/50 [00:19<00:05,  1.89it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.20s. Batch: 0.47s. S_Loss: 2.5264. T_Loss: 3.0916. Mask: 0.9228. :  80%|████████  | 40/50 [00:19<00:04,  2.30it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5260. T_Loss: 3.0919. Mask: 0.9231. :  80%|████████  | 40/50 [00:19<00:04,  2.30it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5260. T_Loss: 3.0919. Mask: 0.9231. :  82%|████████▏ | 41/50 [00:19<00:03,  2.47it/s]total : 1000  current step :  289
total : 1000  current step :  290
total : 1000  current step :  291
Train Iter: 292/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5267. T_Loss: 3.0895. Mask: 0.9234. :  82%|████████▏ | 41/50 [00:20<00:03,  2.47it/s]Train Iter: 292/1000. LR: 0.0000. Data: 0.20s. Batch: 0.48s. S_Loss: 2.5267. T_Loss: 3.0895. Mask: 0.9234. :  84%|████████▍ | 42/50 [00:20<00:03,  2.03it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5263. T_Loss: 3.0939. Mask: 0.9241. :  84%|████████▍ | 42/50 [00:20<00:03,  2.03it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5263. T_Loss: 3.0939. Mask: 0.9241. :  86%|████████▌ | 43/50 [00:20<00:02,  2.40it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.19s. Batch: 0.46s. S_Loss: 2.5263. T_Loss: 3.0977. Mask: 0.9241. :  86%|████████▌ | 43/50 [00:20<00:02,  2.40it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.19s. Batch: 0.46s. S_Loss: 2.5263. T_Loss: 3.0977. Mask: 0.9241. :  88%|████████▊ | 44/50 [00:20<00:02,  2.88it/s]total : 1000  current step :  292
total : 1000  current step :  293
total : 1000  current step :  294
Train Iter: 295/1000. LR: 0.0000. Data: 0.20s. Batch: 0.47s. S_Loss: 2.5273. T_Loss: 3.0986. Mask: 0.9237. :  88%|████████▊ | 44/50 [00:21<00:02,  2.88it/s]Train Iter: 295/1000. LR: 0.0000. Data: 0.20s. Batch: 0.47s. S_Loss: 2.5273. T_Loss: 3.0986. Mask: 0.9237. :  90%|█████████ | 45/50 [00:21<00:02,  2.04it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5270. T_Loss: 3.0991. Mask: 0.9238. :  90%|█████████ | 45/50 [00:21<00:02,  2.04it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5270. T_Loss: 3.0991. Mask: 0.9238. :  92%|█████████▏| 46/50 [00:21<00:01,  2.36it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.19s. Batch: 0.46s. S_Loss: 2.5269. T_Loss: 3.1057. Mask: 0.9243. :  92%|█████████▏| 46/50 [00:21<00:01,  2.36it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.19s. Batch: 0.46s. S_Loss: 2.5269. T_Loss: 3.1057. Mask: 0.9243. :  94%|█████████▍| 47/50 [00:21<00:01,  2.70it/s]total : 1000  current step :  295
total : 1000  current step :  296
total : 1000  current step :  297
Train Iter: 298/1000. LR: 0.0000. Data: 0.20s. Batch: 0.47s. S_Loss: 2.5267. T_Loss: 3.1086. Mask: 0.9243. :  94%|█████████▍| 47/50 [00:22<00:01,  2.70it/s]Train Iter: 298/1000. LR: 0.0000. Data: 0.20s. Batch: 0.47s. S_Loss: 2.5267. T_Loss: 3.1086. Mask: 0.9243. :  96%|█████████▌| 48/50 [00:22<00:01,  1.82it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5264. T_Loss: 3.1126. Mask: 0.9246. :  96%|█████████▌| 48/50 [00:22<00:01,  1.82it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.19s. Batch: 0.47s. S_Loss: 2.5264. T_Loss: 3.1126. Mask: 0.9246. :  98%|█████████▊| 49/50 [00:22<00:00,  2.26it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.19s. Batch: 0.46s. S_Loss: 2.5257. T_Loss: 3.1184. Mask: 0.9248. :  98%|█████████▊| 49/50 [00:23<00:00,  2.26it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.19s. Batch: 0.46s. S_Loss: 2.5257. T_Loss: 3.1184. Mask: 0.9248. : 100%|██████████| 50/50 [00:23<00:00,  2.64it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.19s. Batch: 0.46s. S_Loss: 2.5257. T_Loss: 3.1184. Mask: 0.9248. : 100%|██████████| 50/50 [00:23<00:00,  2.15it/s]
total : 1000  current step :  298
total : 1000  current step :  299
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.1752. top1: 10.94. top5: 83.98. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.1752. top1: 10.94. top5: 83.98. :  12%|█▎        | 1/8 [00:00<00:02,  2.55it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.1929. top1: 8.59. top5: 84.18. :  12%|█▎        | 1/8 [00:00<00:02,  2.55it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.1929. top1: 8.59. top5: 84.18. :  25%|██▌       | 2/8 [00:00<00:01,  3.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.27s. Loss: 2.1936. top1: 7.55. top5: 85.16. :  25%|██▌       | 2/8 [00:00<00:01,  3.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.27s. Loss: 2.1936. top1: 7.55. top5: 85.16. :  38%|███▊      | 3/8 [00:00<00:01,  4.00it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.25s. Loss: 2.1985. top1: 7.91. top5: 85.06. :  38%|███▊      | 3/8 [00:01<00:01,  4.00it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.25s. Loss: 2.1985. top1: 7.91. top5: 85.06. :  50%|█████     | 4/8 [00:01<00:00,  4.32it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.24s. Loss: 2.2355. top1: 6.33. top5: 78.44. :  50%|█████     | 4/8 [00:01<00:00,  4.32it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.24s. Loss: 2.2355. top1: 6.33. top5: 78.44. :  62%|██████▎   | 5/8 [00:01<00:00,  4.57it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.23s. Loss: 2.2592. top1: 5.27. top5: 75.26. :  62%|██████▎   | 5/8 [00:01<00:00,  4.57it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.23s. Loss: 2.2592. top1: 5.27. top5: 75.26. :  75%|███████▌  | 6/8 [00:01<00:00,  4.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.23s. Loss: 2.2791. top1: 4.52. top5: 71.71. :  75%|███████▌  | 6/8 [00:01<00:00,  4.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.23s. Loss: 2.2791. top1: 4.52. top5: 71.71. :  88%|████████▊ | 7/8 [00:01<00:00,  4.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.22s. Loss: 2.2913. top1: 4.05. top5: 69.25. :  88%|████████▊ | 7/8 [00:01<00:00,  4.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.22s. Loss: 2.2913. top1: 4.05. top5: 69.25. : 100%|██████████| 8/8 [00:01<00:00,  5.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.22s. Loss: 2.2913. top1: 4.05. top5: 69.25. : 100%|██████████| 8/8 [00:01<00:00,  4.29it/s]
total : 1000  current step :  300
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.56s. Batch: 0.80s. S_Loss: 2.5042. T_Loss: 3.1733. Mask: 0.9180. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.56s. Batch: 0.80s. S_Loss: 2.5042. T_Loss: 3.1733. Mask: 0.9180. :   2%|▏         | 1/50 [00:00<00:39,  1.24it/s]Train Iter: 302/1000. LR: 0.0189. Data: 0.28s. Batch: 0.54s. S_Loss: 2.0885. T_Loss: 3.1332. Mask: 0.9082. :   2%|▏         | 1/50 [00:01<00:39,  1.24it/s]Train Iter: 302/1000. LR: 0.0189. Data: 0.28s. Batch: 0.54s. S_Loss: 2.0885. T_Loss: 3.1332. Mask: 0.9082. :   4%|▍         | 2/50 [00:01<00:23,  2.03it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.19s. Batch: 0.43s. S_Loss: 1.8529. T_Loss: 3.2112. Mask: 0.9206. :   4%|▍         | 2/50 [00:01<00:23,  2.03it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.19s. Batch: 0.43s. S_Loss: 1.8529. T_Loss: 3.2112. Mask: 0.9206. :   6%|▌         | 3/50 [00:01<00:17,  2.71it/s]total : 1000  current step :  301
total : 1000  current step :  302
total : 1000  current step :  303
Train Iter: 304/1000. LR: 0.0190. Data: 0.27s. Batch: 0.51s. S_Loss: 1.7314. T_Loss: 3.2371. Mask: 0.9131. :   6%|▌         | 3/50 [00:02<00:17,  2.71it/s]Train Iter: 304/1000. LR: 0.0190. Data: 0.27s. Batch: 0.51s. S_Loss: 1.7314. T_Loss: 3.2371. Mask: 0.9131. :   8%|▊         | 4/50 [00:02<00:24,  1.91it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.22s. Batch: 0.51s. S_Loss: 1.6683. T_Loss: 3.2841. Mask: 0.9133. :   8%|▊         | 4/50 [00:02<00:24,  1.91it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.22s. Batch: 0.51s. S_Loss: 1.6683. T_Loss: 3.2841. Mask: 0.9133. :  10%|█         | 5/50 [00:02<00:22,  1.96it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.18s. Batch: 0.49s. S_Loss: 1.6381. T_Loss: 3.3240. Mask: 0.9167. :  10%|█         | 5/50 [00:02<00:22,  1.96it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.18s. Batch: 0.49s. S_Loss: 1.6381. T_Loss: 3.3240. Mask: 0.9167. :  12%|█▏        | 6/50 [00:02<00:20,  2.16it/s]total : 1000  current step :  304
total : 1000  current step :  305
total : 1000  current step :  306
Train Iter: 307/1000. LR: 0.0192. Data: 0.24s. Batch: 0.53s. S_Loss: 1.6206. T_Loss: 3.3278. Mask: 0.9157. :  12%|█▏        | 6/50 [00:03<00:20,  2.16it/s]Train Iter: 307/1000. LR: 0.0192. Data: 0.24s. Batch: 0.53s. S_Loss: 1.6206. T_Loss: 3.3278. Mask: 0.9157. :  14%|█▍        | 7/50 [00:03<00:24,  1.75it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.21s. Batch: 0.51s. S_Loss: 1.6108. T_Loss: 3.3356. Mask: 0.9155. :  14%|█▍        | 7/50 [00:04<00:24,  1.75it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.21s. Batch: 0.51s. S_Loss: 1.6108. T_Loss: 3.3356. Mask: 0.9155. :  16%|█▌        | 8/50 [00:04<00:20,  2.00it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.19s. Batch: 0.48s. S_Loss: 1.6132. T_Loss: 3.3240. Mask: 0.9162. :  16%|█▌        | 8/50 [00:04<00:20,  2.00it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.19s. Batch: 0.48s. S_Loss: 1.6132. T_Loss: 3.3240. Mask: 0.9162. :  18%|█▊        | 9/50 [00:04<00:17,  2.40it/s]total : 1000  current step :  307
total : 1000  current step :  308
total : 1000  current step :  309
Train Iter: 310/1000. LR: 0.0194. Data: 0.23s. Batch: 0.50s. S_Loss: 1.6154. T_Loss: 3.2999. Mask: 0.9152. :  18%|█▊        | 9/50 [00:05<00:17,  2.40it/s]Train Iter: 310/1000. LR: 0.0194. Data: 0.23s. Batch: 0.50s. S_Loss: 1.6154. T_Loss: 3.2999. Mask: 0.9152. :  20%|██        | 10/50 [00:05<00:20,  1.93it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.22s. Batch: 0.48s. S_Loss: 1.6198. T_Loss: 3.2967. Mask: 0.9165. :  20%|██        | 10/50 [00:05<00:20,  1.93it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.22s. Batch: 0.48s. S_Loss: 1.6198. T_Loss: 3.2967. Mask: 0.9165. :  22%|██▏       | 11/50 [00:05<00:16,  2.32it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.21s. Batch: 0.50s. S_Loss: 1.6251. T_Loss: 3.3030. Mask: 0.9167. :  22%|██▏       | 11/50 [00:05<00:16,  2.32it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.21s. Batch: 0.50s. S_Loss: 1.6251. T_Loss: 3.3030. Mask: 0.9167. :  24%|██▍       | 12/50 [00:05<00:19,  1.96it/s]total : 1000  current step :  310
total : 1000  current step :  311
total : 1000  current step :  312
Train Iter: 313/1000. LR: 0.0196. Data: 0.24s. Batch: 0.52s. S_Loss: 1.6294. T_Loss: 3.3010. Mask: 0.9189. :  24%|██▍       | 12/50 [00:06<00:19,  1.96it/s]Train Iter: 313/1000. LR: 0.0196. Data: 0.24s. Batch: 0.52s. S_Loss: 1.6294. T_Loss: 3.3010. Mask: 0.9189. :  26%|██▌       | 13/50 [00:06<00:21,  1.70it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.23s. Batch: 0.50s. S_Loss: 1.6340. T_Loss: 3.2931. Mask: 0.9208. :  26%|██▌       | 13/50 [00:07<00:21,  1.70it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.23s. Batch: 0.50s. S_Loss: 1.6340. T_Loss: 3.2931. Mask: 0.9208. :  28%|██▊       | 14/50 [00:07<00:18,  1.93it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.22s. Batch: 0.50s. S_Loss: 1.6385. T_Loss: 3.2960. Mask: 0.9216. :  28%|██▊       | 14/50 [00:07<00:18,  1.93it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.22s. Batch: 0.50s. S_Loss: 1.6385. T_Loss: 3.2960. Mask: 0.9216. :  30%|███       | 15/50 [00:07<00:17,  2.03it/s]total : 1000  current step :  313
total : 1000  current step :  314
total : 1000  current step :  315
Train Iter: 316/1000. LR: 0.0198. Data: 0.24s. Batch: 0.52s. S_Loss: 1.6420. T_Loss: 3.3051. Mask: 0.9216. :  30%|███       | 15/50 [00:08<00:17,  2.03it/s]Train Iter: 316/1000. LR: 0.0198. Data: 0.24s. Batch: 0.52s. S_Loss: 1.6420. T_Loss: 3.3051. Mask: 0.9216. :  32%|███▏      | 16/50 [00:08<00:19,  1.76it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.23s. Batch: 0.50s. S_Loss: 1.6412. T_Loss: 3.3313. Mask: 0.9216. :  32%|███▏      | 16/50 [00:08<00:19,  1.76it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.23s. Batch: 0.50s. S_Loss: 1.6412. T_Loss: 3.3313. Mask: 0.9216. :  34%|███▍      | 17/50 [00:08<00:15,  2.08it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.22s. Batch: 0.49s. S_Loss: 1.6401. T_Loss: 3.3467. Mask: 0.9206. :  34%|███▍      | 17/50 [00:08<00:15,  2.08it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.22s. Batch: 0.49s. S_Loss: 1.6401. T_Loss: 3.3467. Mask: 0.9206. :  36%|███▌      | 18/50 [00:08<00:13,  2.39it/s]total : 1000  current step :  316
total : 1000  current step :  317
total : 1000  current step :  318
Train Iter: 319/1000. LR: 0.0199. Data: 0.24s. Batch: 0.51s. S_Loss: 1.6356. T_Loss: 3.3481. Mask: 0.9196. :  36%|███▌      | 18/50 [00:09<00:13,  2.39it/s]Train Iter: 319/1000. LR: 0.0199. Data: 0.24s. Batch: 0.51s. S_Loss: 1.6356. T_Loss: 3.3481. Mask: 0.9196. :  38%|███▊      | 19/50 [00:09<00:17,  1.77it/s]total : 1000  current step :  319
Train Iter: 320/1000. LR: 0.0200. Data: 0.24s. Batch: 0.50s. S_Loss: 1.6294. T_Loss: 3.3543. Mask: 0.9199. :  38%|███▊      | 19/50 [00:10<00:17,  1.77it/s]Train Iter: 320/1000. LR: 0.0200. Data: 0.24s. Batch: 0.50s. S_Loss: 1.6294. T_Loss: 3.3543. Mask: 0.9199. :  40%|████      | 20/50 [00:10<00:15,  1.94it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.24s. Batch: 0.51s. S_Loss: 1.6262. T_Loss: 3.3581. Mask: 0.9196. :  40%|████      | 20/50 [00:10<00:15,  1.94it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.24s. Batch: 0.51s. S_Loss: 1.6262. T_Loss: 3.3581. Mask: 0.9196. :  42%|████▏     | 21/50 [00:10<00:15,  1.92it/s]total : 1000  current step :  320
total : 1000  current step :  321
Train Iter: 322/1000. LR: 0.0201. Data: 0.26s. Batch: 0.52s. S_Loss: 1.6174. T_Loss: 3.3749. Mask: 0.9206. :  42%|████▏     | 21/50 [00:11<00:15,  1.92it/s]Train Iter: 322/1000. LR: 0.0201. Data: 0.26s. Batch: 0.52s. S_Loss: 1.6174. T_Loss: 3.3749. Mask: 0.9206. :  44%|████▍     | 22/50 [00:11<00:17,  1.61it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.24s. Batch: 0.51s. S_Loss: 1.6103. T_Loss: 3.3710. Mask: 0.9212. :  44%|████▍     | 22/50 [00:11<00:17,  1.61it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.24s. Batch: 0.51s. S_Loss: 1.6103. T_Loss: 3.3710. Mask: 0.9212. :  46%|████▌     | 23/50 [00:11<00:13,  1.97it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.23s. Batch: 0.50s. S_Loss: 1.6035. T_Loss: 3.3722. Mask: 0.9212. :  46%|████▌     | 23/50 [00:11<00:13,  1.97it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.23s. Batch: 0.50s. S_Loss: 1.6035. T_Loss: 3.3722. Mask: 0.9212. :  48%|████▊     | 24/50 [00:11<00:10,  2.44it/s]total : 1000  current step :  322
total : 1000  current step :  323
total : 1000  current step :  324
Train Iter: 325/1000. LR: 0.0203. Data: 0.25s. Batch: 0.51s. S_Loss: 1.5962. T_Loss: 3.3680. Mask: 0.9219. :  48%|████▊     | 24/50 [00:12<00:10,  2.44it/s]Train Iter: 325/1000. LR: 0.0203. Data: 0.25s. Batch: 0.51s. S_Loss: 1.5962. T_Loss: 3.3680. Mask: 0.9219. :  50%|█████     | 25/50 [00:12<00:12,  1.95it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.24s. Batch: 0.50s. S_Loss: 1.5864. T_Loss: 3.3674. Mask: 0.9225. :  50%|█████     | 25/50 [00:13<00:12,  1.95it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.24s. Batch: 0.50s. S_Loss: 1.5864. T_Loss: 3.3674. Mask: 0.9225. :  52%|█████▏    | 26/50 [00:13<00:11,  2.14it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.23s. Batch: 0.49s. S_Loss: 1.5785. T_Loss: 3.3692. Mask: 0.9226. :  52%|█████▏    | 26/50 [00:13<00:11,  2.14it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.23s. Batch: 0.49s. S_Loss: 1.5785. T_Loss: 3.3692. Mask: 0.9226. :  54%|█████▍    | 27/50 [00:13<00:09,  2.35it/s]total : 1000  current step :  325
total : 1000  current step :  326
total : 1000  current step :  327
Train Iter: 328/1000. LR: 0.0205. Data: 0.24s. Batch: 0.51s. S_Loss: 1.5681. T_Loss: 3.3658. Mask: 0.9219. :  54%|█████▍    | 27/50 [00:14<00:09,  2.35it/s]Train Iter: 328/1000. LR: 0.0205. Data: 0.24s. Batch: 0.51s. S_Loss: 1.5681. T_Loss: 3.3658. Mask: 0.9219. :  56%|█████▌    | 28/50 [00:14<00:12,  1.76it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.23s. Batch: 0.50s. S_Loss: 1.5580. T_Loss: 3.3746. Mask: 0.9225. :  56%|█████▌    | 28/50 [00:14<00:12,  1.76it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.23s. Batch: 0.50s. S_Loss: 1.5580. T_Loss: 3.3746. Mask: 0.9225. :  58%|█████▊    | 29/50 [00:14<00:09,  2.18it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.23s. Batch: 0.49s. S_Loss: 1.5496. T_Loss: 3.3784. Mask: 0.9234. :  58%|█████▊    | 29/50 [00:14<00:09,  2.18it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.23s. Batch: 0.49s. S_Loss: 1.5496. T_Loss: 3.3784. Mask: 0.9234. :  60%|██████    | 30/50 [00:14<00:07,  2.73it/s]total : 1000  current step :  328
total : 1000  current step :  329
total : 1000  current step :  330
Train Iter: 331/1000. LR: 0.0207. Data: 0.24s. Batch: 0.50s. S_Loss: 1.5413. T_Loss: 3.3864. Mask: 0.9233. :  60%|██████    | 30/50 [00:15<00:07,  2.73it/s]Train Iter: 331/1000. LR: 0.0207. Data: 0.24s. Batch: 0.50s. S_Loss: 1.5413. T_Loss: 3.3864. Mask: 0.9233. :  62%|██████▏   | 31/50 [00:15<00:09,  2.06it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.23s. Batch: 0.49s. S_Loss: 1.5330. T_Loss: 3.3949. Mask: 0.9230. :  62%|██████▏   | 31/50 [00:15<00:09,  2.06it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.23s. Batch: 0.49s. S_Loss: 1.5330. T_Loss: 3.3949. Mask: 0.9230. :  64%|██████▍   | 32/50 [00:15<00:07,  2.39it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.23s. Batch: 0.48s. S_Loss: 1.5250. T_Loss: 3.4009. Mask: 0.9228. :  64%|██████▍   | 32/50 [00:15<00:07,  2.39it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.23s. Batch: 0.48s. S_Loss: 1.5250. T_Loss: 3.4009. Mask: 0.9228. :  66%|██████▌   | 33/50 [00:15<00:06,  2.67it/s]total : 1000  current step :  331
total : 1000  current step :  332
total : 1000  current step :  333
Train Iter: 334/1000. LR: 0.0209. Data: 0.24s. Batch: 0.49s. S_Loss: 1.5170. T_Loss: 3.4075. Mask: 0.9220. :  66%|██████▌   | 33/50 [00:16<00:06,  2.67it/s]Train Iter: 334/1000. LR: 0.0209. Data: 0.24s. Batch: 0.49s. S_Loss: 1.5170. T_Loss: 3.4075. Mask: 0.9220. :  68%|██████▊   | 34/50 [00:16<00:08,  1.98it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.24s. Batch: 0.48s. S_Loss: 1.5087. T_Loss: 3.4173. Mask: 0.9225. :  68%|██████▊   | 34/50 [00:16<00:08,  1.98it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.24s. Batch: 0.48s. S_Loss: 1.5087. T_Loss: 3.4173. Mask: 0.9225. :  70%|███████   | 35/50 [00:16<00:06,  2.42it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.23s. Batch: 0.48s. S_Loss: 1.5007. T_Loss: 3.4229. Mask: 0.9230. :  70%|███████   | 35/50 [00:17<00:06,  2.42it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.23s. Batch: 0.48s. S_Loss: 1.5007. T_Loss: 3.4229. Mask: 0.9230. :  72%|███████▏  | 36/50 [00:17<00:05,  2.70it/s]total : 1000  current step :  334
total : 1000  current step :  335
total : 1000  current step :  336
Train Iter: 337/1000. LR: 0.0211. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4933. T_Loss: 3.4249. Mask: 0.9237. :  72%|███████▏  | 36/50 [00:18<00:05,  2.70it/s]Train Iter: 337/1000. LR: 0.0211. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4933. T_Loss: 3.4249. Mask: 0.9237. :  74%|███████▍  | 37/50 [00:18<00:07,  1.86it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.24s. Batch: 0.48s. S_Loss: 1.4860. T_Loss: 3.4259. Mask: 0.9238. :  74%|███████▍  | 37/50 [00:18<00:07,  1.86it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.24s. Batch: 0.48s. S_Loss: 1.4860. T_Loss: 3.4259. Mask: 0.9238. :  76%|███████▌  | 38/50 [00:18<00:05,  2.26it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.23s. Batch: 0.48s. S_Loss: 1.4783. T_Loss: 3.4257. Mask: 0.9243. :  76%|███████▌  | 38/50 [00:18<00:05,  2.26it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.23s. Batch: 0.48s. S_Loss: 1.4783. T_Loss: 3.4257. Mask: 0.9243. :  78%|███████▊  | 39/50 [00:18<00:04,  2.53it/s]total : 1000  current step :  337
total : 1000  current step :  338
total : 1000  current step :  339
Train Iter: 340/1000. LR: 0.0213. Data: 0.25s. Batch: 0.50s. S_Loss: 1.4718. T_Loss: 3.4286. Mask: 0.9242. :  78%|███████▊  | 39/50 [00:19<00:04,  2.53it/s]Train Iter: 340/1000. LR: 0.0213. Data: 0.25s. Batch: 0.50s. S_Loss: 1.4718. T_Loss: 3.4286. Mask: 0.9242. :  80%|████████  | 40/50 [00:19<00:06,  1.49it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4653. T_Loss: 3.4290. Mask: 0.9247. :  80%|████████  | 40/50 [00:20<00:06,  1.49it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4653. T_Loss: 3.4290. Mask: 0.9247. :  82%|████████▏ | 41/50 [00:20<00:04,  1.91it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.24s. Batch: 0.48s. S_Loss: 1.4593. T_Loss: 3.4248. Mask: 0.9241. :  82%|████████▏ | 41/50 [00:20<00:04,  1.91it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.24s. Batch: 0.48s. S_Loss: 1.4593. T_Loss: 3.4248. Mask: 0.9241. :  84%|████████▍ | 42/50 [00:20<00:03,  2.34it/s]total : 1000  current step :  340
total : 1000  current step :  341
total : 1000  current step :  342
Train Iter: 343/1000. LR: 0.0214. Data: 0.25s. Batch: 0.49s. S_Loss: 1.4513. T_Loss: 3.4209. Mask: 0.9244. :  84%|████████▍ | 42/50 [00:21<00:03,  2.34it/s]Train Iter: 343/1000. LR: 0.0214. Data: 0.25s. Batch: 0.49s. S_Loss: 1.4513. T_Loss: 3.4209. Mask: 0.9244. :  86%|████████▌ | 43/50 [00:21<00:04,  1.70it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4443. T_Loss: 3.4145. Mask: 0.9245. :  86%|████████▌ | 43/50 [00:21<00:04,  1.70it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4443. T_Loss: 3.4145. Mask: 0.9245. :  88%|████████▊ | 44/50 [00:21<00:03,  1.91it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4373. T_Loss: 3.4106. Mask: 0.9247. :  88%|████████▊ | 44/50 [00:21<00:03,  1.91it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4373. T_Loss: 3.4106. Mask: 0.9247. :  90%|█████████ | 45/50 [00:21<00:02,  2.31it/s]total : 1000  current step :  343
total : 1000  current step :  344
total : 1000  current step :  345
Train Iter: 346/1000. LR: 0.0216. Data: 0.25s. Batch: 0.50s. S_Loss: 1.4309. T_Loss: 3.4095. Mask: 0.9243. :  90%|█████████ | 45/50 [00:22<00:02,  2.31it/s]Train Iter: 346/1000. LR: 0.0216. Data: 0.25s. Batch: 0.50s. S_Loss: 1.4309. T_Loss: 3.4095. Mask: 0.9243. :  92%|█████████▏| 46/50 [00:22<00:02,  1.64it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.25s. Batch: 0.50s. S_Loss: 1.4247. T_Loss: 3.4078. Mask: 0.9245. :  92%|█████████▏| 46/50 [00:23<00:02,  1.64it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.25s. Batch: 0.50s. S_Loss: 1.4247. T_Loss: 3.4078. Mask: 0.9245. :  94%|█████████▍| 47/50 [00:23<00:01,  1.74it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4194. T_Loss: 3.4131. Mask: 0.9249. :  94%|█████████▍| 47/50 [00:23<00:01,  1.74it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.24s. Batch: 0.49s. S_Loss: 1.4194. T_Loss: 3.4131. Mask: 0.9249. :  96%|█████████▌| 48/50 [00:23<00:00,  2.19it/s]total : 1000  current step :  346
total : 1000  current step :  347
total : 1000  current step :  348
Train Iter: 349/1000. LR: 0.0218. Data: 0.25s. Batch: 0.50s. S_Loss: 1.4132. T_Loss: 3.4207. Mask: 0.9257. :  96%|█████████▌| 48/50 [00:24<00:00,  2.19it/s]Train Iter: 349/1000. LR: 0.0218. Data: 0.25s. Batch: 0.50s. S_Loss: 1.4132. T_Loss: 3.4207. Mask: 0.9257. :  98%|█████████▊| 49/50 [00:24<00:00,  1.68it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.25s. Batch: 0.49s. S_Loss: 1.4078. T_Loss: 3.4302. Mask: 0.9258. :  98%|█████████▊| 49/50 [00:24<00:00,  1.68it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.25s. Batch: 0.49s. S_Loss: 1.4078. T_Loss: 3.4302. Mask: 0.9258. : 100%|██████████| 50/50 [00:24<00:00,  2.03it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.25s. Batch: 0.49s. S_Loss: 1.4078. T_Loss: 3.4302. Mask: 0.9258. : 100%|██████████| 50/50 [00:24<00:00,  2.02it/s]
total : 1000  current step :  349
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.5697. top1: 85.94. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.5697. top1: 85.94. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.75it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.5626. top1: 86.52. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.75it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.5626. top1: 86.52. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.5630. top1: 86.98. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.5630. top1: 86.98. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.5639. top1: 85.74. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.5639. top1: 85.74. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.53it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.5887. top1: 76.17. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.53it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.5887. top1: 76.17. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.6007. top1: 70.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.6007. top1: 70.70. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.6121. top1: 65.46. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.6121. top1: 65.46. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.6201. top1: 62.35. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.6201. top1: 62.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.6201. top1: 62.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]
total : 1000  current step :  350
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.26s. S_Loss: 1.1202. T_Loss: 3.8076. Mask: 0.9375. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.26s. S_Loss: 1.1202. T_Loss: 3.8076. Mask: 0.9375. :   2%|▏         | 1/50 [00:00<00:12,  3.83it/s]total : 1000  current step :  351
Train Iter: 352/1000. LR: 0.0220. Data: 0.32s. Batch: 0.55s. S_Loss: 1.1020. T_Loss: 3.7525. Mask: 0.9434. :   2%|▏         | 1/50 [00:01<00:12,  3.83it/s]Train Iter: 352/1000. LR: 0.0220. Data: 0.32s. Batch: 0.55s. S_Loss: 1.1020. T_Loss: 3.7525. Mask: 0.9434. :   4%|▍         | 2/50 [00:01<00:28,  1.66it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.23s. Batch: 0.45s. S_Loss: 1.1084. T_Loss: 3.7840. Mask: 0.9362. :   4%|▍         | 2/50 [00:01<00:28,  1.66it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.23s. Batch: 0.45s. S_Loss: 1.1084. T_Loss: 3.7840. Mask: 0.9362. :   6%|▌         | 3/50 [00:01<00:21,  2.24it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.19s. Batch: 0.44s. S_Loss: 1.1083. T_Loss: 3.7255. Mask: 0.9297. :   6%|▌         | 3/50 [00:01<00:21,  2.24it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.19s. Batch: 0.44s. S_Loss: 1.1083. T_Loss: 3.7255. Mask: 0.9297. :   8%|▊         | 4/50 [00:01<00:19,  2.35it/s]total : 1000  current step :  352
total : 1000  current step :  353
total : 1000  current step :  354
Train Iter: 355/1000. LR: 0.0222. Data: 0.26s. Batch: 0.53s. S_Loss: 1.1048. T_Loss: 3.6965. Mask: 0.9352. :   8%|▊         | 4/50 [00:02<00:19,  2.35it/s]Train Iter: 355/1000. LR: 0.0222. Data: 0.26s. Batch: 0.53s. S_Loss: 1.1048. T_Loss: 3.6965. Mask: 0.9352. :  10%|█         | 5/50 [00:02<00:27,  1.66it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.22s. Batch: 0.49s. S_Loss: 1.1000. T_Loss: 3.6342. Mask: 0.9303. :  10%|█         | 5/50 [00:02<00:27,  1.66it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.22s. Batch: 0.49s. S_Loss: 1.1000. T_Loss: 3.6342. Mask: 0.9303. :  12%|█▏        | 6/50 [00:02<00:21,  2.04it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.19s. Batch: 0.44s. S_Loss: 1.0977. T_Loss: 3.5929. Mask: 0.9280. :  12%|█▏        | 6/50 [00:03<00:21,  2.04it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.19s. Batch: 0.44s. S_Loss: 1.0977. T_Loss: 3.5929. Mask: 0.9280. :  14%|█▍        | 7/50 [00:03<00:16,  2.63it/s]total : 1000  current step :  355
total : 1000  current step :  356
total : 1000  current step :  357
Train Iter: 358/1000. LR: 0.0224. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0926. T_Loss: 3.5777. Mask: 0.9316. :  14%|█▍        | 7/50 [00:04<00:16,  2.63it/s]Train Iter: 358/1000. LR: 0.0224. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0926. T_Loss: 3.5777. Mask: 0.9316. :  16%|█▌        | 8/50 [00:04<00:23,  1.78it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.22s. Batch: 0.48s. S_Loss: 1.0942. T_Loss: 3.6461. Mask: 0.9345. :  16%|█▌        | 8/50 [00:04<00:23,  1.78it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.22s. Batch: 0.48s. S_Loss: 1.0942. T_Loss: 3.6461. Mask: 0.9345. :  18%|█▊        | 9/50 [00:04<00:19,  2.14it/s]total : 1000  current step :  358
total : 1000  current step :  359
Train Iter: 360/1000. LR: 0.0225. Data: 0.22s. Batch: 0.46s. S_Loss: 1.0989. T_Loss: 3.6710. Mask: 0.9344. :  18%|█▊        | 9/50 [00:04<00:19,  2.14it/s]Train Iter: 360/1000. LR: 0.0225. Data: 0.22s. Batch: 0.46s. S_Loss: 1.0989. T_Loss: 3.6710. Mask: 0.9344. :  20%|██        | 10/50 [00:04<00:17,  2.32it/s]total : 1000  current step :  360
Train Iter: 361/1000. LR: 0.0226. Data: 0.26s. Batch: 0.52s. S_Loss: 1.1008. T_Loss: 3.7143. Mask: 0.9322. :  20%|██        | 10/50 [00:05<00:17,  2.32it/s]Train Iter: 361/1000. LR: 0.0226. Data: 0.26s. Batch: 0.52s. S_Loss: 1.1008. T_Loss: 3.7143. Mask: 0.9322. :  22%|██▏       | 11/50 [00:05<00:24,  1.61it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.24s. Batch: 0.49s. S_Loss: 1.1024. T_Loss: 3.7538. Mask: 0.9261. :  22%|██▏       | 11/50 [00:05<00:24,  1.61it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.24s. Batch: 0.49s. S_Loss: 1.1024. T_Loss: 3.7538. Mask: 0.9261. :  24%|██▍       | 12/50 [00:05<00:18,  2.05it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.23s. Batch: 0.48s. S_Loss: 1.1045. T_Loss: 3.7793. Mask: 0.9198. :  24%|██▍       | 12/50 [00:06<00:18,  2.05it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.23s. Batch: 0.48s. S_Loss: 1.1045. T_Loss: 3.7793. Mask: 0.9198. :  26%|██▌       | 13/50 [00:06<00:15,  2.31it/s]total : 1000  current step :  361
total : 1000  current step :  362
total : 1000  current step :  363
Train Iter: 364/1000. LR: 0.0228. Data: 0.26s. Batch: 0.51s. S_Loss: 1.1007. T_Loss: 3.7838. Mask: 0.9155. :  26%|██▌       | 13/50 [00:07<00:15,  2.31it/s]Train Iter: 364/1000. LR: 0.0228. Data: 0.26s. Batch: 0.51s. S_Loss: 1.1007. T_Loss: 3.7838. Mask: 0.9155. :  28%|██▊       | 14/50 [00:07<00:20,  1.72it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0979. T_Loss: 3.7966. Mask: 0.9146. :  28%|██▊       | 14/50 [00:07<00:20,  1.72it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0979. T_Loss: 3.7966. Mask: 0.9146. :  30%|███       | 15/50 [00:07<00:16,  2.17it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.23s. Batch: 0.47s. S_Loss: 1.0965. T_Loss: 3.7892. Mask: 0.9099. :  30%|███       | 15/50 [00:07<00:16,  2.17it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.23s. Batch: 0.47s. S_Loss: 1.0965. T_Loss: 3.7892. Mask: 0.9099. :  32%|███▏      | 16/50 [00:07<00:13,  2.50it/s]total : 1000  current step :  364
total : 1000  current step :  365
total : 1000  current step :  366
Train Iter: 367/1000. LR: 0.0229. Data: 0.26s. Batch: 0.50s. S_Loss: 1.0942. T_Loss: 3.7881. Mask: 0.9083. :  32%|███▏      | 16/50 [00:08<00:13,  2.50it/s]Train Iter: 367/1000. LR: 0.0229. Data: 0.26s. Batch: 0.50s. S_Loss: 1.0942. T_Loss: 3.7881. Mask: 0.9083. :  34%|███▍      | 17/50 [00:08<00:19,  1.68it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0948. T_Loss: 3.7864. Mask: 0.9076. :  34%|███▍      | 17/50 [00:08<00:19,  1.68it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0948. T_Loss: 3.7864. Mask: 0.9076. :  36%|███▌      | 18/50 [00:08<00:16,  1.97it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.23s. Batch: 0.48s. S_Loss: 1.0950. T_Loss: 3.7890. Mask: 0.9083. :  36%|███▌      | 18/50 [00:09<00:16,  1.97it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.23s. Batch: 0.48s. S_Loss: 1.0950. T_Loss: 3.7890. Mask: 0.9083. :  38%|███▊      | 19/50 [00:09<00:12,  2.46it/s]total : 1000  current step :  367
total : 1000  current step :  368
total : 1000  current step :  369
Train Iter: 370/1000. LR: 0.0231. Data: 0.26s. Batch: 0.51s. S_Loss: 1.0939. T_Loss: 3.7897. Mask: 0.9088. :  38%|███▊      | 19/50 [00:10<00:12,  2.46it/s]Train Iter: 370/1000. LR: 0.0231. Data: 0.26s. Batch: 0.51s. S_Loss: 1.0939. T_Loss: 3.7897. Mask: 0.9088. :  40%|████      | 20/50 [00:10<00:18,  1.65it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.25s. Batch: 0.49s. S_Loss: 1.0945. T_Loss: 3.7965. Mask: 0.9094. :  40%|████      | 20/50 [00:10<00:18,  1.65it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.25s. Batch: 0.49s. S_Loss: 1.0945. T_Loss: 3.7965. Mask: 0.9094. :  42%|████▏     | 21/50 [00:10<00:14,  2.01it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0960. T_Loss: 3.8159. Mask: 0.9089. :  42%|████▏     | 21/50 [00:10<00:14,  2.01it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0960. T_Loss: 3.8159. Mask: 0.9089. :  44%|████▍     | 22/50 [00:10<00:12,  2.24it/s]total : 1000  current step :  370
total : 1000  current step :  371
total : 1000  current step :  372
Train Iter: 373/1000. LR: 0.0233. Data: 0.26s. Batch: 0.51s. S_Loss: 1.0968. T_Loss: 3.8673. Mask: 0.9100. :  44%|████▍     | 22/50 [00:11<00:12,  2.24it/s]Train Iter: 373/1000. LR: 0.0233. Data: 0.26s. Batch: 0.51s. S_Loss: 1.0968. T_Loss: 3.8673. Mask: 0.9100. :  46%|████▌     | 23/50 [00:11<00:16,  1.59it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0942. T_Loss: 3.8669. Mask: 0.9089. :  46%|████▌     | 23/50 [00:12<00:16,  1.59it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0942. T_Loss: 3.8669. Mask: 0.9089. :  48%|████▊     | 24/50 [00:12<00:13,  1.86it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0938. T_Loss: 3.8551. Mask: 0.9072. :  48%|████▊     | 24/50 [00:12<00:13,  1.86it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0938. T_Loss: 3.8551. Mask: 0.9072. :  50%|█████     | 25/50 [00:12<00:10,  2.34it/s]total : 1000  current step :  373
total : 1000  current step :  374
total : 1000  current step :  375
Train Iter: 376/1000. LR: 0.0235. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0915. T_Loss: 3.8494. Mask: 0.9064. :  50%|█████     | 25/50 [00:13<00:10,  2.34it/s]Train Iter: 376/1000. LR: 0.0235. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0915. T_Loss: 3.8494. Mask: 0.9064. :  52%|█████▏    | 26/50 [00:13<00:13,  1.77it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0907. T_Loss: 3.8603. Mask: 0.9073. :  52%|█████▏    | 26/50 [00:13<00:13,  1.77it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0907. T_Loss: 3.8603. Mask: 0.9073. :  54%|█████▍    | 27/50 [00:13<00:10,  2.11it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0893. T_Loss: 3.8648. Mask: 0.9074. :  54%|█████▍    | 27/50 [00:13<00:10,  2.11it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0893. T_Loss: 3.8648. Mask: 0.9074. :  56%|█████▌    | 28/50 [00:13<00:09,  2.38it/s]total : 1000  current step :  376
total : 1000  current step :  377
total : 1000  current step :  378
Train Iter: 379/1000. LR: 0.0237. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0886. T_Loss: 3.8663. Mask: 0.9067. :  56%|█████▌    | 28/50 [00:14<00:09,  2.38it/s]Train Iter: 379/1000. LR: 0.0237. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0886. T_Loss: 3.8663. Mask: 0.9067. :  58%|█████▊    | 29/50 [00:14<00:12,  1.72it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0868. T_Loss: 3.8641. Mask: 0.9059. :  58%|█████▊    | 29/50 [00:14<00:12,  1.72it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0868. T_Loss: 3.8641. Mask: 0.9059. :  60%|██████    | 30/50 [00:14<00:09,  2.15it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0847. T_Loss: 3.8629. Mask: 0.9054. :  60%|██████    | 30/50 [00:15<00:09,  2.15it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0847. T_Loss: 3.8629. Mask: 0.9054. :  62%|██████▏   | 31/50 [00:15<00:08,  2.27it/s]total : 1000  current step :  379
total : 1000  current step :  380
total : 1000  current step :  381
Train Iter: 382/1000. LR: 0.0239. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0832. T_Loss: 3.8648. Mask: 0.9064. :  62%|██████▏   | 31/50 [00:16<00:08,  2.27it/s]Train Iter: 382/1000. LR: 0.0239. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0832. T_Loss: 3.8648. Mask: 0.9064. :  64%|██████▍   | 32/50 [00:16<00:12,  1.42it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0820. T_Loss: 3.8604. Mask: 0.9068. :  64%|██████▍   | 32/50 [00:16<00:12,  1.42it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0820. T_Loss: 3.8604. Mask: 0.9068. :  66%|██████▌   | 33/50 [00:16<00:09,  1.83it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0808. T_Loss: 3.8569. Mask: 0.9064. :  66%|██████▌   | 33/50 [00:17<00:09,  1.83it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0808. T_Loss: 3.8569. Mask: 0.9064. :  68%|██████▊   | 34/50 [00:17<00:07,  2.08it/s]total : 1000  current step :  382
total : 1000  current step :  383
total : 1000  current step :  384
Train Iter: 385/1000. LR: 0.0241. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0808. T_Loss: 3.8589. Mask: 0.9066. :  68%|██████▊   | 34/50 [00:18<00:07,  2.08it/s]Train Iter: 385/1000. LR: 0.0241. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0808. T_Loss: 3.8589. Mask: 0.9066. :  70%|███████   | 35/50 [00:18<00:09,  1.55it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0803. T_Loss: 3.8640. Mask: 0.9067. :  70%|███████   | 35/50 [00:18<00:09,  1.55it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0803. T_Loss: 3.8640. Mask: 0.9067. :  72%|███████▏  | 36/50 [00:18<00:07,  1.87it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0803. T_Loss: 3.8664. Mask: 0.9072. :  72%|███████▏  | 36/50 [00:18<00:07,  1.87it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0803. T_Loss: 3.8664. Mask: 0.9072. :  74%|███████▍  | 37/50 [00:18<00:05,  2.35it/s]total : 1000  current step :  385
total : 1000  current step :  386
total : 1000  current step :  387
Train Iter: 388/1000. LR: 0.0243. Data: 0.26s. Batch: 0.51s. S_Loss: 1.0808. T_Loss: 3.8626. Mask: 0.9065. :  74%|███████▍  | 37/50 [00:19<00:05,  2.35it/s]Train Iter: 388/1000. LR: 0.0243. Data: 0.26s. Batch: 0.51s. S_Loss: 1.0808. T_Loss: 3.8626. Mask: 0.9065. :  76%|███████▌  | 38/50 [00:19<00:07,  1.64it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0798. T_Loss: 3.8593. Mask: 0.9064. :  76%|███████▌  | 38/50 [00:20<00:07,  1.64it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0798. T_Loss: 3.8593. Mask: 0.9064. :  78%|███████▊  | 39/50 [00:20<00:06,  1.78it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0781. T_Loss: 3.8561. Mask: 0.9069. :  78%|███████▊  | 39/50 [00:20<00:06,  1.78it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0781. T_Loss: 3.8561. Mask: 0.9069. :  80%|████████  | 40/50 [00:20<00:04,  2.27it/s]total : 1000  current step :  388
total : 1000  current step :  389
total : 1000  current step :  390
Train Iter: 391/1000. LR: 0.0244. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0758. T_Loss: 3.8466. Mask: 0.9073. :  80%|████████  | 40/50 [00:21<00:04,  2.27it/s]Train Iter: 391/1000. LR: 0.0244. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0758. T_Loss: 3.8466. Mask: 0.9073. :  82%|████████▏ | 41/50 [00:21<00:05,  1.63it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0746. T_Loss: 3.8415. Mask: 0.9081. :  82%|████████▏ | 41/50 [00:21<00:05,  1.63it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0746. T_Loss: 3.8415. Mask: 0.9081. :  84%|████████▍ | 42/50 [00:21<00:04,  1.91it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0741. T_Loss: 3.8447. Mask: 0.9094. :  84%|████████▍ | 42/50 [00:21<00:04,  1.91it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0741. T_Loss: 3.8447. Mask: 0.9094. :  86%|████████▌ | 43/50 [00:21<00:03,  2.28it/s]total : 1000  current step :  391
total : 1000  current step :  392
total : 1000  current step :  393
Train Iter: 394/1000. LR: 0.0246. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0735. T_Loss: 3.8529. Mask: 0.9100. :  86%|████████▌ | 43/50 [00:22<00:03,  2.28it/s]Train Iter: 394/1000. LR: 0.0246. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0735. T_Loss: 3.8529. Mask: 0.9100. :  88%|████████▊ | 44/50 [00:22<00:03,  1.74it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0730. T_Loss: 3.8619. Mask: 0.9104. :  88%|████████▊ | 44/50 [00:22<00:03,  1.74it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0730. T_Loss: 3.8619. Mask: 0.9104. :  90%|█████████ | 45/50 [00:22<00:02,  2.00it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0734. T_Loss: 3.8634. Mask: 0.9104. :  90%|█████████ | 45/50 [00:23<00:02,  2.00it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0734. T_Loss: 3.8634. Mask: 0.9104. :  92%|█████████▏| 46/50 [00:23<00:01,  2.19it/s]total : 1000  current step :  394
total : 1000  current step :  395
total : 1000  current step :  396
Train Iter: 397/1000. LR: 0.0248. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0722. T_Loss: 3.8611. Mask: 0.9106. :  92%|█████████▏| 46/50 [00:24<00:01,  2.19it/s]Train Iter: 397/1000. LR: 0.0248. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0722. T_Loss: 3.8611. Mask: 0.9106. :  94%|█████████▍| 47/50 [00:24<00:01,  1.73it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0718. T_Loss: 3.8608. Mask: 0.9107. :  94%|█████████▍| 47/50 [00:24<00:01,  1.73it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0718. T_Loss: 3.8608. Mask: 0.9107. :  96%|█████████▌| 48/50 [00:24<00:01,  1.85it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0706. T_Loss: 3.8597. Mask: 0.9114. :  96%|█████████▌| 48/50 [00:24<00:01,  1.85it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0706. T_Loss: 3.8597. Mask: 0.9114. :  98%|█████████▊| 49/50 [00:24<00:00,  2.11it/s]total : 1000  current step :  397
total : 1000  current step :  398
total : 1000  current step :  399
Train Iter: 400/1000. LR: 0.0250. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0690. T_Loss: 3.8531. Mask: 0.9116. :  98%|█████████▊| 49/50 [00:26<00:00,  2.11it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0690. T_Loss: 3.8531. Mask: 0.9116. : 100%|██████████| 50/50 [00:26<00:00,  1.47it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0690. T_Loss: 3.8531. Mask: 0.9116. : 100%|██████████| 50/50 [00:26<00:00,  1.91it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.4060. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.4060. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.00it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.3740. top1: 85.55. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.00it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.3740. top1: 85.55. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.83it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.3738. top1: 86.07. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.83it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.3738. top1: 86.07. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.3794. top1: 84.86. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.3794. top1: 84.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.4261. top1: 79.38. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.4261. top1: 79.38. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.4528. top1: 77.47. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.4528. top1: 77.47. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.4765. top1: 74.16. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.4765. top1: 74.16. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.4906. top1: 72.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.4906. top1: 72.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.4906. top1: 72.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]
total : 1000  current step :  400
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.22s. S_Loss: 1.0266. T_Loss: 3.7980. Mask: 0.9258. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.22s. S_Loss: 1.0266. T_Loss: 3.7980. Mask: 0.9258. :   2%|▏         | 1/50 [00:00<00:11,  4.42it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.22s. S_Loss: 1.0317. T_Loss: 3.7740. Mask: 0.9336. :   2%|▏         | 1/50 [00:00<00:11,  4.42it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.22s. S_Loss: 1.0317. T_Loss: 3.7740. Mask: 0.9336. :   4%|▍         | 2/50 [00:00<00:10,  4.58it/s]total : 1000  current step :  401
total : 1000  current step :  402
Train Iter: 403/1000. LR: 0.0252. Data: 0.21s. Batch: 0.42s. S_Loss: 1.0361. T_Loss: 3.7970. Mask: 0.9258. :   4%|▍         | 2/50 [00:01<00:10,  4.58it/s]Train Iter: 403/1000. LR: 0.0252. Data: 0.21s. Batch: 0.42s. S_Loss: 1.0361. T_Loss: 3.7970. Mask: 0.9258. :   6%|▌         | 3/50 [00:01<00:23,  2.04it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.19s. Batch: 0.41s. S_Loss: 1.0404. T_Loss: 3.8207. Mask: 0.9189. :   6%|▌         | 3/50 [00:01<00:23,  2.04it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.19s. Batch: 0.41s. S_Loss: 1.0404. T_Loss: 3.8207. Mask: 0.9189. :   8%|▊         | 4/50 [00:01<00:21,  2.18it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.16s. Batch: 0.38s. S_Loss: 1.0431. T_Loss: 3.8849. Mask: 0.9180. :   8%|▊         | 4/50 [00:01<00:21,  2.18it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.16s. Batch: 0.38s. S_Loss: 1.0431. T_Loss: 3.8849. Mask: 0.9180. :  10%|█         | 5/50 [00:01<00:17,  2.57it/s]total : 1000  current step :  403
total : 1000  current step :  404
total : 1000  current step :  405
Train Iter: 406/1000. LR: 0.0254. Data: 0.25s. Batch: 0.47s. S_Loss: 1.0326. T_Loss: 3.8494. Mask: 0.9225. :  10%|█         | 5/50 [00:02<00:17,  2.57it/s]Train Iter: 406/1000. LR: 0.0254. Data: 0.25s. Batch: 0.47s. S_Loss: 1.0326. T_Loss: 3.8494. Mask: 0.9225. :  12%|█▏        | 6/50 [00:02<00:25,  1.75it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.23s. Batch: 0.45s. S_Loss: 1.0281. T_Loss: 3.8453. Mask: 0.9219. :  12%|█▏        | 6/50 [00:03<00:25,  1.75it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.23s. Batch: 0.45s. S_Loss: 1.0281. T_Loss: 3.8453. Mask: 0.9219. :  14%|█▍        | 7/50 [00:03<00:20,  2.10it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.21s. Batch: 0.44s. S_Loss: 1.0272. T_Loss: 3.8323. Mask: 0.9189. :  14%|█▍        | 7/50 [00:03<00:20,  2.10it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.21s. Batch: 0.44s. S_Loss: 1.0272. T_Loss: 3.8323. Mask: 0.9189. :  16%|█▌        | 8/50 [00:03<00:19,  2.19it/s]total : 1000  current step :  406
total : 1000  current step :  407
total : 1000  current step :  408
Train Iter: 409/1000. LR: 0.0256. Data: 0.26s. Batch: 0.48s. S_Loss: 1.0294. T_Loss: 3.8491. Mask: 0.9219. :  16%|█▌        | 8/50 [00:04<00:19,  2.19it/s]Train Iter: 409/1000. LR: 0.0256. Data: 0.26s. Batch: 0.48s. S_Loss: 1.0294. T_Loss: 3.8491. Mask: 0.9219. :  18%|█▊        | 9/50 [00:04<00:23,  1.77it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.25s. Batch: 0.48s. S_Loss: 1.0329. T_Loss: 3.8584. Mask: 0.9219. :  18%|█▊        | 9/50 [00:04<00:23,  1.77it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.25s. Batch: 0.48s. S_Loss: 1.0329. T_Loss: 3.8584. Mask: 0.9219. :  20%|██        | 10/50 [00:04<00:21,  1.86it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.23s. Batch: 0.46s. S_Loss: 1.0333. T_Loss: 3.8616. Mask: 0.9205. :  20%|██        | 10/50 [00:05<00:21,  1.86it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.23s. Batch: 0.46s. S_Loss: 1.0333. T_Loss: 3.8616. Mask: 0.9205. :  22%|██▏       | 11/50 [00:05<00:17,  2.22it/s]total : 1000  current step :  409
total : 1000  current step :  410
total : 1000  current step :  411
Train Iter: 412/1000. LR: 0.0258. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0329. T_Loss: 3.8589. Mask: 0.9196. :  22%|██▏       | 11/50 [00:05<00:17,  2.22it/s]Train Iter: 412/1000. LR: 0.0258. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0329. T_Loss: 3.8589. Mask: 0.9196. :  24%|██▍       | 12/50 [00:05<00:21,  1.74it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.26s. Batch: 0.48s. S_Loss: 1.0312. T_Loss: 3.8706. Mask: 0.9204. :  24%|██▍       | 12/50 [00:06<00:21,  1.74it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.26s. Batch: 0.48s. S_Loss: 1.0312. T_Loss: 3.8706. Mask: 0.9204. :  26%|██▌       | 13/50 [00:06<00:18,  2.00it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.24s. Batch: 0.48s. S_Loss: 1.0277. T_Loss: 3.8730. Mask: 0.9224. :  26%|██▌       | 13/50 [00:06<00:18,  2.00it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.24s. Batch: 0.48s. S_Loss: 1.0277. T_Loss: 3.8730. Mask: 0.9224. :  28%|██▊       | 14/50 [00:06<00:17,  2.10it/s]total : 1000  current step :  412
total : 1000  current step :  413
total : 1000  current step :  414
Train Iter: 415/1000. LR: 0.0259. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0270. T_Loss: 3.8613. Mask: 0.9229. :  28%|██▊       | 14/50 [00:07<00:17,  2.10it/s]Train Iter: 415/1000. LR: 0.0259. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0270. T_Loss: 3.8613. Mask: 0.9229. :  30%|███       | 15/50 [00:07<00:21,  1.64it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0274. T_Loss: 3.8618. Mask: 0.9241. :  30%|███       | 15/50 [00:07<00:21,  1.64it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0274. T_Loss: 3.8618. Mask: 0.9241. :  32%|███▏      | 16/50 [00:07<00:17,  1.94it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.26s. Batch: 0.49s. S_Loss: 1.0283. T_Loss: 3.8566. Mask: 0.9233. :  32%|███▏      | 16/50 [00:08<00:17,  1.94it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.26s. Batch: 0.49s. S_Loss: 1.0283. T_Loss: 3.8566. Mask: 0.9233. :  34%|███▍      | 17/50 [00:08<00:16,  2.02it/s]total : 1000  current step :  415
total : 1000  current step :  416
total : 1000  current step :  417
Train Iter: 418/1000. LR: 0.0261. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0287. T_Loss: 3.8860. Mask: 0.9240. :  34%|███▍      | 17/50 [00:09<00:16,  2.02it/s]Train Iter: 418/1000. LR: 0.0261. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0287. T_Loss: 3.8860. Mask: 0.9240. :  36%|███▌      | 18/50 [00:09<00:19,  1.68it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.28s. Batch: 0.50s. S_Loss: 1.0276. T_Loss: 3.9034. Mask: 0.9262. :  36%|███▌      | 18/50 [00:09<00:19,  1.68it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.28s. Batch: 0.50s. S_Loss: 1.0276. T_Loss: 3.9034. Mask: 0.9262. :  38%|███▊      | 19/50 [00:09<00:15,  1.97it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0288. T_Loss: 3.9262. Mask: 0.9264. :  38%|███▊      | 19/50 [00:09<00:15,  1.97it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0288. T_Loss: 3.9262. Mask: 0.9264. :  40%|████      | 20/50 [00:09<00:13,  2.15it/s]total : 1000  current step :  418
total : 1000  current step :  419
total : 1000  current step :  420
Train Iter: 421/1000. LR: 0.0263. Data: 0.29s. Batch: 0.52s. S_Loss: 1.0317. T_Loss: 3.9467. Mask: 0.9245. :  40%|████      | 20/50 [00:10<00:13,  2.15it/s]Train Iter: 421/1000. LR: 0.0263. Data: 0.29s. Batch: 0.52s. S_Loss: 1.0317. T_Loss: 3.9467. Mask: 0.9245. :  42%|████▏     | 21/50 [00:10<00:18,  1.58it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.28s. Batch: 0.50s. S_Loss: 1.0330. T_Loss: 3.9522. Mask: 0.9238. :  42%|████▏     | 21/50 [00:11<00:18,  1.58it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.28s. Batch: 0.50s. S_Loss: 1.0330. T_Loss: 3.9522. Mask: 0.9238. :  44%|████▍     | 22/50 [00:11<00:14,  1.95it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0307. T_Loss: 3.9530. Mask: 0.9241. :  44%|████▍     | 22/50 [00:11<00:14,  1.95it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0307. T_Loss: 3.9530. Mask: 0.9241. :  46%|████▌     | 23/50 [00:11<00:11,  2.28it/s]total : 1000  current step :  421
total : 1000  current step :  422
total : 1000  current step :  423
Train Iter: 424/1000. LR: 0.0265. Data: 0.29s. Batch: 0.52s. S_Loss: 1.0301. T_Loss: 3.9489. Mask: 0.9245. :  46%|████▌     | 23/50 [00:12<00:11,  2.28it/s]Train Iter: 424/1000. LR: 0.0265. Data: 0.29s. Batch: 0.52s. S_Loss: 1.0301. T_Loss: 3.9489. Mask: 0.9245. :  48%|████▊     | 24/50 [00:12<00:16,  1.61it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0291. T_Loss: 3.9592. Mask: 0.9248. :  48%|████▊     | 24/50 [00:12<00:16,  1.61it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0291. T_Loss: 3.9592. Mask: 0.9248. :  50%|█████     | 25/50 [00:12<00:12,  1.96it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.27s. Batch: 0.50s. S_Loss: 1.0304. T_Loss: 3.9604. Mask: 0.9244. :  50%|█████     | 25/50 [00:12<00:12,  1.96it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.27s. Batch: 0.50s. S_Loss: 1.0304. T_Loss: 3.9604. Mask: 0.9244. :  52%|█████▏    | 26/50 [00:12<00:10,  2.30it/s]total : 1000  current step :  424
total : 1000  current step :  425
total : 1000  current step :  426
Train Iter: 427/1000. LR: 0.0267. Data: 0.29s. Batch: 0.51s. S_Loss: 1.0306. T_Loss: 3.9655. Mask: 0.9240. :  52%|█████▏    | 26/50 [00:13<00:10,  2.30it/s]Train Iter: 427/1000. LR: 0.0267. Data: 0.29s. Batch: 0.51s. S_Loss: 1.0306. T_Loss: 3.9655. Mask: 0.9240. :  54%|█████▍    | 27/50 [00:13<00:13,  1.72it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0309. T_Loss: 3.9735. Mask: 0.9238. :  54%|█████▍    | 27/50 [00:14<00:13,  1.72it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0309. T_Loss: 3.9735. Mask: 0.9238. :  56%|█████▌    | 28/50 [00:14<00:11,  1.89it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.27s. Batch: 0.50s. S_Loss: 1.0296. T_Loss: 3.9787. Mask: 0.9236. :  56%|█████▌    | 28/50 [00:14<00:11,  1.89it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.27s. Batch: 0.50s. S_Loss: 1.0296. T_Loss: 3.9787. Mask: 0.9236. :  58%|█████▊    | 29/50 [00:14<00:10,  2.03it/s]total : 1000  current step :  427
total : 1000  current step :  428
total : 1000  current step :  429
Train Iter: 430/1000. LR: 0.0269. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0296. T_Loss: 3.9942. Mask: 0.9243. :  58%|█████▊    | 29/50 [00:15<00:10,  2.03it/s]Train Iter: 430/1000. LR: 0.0269. Data: 0.28s. Batch: 0.51s. S_Loss: 1.0296. T_Loss: 3.9942. Mask: 0.9243. :  60%|██████    | 30/50 [00:15<00:11,  1.69it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.28s. Batch: 0.52s. S_Loss: 1.0306. T_Loss: 3.9937. Mask: 0.9239. :  60%|██████    | 30/50 [00:16<00:11,  1.69it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.28s. Batch: 0.52s. S_Loss: 1.0306. T_Loss: 3.9937. Mask: 0.9239. :  62%|██████▏   | 31/50 [00:16<00:11,  1.67it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.27s. Batch: 0.51s. S_Loss: 1.0305. T_Loss: 3.9928. Mask: 0.9240. :  62%|██████▏   | 31/50 [00:16<00:11,  1.67it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.27s. Batch: 0.51s. S_Loss: 1.0305. T_Loss: 3.9928. Mask: 0.9240. :  64%|██████▍   | 32/50 [00:16<00:09,  1.89it/s]total : 1000  current step :  430
total : 1000  current step :  431
total : 1000  current step :  432
Train Iter: 433/1000. LR: 0.0271. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0300. T_Loss: 3.9857. Mask: 0.9239. :  64%|██████▍   | 32/50 [00:17<00:09,  1.89it/s]Train Iter: 433/1000. LR: 0.0271. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0300. T_Loss: 3.9857. Mask: 0.9239. :  66%|██████▌   | 33/50 [00:17<00:11,  1.50it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.28s. Batch: 0.52s. S_Loss: 1.0297. T_Loss: 3.9862. Mask: 0.9241. :  66%|██████▌   | 33/50 [00:17<00:11,  1.50it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.28s. Batch: 0.52s. S_Loss: 1.0297. T_Loss: 3.9862. Mask: 0.9241. :  68%|██████▊   | 34/50 [00:17<00:09,  1.76it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0293. T_Loss: 3.9941. Mask: 0.9240. :  68%|██████▊   | 34/50 [00:18<00:09,  1.76it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0293. T_Loss: 3.9941. Mask: 0.9240. :  70%|███████   | 35/50 [00:18<00:07,  1.98it/s]total : 1000  current step :  433
total : 1000  current step :  434
total : 1000  current step :  435
Train Iter: 436/1000. LR: 0.0273. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0285. T_Loss: 3.9883. Mask: 0.9230. :  70%|███████   | 35/50 [00:19<00:07,  1.98it/s]Train Iter: 436/1000. LR: 0.0273. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0285. T_Loss: 3.9883. Mask: 0.9230. :  72%|███████▏  | 36/50 [00:19<00:08,  1.56it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0292. T_Loss: 3.9895. Mask: 0.9219. :  72%|███████▏  | 36/50 [00:19<00:08,  1.56it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0292. T_Loss: 3.9895. Mask: 0.9219. :  74%|███████▍  | 37/50 [00:19<00:07,  1.65it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0305. T_Loss: 4.0026. Mask: 0.9211. :  74%|███████▍  | 37/50 [00:19<00:07,  1.65it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0305. T_Loss: 4.0026. Mask: 0.9211. :  76%|███████▌  | 38/50 [00:19<00:06,  1.90it/s]total : 1000  current step :  436
total : 1000  current step :  437
total : 1000  current step :  438
Train Iter: 439/1000. LR: 0.0274. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0311. T_Loss: 4.0068. Mask: 0.9203. :  76%|███████▌  | 38/50 [00:20<00:06,  1.90it/s]Train Iter: 439/1000. LR: 0.0274. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0311. T_Loss: 4.0068. Mask: 0.9203. :  78%|███████▊  | 39/50 [00:20<00:07,  1.52it/s]total : 1000  current step :  439
Train Iter: 440/1000. LR: 0.0275. Data: 0.29s. Batch: 0.54s. S_Loss: 1.0316. T_Loss: 4.0181. Mask: 0.9204. :  78%|███████▊  | 39/50 [00:21<00:07,  1.52it/s]Train Iter: 440/1000. LR: 0.0275. Data: 0.29s. Batch: 0.54s. S_Loss: 1.0316. T_Loss: 4.0181. Mask: 0.9204. :  80%|████████  | 40/50 [00:21<00:06,  1.45it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.29s. Batch: 0.54s. S_Loss: 1.0328. T_Loss: 4.0329. Mask: 0.9197. :  80%|████████  | 40/50 [00:22<00:06,  1.45it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.29s. Batch: 0.54s. S_Loss: 1.0328. T_Loss: 4.0329. Mask: 0.9197. :  82%|████████▏ | 41/50 [00:22<00:06,  1.45it/s]total : 1000  current step :  440
total : 1000  current step :  441
Train Iter: 442/1000. LR: 0.0276. Data: 0.30s. Batch: 0.56s. S_Loss: 1.0331. T_Loss: 4.0344. Mask: 0.9192. :  82%|████████▏ | 41/50 [00:23<00:06,  1.45it/s]Train Iter: 442/1000. LR: 0.0276. Data: 0.30s. Batch: 0.56s. S_Loss: 1.0331. T_Loss: 4.0344. Mask: 0.9192. :  84%|████████▍ | 42/50 [00:23<00:06,  1.24it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0320. T_Loss: 4.0353. Mask: 0.9192. :  84%|████████▍ | 42/50 [00:23<00:06,  1.24it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0320. T_Loss: 4.0353. Mask: 0.9192. :  86%|████████▌ | 43/50 [00:23<00:04,  1.47it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0315. T_Loss: 4.0435. Mask: 0.9201. :  86%|████████▌ | 43/50 [00:24<00:04,  1.47it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0315. T_Loss: 4.0435. Mask: 0.9201. :  88%|████████▊ | 44/50 [00:24<00:03,  1.60it/s]total : 1000  current step :  442
total : 1000  current step :  443
total : 1000  current step :  444
Train Iter: 445/1000. LR: 0.0278. Data: 0.31s. Batch: 0.57s. S_Loss: 1.0316. T_Loss: 4.0528. Mask: 0.9207. :  88%|████████▊ | 44/50 [00:25<00:03,  1.60it/s]Train Iter: 445/1000. LR: 0.0278. Data: 0.31s. Batch: 0.57s. S_Loss: 1.0316. T_Loss: 4.0528. Mask: 0.9207. :  90%|█████████ | 45/50 [00:25<00:04,  1.22it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.30s. Batch: 0.56s. S_Loss: 1.0310. T_Loss: 4.0520. Mask: 0.9209. :  90%|█████████ | 45/50 [00:25<00:04,  1.22it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.30s. Batch: 0.56s. S_Loss: 1.0310. T_Loss: 4.0520. Mask: 0.9209. :  92%|█████████▏| 46/50 [00:25<00:02,  1.60it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0301. T_Loss: 4.0509. Mask: 0.9214. :  92%|█████████▏| 46/50 [00:25<00:02,  1.60it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0301. T_Loss: 4.0509. Mask: 0.9214. :  94%|█████████▍| 47/50 [00:25<00:01,  2.07it/s]total : 1000  current step :  445
total : 1000  current step :  446
total : 1000  current step :  447
Train Iter: 448/1000. LR: 0.0280. Data: 0.31s. Batch: 0.56s. S_Loss: 1.0306. T_Loss: 4.0612. Mask: 0.9220. :  94%|█████████▍| 47/50 [00:26<00:01,  2.07it/s]Train Iter: 448/1000. LR: 0.0280. Data: 0.31s. Batch: 0.56s. S_Loss: 1.0306. T_Loss: 4.0612. Mask: 0.9220. :  96%|█████████▌| 48/50 [00:26<00:01,  1.58it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0298. T_Loss: 4.0616. Mask: 0.9221. :  96%|█████████▌| 48/50 [00:27<00:01,  1.58it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0298. T_Loss: 4.0616. Mask: 0.9221. :  98%|█████████▊| 49/50 [00:27<00:00,  1.91it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0294. T_Loss: 4.0676. Mask: 0.9226. :  98%|█████████▊| 49/50 [00:27<00:00,  1.91it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0294. T_Loss: 4.0676. Mask: 0.9226. : 100%|██████████| 50/50 [00:27<00:00,  2.27it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.30s. Batch: 0.55s. S_Loss: 1.0294. T_Loss: 4.0676. Mask: 0.9226. : 100%|██████████| 50/50 [00:27<00:00,  1.82it/s]
total : 1000  current step :  448
total : 1000  current step :  449
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.2703. top1: 80.08. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.2703. top1: 80.08. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.2352. top1: 82.03. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.2352. top1: 82.03. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.2343. top1: 82.68. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.2343. top1: 82.68. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.96it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.2409. top1: 82.13. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.96it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.2409. top1: 82.13. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.15it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.2861. top1: 78.91. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.15it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.2861. top1: 78.91. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.3088. top1: 77.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.3088. top1: 77.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.3319. top1: 76.12. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.3319. top1: 76.12. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.3463. top1: 74.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.3463. top1: 74.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.3463. top1: 74.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.23it/s]
total : 1000  current step :  450
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.81s. Batch: 1.03s. S_Loss: 1.0157. T_Loss: 4.2786. Mask: 0.9219. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.81s. Batch: 1.03s. S_Loss: 1.0157. T_Loss: 4.2786. Mask: 0.9219. :   2%|▏         | 1/50 [00:01<00:50,  1.03s/it]Train Iter: 452/1000. LR: 0.0282. Data: 0.46s. Batch: 0.70s. S_Loss: 1.0433. T_Loss: 4.4193. Mask: 0.9238. :   2%|▏         | 1/50 [00:01<00:50,  1.03s/it]Train Iter: 452/1000. LR: 0.0282. Data: 0.46s. Batch: 0.70s. S_Loss: 1.0433. T_Loss: 4.4193. Mask: 0.9238. :   4%|▍         | 2/50 [00:01<00:31,  1.54it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.31s. Batch: 0.53s. S_Loss: 1.0445. T_Loss: 4.6481. Mask: 0.9232. :   4%|▍         | 2/50 [00:01<00:31,  1.54it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.31s. Batch: 0.53s. S_Loss: 1.0445. T_Loss: 4.6481. Mask: 0.9232. :   6%|▌         | 3/50 [00:01<00:20,  2.33it/s]total : 1000  current step :  451
total : 1000  current step :  452
total : 1000  current step :  453
Train Iter: 454/1000. LR: 0.0284. Data: 0.40s. Batch: 0.61s. S_Loss: 1.0579. T_Loss: 4.6254. Mask: 0.9121. :   6%|▌         | 3/50 [00:02<00:20,  2.33it/s]Train Iter: 454/1000. LR: 0.0284. Data: 0.40s. Batch: 0.61s. S_Loss: 1.0579. T_Loss: 4.6254. Mask: 0.9121. :   8%|▊         | 4/50 [00:02<00:27,  1.67it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0550. T_Loss: 4.6529. Mask: 0.9125. :   8%|▊         | 4/50 [00:02<00:27,  1.67it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0550. T_Loss: 4.6529. Mask: 0.9125. :  10%|█         | 5/50 [00:02<00:21,  2.06it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.28s. Batch: 0.49s. S_Loss: 1.0494. T_Loss: 4.6304. Mask: 0.9017. :  10%|█         | 5/50 [00:02<00:21,  2.06it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.28s. Batch: 0.49s. S_Loss: 1.0494. T_Loss: 4.6304. Mask: 0.9017. :  12%|█▏        | 6/50 [00:02<00:17,  2.52it/s]total : 1000  current step :  454
total : 1000  current step :  455
total : 1000  current step :  456
Train Iter: 457/1000. LR: 0.0286. Data: 0.33s. Batch: 0.55s. S_Loss: 1.0432. T_Loss: 4.6467. Mask: 0.9029. :  12%|█▏        | 6/50 [00:03<00:17,  2.52it/s]Train Iter: 457/1000. LR: 0.0286. Data: 0.33s. Batch: 0.55s. S_Loss: 1.0432. T_Loss: 4.6467. Mask: 0.9029. :  14%|█▍        | 7/50 [00:03<00:23,  1.79it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.30s. Batch: 0.51s. S_Loss: 1.0438. T_Loss: 4.6285. Mask: 0.9019. :  14%|█▍        | 7/50 [00:04<00:23,  1.79it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.30s. Batch: 0.51s. S_Loss: 1.0438. T_Loss: 4.6285. Mask: 0.9019. :  16%|█▌        | 8/50 [00:04<00:19,  2.16it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.28s. Batch: 0.48s. S_Loss: 1.0464. T_Loss: 4.5959. Mask: 0.9028. :  16%|█▌        | 8/50 [00:04<00:19,  2.16it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.28s. Batch: 0.48s. S_Loss: 1.0464. T_Loss: 4.5959. Mask: 0.9028. :  18%|█▊        | 9/50 [00:04<00:16,  2.47it/s]total : 1000  current step :  457
total : 1000  current step :  458
total : 1000  current step :  459
Train Iter: 460/1000. LR: 0.0287. Data: 0.32s. Batch: 0.52s. S_Loss: 1.0467. T_Loss: 4.5888. Mask: 0.9086. :  18%|█▊        | 9/50 [00:05<00:16,  2.47it/s]Train Iter: 460/1000. LR: 0.0287. Data: 0.32s. Batch: 0.52s. S_Loss: 1.0467. T_Loss: 4.5888. Mask: 0.9086. :  20%|██        | 10/50 [00:05<00:21,  1.90it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.30s. Batch: 0.49s. S_Loss: 1.0489. T_Loss: 4.5922. Mask: 0.9091. :  20%|██        | 10/50 [00:05<00:21,  1.90it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.30s. Batch: 0.49s. S_Loss: 1.0489. T_Loss: 4.5922. Mask: 0.9091. :  22%|██▏       | 11/50 [00:05<00:17,  2.21it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.28s. Batch: 0.49s. S_Loss: 1.0489. T_Loss: 4.5750. Mask: 0.9108. :  22%|██▏       | 11/50 [00:05<00:17,  2.21it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.28s. Batch: 0.49s. S_Loss: 1.0489. T_Loss: 4.5750. Mask: 0.9108. :  24%|██▍       | 12/50 [00:05<00:17,  2.21it/s]total : 1000  current step :  460
total : 1000  current step :  461
total : 1000  current step :  462
Train Iter: 463/1000. LR: 0.0289. Data: 0.32s. Batch: 0.52s. S_Loss: 1.0527. T_Loss: 4.5597. Mask: 0.9096. :  24%|██▍       | 12/50 [00:06<00:17,  2.21it/s]Train Iter: 463/1000. LR: 0.0289. Data: 0.32s. Batch: 0.52s. S_Loss: 1.0527. T_Loss: 4.5597. Mask: 0.9096. :  26%|██▌       | 13/50 [00:06<00:21,  1.70it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.30s. Batch: 0.51s. S_Loss: 1.0543. T_Loss: 4.5421. Mask: 0.9124. :  26%|██▌       | 13/50 [00:07<00:21,  1.70it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.30s. Batch: 0.51s. S_Loss: 1.0543. T_Loss: 4.5421. Mask: 0.9124. :  28%|██▊       | 14/50 [00:07<00:18,  1.92it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.29s. Batch: 0.50s. S_Loss: 1.0534. T_Loss: 4.5281. Mask: 0.9122. :  28%|██▊       | 14/50 [00:07<00:18,  1.92it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.29s. Batch: 0.50s. S_Loss: 1.0534. T_Loss: 4.5281. Mask: 0.9122. :  30%|███       | 15/50 [00:07<00:16,  2.08it/s]total : 1000  current step :  463
total : 1000  current step :  464
total : 1000  current step :  465
Train Iter: 466/1000. LR: 0.0291. Data: 0.32s. Batch: 0.55s. S_Loss: 1.0526. T_Loss: 4.5485. Mask: 0.9143. :  30%|███       | 15/50 [00:08<00:16,  2.08it/s]Train Iter: 466/1000. LR: 0.0291. Data: 0.32s. Batch: 0.55s. S_Loss: 1.0526. T_Loss: 4.5485. Mask: 0.9143. :  32%|███▏      | 16/50 [00:08<00:23,  1.43it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.30s. Batch: 0.53s. S_Loss: 1.0523. T_Loss: 4.5735. Mask: 0.9154. :  32%|███▏      | 16/50 [00:09<00:23,  1.43it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.30s. Batch: 0.53s. S_Loss: 1.0523. T_Loss: 4.5735. Mask: 0.9154. :  34%|███▍      | 17/50 [00:09<00:19,  1.70it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.29s. Batch: 0.52s. S_Loss: 1.0519. T_Loss: 4.5749. Mask: 0.9151. :  34%|███▍      | 17/50 [00:09<00:19,  1.70it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.29s. Batch: 0.52s. S_Loss: 1.0519. T_Loss: 4.5749. Mask: 0.9151. :  36%|███▌      | 18/50 [00:09<00:15,  2.04it/s]total : 1000  current step :  466
total : 1000  current step :  467
total : 1000  current step :  468
Train Iter: 469/1000. LR: 0.0293. Data: 0.31s. Batch: 0.54s. S_Loss: 1.0514. T_Loss: 4.5782. Mask: 0.9143. :  36%|███▌      | 18/50 [00:10<00:15,  2.04it/s]Train Iter: 469/1000. LR: 0.0293. Data: 0.31s. Batch: 0.54s. S_Loss: 1.0514. T_Loss: 4.5782. Mask: 0.9143. :  38%|███▊      | 19/50 [00:10<00:19,  1.61it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0503. T_Loss: 4.5876. Mask: 0.9146. :  38%|███▊      | 19/50 [00:10<00:19,  1.61it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0503. T_Loss: 4.5876. Mask: 0.9146. :  40%|████      | 20/50 [00:10<00:15,  1.90it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.28s. Batch: 0.52s. S_Loss: 1.0497. T_Loss: 4.5812. Mask: 0.9150. :  40%|████      | 20/50 [00:10<00:15,  1.90it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.28s. Batch: 0.52s. S_Loss: 1.0497. T_Loss: 4.5812. Mask: 0.9150. :  42%|████▏     | 21/50 [00:10<00:13,  2.19it/s]total : 1000  current step :  469
total : 1000  current step :  470
total : 1000  current step :  471
Train Iter: 472/1000. LR: 0.0295. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0480. T_Loss: 4.5797. Mask: 0.9158. :  42%|████▏     | 21/50 [00:11<00:13,  2.19it/s]Train Iter: 472/1000. LR: 0.0295. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0480. T_Loss: 4.5797. Mask: 0.9158. :  44%|████▍     | 22/50 [00:11<00:16,  1.69it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0462. T_Loss: 4.5681. Mask: 0.9159. :  44%|████▍     | 22/50 [00:12<00:16,  1.69it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0462. T_Loss: 4.5681. Mask: 0.9159. :  46%|████▌     | 23/50 [00:12<00:15,  1.77it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0439. T_Loss: 4.5388. Mask: 0.9165. :  46%|████▌     | 23/50 [00:12<00:15,  1.77it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0439. T_Loss: 4.5388. Mask: 0.9165. :  48%|████▊     | 24/50 [00:12<00:12,  2.12it/s]total : 1000  current step :  472
total : 1000  current step :  473
total : 1000  current step :  474
Train Iter: 475/1000. LR: 0.0297. Data: 0.28s. Batch: 0.54s. S_Loss: 1.0448. T_Loss: 4.5372. Mask: 0.9166. :  48%|████▊     | 24/50 [00:13<00:12,  2.12it/s]Train Iter: 475/1000. LR: 0.0297. Data: 0.28s. Batch: 0.54s. S_Loss: 1.0448. T_Loss: 4.5372. Mask: 0.9166. :  50%|█████     | 25/50 [00:13<00:14,  1.70it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0437. T_Loss: 4.5496. Mask: 0.9181. :  50%|█████     | 25/50 [00:13<00:14,  1.70it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0437. T_Loss: 4.5496. Mask: 0.9181. :  52%|█████▏    | 26/50 [00:13<00:12,  1.94it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0422. T_Loss: 4.5416. Mask: 0.9184. :  52%|█████▏    | 26/50 [00:14<00:12,  1.94it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0422. T_Loss: 4.5416. Mask: 0.9184. :  54%|█████▍    | 27/50 [00:14<00:10,  2.16it/s]total : 1000  current step :  475
total : 1000  current step :  476
total : 1000  current step :  477
Train Iter: 478/1000. LR: 0.0299. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0422. T_Loss: 4.5475. Mask: 0.9187. :  54%|█████▍    | 27/50 [00:14<00:10,  2.16it/s]Train Iter: 478/1000. LR: 0.0299. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0422. T_Loss: 4.5475. Mask: 0.9187. :  56%|█████▌    | 28/50 [00:14<00:12,  1.82it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0421. T_Loss: 4.5405. Mask: 0.9178. :  56%|█████▌    | 28/50 [00:15<00:12,  1.82it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0421. T_Loss: 4.5405. Mask: 0.9178. :  58%|█████▊    | 29/50 [00:15<00:09,  2.12it/s]total : 1000  current step :  478
total : 1000  current step :  479
Train Iter: 480/1000. LR: 0.0300. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0420. T_Loss: 4.5292. Mask: 0.9173. :  58%|█████▊    | 29/50 [00:15<00:09,  2.12it/s]Train Iter: 480/1000. LR: 0.0300. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0420. T_Loss: 4.5292. Mask: 0.9173. :  60%|██████    | 30/50 [00:15<00:09,  2.01it/s]total : 1000  current step :  480
Train Iter: 481/1000. LR: 0.0301. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0424. T_Loss: 4.5396. Mask: 0.9181. :  60%|██████    | 30/50 [00:16<00:09,  2.01it/s]Train Iter: 481/1000. LR: 0.0301. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0424. T_Loss: 4.5396. Mask: 0.9181. :  62%|██████▏   | 31/50 [00:16<00:11,  1.67it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0436. T_Loss: 4.5534. Mask: 0.9183. :  62%|██████▏   | 31/50 [00:16<00:11,  1.67it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.27s. Batch: 0.52s. S_Loss: 1.0436. T_Loss: 4.5534. Mask: 0.9183. :  64%|██████▍   | 32/50 [00:16<00:09,  1.99it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0430. T_Loss: 4.5459. Mask: 0.9186. :  64%|██████▍   | 32/50 [00:17<00:09,  1.99it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0430. T_Loss: 4.5459. Mask: 0.9186. :  66%|██████▌   | 33/50 [00:17<00:08,  2.09it/s]total : 1000  current step :  481
total : 1000  current step :  482
total : 1000  current step :  483
Train Iter: 484/1000. LR: 0.0302. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0429. T_Loss: 4.5484. Mask: 0.9191. :  66%|██████▌   | 33/50 [00:18<00:08,  2.09it/s]Train Iter: 484/1000. LR: 0.0302. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0429. T_Loss: 4.5484. Mask: 0.9191. :  68%|██████▊   | 34/50 [00:18<00:09,  1.60it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0429. T_Loss: 4.5564. Mask: 0.9194. :  68%|██████▊   | 34/50 [00:18<00:09,  1.60it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0429. T_Loss: 4.5564. Mask: 0.9194. :  70%|███████   | 35/50 [00:18<00:07,  1.92it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0439. T_Loss: 4.5642. Mask: 0.9195. :  70%|███████   | 35/50 [00:18<00:07,  1.92it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0439. T_Loss: 4.5642. Mask: 0.9195. :  72%|███████▏  | 36/50 [00:18<00:06,  2.29it/s]total : 1000  current step :  484
total : 1000  current step :  485
total : 1000  current step :  486
Train Iter: 487/1000. LR: 0.0304. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0434. T_Loss: 4.5629. Mask: 0.9191. :  72%|███████▏  | 36/50 [00:19<00:06,  2.29it/s]Train Iter: 487/1000. LR: 0.0304. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0434. T_Loss: 4.5629. Mask: 0.9191. :  74%|███████▍  | 37/50 [00:19<00:07,  1.70it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0428. T_Loss: 4.5620. Mask: 0.9193. :  74%|███████▍  | 37/50 [00:20<00:07,  1.70it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0428. T_Loss: 4.5620. Mask: 0.9193. :  76%|███████▌  | 38/50 [00:20<00:06,  1.93it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0434. T_Loss: 4.5704. Mask: 0.9202. :  76%|███████▌  | 38/50 [00:20<00:06,  1.93it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0434. T_Loss: 4.5704. Mask: 0.9202. :  78%|███████▊  | 39/50 [00:20<00:04,  2.28it/s]total : 1000  current step :  487
total : 1000  current step :  488
total : 1000  current step :  489
Train Iter: 490/1000. LR: 0.0306. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0428. T_Loss: 4.5787. Mask: 0.9202. :  78%|███████▊  | 39/50 [00:21<00:04,  2.28it/s]Train Iter: 490/1000. LR: 0.0306. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0428. T_Loss: 4.5787. Mask: 0.9202. :  80%|████████  | 40/50 [00:21<00:05,  1.74it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0416. T_Loss: 4.5804. Mask: 0.9200. :  80%|████████  | 40/50 [00:21<00:05,  1.74it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0416. T_Loss: 4.5804. Mask: 0.9200. :  82%|████████▏ | 41/50 [00:21<00:04,  1.95it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0409. T_Loss: 4.5852. Mask: 0.9199. :  82%|████████▏ | 41/50 [00:21<00:04,  1.95it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0409. T_Loss: 4.5852. Mask: 0.9199. :  84%|████████▍ | 42/50 [00:21<00:03,  2.41it/s]total : 1000  current step :  490
total : 1000  current step :  491
total : 1000  current step :  492
Train Iter: 493/1000. LR: 0.0308. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0397. T_Loss: 4.5819. Mask: 0.9203. :  84%|████████▍ | 42/50 [00:22<00:03,  2.41it/s]Train Iter: 493/1000. LR: 0.0308. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0397. T_Loss: 4.5819. Mask: 0.9203. :  86%|████████▌ | 43/50 [00:22<00:03,  1.83it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0391. T_Loss: 4.5794. Mask: 0.9203. :  86%|████████▌ | 43/50 [00:23<00:03,  1.83it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0391. T_Loss: 4.5794. Mask: 0.9203. :  88%|████████▊ | 44/50 [00:23<00:03,  1.79it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0395. T_Loss: 4.5848. Mask: 0.9210. :  88%|████████▊ | 44/50 [00:23<00:03,  1.79it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0395. T_Loss: 4.5848. Mask: 0.9210. :  90%|█████████ | 45/50 [00:23<00:02,  2.09it/s]total : 1000  current step :  493
total : 1000  current step :  494
total : 1000  current step :  495
Train Iter: 496/1000. LR: 0.0310. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0380. T_Loss: 4.5763. Mask: 0.9210. :  90%|█████████ | 45/50 [00:24<00:02,  2.09it/s]Train Iter: 496/1000. LR: 0.0310. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0380. T_Loss: 4.5763. Mask: 0.9210. :  92%|█████████▏| 46/50 [00:24<00:02,  1.53it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0368. T_Loss: 4.5677. Mask: 0.9205. :  92%|█████████▏| 46/50 [00:24<00:02,  1.53it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0368. T_Loss: 4.5677. Mask: 0.9205. :  94%|█████████▍| 47/50 [00:24<00:01,  1.77it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0359. T_Loss: 4.5594. Mask: 0.9211. :  94%|█████████▍| 47/50 [00:25<00:01,  1.77it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0359. T_Loss: 4.5594. Mask: 0.9211. :  96%|█████████▌| 48/50 [00:25<00:00,  2.13it/s]total : 1000  current step :  496
total : 1000  current step :  497
total : 1000  current step :  498
Train Iter: 499/1000. LR: 0.0312. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0357. T_Loss: 4.5565. Mask: 0.9216. :  96%|█████████▌| 48/50 [00:26<00:00,  2.13it/s]Train Iter: 499/1000. LR: 0.0312. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0357. T_Loss: 4.5565. Mask: 0.9216. :  98%|█████████▊| 49/50 [00:26<00:00,  1.58it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0356. T_Loss: 4.5590. Mask: 0.9220. :  98%|█████████▊| 49/50 [00:26<00:00,  1.58it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0356. T_Loss: 4.5590. Mask: 0.9220. : 100%|██████████| 50/50 [00:26<00:00,  1.84it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0356. T_Loss: 4.5590. Mask: 0.9220. : 100%|██████████| 50/50 [00:26<00:00,  1.89it/s]
total : 1000  current step :  499
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.1943. top1: 76.17. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.1943. top1: 76.17. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.35it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1579. top1: 79.69. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.35it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1579. top1: 79.69. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1564. top1: 80.21. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1564. top1: 80.21. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.86it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1629. top1: 79.69. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.86it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1629. top1: 79.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1955. top1: 77.81. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1955. top1: 77.81. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  4.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.2069. top1: 77.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  4.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.2069. top1: 77.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.2231. top1: 76.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.2231. top1: 76.73. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.2345. top1: 76.00. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.2345. top1: 76.00. top5: 100.00. : 100%|██████████| 8/8 [00:01<00:00,  4.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.2345. top1: 76.00. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.89it/s]
total : 1000  current step :  500
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.59s. S_Loss: 0.9702. T_Loss: 4.5190. Mask: 0.9727. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.59s. S_Loss: 0.9702. T_Loss: 4.5190. Mask: 0.9727. :   2%|▏         | 1/50 [00:00<00:28,  1.69it/s]total : 1000  current step :  501
Train Iter: 502/1000. LR: 0.0314. Data: 0.30s. Batch: 0.70s. S_Loss: 0.9756. T_Loss: 4.4181. Mask: 0.9531. :   2%|▏         | 1/50 [00:01<00:28,  1.69it/s]Train Iter: 502/1000. LR: 0.0314. Data: 0.30s. Batch: 0.70s. S_Loss: 0.9756. T_Loss: 4.4181. Mask: 0.9531. :   4%|▍         | 2/50 [00:01<00:34,  1.38it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.22s. Batch: 0.59s. S_Loss: 0.9862. T_Loss: 4.3625. Mask: 0.9427. :   4%|▍         | 2/50 [00:01<00:34,  1.38it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.22s. Batch: 0.59s. S_Loss: 0.9862. T_Loss: 4.3625. Mask: 0.9427. :   6%|▌         | 3/50 [00:01<00:26,  1.79it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.17s. Batch: 0.50s. S_Loss: 0.9889. T_Loss: 4.4260. Mask: 0.9385. :   6%|▌         | 3/50 [00:02<00:26,  1.79it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.17s. Batch: 0.50s. S_Loss: 0.9889. T_Loss: 4.4260. Mask: 0.9385. :   8%|▊         | 4/50 [00:02<00:20,  2.30it/s]total : 1000  current step :  502
total : 1000  current step :  503
total : 1000  current step :  504
Train Iter: 505/1000. LR: 0.0316. Data: 0.25s. Batch: 0.55s. S_Loss: 1.0022. T_Loss: 4.5185. Mask: 0.9383. :   8%|▊         | 4/50 [00:02<00:20,  2.30it/s]Train Iter: 505/1000. LR: 0.0316. Data: 0.25s. Batch: 0.55s. S_Loss: 1.0022. T_Loss: 4.5185. Mask: 0.9383. :  10%|█         | 5/50 [00:02<00:24,  1.81it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0089. T_Loss: 4.5512. Mask: 0.9316. :  10%|█         | 5/50 [00:03<00:24,  1.81it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0089. T_Loss: 4.5512. Mask: 0.9316. :  12%|█▏        | 6/50 [00:03<00:19,  2.24it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.20s. Batch: 0.46s. S_Loss: 1.0127. T_Loss: 4.6321. Mask: 0.9302. :  12%|█▏        | 6/50 [00:03<00:19,  2.24it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.20s. Batch: 0.46s. S_Loss: 1.0127. T_Loss: 4.6321. Mask: 0.9302. :  14%|█▍        | 7/50 [00:03<00:16,  2.65it/s]total : 1000  current step :  505
total : 1000  current step :  506
total : 1000  current step :  507
Train Iter: 508/1000. LR: 0.0318. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0142. T_Loss: 4.6546. Mask: 0.9292. :  14%|█▍        | 7/50 [00:03<00:16,  2.65it/s]Train Iter: 508/1000. LR: 0.0318. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0142. T_Loss: 4.6546. Mask: 0.9292. :  16%|█▌        | 8/50 [00:03<00:19,  2.11it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.22s. Batch: 0.47s. S_Loss: 1.0136. T_Loss: 4.6733. Mask: 0.9266. :  16%|█▌        | 8/50 [00:04<00:19,  2.11it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.22s. Batch: 0.47s. S_Loss: 1.0136. T_Loss: 4.6733. Mask: 0.9266. :  18%|█▊        | 9/50 [00:04<00:17,  2.38it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.20s. Batch: 0.45s. S_Loss: 1.0144. T_Loss: 4.6943. Mask: 0.9277. :  18%|█▊        | 9/50 [00:04<00:17,  2.38it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.20s. Batch: 0.45s. S_Loss: 1.0144. T_Loss: 4.6943. Mask: 0.9277. :  20%|██        | 10/50 [00:04<00:15,  2.65it/s]total : 1000  current step :  508
total : 1000  current step :  509
total : 1000  current step :  510
Train Iter: 511/1000. LR: 0.0319. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0105. T_Loss: 4.6903. Mask: 0.9276. :  20%|██        | 10/50 [00:05<00:15,  2.65it/s]Train Iter: 511/1000. LR: 0.0319. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0105. T_Loss: 4.6903. Mask: 0.9276. :  22%|██▏       | 11/50 [00:05<00:21,  1.81it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.22s. Batch: 0.48s. S_Loss: 1.0116. T_Loss: 4.6947. Mask: 0.9277. :  22%|██▏       | 11/50 [00:05<00:21,  1.81it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.22s. Batch: 0.48s. S_Loss: 1.0116. T_Loss: 4.6947. Mask: 0.9277. :  24%|██▍       | 12/50 [00:05<00:18,  2.10it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.21s. Batch: 0.46s. S_Loss: 1.0175. T_Loss: 4.7137. Mask: 0.9288. :  24%|██▍       | 12/50 [00:05<00:18,  2.10it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.21s. Batch: 0.46s. S_Loss: 1.0175. T_Loss: 4.7137. Mask: 0.9288. :  26%|██▌       | 13/50 [00:05<00:14,  2.59it/s]total : 1000  current step :  511
total : 1000  current step :  512
total : 1000  current step :  513
Train Iter: 514/1000. LR: 0.0321. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0197. T_Loss: 4.7320. Mask: 0.9286. :  26%|██▌       | 13/50 [00:06<00:14,  2.59it/s]Train Iter: 514/1000. LR: 0.0321. Data: 0.24s. Batch: 0.49s. S_Loss: 1.0197. T_Loss: 4.7320. Mask: 0.9286. :  28%|██▊       | 14/50 [00:06<00:19,  1.80it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0180. T_Loss: 4.7204. Mask: 0.9292. :  28%|██▊       | 14/50 [00:07<00:19,  1.80it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0180. T_Loss: 4.7204. Mask: 0.9292. :  30%|███       | 15/50 [00:07<00:17,  1.95it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.21s. Batch: 0.48s. S_Loss: 1.0174. T_Loss: 4.7101. Mask: 0.9294. :  30%|███       | 15/50 [00:07<00:17,  1.95it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.21s. Batch: 0.48s. S_Loss: 1.0174. T_Loss: 4.7101. Mask: 0.9294. :  32%|███▏      | 16/50 [00:07<00:15,  2.18it/s]total : 1000  current step :  514
total : 1000  current step :  515
total : 1000  current step :  516
Train Iter: 517/1000. LR: 0.0323. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0166. T_Loss: 4.7045. Mask: 0.9313. :  32%|███▏      | 16/50 [00:08<00:15,  2.18it/s]Train Iter: 517/1000. LR: 0.0323. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0166. T_Loss: 4.7045. Mask: 0.9313. :  34%|███▍      | 17/50 [00:08<00:19,  1.65it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0150. T_Loss: 4.6685. Mask: 0.9306. :  34%|███▍      | 17/50 [00:08<00:19,  1.65it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0150. T_Loss: 4.6685. Mask: 0.9306. :  36%|███▌      | 18/50 [00:08<00:16,  1.97it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.21s. Batch: 0.48s. S_Loss: 1.0125. T_Loss: 4.6397. Mask: 0.9319. :  36%|███▌      | 18/50 [00:09<00:16,  1.97it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.21s. Batch: 0.48s. S_Loss: 1.0125. T_Loss: 4.6397. Mask: 0.9319. :  38%|███▊      | 19/50 [00:09<00:13,  2.27it/s]total : 1000  current step :  517
total : 1000  current step :  518
total : 1000  current step :  519
Train Iter: 520/1000. LR: 0.0325. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0088. T_Loss: 4.5974. Mask: 0.9334. :  38%|███▊      | 19/50 [00:10<00:13,  2.27it/s]Train Iter: 520/1000. LR: 0.0325. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0088. T_Loss: 4.5974. Mask: 0.9334. :  40%|████      | 20/50 [00:10<00:18,  1.58it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0086. T_Loss: 4.5781. Mask: 0.9330. :  40%|████      | 20/50 [00:10<00:18,  1.58it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0086. T_Loss: 4.5781. Mask: 0.9330. :  42%|████▏     | 21/50 [00:10<00:19,  1.51it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0055. T_Loss: 4.5571. Mask: 0.9347. :  42%|████▏     | 21/50 [00:11<00:19,  1.51it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0055. T_Loss: 4.5571. Mask: 0.9347. :  44%|████▍     | 22/50 [00:11<00:15,  1.80it/s]total : 1000  current step :  520
total : 1000  current step :  521
total : 1000  current step :  522
Train Iter: 523/1000. LR: 0.0327. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0040. T_Loss: 4.5434. Mask: 0.9339. :  44%|████▍     | 22/50 [00:12<00:15,  1.80it/s]Train Iter: 523/1000. LR: 0.0327. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0040. T_Loss: 4.5434. Mask: 0.9339. :  46%|████▌     | 23/50 [00:12<00:17,  1.52it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0039. T_Loss: 4.5351. Mask: 0.9338. :  46%|████▌     | 23/50 [00:12<00:17,  1.52it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0039. T_Loss: 4.5351. Mask: 0.9338. :  48%|████▊     | 24/50 [00:12<00:15,  1.69it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0034. T_Loss: 4.5397. Mask: 0.9339. :  48%|████▊     | 24/50 [00:12<00:15,  1.69it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0034. T_Loss: 4.5397. Mask: 0.9339. :  50%|█████     | 25/50 [00:12<00:11,  2.09it/s]total : 1000  current step :  523
total : 1000  current step :  524
total : 1000  current step :  525
Train Iter: 526/1000. LR: 0.0329. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0038. T_Loss: 4.5512. Mask: 0.9337. :  50%|█████     | 25/50 [00:13<00:11,  2.09it/s]Train Iter: 526/1000. LR: 0.0329. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0038. T_Loss: 4.5512. Mask: 0.9337. :  52%|█████▏    | 26/50 [00:13<00:14,  1.68it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0044. T_Loss: 4.5666. Mask: 0.9326. :  52%|█████▏    | 26/50 [00:14<00:14,  1.68it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0044. T_Loss: 4.5666. Mask: 0.9326. :  54%|█████▍    | 27/50 [00:14<00:11,  1.93it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0050. T_Loss: 4.5911. Mask: 0.9321. :  54%|█████▍    | 27/50 [00:14<00:11,  1.93it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0050. T_Loss: 4.5911. Mask: 0.9321. :  56%|█████▌    | 28/50 [00:14<00:10,  2.17it/s]total : 1000  current step :  526
total : 1000  current step :  527
total : 1000  current step :  528
Train Iter: 529/1000. LR: 0.0331. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0046. T_Loss: 4.6113. Mask: 0.9306. :  56%|█████▌    | 28/50 [00:15<00:10,  2.17it/s]Train Iter: 529/1000. LR: 0.0331. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0046. T_Loss: 4.6113. Mask: 0.9306. :  58%|█████▊    | 29/50 [00:15<00:13,  1.56it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0054. T_Loss: 4.6351. Mask: 0.9296. :  58%|█████▊    | 29/50 [00:15<00:13,  1.56it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0054. T_Loss: 4.6351. Mask: 0.9296. :  60%|██████    | 30/50 [00:15<00:10,  1.90it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0069. T_Loss: 4.6566. Mask: 0.9282. :  60%|██████    | 30/50 [00:16<00:10,  1.90it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0069. T_Loss: 4.6566. Mask: 0.9282. :  62%|██████▏   | 31/50 [00:16<00:08,  2.14it/s]total : 1000  current step :  529
total : 1000  current step :  530
total : 1000  current step :  531
Train Iter: 532/1000. LR: 0.0333. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0076. T_Loss: 4.6698. Mask: 0.9276. :  62%|██████▏   | 31/50 [00:17<00:08,  2.14it/s]Train Iter: 532/1000. LR: 0.0333. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0076. T_Loss: 4.6698. Mask: 0.9276. :  64%|██████▍   | 32/50 [00:17<00:11,  1.59it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0096. T_Loss: 4.6794. Mask: 0.9265. :  64%|██████▍   | 32/50 [00:17<00:11,  1.59it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0096. T_Loss: 4.6794. Mask: 0.9265. :  66%|██████▌   | 33/50 [00:17<00:10,  1.68it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0111. T_Loss: 4.6749. Mask: 0.9254. :  66%|██████▌   | 33/50 [00:17<00:10,  1.68it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0111. T_Loss: 4.6749. Mask: 0.9254. :  68%|██████▊   | 34/50 [00:17<00:08,  1.84it/s]total : 1000  current step :  532
total : 1000  current step :  533
total : 1000  current step :  534
Train Iter: 535/1000. LR: 0.0334. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0108. T_Loss: 4.6753. Mask: 0.9259. :  68%|██████▊   | 34/50 [00:18<00:08,  1.84it/s]Train Iter: 535/1000. LR: 0.0334. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0108. T_Loss: 4.6753. Mask: 0.9259. :  70%|███████   | 35/50 [00:18<00:09,  1.52it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0124. T_Loss: 4.6718. Mask: 0.9252. :  70%|███████   | 35/50 [00:19<00:09,  1.52it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0124. T_Loss: 4.6718. Mask: 0.9252. :  72%|███████▏  | 36/50 [00:19<00:07,  1.82it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0136. T_Loss: 4.6716. Mask: 0.9250. :  72%|███████▏  | 36/50 [00:19<00:07,  1.82it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0136. T_Loss: 4.6716. Mask: 0.9250. :  74%|███████▍  | 37/50 [00:19<00:06,  2.05it/s]total : 1000  current step :  535
total : 1000  current step :  536
total : 1000  current step :  537
Train Iter: 538/1000. LR: 0.0336. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0140. T_Loss: 4.6577. Mask: 0.9250. :  74%|███████▍  | 37/50 [00:20<00:06,  2.05it/s]Train Iter: 538/1000. LR: 0.0336. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0140. T_Loss: 4.6577. Mask: 0.9250. :  76%|███████▌  | 38/50 [00:20<00:07,  1.66it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0139. T_Loss: 4.6442. Mask: 0.9250. :  76%|███████▌  | 38/50 [00:20<00:07,  1.66it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0139. T_Loss: 4.6442. Mask: 0.9250. :  78%|███████▊  | 39/50 [00:20<00:05,  1.92it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0146. T_Loss: 4.6463. Mask: 0.9257. :  78%|███████▊  | 39/50 [00:21<00:05,  1.92it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0146. T_Loss: 4.6463. Mask: 0.9257. :  80%|████████  | 40/50 [00:21<00:04,  2.20it/s]total : 1000  current step :  538
total : 1000  current step :  539
total : 1000  current step :  540
Train Iter: 541/1000. LR: 0.0338. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0150. T_Loss: 4.6423. Mask: 0.9257. :  80%|████████  | 40/50 [00:22<00:04,  2.20it/s]Train Iter: 541/1000. LR: 0.0338. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0150. T_Loss: 4.6423. Mask: 0.9257. :  82%|████████▏ | 41/50 [00:22<00:05,  1.58it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0148. T_Loss: 4.6470. Mask: 0.9265. :  82%|████████▏ | 41/50 [00:22<00:05,  1.58it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0148. T_Loss: 4.6470. Mask: 0.9265. :  84%|████████▍ | 42/50 [00:22<00:04,  1.77it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0144. T_Loss: 4.6444. Mask: 0.9266. :  84%|████████▍ | 42/50 [00:22<00:04,  1.77it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0144. T_Loss: 4.6444. Mask: 0.9266. :  86%|████████▌ | 43/50 [00:22<00:03,  2.04it/s]total : 1000  current step :  541
total : 1000  current step :  542
total : 1000  current step :  543
Train Iter: 544/1000. LR: 0.0340. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0144. T_Loss: 4.6444. Mask: 0.9268. :  86%|████████▌ | 43/50 [00:23<00:03,  2.04it/s]Train Iter: 544/1000. LR: 0.0340. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0144. T_Loss: 4.6444. Mask: 0.9268. :  88%|████████▊ | 44/50 [00:23<00:03,  1.62it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0144. T_Loss: 4.6419. Mask: 0.9271. :  88%|████████▊ | 44/50 [00:24<00:03,  1.62it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0144. T_Loss: 4.6419. Mask: 0.9271. :  90%|█████████ | 45/50 [00:24<00:02,  1.83it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0159. T_Loss: 4.6472. Mask: 0.9271. :  90%|█████████ | 45/50 [00:24<00:02,  1.83it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0159. T_Loss: 4.6472. Mask: 0.9271. :  92%|█████████▏| 46/50 [00:24<00:01,  2.15it/s]total : 1000  current step :  544
total : 1000  current step :  545
total : 1000  current step :  546
Train Iter: 547/1000. LR: 0.0342. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0159. T_Loss: 4.6386. Mask: 0.9270. :  92%|█████████▏| 46/50 [00:25<00:01,  2.15it/s]Train Iter: 547/1000. LR: 0.0342. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0159. T_Loss: 4.6386. Mask: 0.9270. :  94%|█████████▍| 47/50 [00:25<00:01,  1.77it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0157. T_Loss: 4.6308. Mask: 0.9272. :  94%|█████████▍| 47/50 [00:25<00:01,  1.77it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0157. T_Loss: 4.6308. Mask: 0.9272. :  96%|█████████▌| 48/50 [00:25<00:01,  1.95it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0150. T_Loss: 4.6217. Mask: 0.9275. :  96%|█████████▌| 48/50 [00:25<00:01,  1.95it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0150. T_Loss: 4.6217. Mask: 0.9275. :  98%|█████████▊| 49/50 [00:25<00:00,  2.18it/s]total : 1000  current step :  547
total : 1000  current step :  548
total : 1000  current step :  549
Train Iter: 550/1000. LR: 0.0344. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0139. T_Loss: 4.6055. Mask: 0.9276. :  98%|█████████▊| 49/50 [00:27<00:00,  2.18it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0139. T_Loss: 4.6055. Mask: 0.9276. : 100%|██████████| 50/50 [00:27<00:00,  1.36it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0139. T_Loss: 4.6055. Mask: 0.9276. : 100%|██████████| 50/50 [00:27<00:00,  1.83it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.1817. top1: 75.39. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.1817. top1: 75.39. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.19it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1405. top1: 78.32. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.19it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1405. top1: 78.32. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1388. top1: 78.65. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1388. top1: 78.65. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1444. top1: 77.54. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1444. top1: 77.54. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1589. top1: 77.42. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1589. top1: 77.42. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  4.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1579. top1: 77.99. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  4.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1579. top1: 77.99. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1653. top1: 77.90. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1653. top1: 77.90. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1722. top1: 77.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1722. top1: 77.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.67it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1722. top1: 77.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.41it/s]
total : 1000  current step :  550
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.24s. S_Loss: 0.9734. T_Loss: 4.1917. Mask: 0.9414. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.24s. S_Loss: 0.9734. T_Loss: 4.1917. Mask: 0.9414. :   2%|▏         | 1/50 [00:00<00:12,  4.06it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.20s. S_Loss: 0.9940. T_Loss: 4.2517. Mask: 0.9316. :   2%|▏         | 1/50 [00:00<00:12,  4.06it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.20s. S_Loss: 0.9940. T_Loss: 4.2517. Mask: 0.9316. :   4%|▍         | 2/50 [00:00<00:09,  5.13it/s]total : 1000  current step :  551
total : 1000  current step :  552
Train Iter: 553/1000. LR: 0.0346. Data: 0.22s. Batch: 0.44s. S_Loss: 0.9944. T_Loss: 4.3249. Mask: 0.9375. :   4%|▍         | 2/50 [00:01<00:09,  5.13it/s]Train Iter: 553/1000. LR: 0.0346. Data: 0.22s. Batch: 0.44s. S_Loss: 0.9944. T_Loss: 4.3249. Mask: 0.9375. :   6%|▌         | 3/50 [00:01<00:24,  1.89it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.17s. Batch: 0.39s. S_Loss: 0.9965. T_Loss: 4.3824. Mask: 0.9365. :   6%|▌         | 3/50 [00:01<00:24,  1.89it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.17s. Batch: 0.39s. S_Loss: 0.9965. T_Loss: 4.3824. Mask: 0.9365. :   8%|▊         | 4/50 [00:01<00:18,  2.45it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.17s. Batch: 0.38s. S_Loss: 0.9949. T_Loss: 4.3354. Mask: 0.9305. :   8%|▊         | 4/50 [00:01<00:18,  2.45it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.17s. Batch: 0.38s. S_Loss: 0.9949. T_Loss: 4.3354. Mask: 0.9305. :  10%|█         | 5/50 [00:01<00:17,  2.59it/s]total : 1000  current step :  553
total : 1000  current step :  554
total : 1000  current step :  555
Train Iter: 556/1000. LR: 0.0347. Data: 0.24s. Batch: 0.45s. S_Loss: 0.9912. T_Loss: 4.3464. Mask: 0.9290. :  10%|█         | 5/50 [00:02<00:17,  2.59it/s]Train Iter: 556/1000. LR: 0.0347. Data: 0.24s. Batch: 0.45s. S_Loss: 0.9912. T_Loss: 4.3464. Mask: 0.9290. :  12%|█▏        | 6/50 [00:02<00:23,  1.86it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.23s. Batch: 0.47s. S_Loss: 0.9956. T_Loss: 4.3481. Mask: 0.9308. :  12%|█▏        | 6/50 [00:03<00:23,  1.86it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.23s. Batch: 0.47s. S_Loss: 0.9956. T_Loss: 4.3481. Mask: 0.9308. :  14%|█▍        | 7/50 [00:03<00:23,  1.86it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.20s. Batch: 0.44s. S_Loss: 0.9971. T_Loss: 4.3425. Mask: 0.9302. :  14%|█▍        | 7/50 [00:03<00:23,  1.86it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.20s. Batch: 0.44s. S_Loss: 0.9971. T_Loss: 4.3425. Mask: 0.9302. :  16%|█▌        | 8/50 [00:03<00:18,  2.27it/s]total : 1000  current step :  556
total : 1000  current step :  557
total : 1000  current step :  558
Train Iter: 559/1000. LR: 0.0349. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9969. T_Loss: 4.3650. Mask: 0.9280. :  16%|█▌        | 8/50 [00:04<00:18,  2.27it/s]Train Iter: 559/1000. LR: 0.0349. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9969. T_Loss: 4.3650. Mask: 0.9280. :  18%|█▊        | 9/50 [00:04<00:23,  1.72it/s]total : 1000  current step :  559
Train Iter: 560/1000. LR: 0.0350. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9973. T_Loss: 4.3743. Mask: 0.9277. :  18%|█▊        | 9/50 [00:04<00:23,  1.72it/s]Train Iter: 560/1000. LR: 0.0350. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9973. T_Loss: 4.3743. Mask: 0.9277. :  20%|██        | 10/50 [00:04<00:23,  1.70it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.24s. Batch: 0.49s. S_Loss: 0.9999. T_Loss: 4.4243. Mask: 0.9286. :  20%|██        | 10/50 [00:05<00:23,  1.70it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.24s. Batch: 0.49s. S_Loss: 0.9999. T_Loss: 4.4243. Mask: 0.9286. :  22%|██▏       | 11/50 [00:05<00:20,  1.92it/s]total : 1000  current step :  560
total : 1000  current step :  561
Train Iter: 562/1000. LR: 0.0351. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9974. T_Loss: 4.4691. Mask: 0.9287. :  22%|██▏       | 11/50 [00:06<00:20,  1.92it/s]Train Iter: 562/1000. LR: 0.0351. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9974. T_Loss: 4.4691. Mask: 0.9287. :  24%|██▍       | 12/50 [00:06<00:23,  1.63it/s]Train Iter: 563/1000. LR: 0.0352. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9941. T_Loss: 4.4664. Mask: 0.9288. :  24%|██▍       | 12/50 [00:06<00:23,  1.63it/s]Train Iter: 563/1000. LR: 0.0352. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9941. T_Loss: 4.4664. Mask: 0.9288. :  26%|██▌       | 13/50 [00:06<00:18,  1.97it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.24s. Batch: 0.48s. S_Loss: 0.9928. T_Loss: 4.4745. Mask: 0.9280. :  26%|██▌       | 13/50 [00:06<00:18,  1.97it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.24s. Batch: 0.48s. S_Loss: 0.9928. T_Loss: 4.4745. Mask: 0.9280. :  28%|██▊       | 14/50 [00:06<00:16,  2.18it/s]total : 1000  current step :  562
total : 1000  current step :  563
total : 1000  current step :  564
Train Iter: 565/1000. LR: 0.0353. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9929. T_Loss: 4.4716. Mask: 0.9273. :  28%|██▊       | 14/50 [00:07<00:16,  2.18it/s]Train Iter: 565/1000. LR: 0.0353. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9929. T_Loss: 4.4716. Mask: 0.9273. :  30%|███       | 15/50 [00:07<00:19,  1.81it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.24s. Batch: 0.49s. S_Loss: 0.9947. T_Loss: 4.4890. Mask: 0.9280. :  30%|███       | 15/50 [00:07<00:19,  1.81it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.24s. Batch: 0.49s. S_Loss: 0.9947. T_Loss: 4.4890. Mask: 0.9280. :  32%|███▏      | 16/50 [00:07<00:16,  2.12it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.23s. Batch: 0.48s. S_Loss: 0.9949. T_Loss: 4.5099. Mask: 0.9301. :  32%|███▏      | 16/50 [00:08<00:16,  2.12it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.23s. Batch: 0.48s. S_Loss: 0.9949. T_Loss: 4.5099. Mask: 0.9301. :  34%|███▍      | 17/50 [00:08<00:14,  2.25it/s]total : 1000  current step :  565
total : 1000  current step :  566
total : 1000  current step :  567
Train Iter: 568/1000. LR: 0.0355. Data: 0.25s. Batch: 0.51s. S_Loss: 0.9949. T_Loss: 4.5180. Mask: 0.9303. :  34%|███▍      | 17/50 [00:09<00:14,  2.25it/s]Train Iter: 568/1000. LR: 0.0355. Data: 0.25s. Batch: 0.51s. S_Loss: 0.9949. T_Loss: 4.5180. Mask: 0.9303. :  36%|███▌      | 18/50 [00:09<00:18,  1.73it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.24s. Batch: 0.49s. S_Loss: 0.9933. T_Loss: 4.5113. Mask: 0.9311. :  36%|███▌      | 18/50 [00:09<00:18,  1.73it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.24s. Batch: 0.49s. S_Loss: 0.9933. T_Loss: 4.5113. Mask: 0.9311. :  38%|███▊      | 19/50 [00:09<00:14,  2.08it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.23s. Batch: 0.48s. S_Loss: 0.9927. T_Loss: 4.5013. Mask: 0.9307. :  38%|███▊      | 19/50 [00:09<00:14,  2.08it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.23s. Batch: 0.48s. S_Loss: 0.9927. T_Loss: 4.5013. Mask: 0.9307. :  40%|████      | 20/50 [00:09<00:13,  2.30it/s]total : 1000  current step :  568
total : 1000  current step :  569
total : 1000  current step :  570
Train Iter: 571/1000. LR: 0.0357. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9933. T_Loss: 4.4857. Mask: 0.9315. :  40%|████      | 20/50 [00:10<00:13,  2.30it/s]Train Iter: 571/1000. LR: 0.0357. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9933. T_Loss: 4.4857. Mask: 0.9315. :  42%|████▏     | 21/50 [00:10<00:16,  1.78it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.23s. Batch: 0.49s. S_Loss: 0.9914. T_Loss: 4.4835. Mask: 0.9331. :  42%|████▏     | 21/50 [00:10<00:16,  1.78it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.23s. Batch: 0.49s. S_Loss: 0.9914. T_Loss: 4.4835. Mask: 0.9331. :  44%|████▍     | 22/50 [00:10<00:13,  2.12it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.22s. Batch: 0.48s. S_Loss: 0.9911. T_Loss: 4.4688. Mask: 0.9336. :  44%|████▍     | 22/50 [00:11<00:13,  2.12it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.22s. Batch: 0.48s. S_Loss: 0.9911. T_Loss: 4.4688. Mask: 0.9336. :  46%|████▌     | 23/50 [00:11<00:10,  2.47it/s]total : 1000  current step :  571
total : 1000  current step :  572
total : 1000  current step :  573
Train Iter: 574/1000. LR: 0.0359. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9913. T_Loss: 4.4607. Mask: 0.9331. :  46%|████▌     | 23/50 [00:12<00:10,  2.47it/s]Train Iter: 574/1000. LR: 0.0359. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9913. T_Loss: 4.4607. Mask: 0.9331. :  48%|████▊     | 24/50 [00:12<00:15,  1.71it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.23s. Batch: 0.49s. S_Loss: 0.9931. T_Loss: 4.4506. Mask: 0.9319. :  48%|████▊     | 24/50 [00:12<00:15,  1.71it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.23s. Batch: 0.49s. S_Loss: 0.9931. T_Loss: 4.4506. Mask: 0.9319. :  50%|█████     | 25/50 [00:12<00:12,  2.01it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.22s. Batch: 0.48s. S_Loss: 0.9925. T_Loss: 4.4419. Mask: 0.9312. :  50%|█████     | 25/50 [00:12<00:12,  2.01it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.22s. Batch: 0.48s. S_Loss: 0.9925. T_Loss: 4.4419. Mask: 0.9312. :  52%|█████▏    | 26/50 [00:12<00:10,  2.35it/s]total : 1000  current step :  574
total : 1000  current step :  575
total : 1000  current step :  576
Train Iter: 577/1000. LR: 0.0361. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9921. T_Loss: 4.4404. Mask: 0.9314. :  52%|█████▏    | 26/50 [00:13<00:10,  2.35it/s]Train Iter: 577/1000. LR: 0.0361. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9921. T_Loss: 4.4404. Mask: 0.9314. :  54%|█████▍    | 27/50 [00:13<00:13,  1.75it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9916. T_Loss: 4.4359. Mask: 0.9319. :  54%|█████▍    | 27/50 [00:14<00:13,  1.75it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9916. T_Loss: 4.4359. Mask: 0.9319. :  56%|█████▌    | 28/50 [00:14<00:11,  1.85it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.22s. Batch: 0.49s. S_Loss: 0.9916. T_Loss: 4.4400. Mask: 0.9314. :  56%|█████▌    | 28/50 [00:14<00:11,  1.85it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.22s. Batch: 0.49s. S_Loss: 0.9916. T_Loss: 4.4400. Mask: 0.9314. :  58%|█████▊    | 29/50 [00:14<00:09,  2.25it/s]total : 1000  current step :  577
total : 1000  current step :  578
total : 1000  current step :  579
Train Iter: 580/1000. LR: 0.0362. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9944. T_Loss: 4.4431. Mask: 0.9307. :  58%|█████▊    | 29/50 [00:15<00:09,  2.25it/s]Train Iter: 580/1000. LR: 0.0362. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9944. T_Loss: 4.4431. Mask: 0.9307. :  60%|██████    | 30/50 [00:15<00:11,  1.73it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9943. T_Loss: 4.4511. Mask: 0.9315. :  60%|██████    | 30/50 [00:15<00:11,  1.73it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9943. T_Loss: 4.4511. Mask: 0.9315. :  62%|██████▏   | 31/50 [00:15<00:09,  1.94it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.22s. Batch: 0.49s. S_Loss: 0.9967. T_Loss: 4.4621. Mask: 0.9312. :  62%|██████▏   | 31/50 [00:15<00:09,  1.94it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.22s. Batch: 0.49s. S_Loss: 0.9967. T_Loss: 4.4621. Mask: 0.9312. :  64%|██████▍   | 32/50 [00:15<00:07,  2.26it/s]total : 1000  current step :  580
total : 1000  current step :  581
total : 1000  current step :  582
Train Iter: 583/1000. LR: 0.0364. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9980. T_Loss: 4.4707. Mask: 0.9305. :  64%|██████▍   | 32/50 [00:16<00:07,  2.26it/s]Train Iter: 583/1000. LR: 0.0364. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9980. T_Loss: 4.4707. Mask: 0.9305. :  66%|██████▌   | 33/50 [00:16<00:10,  1.68it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9986. T_Loss: 4.4786. Mask: 0.9307. :  66%|██████▌   | 33/50 [00:17<00:10,  1.68it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9986. T_Loss: 4.4786. Mask: 0.9307. :  68%|██████▊   | 34/50 [00:17<00:08,  1.88it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0001. T_Loss: 4.4789. Mask: 0.9309. :  68%|██████▊   | 34/50 [00:17<00:08,  1.88it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0001. T_Loss: 4.4789. Mask: 0.9309. :  70%|███████   | 35/50 [00:17<00:07,  2.14it/s]total : 1000  current step :  583
total : 1000  current step :  584
total : 1000  current step :  585
Train Iter: 586/1000. LR: 0.0366. Data: 0.23s. Batch: 0.51s. S_Loss: 1.0006. T_Loss: 4.4777. Mask: 0.9308. :  70%|███████   | 35/50 [00:18<00:07,  2.14it/s]Train Iter: 586/1000. LR: 0.0366. Data: 0.23s. Batch: 0.51s. S_Loss: 1.0006. T_Loss: 4.4777. Mask: 0.9308. :  72%|███████▏  | 36/50 [00:18<00:08,  1.67it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0018. T_Loss: 4.4792. Mask: 0.9303. :  72%|███████▏  | 36/50 [00:18<00:08,  1.67it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0018. T_Loss: 4.4792. Mask: 0.9303. :  74%|███████▍  | 37/50 [00:18<00:06,  1.91it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0021. T_Loss: 4.4776. Mask: 0.9304. :  74%|███████▍  | 37/50 [00:18<00:06,  1.91it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0021. T_Loss: 4.4776. Mask: 0.9304. :  76%|███████▌  | 38/50 [00:18<00:05,  2.24it/s]total : 1000  current step :  586
total : 1000  current step :  587
total : 1000  current step :  588
Train Iter: 589/1000. LR: 0.0368. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0026. T_Loss: 4.4785. Mask: 0.9304. :  76%|███████▌  | 38/50 [00:19<00:05,  2.24it/s]Train Iter: 589/1000. LR: 0.0368. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0026. T_Loss: 4.4785. Mask: 0.9304. :  78%|███████▊  | 39/50 [00:19<00:05,  1.84it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0029. T_Loss: 4.4813. Mask: 0.9305. :  78%|███████▊  | 39/50 [00:19<00:05,  1.84it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0029. T_Loss: 4.4813. Mask: 0.9305. :  80%|████████  | 40/50 [00:19<00:04,  2.14it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0027. T_Loss: 4.4841. Mask: 0.9309. :  80%|████████  | 40/50 [00:20<00:04,  2.14it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0027. T_Loss: 4.4841. Mask: 0.9309. :  82%|████████▏ | 41/50 [00:20<00:03,  2.32it/s]total : 1000  current step :  589
total : 1000  current step :  590
total : 1000  current step :  591
Train Iter: 592/1000. LR: 0.0370. Data: 0.23s. Batch: 0.51s. S_Loss: 1.0026. T_Loss: 4.4867. Mask: 0.9300. :  82%|████████▏ | 41/50 [00:21<00:03,  2.32it/s]Train Iter: 592/1000. LR: 0.0370. Data: 0.23s. Batch: 0.51s. S_Loss: 1.0026. T_Loss: 4.4867. Mask: 0.9300. :  84%|████████▍ | 42/50 [00:21<00:04,  1.62it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0022. T_Loss: 4.4807. Mask: 0.9299. :  84%|████████▍ | 42/50 [00:21<00:04,  1.62it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0022. T_Loss: 4.4807. Mask: 0.9299. :  86%|████████▌ | 43/50 [00:21<00:03,  1.96it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0019. T_Loss: 4.4771. Mask: 0.9300. :  86%|████████▌ | 43/50 [00:21<00:03,  1.96it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0019. T_Loss: 4.4771. Mask: 0.9300. :  88%|████████▊ | 44/50 [00:21<00:02,  2.19it/s]total : 1000  current step :  592
total : 1000  current step :  593
total : 1000  current step :  594
Train Iter: 595/1000. LR: 0.0372. Data: 0.23s. Batch: 0.51s. S_Loss: 1.0016. T_Loss: 4.4811. Mask: 0.9301. :  88%|████████▊ | 44/50 [00:22<00:02,  2.19it/s]Train Iter: 595/1000. LR: 0.0372. Data: 0.23s. Batch: 0.51s. S_Loss: 1.0016. T_Loss: 4.4811. Mask: 0.9301. :  90%|█████████ | 45/50 [00:22<00:02,  1.69it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0021. T_Loss: 4.4882. Mask: 0.9305. :  90%|█████████ | 45/50 [00:23<00:02,  1.69it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0021. T_Loss: 4.4882. Mask: 0.9305. :  92%|█████████▏| 46/50 [00:23<00:01,  2.16it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0023. T_Loss: 4.4873. Mask: 0.9302. :  92%|█████████▏| 46/50 [00:23<00:01,  2.16it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0023. T_Loss: 4.4873. Mask: 0.9302. :  94%|█████████▍| 47/50 [00:23<00:01,  2.31it/s]total : 1000  current step :  595
total : 1000  current step :  596
total : 1000  current step :  597
Train Iter: 598/1000. LR: 0.0374. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0024. T_Loss: 4.4937. Mask: 0.9303. :  94%|█████████▍| 47/50 [00:24<00:01,  2.31it/s]Train Iter: 598/1000. LR: 0.0374. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0024. T_Loss: 4.4937. Mask: 0.9303. :  96%|█████████▌| 48/50 [00:24<00:01,  1.88it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0042. T_Loss: 4.5059. Mask: 0.9303. :  96%|█████████▌| 48/50 [00:24<00:01,  1.88it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0042. T_Loss: 4.5059. Mask: 0.9303. :  98%|█████████▊| 49/50 [00:24<00:00,  1.93it/s]total : 1000  current step :  598
total : 1000  current step :  599
Train Iter: 600/1000. LR: 0.0375. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0056. T_Loss: 4.5133. Mask: 0.9302. :  98%|█████████▊| 49/50 [00:25<00:00,  1.93it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0056. T_Loss: 4.5133. Mask: 0.9302. : 100%|██████████| 50/50 [00:25<00:00,  2.01it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.22s. Batch: 0.50s. S_Loss: 1.0056. T_Loss: 4.5133. Mask: 0.9302. : 100%|██████████| 50/50 [00:25<00:00,  1.99it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.2281. top1: 71.88. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.2281. top1: 71.88. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.21it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1774. top1: 73.83. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  2.21it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1774. top1: 73.83. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:01,  3.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1752. top1: 74.48. top5: 99.74. :  25%|██▌       | 2/8 [00:00<00:01,  3.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1752. top1: 74.48. top5: 99.74. :  38%|███▊      | 3/8 [00:00<00:01,  3.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1797. top1: 73.24. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  3.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1797. top1: 73.24. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:00,  4.10it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1692. top1: 74.69. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.10it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1692. top1: 74.69. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.31it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1515. top1: 76.17. top5: 99.87. :  62%|██████▎   | 5/8 [00:01<00:00,  4.31it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1515. top1: 76.17. top5: 99.87. :  75%|███████▌  | 6/8 [00:01<00:00,  4.52it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1469. top1: 77.06. top5: 99.89. :  75%|███████▌  | 6/8 [00:01<00:00,  4.52it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1469. top1: 77.06. top5: 99.89. :  88%|████████▊ | 7/8 [00:01<00:00,  4.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1473. top1: 77.00. top5: 99.90. :  88%|████████▊ | 7/8 [00:01<00:00,  4.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1473. top1: 77.00. top5: 99.90. : 100%|██████████| 8/8 [00:01<00:00,  4.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1473. top1: 77.00. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  3.97it/s]
total : 1000  current step :  600
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.65s. Batch: 0.83s. S_Loss: 1.0199. T_Loss: 5.1506. Mask: 0.9492. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.65s. Batch: 0.83s. S_Loss: 1.0199. T_Loss: 5.1506. Mask: 0.9492. :   2%|▏         | 1/50 [00:00<00:40,  1.20it/s]Train Iter: 602/1000. LR: 0.0376. Data: 0.39s. Batch: 0.61s. S_Loss: 1.0119. T_Loss: 5.0638. Mask: 0.9414. :   2%|▏         | 1/50 [00:01<00:40,  1.20it/s]Train Iter: 602/1000. LR: 0.0376. Data: 0.39s. Batch: 0.61s. S_Loss: 1.0119. T_Loss: 5.0638. Mask: 0.9414. :   4%|▍         | 2/50 [00:01<00:27,  1.76it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.27s. Batch: 0.48s. S_Loss: 1.0070. T_Loss: 4.8647. Mask: 0.9349. :   4%|▍         | 2/50 [00:01<00:27,  1.76it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.27s. Batch: 0.48s. S_Loss: 1.0070. T_Loss: 4.8647. Mask: 0.9349. :   6%|▌         | 3/50 [00:01<00:19,  2.44it/s]total : 1000  current step :  601
total : 1000  current step :  602
total : 1000  current step :  603
Train Iter: 604/1000. LR: 0.0378. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0091. T_Loss: 4.7945. Mask: 0.9365. :   6%|▌         | 3/50 [00:02<00:19,  2.44it/s]Train Iter: 604/1000. LR: 0.0378. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0091. T_Loss: 4.7945. Mask: 0.9365. :   8%|▊         | 4/50 [00:02<00:26,  1.74it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.29s. Batch: 0.50s. S_Loss: 1.0096. T_Loss: 4.7273. Mask: 0.9359. :   8%|▊         | 4/50 [00:02<00:26,  1.74it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.29s. Batch: 0.50s. S_Loss: 1.0096. T_Loss: 4.7273. Mask: 0.9359. :  10%|█         | 5/50 [00:02<00:20,  2.19it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.26s. Batch: 0.50s. S_Loss: 1.0124. T_Loss: 4.6773. Mask: 0.9310. :  10%|█         | 5/50 [00:02<00:20,  2.19it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.26s. Batch: 0.50s. S_Loss: 1.0124. T_Loss: 4.6773. Mask: 0.9310. :  12%|█▏        | 6/50 [00:02<00:20,  2.15it/s]total : 1000  current step :  604
total : 1000  current step :  605
total : 1000  current step :  606
Train Iter: 607/1000. LR: 0.0379. Data: 0.31s. Batch: 0.56s. S_Loss: 1.0098. T_Loss: 4.6552. Mask: 0.9330. :  12%|█▏        | 6/50 [00:03<00:20,  2.15it/s]Train Iter: 607/1000. LR: 0.0379. Data: 0.31s. Batch: 0.56s. S_Loss: 1.0098. T_Loss: 4.6552. Mask: 0.9330. :  14%|█▍        | 7/50 [00:03<00:26,  1.61it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0062. T_Loss: 4.6055. Mask: 0.9321. :  14%|█▍        | 7/50 [00:04<00:26,  1.61it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.28s. Batch: 0.53s. S_Loss: 1.0062. T_Loss: 4.6055. Mask: 0.9321. :  16%|█▌        | 8/50 [00:04<00:21,  1.92it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.25s. Batch: 0.49s. S_Loss: 1.0009. T_Loss: 4.5516. Mask: 0.9336. :  16%|█▌        | 8/50 [00:04<00:21,  1.92it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.25s. Batch: 0.49s. S_Loss: 1.0009. T_Loss: 4.5516. Mask: 0.9336. :  18%|█▊        | 9/50 [00:04<00:16,  2.44it/s]total : 1000  current step :  607
total : 1000  current step :  608
total : 1000  current step :  609
Train Iter: 610/1000. LR: 0.0381. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9975. T_Loss: 4.5200. Mask: 0.9348. :  18%|█▊        | 9/50 [00:05<00:16,  2.44it/s]Train Iter: 610/1000. LR: 0.0381. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9975. T_Loss: 4.5200. Mask: 0.9348. :  20%|██        | 10/50 [00:05<00:22,  1.75it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9984. T_Loss: 4.5199. Mask: 0.9347. :  20%|██        | 10/50 [00:05<00:22,  1.75it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9984. T_Loss: 4.5199. Mask: 0.9347. :  22%|██▏       | 11/50 [00:05<00:19,  2.05it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0011. T_Loss: 4.5584. Mask: 0.9336. :  22%|██▏       | 11/50 [00:05<00:19,  2.05it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0011. T_Loss: 4.5584. Mask: 0.9336. :  24%|██▍       | 12/50 [00:05<00:16,  2.26it/s]total : 1000  current step :  610
total : 1000  current step :  611
total : 1000  current step :  612
Train Iter: 613/1000. LR: 0.0383. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0008. T_Loss: 4.5414. Mask: 0.9348. :  24%|██▍       | 12/50 [00:06<00:16,  2.26it/s]Train Iter: 613/1000. LR: 0.0383. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0008. T_Loss: 4.5414. Mask: 0.9348. :  26%|██▌       | 13/50 [00:06<00:21,  1.69it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.25s. Batch: 0.52s. S_Loss: 0.9989. T_Loss: 4.5582. Mask: 0.9355. :  26%|██▌       | 13/50 [00:07<00:21,  1.69it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.25s. Batch: 0.52s. S_Loss: 0.9989. T_Loss: 4.5582. Mask: 0.9355. :  28%|██▊       | 14/50 [00:07<00:18,  1.93it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9983. T_Loss: 4.5492. Mask: 0.9341. :  28%|██▊       | 14/50 [00:07<00:18,  1.93it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.23s. Batch: 0.50s. S_Loss: 0.9983. T_Loss: 4.5492. Mask: 0.9341. :  30%|███       | 15/50 [00:07<00:16,  2.18it/s]total : 1000  current step :  613
total : 1000  current step :  614
total : 1000  current step :  615
Train Iter: 616/1000. LR: 0.0385. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9970. T_Loss: 4.5632. Mask: 0.9341. :  30%|███       | 15/50 [00:08<00:16,  2.18it/s]Train Iter: 616/1000. LR: 0.0385. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9970. T_Loss: 4.5632. Mask: 0.9341. :  32%|███▏      | 16/50 [00:08<00:20,  1.65it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.25s. Batch: 0.52s. S_Loss: 0.9955. T_Loss: 4.5440. Mask: 0.9320. :  32%|███▏      | 16/50 [00:08<00:20,  1.65it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.25s. Batch: 0.52s. S_Loss: 0.9955. T_Loss: 4.5440. Mask: 0.9320. :  34%|███▍      | 17/50 [00:08<00:17,  1.86it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.24s. Batch: 0.52s. S_Loss: 0.9944. T_Loss: 4.5445. Mask: 0.9316. :  34%|███▍      | 17/50 [00:09<00:17,  1.86it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.24s. Batch: 0.52s. S_Loss: 0.9944. T_Loss: 4.5445. Mask: 0.9316. :  36%|███▌      | 18/50 [00:09<00:16,  1.95it/s]total : 1000  current step :  616
total : 1000  current step :  617
total : 1000  current step :  618
Train Iter: 619/1000. LR: 0.0387. Data: 0.27s. Batch: 0.54s. S_Loss: 0.9947. T_Loss: 4.5464. Mask: 0.9317. :  36%|███▌      | 18/50 [00:10<00:16,  1.95it/s]Train Iter: 619/1000. LR: 0.0387. Data: 0.27s. Batch: 0.54s. S_Loss: 0.9947. T_Loss: 4.5464. Mask: 0.9317. :  38%|███▊      | 19/50 [00:10<00:19,  1.56it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.26s. Batch: 0.54s. S_Loss: 0.9963. T_Loss: 4.5578. Mask: 0.9313. :  38%|███▊      | 19/50 [00:10<00:19,  1.56it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.26s. Batch: 0.54s. S_Loss: 0.9963. T_Loss: 4.5578. Mask: 0.9313. :  40%|████      | 20/50 [00:10<00:18,  1.63it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.25s. Batch: 0.52s. S_Loss: 0.9958. T_Loss: 4.5582. Mask: 0.9317. :  40%|████      | 20/50 [00:11<00:18,  1.63it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.25s. Batch: 0.52s. S_Loss: 0.9958. T_Loss: 4.5582. Mask: 0.9317. :  42%|████▏     | 21/50 [00:11<00:13,  2.09it/s]total : 1000  current step :  619
total : 1000  current step :  620
total : 1000  current step :  621
Train Iter: 622/1000. LR: 0.0389. Data: 0.27s. Batch: 0.54s. S_Loss: 0.9952. T_Loss: 4.5402. Mask: 0.9315. :  42%|████▏     | 21/50 [00:11<00:13,  2.09it/s]Train Iter: 622/1000. LR: 0.0389. Data: 0.27s. Batch: 0.54s. S_Loss: 0.9952. T_Loss: 4.5402. Mask: 0.9315. :  44%|████▍     | 22/50 [00:11<00:16,  1.65it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9939. T_Loss: 4.5261. Mask: 0.9314. :  44%|████▍     | 22/50 [00:12<00:16,  1.65it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9939. T_Loss: 4.5261. Mask: 0.9314. :  46%|████▌     | 23/50 [00:12<00:14,  1.91it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.25s. Batch: 0.52s. S_Loss: 0.9940. T_Loss: 4.5118. Mask: 0.9320. :  46%|████▌     | 23/50 [00:12<00:14,  1.91it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.25s. Batch: 0.52s. S_Loss: 0.9940. T_Loss: 4.5118. Mask: 0.9320. :  48%|████▊     | 24/50 [00:12<00:10,  2.38it/s]total : 1000  current step :  622
total : 1000  current step :  623
total : 1000  current step :  624
Train Iter: 625/1000. LR: 0.0391. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9945. T_Loss: 4.5137. Mask: 0.9323. :  48%|████▊     | 24/50 [00:13<00:10,  2.38it/s]Train Iter: 625/1000. LR: 0.0391. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9945. T_Loss: 4.5137. Mask: 0.9323. :  50%|█████     | 25/50 [00:13<00:13,  1.85it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9964. T_Loss: 4.5037. Mask: 0.9318. :  50%|█████     | 25/50 [00:13<00:13,  1.85it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9964. T_Loss: 4.5037. Mask: 0.9318. :  52%|█████▏    | 26/50 [00:13<00:10,  2.24it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.25s. Batch: 0.51s. S_Loss: 0.9970. T_Loss: 4.5069. Mask: 0.9310. :  52%|█████▏    | 26/50 [00:13<00:10,  2.24it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.25s. Batch: 0.51s. S_Loss: 0.9970. T_Loss: 4.5069. Mask: 0.9310. :  54%|█████▍    | 27/50 [00:13<00:10,  2.22it/s]total : 1000  current step :  625
total : 1000  current step :  626
total : 1000  current step :  627
Train Iter: 628/1000. LR: 0.0393. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9974. T_Loss: 4.5061. Mask: 0.9304. :  54%|█████▍    | 27/50 [00:14<00:10,  2.22it/s]Train Iter: 628/1000. LR: 0.0393. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9974. T_Loss: 4.5061. Mask: 0.9304. :  56%|█████▌    | 28/50 [00:14<00:13,  1.69it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9978. T_Loss: 4.5085. Mask: 0.9297. :  56%|█████▌    | 28/50 [00:15<00:13,  1.69it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9978. T_Loss: 4.5085. Mask: 0.9297. :  58%|█████▊    | 29/50 [00:15<00:10,  2.07it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0003. T_Loss: 4.5170. Mask: 0.9289. :  58%|█████▊    | 29/50 [00:15<00:10,  2.07it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0003. T_Loss: 4.5170. Mask: 0.9289. :  60%|██████    | 30/50 [00:15<00:08,  2.46it/s]total : 1000  current step :  628
total : 1000  current step :  629
total : 1000  current step :  630
Train Iter: 631/1000. LR: 0.0394. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9997. T_Loss: 4.5110. Mask: 0.9291. :  60%|██████    | 30/50 [00:16<00:08,  2.46it/s]Train Iter: 631/1000. LR: 0.0394. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9997. T_Loss: 4.5110. Mask: 0.9291. :  62%|██████▏   | 31/50 [00:16<00:10,  1.75it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0010. T_Loss: 4.5122. Mask: 0.9286. :  62%|██████▏   | 31/50 [00:16<00:10,  1.75it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0010. T_Loss: 4.5122. Mask: 0.9286. :  64%|██████▍   | 32/50 [00:16<00:08,  2.02it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0019. T_Loss: 4.5222. Mask: 0.9285. :  64%|██████▍   | 32/50 [00:16<00:08,  2.02it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0019. T_Loss: 4.5222. Mask: 0.9285. :  66%|██████▌   | 33/50 [00:16<00:07,  2.41it/s]total : 1000  current step :  631
total : 1000  current step :  632
total : 1000  current step :  633
Train Iter: 634/1000. LR: 0.0396. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0032. T_Loss: 4.5379. Mask: 0.9278. :  66%|██████▌   | 33/50 [00:17<00:07,  2.41it/s]Train Iter: 634/1000. LR: 0.0396. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0032. T_Loss: 4.5379. Mask: 0.9278. :  68%|██████▊   | 34/50 [00:17<00:10,  1.60it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0047. T_Loss: 4.5412. Mask: 0.9266. :  68%|██████▊   | 34/50 [00:18<00:10,  1.60it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0047. T_Loss: 4.5412. Mask: 0.9266. :  70%|███████   | 35/50 [00:18<00:07,  1.98it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0047. T_Loss: 4.5491. Mask: 0.9265. :  70%|███████   | 35/50 [00:18<00:07,  1.98it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0047. T_Loss: 4.5491. Mask: 0.9265. :  72%|███████▏  | 36/50 [00:18<00:06,  2.32it/s]total : 1000  current step :  634
total : 1000  current step :  635
total : 1000  current step :  636
Train Iter: 637/1000. LR: 0.0398. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0045. T_Loss: 4.5544. Mask: 0.9269. :  72%|███████▏  | 36/50 [00:19<00:06,  2.32it/s]Train Iter: 637/1000. LR: 0.0398. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0045. T_Loss: 4.5544. Mask: 0.9269. :  74%|███████▍  | 37/50 [00:19<00:07,  1.77it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.24s. Batch: 0.52s. S_Loss: 1.0046. T_Loss: 4.5542. Mask: 0.9266. :  74%|███████▍  | 37/50 [00:19<00:07,  1.77it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.24s. Batch: 0.52s. S_Loss: 1.0046. T_Loss: 4.5542. Mask: 0.9266. :  76%|███████▌  | 38/50 [00:19<00:06,  2.00it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0040. T_Loss: 4.5481. Mask: 0.9269. :  76%|███████▌  | 38/50 [00:19<00:06,  2.00it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.24s. Batch: 0.51s. S_Loss: 1.0040. T_Loss: 4.5481. Mask: 0.9269. :  78%|███████▊  | 39/50 [00:19<00:04,  2.33it/s]total : 1000  current step :  637
total : 1000  current step :  638
total : 1000  current step :  639
Train Iter: 640/1000. LR: 0.0400. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0031. T_Loss: 4.5404. Mask: 0.9273. :  78%|███████▊  | 39/50 [00:21<00:04,  2.33it/s]Train Iter: 640/1000. LR: 0.0400. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0031. T_Loss: 4.5404. Mask: 0.9273. :  80%|████████  | 40/50 [00:21<00:06,  1.52it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0026. T_Loss: 4.5306. Mask: 0.9283. :  80%|████████  | 40/50 [00:21<00:06,  1.52it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0026. T_Loss: 4.5306. Mask: 0.9283. :  82%|████████▏ | 41/50 [00:21<00:06,  1.46it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0018. T_Loss: 4.5180. Mask: 0.9278. :  82%|████████▏ | 41/50 [00:22<00:06,  1.46it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0018. T_Loss: 4.5180. Mask: 0.9278. :  84%|████████▍ | 42/50 [00:22<00:04,  1.78it/s]total : 1000  current step :  640
total : 1000  current step :  641
total : 1000  current step :  642
Train Iter: 643/1000. LR: 0.0402. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0017. T_Loss: 4.5153. Mask: 0.9282. :  84%|████████▍ | 42/50 [00:23<00:04,  1.78it/s]Train Iter: 643/1000. LR: 0.0402. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0017. T_Loss: 4.5153. Mask: 0.9282. :  86%|████████▌ | 43/50 [00:23<00:04,  1.51it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0031. T_Loss: 4.5239. Mask: 0.9290. :  86%|████████▌ | 43/50 [00:23<00:04,  1.51it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0031. T_Loss: 4.5239. Mask: 0.9290. :  88%|████████▊ | 44/50 [00:23<00:03,  1.75it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0042. T_Loss: 4.5272. Mask: 0.9296. :  88%|████████▊ | 44/50 [00:23<00:03,  1.75it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0042. T_Loss: 4.5272. Mask: 0.9296. :  90%|█████████ | 45/50 [00:23<00:02,  2.10it/s]total : 1000  current step :  643
total : 1000  current step :  644
total : 1000  current step :  645
Train Iter: 646/1000. LR: 0.0404. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0039. T_Loss: 4.5230. Mask: 0.9292. :  90%|█████████ | 45/50 [00:24<00:02,  2.10it/s]Train Iter: 646/1000. LR: 0.0404. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0039. T_Loss: 4.5230. Mask: 0.9292. :  92%|█████████▏| 46/50 [00:24<00:02,  1.77it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0034. T_Loss: 4.5252. Mask: 0.9294. :  92%|█████████▏| 46/50 [00:24<00:02,  1.77it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0034. T_Loss: 4.5252. Mask: 0.9294. :  94%|█████████▍| 47/50 [00:24<00:01,  2.03it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0027. T_Loss: 4.5220. Mask: 0.9288. :  94%|█████████▍| 47/50 [00:25<00:01,  2.03it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0027. T_Loss: 4.5220. Mask: 0.9288. :  96%|█████████▌| 48/50 [00:25<00:00,  2.10it/s]total : 1000  current step :  646
total : 1000  current step :  647
total : 1000  current step :  648
Train Iter: 649/1000. LR: 0.0406. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0023. T_Loss: 4.5264. Mask: 0.9292. :  96%|█████████▌| 48/50 [00:26<00:00,  2.10it/s]Train Iter: 649/1000. LR: 0.0406. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0023. T_Loss: 4.5264. Mask: 0.9292. :  98%|█████████▊| 49/50 [00:26<00:00,  1.59it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0025. T_Loss: 4.5321. Mask: 0.9295. :  98%|█████████▊| 49/50 [00:26<00:00,  1.59it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0025. T_Loss: 4.5321. Mask: 0.9295. : 100%|██████████| 50/50 [00:26<00:00,  1.97it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.25s. Batch: 0.53s. S_Loss: 1.0025. T_Loss: 4.5321. Mask: 0.9295. : 100%|██████████| 50/50 [00:26<00:00,  1.90it/s]
total : 1000  current step :  649
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.2815. top1: 70.31. top5: 99.61. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.2815. top1: 70.31. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:03,  1.87it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.2211. top1: 72.07. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:03,  1.87it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.2211. top1: 72.07. top5: 99.61. :  25%|██▌       | 2/8 [00:00<00:02,  2.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.2197. top1: 70.96. top5: 99.48. :  25%|██▌       | 2/8 [00:00<00:02,  2.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.2197. top1: 70.96. top5: 99.48. :  38%|███▊      | 3/8 [00:00<00:01,  3.35it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.2245. top1: 70.21. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  3.35it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.2245. top1: 70.21. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.68it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1933. top1: 73.05. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.68it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1933. top1: 73.05. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1616. top1: 75.33. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1616. top1: 75.33. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1474. top1: 76.62. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1474. top1: 76.62. top5: 99.67. :  88%|████████▊ | 7/8 [00:01<00:00,  4.28it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1421. top1: 76.95. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  4.28it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1421. top1: 76.95. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  4.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1421. top1: 76.95. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.74it/s]
total : 1000  current step :  650
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.01s. Batch: 0.19s. S_Loss: 1.0298. T_Loss: 4.5349. Mask: 0.9219. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.01s. Batch: 0.19s. S_Loss: 1.0298. T_Loss: 4.5349. Mask: 0.9219. :   2%|▏         | 1/50 [00:00<00:09,  5.25it/s]total : 1000  current step :  651
Train Iter: 652/1000. LR: 0.0408. Data: 0.32s. Batch: 0.58s. S_Loss: 1.0297. T_Loss: 4.5469. Mask: 0.9336. :   2%|▏         | 1/50 [00:01<00:09,  5.25it/s]Train Iter: 652/1000. LR: 0.0408. Data: 0.32s. Batch: 0.58s. S_Loss: 1.0297. T_Loss: 4.5469. Mask: 0.9336. :   4%|▍         | 2/50 [00:01<00:31,  1.53it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.21s. Batch: 0.51s. S_Loss: 1.0438. T_Loss: 4.5498. Mask: 0.9401. :   4%|▍         | 2/50 [00:01<00:31,  1.53it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.21s. Batch: 0.51s. S_Loss: 1.0438. T_Loss: 4.5498. Mask: 0.9401. :   6%|▌         | 3/50 [00:01<00:24,  1.90it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.16s. Batch: 0.44s. S_Loss: 1.0447. T_Loss: 4.5913. Mask: 0.9404. :   6%|▌         | 3/50 [00:01<00:24,  1.90it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.16s. Batch: 0.44s. S_Loss: 1.0447. T_Loss: 4.5913. Mask: 0.9404. :   8%|▊         | 4/50 [00:01<00:18,  2.44it/s]total : 1000  current step :  652
total : 1000  current step :  653
total : 1000  current step :  654
Train Iter: 655/1000. LR: 0.0409. Data: 0.24s. Batch: 0.52s. S_Loss: 1.0438. T_Loss: 4.5933. Mask: 0.9414. :   8%|▊         | 4/50 [00:02<00:18,  2.44it/s]Train Iter: 655/1000. LR: 0.0409. Data: 0.24s. Batch: 0.52s. S_Loss: 1.0438. T_Loss: 4.5933. Mask: 0.9414. :  10%|█         | 5/50 [00:02<00:25,  1.80it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.20s. Batch: 0.47s. S_Loss: 1.0358. T_Loss: 4.5966. Mask: 0.9434. :  10%|█         | 5/50 [00:02<00:25,  1.80it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.20s. Batch: 0.47s. S_Loss: 1.0358. T_Loss: 4.5966. Mask: 0.9434. :  12%|█▏        | 6/50 [00:02<00:19,  2.21it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.18s. Batch: 0.45s. S_Loss: 1.0286. T_Loss: 4.5664. Mask: 0.9425. :  12%|█▏        | 6/50 [00:03<00:19,  2.21it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.18s. Batch: 0.45s. S_Loss: 1.0286. T_Loss: 4.5664. Mask: 0.9425. :  14%|█▍        | 7/50 [00:03<00:17,  2.52it/s]total : 1000  current step :  655
total : 1000  current step :  656
total : 1000  current step :  657
Train Iter: 658/1000. LR: 0.0411. Data: 0.23s. Batch: 0.48s. S_Loss: 1.0230. T_Loss: 4.5137. Mask: 0.9429. :  14%|█▍        | 7/50 [00:03<00:17,  2.52it/s]Train Iter: 658/1000. LR: 0.0411. Data: 0.23s. Batch: 0.48s. S_Loss: 1.0230. T_Loss: 4.5137. Mask: 0.9429. :  16%|█▌        | 8/50 [00:03<00:21,  1.97it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.21s. Batch: 0.47s. S_Loss: 1.0191. T_Loss: 4.4520. Mask: 0.9397. :  16%|█▌        | 8/50 [00:04<00:21,  1.97it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.21s. Batch: 0.47s. S_Loss: 1.0191. T_Loss: 4.4520. Mask: 0.9397. :  18%|█▊        | 9/50 [00:04<00:18,  2.21it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.19s. Batch: 0.45s. S_Loss: 1.0218. T_Loss: 4.4845. Mask: 0.9406. :  18%|█▊        | 9/50 [00:04<00:18,  2.21it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.19s. Batch: 0.45s. S_Loss: 1.0218. T_Loss: 4.4845. Mask: 0.9406. :  20%|██        | 10/50 [00:04<00:16,  2.44it/s]total : 1000  current step :  658
total : 1000  current step :  659
total : 1000  current step :  660
Train Iter: 661/1000. LR: 0.0413. Data: 0.23s. Batch: 0.48s. S_Loss: 1.0245. T_Loss: 4.4909. Mask: 0.9400. :  20%|██        | 10/50 [00:05<00:16,  2.44it/s]Train Iter: 661/1000. LR: 0.0413. Data: 0.23s. Batch: 0.48s. S_Loss: 1.0245. T_Loss: 4.4909. Mask: 0.9400. :  22%|██▏       | 11/50 [00:05<00:20,  1.93it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0228. T_Loss: 4.4801. Mask: 0.9388. :  22%|██▏       | 11/50 [00:05<00:20,  1.93it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0228. T_Loss: 4.4801. Mask: 0.9388. :  24%|██▍       | 12/50 [00:05<00:20,  1.87it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.20s. Batch: 0.48s. S_Loss: 1.0202. T_Loss: 4.4869. Mask: 0.9366. :  24%|██▍       | 12/50 [00:06<00:20,  1.87it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.20s. Batch: 0.48s. S_Loss: 1.0202. T_Loss: 4.4869. Mask: 0.9366. :  26%|██▌       | 13/50 [00:06<00:17,  2.07it/s]total : 1000  current step :  661
total : 1000  current step :  662
total : 1000  current step :  663
Train Iter: 664/1000. LR: 0.0415. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0174. T_Loss: 4.4787. Mask: 0.9378. :  26%|██▌       | 13/50 [00:06<00:17,  2.07it/s]Train Iter: 664/1000. LR: 0.0415. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0174. T_Loss: 4.4787. Mask: 0.9378. :  28%|██▊       | 14/50 [00:06<00:20,  1.75it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0173. T_Loss: 4.4834. Mask: 0.9388. :  28%|██▊       | 14/50 [00:07<00:20,  1.75it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0173. T_Loss: 4.4834. Mask: 0.9388. :  30%|███       | 15/50 [00:07<00:17,  1.96it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.21s. Batch: 0.48s. S_Loss: 1.0196. T_Loss: 4.5014. Mask: 0.9392. :  30%|███       | 15/50 [00:07<00:17,  1.96it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.21s. Batch: 0.48s. S_Loss: 1.0196. T_Loss: 4.5014. Mask: 0.9392. :  32%|███▏      | 16/50 [00:07<00:15,  2.15it/s]total : 1000  current step :  664
total : 1000  current step :  665
total : 1000  current step :  666
Train Iter: 667/1000. LR: 0.0417. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0182. T_Loss: 4.4879. Mask: 0.9393. :  32%|███▏      | 16/50 [00:08<00:15,  2.15it/s]Train Iter: 667/1000. LR: 0.0417. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0182. T_Loss: 4.4879. Mask: 0.9393. :  34%|███▍      | 17/50 [00:08<00:18,  1.82it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.23s. Batch: 0.49s. S_Loss: 1.0157. T_Loss: 4.4904. Mask: 0.9390. :  34%|███▍      | 17/50 [00:08<00:18,  1.82it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.23s. Batch: 0.49s. S_Loss: 1.0157. T_Loss: 4.4904. Mask: 0.9390. :  36%|███▌      | 18/50 [00:08<00:15,  2.08it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.21s. Batch: 0.49s. S_Loss: 1.0135. T_Loss: 4.4867. Mask: 0.9389. :  36%|███▌      | 18/50 [00:09<00:15,  2.08it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.21s. Batch: 0.49s. S_Loss: 1.0135. T_Loss: 4.4867. Mask: 0.9389. :  38%|███▊      | 19/50 [00:09<00:15,  2.05it/s]total : 1000  current step :  667
total : 1000  current step :  668
total : 1000  current step :  669
Train Iter: 670/1000. LR: 0.0419. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0149. T_Loss: 4.5019. Mask: 0.9381. :  38%|███▊      | 19/50 [00:09<00:15,  2.05it/s]Train Iter: 670/1000. LR: 0.0419. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0149. T_Loss: 4.5019. Mask: 0.9381. :  40%|████      | 20/50 [00:09<00:16,  1.81it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.23s. Batch: 0.49s. S_Loss: 1.0131. T_Loss: 4.5210. Mask: 0.9390. :  40%|████      | 20/50 [00:10<00:16,  1.81it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.23s. Batch: 0.49s. S_Loss: 1.0131. T_Loss: 4.5210. Mask: 0.9390. :  42%|████▏     | 21/50 [00:10<00:14,  1.99it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0130. T_Loss: 4.5468. Mask: 0.9382. :  42%|████▏     | 21/50 [00:10<00:14,  1.99it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.22s. Batch: 0.49s. S_Loss: 1.0130. T_Loss: 4.5468. Mask: 0.9382. :  44%|████▍     | 22/50 [00:10<00:12,  2.20it/s]total : 1000  current step :  670
total : 1000  current step :  671
total : 1000  current step :  672
Train Iter: 673/1000. LR: 0.0421. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0098. T_Loss: 4.5320. Mask: 0.9372. :  44%|████▍     | 22/50 [00:11<00:12,  2.20it/s]Train Iter: 673/1000. LR: 0.0421. Data: 0.23s. Batch: 0.50s. S_Loss: 1.0098. T_Loss: 4.5320. Mask: 0.9372. :  46%|████▌     | 23/50 [00:11<00:14,  1.87it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.23s. Batch: 0.48s. S_Loss: 1.0093. T_Loss: 4.5560. Mask: 0.9368. :  46%|████▌     | 23/50 [00:11<00:14,  1.87it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.23s. Batch: 0.48s. S_Loss: 1.0093. T_Loss: 4.5560. Mask: 0.9368. :  48%|████▊     | 24/50 [00:11<00:11,  2.25it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.22s. Batch: 0.48s. S_Loss: 1.0084. T_Loss: 4.5667. Mask: 0.9369. :  48%|████▊     | 24/50 [00:12<00:11,  2.25it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.22s. Batch: 0.48s. S_Loss: 1.0084. T_Loss: 4.5667. Mask: 0.9369. :  50%|█████     | 25/50 [00:12<00:11,  2.25it/s]total : 1000  current step :  673
total : 1000  current step :  674
total : 1000  current step :  675
Train Iter: 676/1000. LR: 0.0423. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0072. T_Loss: 4.5629. Mask: 0.9361. :  50%|█████     | 25/50 [00:13<00:11,  2.25it/s]Train Iter: 676/1000. LR: 0.0423. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0072. T_Loss: 4.5629. Mask: 0.9361. :  52%|█████▏    | 26/50 [00:13<00:13,  1.72it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.23s. Batch: 0.49s. S_Loss: 1.0053. T_Loss: 4.5621. Mask: 0.9365. :  52%|█████▏    | 26/50 [00:13<00:13,  1.72it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.23s. Batch: 0.49s. S_Loss: 1.0053. T_Loss: 4.5621. Mask: 0.9365. :  54%|█████▍    | 27/50 [00:13<00:10,  2.16it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.23s. Batch: 0.49s. S_Loss: 1.0044. T_Loss: 4.5499. Mask: 0.9357. :  54%|█████▍    | 27/50 [00:13<00:10,  2.16it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.23s. Batch: 0.49s. S_Loss: 1.0044. T_Loss: 4.5499. Mask: 0.9357. :  56%|█████▌    | 28/50 [00:13<00:10,  2.19it/s]total : 1000  current step :  676
total : 1000  current step :  677
total : 1000  current step :  678
Train Iter: 679/1000. LR: 0.0424. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0045. T_Loss: 4.5435. Mask: 0.9356. :  56%|█████▌    | 28/50 [00:14<00:10,  2.19it/s]Train Iter: 679/1000. LR: 0.0424. Data: 0.24s. Batch: 0.50s. S_Loss: 1.0045. T_Loss: 4.5435. Mask: 0.9356. :  58%|█████▊    | 29/50 [00:14<00:12,  1.74it/s]total : 1000  current step :  679
Train Iter: 680/1000. LR: 0.0425. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0039. T_Loss: 4.5323. Mask: 0.9365. :  58%|█████▊    | 29/50 [00:15<00:12,  1.74it/s]Train Iter: 680/1000. LR: 0.0425. Data: 0.25s. Batch: 0.50s. S_Loss: 1.0039. T_Loss: 4.5323. Mask: 0.9365. :  60%|██████    | 30/50 [00:15<00:11,  1.73it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0036. T_Loss: 4.5284. Mask: 0.9374. :  60%|██████    | 30/50 [00:15<00:11,  1.73it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0036. T_Loss: 4.5284. Mask: 0.9374. :  62%|██████▏   | 31/50 [00:15<00:11,  1.62it/s]total : 1000  current step :  680
total : 1000  current step :  681
Train Iter: 682/1000. LR: 0.0426. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0032. T_Loss: 4.5120. Mask: 0.9377. :  62%|██████▏   | 31/50 [00:16<00:11,  1.62it/s]Train Iter: 682/1000. LR: 0.0426. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0032. T_Loss: 4.5120. Mask: 0.9377. :  64%|██████▍   | 32/50 [00:16<00:13,  1.33it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0032. T_Loss: 4.5085. Mask: 0.9377. :  64%|██████▍   | 32/50 [00:17<00:13,  1.33it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.26s. Batch: 0.52s. S_Loss: 1.0032. T_Loss: 4.5085. Mask: 0.9377. :  66%|██████▌   | 33/50 [00:17<00:11,  1.47it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0023. T_Loss: 4.5022. Mask: 0.9377. :  66%|██████▌   | 33/50 [00:17<00:11,  1.47it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.25s. Batch: 0.51s. S_Loss: 1.0023. T_Loss: 4.5022. Mask: 0.9377. :  68%|██████▊   | 34/50 [00:17<00:08,  1.88it/s]total : 1000  current step :  682
total : 1000  current step :  683
total : 1000  current step :  684
Train Iter: 685/1000. LR: 0.0428. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0025. T_Loss: 4.5036. Mask: 0.9376. :  68%|██████▊   | 34/50 [00:18<00:08,  1.88it/s]Train Iter: 685/1000. LR: 0.0428. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0025. T_Loss: 4.5036. Mask: 0.9376. :  70%|███████   | 35/50 [00:18<00:10,  1.39it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0031. T_Loss: 4.4991. Mask: 0.9372. :  70%|███████   | 35/50 [00:19<00:10,  1.39it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0031. T_Loss: 4.4991. Mask: 0.9372. :  72%|███████▏  | 36/50 [00:19<00:08,  1.67it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0047. T_Loss: 4.4960. Mask: 0.9370. :  72%|███████▏  | 36/50 [00:19<00:08,  1.67it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.25s. Batch: 0.52s. S_Loss: 1.0047. T_Loss: 4.4960. Mask: 0.9370. :  74%|███████▍  | 37/50 [00:19<00:06,  1.97it/s]total : 1000  current step :  685
total : 1000  current step :  686
total : 1000  current step :  687
Train Iter: 688/1000. LR: 0.0430. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0062. T_Loss: 4.5076. Mask: 0.9367. :  74%|███████▍  | 37/50 [00:20<00:06,  1.97it/s]Train Iter: 688/1000. LR: 0.0430. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0062. T_Loss: 4.5076. Mask: 0.9367. :  76%|███████▌  | 38/50 [00:20<00:08,  1.42it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0078. T_Loss: 4.5069. Mask: 0.9364. :  76%|███████▌  | 38/50 [00:20<00:08,  1.42it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0078. T_Loss: 4.5069. Mask: 0.9364. :  78%|███████▊  | 39/50 [00:20<00:06,  1.66it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0082. T_Loss: 4.5124. Mask: 0.9358. :  78%|███████▊  | 39/50 [00:21<00:06,  1.66it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0082. T_Loss: 4.5124. Mask: 0.9358. :  80%|████████  | 40/50 [00:21<00:05,  1.80it/s]total : 1000  current step :  688
total : 1000  current step :  689
total : 1000  current step :  690
Train Iter: 691/1000. LR: 0.0432. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0075. T_Loss: 4.5122. Mask: 0.9351. :  80%|████████  | 40/50 [00:22<00:05,  1.80it/s]Train Iter: 691/1000. LR: 0.0432. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0075. T_Loss: 4.5122. Mask: 0.9351. :  82%|████████▏ | 41/50 [00:22<00:06,  1.47it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.26s. Batch: 0.54s. S_Loss: 1.0084. T_Loss: 4.5214. Mask: 0.9346. :  82%|████████▏ | 41/50 [00:22<00:06,  1.47it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.26s. Batch: 0.54s. S_Loss: 1.0084. T_Loss: 4.5214. Mask: 0.9346. :  84%|████████▍ | 42/50 [00:22<00:04,  1.76it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0083. T_Loss: 4.5214. Mask: 0.9340. :  84%|████████▍ | 42/50 [00:22<00:04,  1.76it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0083. T_Loss: 4.5214. Mask: 0.9340. :  86%|████████▌ | 43/50 [00:22<00:03,  2.11it/s]total : 1000  current step :  691
total : 1000  current step :  692
total : 1000  current step :  693
Train Iter: 694/1000. LR: 0.0434. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0078. T_Loss: 4.5209. Mask: 0.9339. :  86%|████████▌ | 43/50 [00:23<00:03,  2.11it/s]Train Iter: 694/1000. LR: 0.0434. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0078. T_Loss: 4.5209. Mask: 0.9339. :  88%|████████▊ | 44/50 [00:23<00:03,  1.60it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0085. T_Loss: 4.5321. Mask: 0.9336. :  88%|████████▊ | 44/50 [00:24<00:03,  1.60it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0085. T_Loss: 4.5321. Mask: 0.9336. :  90%|█████████ | 45/50 [00:24<00:02,  1.90it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0082. T_Loss: 4.5319. Mask: 0.9338. :  90%|█████████ | 45/50 [00:24<00:02,  1.90it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0082. T_Loss: 4.5319. Mask: 0.9338. :  92%|█████████▏| 46/50 [00:24<00:01,  2.08it/s]total : 1000  current step :  694
total : 1000  current step :  695
total : 1000  current step :  696
Train Iter: 697/1000. LR: 0.0436. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0083. T_Loss: 4.5342. Mask: 0.9338. :  92%|█████████▏| 46/50 [00:25<00:01,  2.08it/s]Train Iter: 697/1000. LR: 0.0436. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0083. T_Loss: 4.5342. Mask: 0.9338. :  94%|█████████▍| 47/50 [00:25<00:01,  1.50it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0093. T_Loss: 4.5392. Mask: 0.9338. :  94%|█████████▍| 47/50 [00:25<00:01,  1.50it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.27s. Batch: 0.53s. S_Loss: 1.0093. T_Loss: 4.5392. Mask: 0.9338. :  96%|█████████▌| 48/50 [00:25<00:01,  1.96it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0092. T_Loss: 4.5413. Mask: 0.9339. :  96%|█████████▌| 48/50 [00:26<00:01,  1.96it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.26s. Batch: 0.53s. S_Loss: 1.0092. T_Loss: 4.5413. Mask: 0.9339. :  98%|█████████▊| 49/50 [00:26<00:00,  1.97it/s]total : 1000  current step :  697
total : 1000  current step :  698
total : 1000  current step :  699
Train Iter: 700/1000. LR: 0.0438. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0091. T_Loss: 4.5450. Mask: 0.9343. :  98%|█████████▊| 49/50 [00:27<00:00,  1.97it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0091. T_Loss: 4.5450. Mask: 0.9343. : 100%|██████████| 50/50 [00:27<00:00,  1.55it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.27s. Batch: 0.54s. S_Loss: 1.0091. T_Loss: 4.5450. Mask: 0.9343. : 100%|██████████| 50/50 [00:27<00:00,  1.84it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.2770. top1: 69.92. top5: 99.61. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.2770. top1: 69.92. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:03,  2.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.2152. top1: 72.27. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:03,  2.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.2152. top1: 72.27. top5: 99.61. :  25%|██▌       | 2/8 [00:00<00:01,  3.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.2126. top1: 71.74. top5: 99.48. :  25%|██▌       | 2/8 [00:00<00:01,  3.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.2126. top1: 71.74. top5: 99.48. :  38%|███▊      | 3/8 [00:00<00:01,  3.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.2190. top1: 70.90. top5: 99.51. :  38%|███▊      | 3/8 [00:01<00:01,  3.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.2190. top1: 70.90. top5: 99.51. :  50%|█████     | 4/8 [00:01<00:00,  4.05it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1870. top1: 73.52. top5: 99.45. :  50%|█████     | 4/8 [00:01<00:00,  4.05it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1870. top1: 73.52. top5: 99.45. :  62%|██████▎   | 5/8 [00:01<00:00,  4.22it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1548. top1: 75.33. top5: 99.54. :  62%|██████▎   | 5/8 [00:01<00:00,  4.22it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1548. top1: 75.33. top5: 99.54. :  75%|███████▌  | 6/8 [00:01<00:00,  3.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1398. top1: 76.62. top5: 99.44. :  75%|███████▌  | 6/8 [00:01<00:00,  3.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1398. top1: 76.62. top5: 99.44. :  88%|████████▊ | 7/8 [00:01<00:00,  3.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1353. top1: 76.90. top5: 99.50. :  88%|████████▊ | 7/8 [00:02<00:00,  3.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1353. top1: 76.90. top5: 99.50. : 100%|██████████| 8/8 [00:02<00:00,  4.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1353. top1: 76.90. top5: 99.50. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]
total : 1000  current step :  700
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.25s. S_Loss: 0.9941. T_Loss: 4.3539. Mask: 0.9336. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.25s. S_Loss: 0.9941. T_Loss: 4.3539. Mask: 0.9336. :   2%|▏         | 1/50 [00:00<00:12,  4.04it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.22s. S_Loss: 1.0028. T_Loss: 4.2322. Mask: 0.9316. :   2%|▏         | 1/50 [00:00<00:12,  4.04it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.22s. S_Loss: 1.0028. T_Loss: 4.2322. Mask: 0.9316. :   4%|▍         | 2/50 [00:00<00:10,  4.70it/s]total : 1000  current step :  701
total : 1000  current step :  702
Train Iter: 703/1000. LR: 0.0439. Data: 0.22s. Batch: 0.41s. S_Loss: 0.9990. T_Loss: 4.2973. Mask: 0.9401. :   4%|▍         | 2/50 [00:01<00:10,  4.70it/s]Train Iter: 703/1000. LR: 0.0439. Data: 0.22s. Batch: 0.41s. S_Loss: 0.9990. T_Loss: 4.2973. Mask: 0.9401. :   6%|▌         | 3/50 [00:01<00:22,  2.10it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.21s. Batch: 0.45s. S_Loss: 1.0001. T_Loss: 4.3250. Mask: 0.9473. :   6%|▌         | 3/50 [00:01<00:22,  2.10it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.21s. Batch: 0.45s. S_Loss: 1.0001. T_Loss: 4.3250. Mask: 0.9473. :   8%|▊         | 4/50 [00:01<00:23,  1.93it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.17s. Batch: 0.40s. S_Loss: 1.0068. T_Loss: 4.3362. Mask: 0.9500. :   8%|▊         | 4/50 [00:02<00:23,  1.93it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.17s. Batch: 0.40s. S_Loss: 1.0068. T_Loss: 4.3362. Mask: 0.9500. :  10%|█         | 5/50 [00:02<00:18,  2.43it/s]total : 1000  current step :  703
total : 1000  current step :  704
total : 1000  current step :  705
Train Iter: 706/1000. LR: 0.0441. Data: 0.24s. Batch: 0.46s. S_Loss: 1.0011. T_Loss: 4.2984. Mask: 0.9505. :  10%|█         | 5/50 [00:02<00:18,  2.43it/s]Train Iter: 706/1000. LR: 0.0441. Data: 0.24s. Batch: 0.46s. S_Loss: 1.0011. T_Loss: 4.2984. Mask: 0.9505. :  12%|█▏        | 6/50 [00:02<00:23,  1.90it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.23s. Batch: 0.44s. S_Loss: 1.0011. T_Loss: 4.2963. Mask: 0.9487. :  12%|█▏        | 6/50 [00:03<00:23,  1.90it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.23s. Batch: 0.44s. S_Loss: 1.0011. T_Loss: 4.2963. Mask: 0.9487. :  14%|█▍        | 7/50 [00:03<00:19,  2.20it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.22s. Batch: 0.42s. S_Loss: 1.0033. T_Loss: 4.3157. Mask: 0.9478. :  14%|█▍        | 7/50 [00:03<00:19,  2.20it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.22s. Batch: 0.42s. S_Loss: 1.0033. T_Loss: 4.3157. Mask: 0.9478. :  16%|█▌        | 8/50 [00:03<00:17,  2.45it/s]total : 1000  current step :  706
total : 1000  current step :  707
total : 1000  current step :  708
Train Iter: 709/1000. LR: 0.0443. Data: 0.29s. Batch: 0.49s. S_Loss: 1.0039. T_Loss: 4.3287. Mask: 0.9479. :  16%|█▌        | 8/50 [00:04<00:17,  2.45it/s]Train Iter: 709/1000. LR: 0.0443. Data: 0.29s. Batch: 0.49s. S_Loss: 1.0039. T_Loss: 4.3287. Mask: 0.9479. :  18%|█▊        | 9/50 [00:04<00:24,  1.66it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.28s. Batch: 0.48s. S_Loss: 1.0034. T_Loss: 4.3131. Mask: 0.9473. :  18%|█▊        | 9/50 [00:04<00:24,  1.66it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.28s. Batch: 0.48s. S_Loss: 1.0034. T_Loss: 4.3131. Mask: 0.9473. :  20%|██        | 10/50 [00:04<00:21,  1.89it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.26s. Batch: 0.48s. S_Loss: 1.0051. T_Loss: 4.3378. Mask: 0.9482. :  20%|██        | 10/50 [00:05<00:21,  1.89it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.26s. Batch: 0.48s. S_Loss: 1.0051. T_Loss: 4.3378. Mask: 0.9482. :  22%|██▏       | 11/50 [00:05<00:20,  1.94it/s]total : 1000  current step :  709
total : 1000  current step :  710
total : 1000  current step :  711
Train Iter: 712/1000. LR: 0.0445. Data: 0.30s. Batch: 0.52s. S_Loss: 1.0054. T_Loss: 4.3520. Mask: 0.9469. :  22%|██▏       | 11/50 [00:06<00:20,  1.94it/s]Train Iter: 712/1000. LR: 0.0445. Data: 0.30s. Batch: 0.52s. S_Loss: 1.0054. T_Loss: 4.3520. Mask: 0.9469. :  24%|██▍       | 12/50 [00:06<00:24,  1.54it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.29s. Batch: 0.50s. S_Loss: 1.0041. T_Loss: 4.3412. Mask: 0.9450. :  24%|██▍       | 12/50 [00:06<00:24,  1.54it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.29s. Batch: 0.50s. S_Loss: 1.0041. T_Loss: 4.3412. Mask: 0.9450. :  26%|██▌       | 13/50 [00:06<00:20,  1.84it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0027. T_Loss: 4.3394. Mask: 0.9431. :  26%|██▌       | 13/50 [00:06<00:20,  1.84it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0027. T_Loss: 4.3394. Mask: 0.9431. :  28%|██▊       | 14/50 [00:06<00:17,  2.10it/s]total : 1000  current step :  712
total : 1000  current step :  713
total : 1000  current step :  714
Train Iter: 715/1000. LR: 0.0447. Data: 0.30s. Batch: 0.52s. S_Loss: 1.0021. T_Loss: 4.3480. Mask: 0.9435. :  28%|██▊       | 14/50 [00:07<00:17,  2.10it/s]Train Iter: 715/1000. LR: 0.0447. Data: 0.30s. Batch: 0.52s. S_Loss: 1.0021. T_Loss: 4.3480. Mask: 0.9435. :  30%|███       | 15/50 [00:07<00:22,  1.57it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.29s. Batch: 0.51s. S_Loss: 1.0025. T_Loss: 4.3570. Mask: 0.9417. :  30%|███       | 15/50 [00:08<00:22,  1.57it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.29s. Batch: 0.51s. S_Loss: 1.0025. T_Loss: 4.3570. Mask: 0.9417. :  32%|███▏      | 16/50 [00:08<00:18,  1.87it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9997. T_Loss: 4.3654. Mask: 0.9416. :  32%|███▏      | 16/50 [00:08<00:18,  1.87it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9997. T_Loss: 4.3654. Mask: 0.9416. :  34%|███▍      | 17/50 [00:08<00:15,  2.14it/s]total : 1000  current step :  715
total : 1000  current step :  716
total : 1000  current step :  717
Train Iter: 718/1000. LR: 0.0449. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9993. T_Loss: 4.3606. Mask: 0.9414. :  34%|███▍      | 17/50 [00:09<00:15,  2.14it/s]Train Iter: 718/1000. LR: 0.0449. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9993. T_Loss: 4.3606. Mask: 0.9414. :  36%|███▌      | 18/50 [00:09<00:22,  1.42it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0001. T_Loss: 4.3751. Mask: 0.9418. :  36%|███▌      | 18/50 [00:10<00:22,  1.42it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0001. T_Loss: 4.3751. Mask: 0.9418. :  38%|███▊      | 19/50 [00:10<00:18,  1.64it/s]total : 1000  current step :  718
total : 1000  current step :  719
Train Iter: 720/1000. LR: 0.0450. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0004. T_Loss: 4.3753. Mask: 0.9418. :  38%|███▊      | 19/50 [00:10<00:18,  1.64it/s]Train Iter: 720/1000. LR: 0.0450. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0004. T_Loss: 4.3753. Mask: 0.9418. :  40%|████      | 20/50 [00:10<00:16,  1.78it/s]total : 1000  current step :  720
Train Iter: 721/1000. LR: 0.0451. Data: 0.31s. Batch: 0.55s. S_Loss: 1.0006. T_Loss: 4.3722. Mask: 0.9407. :  40%|████      | 20/50 [00:11<00:16,  1.78it/s]Train Iter: 721/1000. LR: 0.0451. Data: 0.31s. Batch: 0.55s. S_Loss: 1.0006. T_Loss: 4.3722. Mask: 0.9407. :  42%|████▏     | 21/50 [00:11<00:20,  1.40it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.30s. Batch: 0.54s. S_Loss: 1.0012. T_Loss: 4.3913. Mask: 0.9414. :  42%|████▏     | 21/50 [00:11<00:20,  1.40it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.30s. Batch: 0.54s. S_Loss: 1.0012. T_Loss: 4.3913. Mask: 0.9414. :  44%|████▍     | 22/50 [00:11<00:16,  1.69it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0016. T_Loss: 4.3998. Mask: 0.9411. :  44%|████▍     | 22/50 [00:12<00:16,  1.69it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.29s. Batch: 0.53s. S_Loss: 1.0016. T_Loss: 4.3998. Mask: 0.9411. :  46%|████▌     | 23/50 [00:12<00:13,  1.93it/s]total : 1000  current step :  721
total : 1000  current step :  722
total : 1000  current step :  723
Train Iter: 724/1000. LR: 0.0453. Data: 0.31s. Batch: 0.55s. S_Loss: 1.0031. T_Loss: 4.4141. Mask: 0.9408. :  46%|████▌     | 23/50 [00:13<00:13,  1.93it/s]Train Iter: 724/1000. LR: 0.0453. Data: 0.31s. Batch: 0.55s. S_Loss: 1.0031. T_Loss: 4.4141. Mask: 0.9408. :  48%|████▊     | 24/50 [00:13<00:17,  1.50it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.30s. Batch: 0.54s. S_Loss: 1.0020. T_Loss: 4.4084. Mask: 0.9400. :  48%|████▊     | 24/50 [00:13<00:17,  1.50it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.30s. Batch: 0.54s. S_Loss: 1.0020. T_Loss: 4.4084. Mask: 0.9400. :  50%|█████     | 25/50 [00:13<00:13,  1.87it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.30s. Batch: 0.54s. S_Loss: 1.0012. T_Loss: 4.3978. Mask: 0.9402. :  50%|█████     | 25/50 [00:14<00:13,  1.87it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.30s. Batch: 0.54s. S_Loss: 1.0012. T_Loss: 4.3978. Mask: 0.9402. :  52%|█████▏    | 26/50 [00:14<00:12,  1.93it/s]total : 1000  current step :  724
total : 1000  current step :  725
total : 1000  current step :  726
Train Iter: 727/1000. LR: 0.0454. Data: 0.31s. Batch: 0.55s. S_Loss: 1.0017. T_Loss: 4.3920. Mask: 0.9395. :  52%|█████▏    | 26/50 [00:14<00:12,  1.93it/s]Train Iter: 727/1000. LR: 0.0454. Data: 0.31s. Batch: 0.55s. S_Loss: 1.0017. T_Loss: 4.3920. Mask: 0.9395. :  54%|█████▍    | 27/50 [00:14<00:15,  1.53it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.31s. Batch: 0.55s. S_Loss: 1.0008. T_Loss: 4.3765. Mask: 0.9392. :  54%|█████▍    | 27/50 [00:15<00:15,  1.53it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.31s. Batch: 0.55s. S_Loss: 1.0008. T_Loss: 4.3765. Mask: 0.9392. :  56%|█████▌    | 28/50 [00:15<00:12,  1.72it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.30s. Batch: 0.54s. S_Loss: 1.0008. T_Loss: 4.3744. Mask: 0.9394. :  56%|█████▌    | 28/50 [00:15<00:12,  1.72it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.30s. Batch: 0.54s. S_Loss: 1.0008. T_Loss: 4.3744. Mask: 0.9394. :  58%|█████▊    | 29/50 [00:15<00:10,  1.95it/s]total : 1000  current step :  727
total : 1000  current step :  728
total : 1000  current step :  729
Train Iter: 730/1000. LR: 0.0456. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9999. T_Loss: 4.3662. Mask: 0.9400. :  58%|█████▊    | 29/50 [00:16<00:10,  1.95it/s]Train Iter: 730/1000. LR: 0.0456. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9999. T_Loss: 4.3662. Mask: 0.9400. :  60%|██████    | 30/50 [00:16<00:12,  1.59it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9999. T_Loss: 4.3754. Mask: 0.9398. :  60%|██████    | 30/50 [00:17<00:12,  1.59it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9999. T_Loss: 4.3754. Mask: 0.9398. :  62%|██████▏   | 31/50 [00:17<00:11,  1.69it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9991. T_Loss: 4.3658. Mask: 0.9399. :  62%|██████▏   | 31/50 [00:17<00:11,  1.69it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9991. T_Loss: 4.3658. Mask: 0.9399. :  64%|██████▍   | 32/50 [00:17<00:10,  1.76it/s]total : 1000  current step :  730
total : 1000  current step :  731
total : 1000  current step :  732
Train Iter: 733/1000. LR: 0.0458. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9987. T_Loss: 4.3647. Mask: 0.9400. :  64%|██████▍   | 32/50 [00:18<00:10,  1.76it/s]Train Iter: 733/1000. LR: 0.0458. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9987. T_Loss: 4.3647. Mask: 0.9400. :  66%|██████▌   | 33/50 [00:18<00:11,  1.50it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9975. T_Loss: 4.3547. Mask: 0.9400. :  66%|██████▌   | 33/50 [00:19<00:11,  1.50it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9975. T_Loss: 4.3547. Mask: 0.9400. :  68%|██████▊   | 34/50 [00:19<00:09,  1.64it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9963. T_Loss: 4.3462. Mask: 0.9407. :  68%|██████▊   | 34/50 [00:19<00:09,  1.64it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9963. T_Loss: 4.3462. Mask: 0.9407. :  70%|███████   | 35/50 [00:19<00:07,  1.90it/s]total : 1000  current step :  733
total : 1000  current step :  734
total : 1000  current step :  735
Train Iter: 736/1000. LR: 0.0460. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9963. T_Loss: 4.3424. Mask: 0.9401. :  70%|███████   | 35/50 [00:20<00:07,  1.90it/s]Train Iter: 736/1000. LR: 0.0460. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9963. T_Loss: 4.3424. Mask: 0.9401. :  72%|███████▏  | 36/50 [00:20<00:09,  1.49it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9957. T_Loss: 4.3391. Mask: 0.9407. :  72%|███████▏  | 36/50 [00:20<00:09,  1.49it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9957. T_Loss: 4.3391. Mask: 0.9407. :  74%|███████▍  | 37/50 [00:20<00:08,  1.61it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9962. T_Loss: 4.3379. Mask: 0.9409. :  74%|███████▍  | 37/50 [00:21<00:08,  1.61it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9962. T_Loss: 4.3379. Mask: 0.9409. :  76%|███████▌  | 38/50 [00:21<00:06,  1.97it/s]total : 1000  current step :  736
total : 1000  current step :  737
total : 1000  current step :  738
Train Iter: 739/1000. LR: 0.0462. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9963. T_Loss: 4.3384. Mask: 0.9411. :  76%|███████▌  | 38/50 [00:22<00:06,  1.97it/s]Train Iter: 739/1000. LR: 0.0462. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9963. T_Loss: 4.3384. Mask: 0.9411. :  78%|███████▊  | 39/50 [00:22<00:08,  1.37it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9961. T_Loss: 4.3388. Mask: 0.9412. :  78%|███████▊  | 39/50 [00:22<00:08,  1.37it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9961. T_Loss: 4.3388. Mask: 0.9412. :  80%|████████  | 40/50 [00:22<00:05,  1.68it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9955. T_Loss: 4.3310. Mask: 0.9406. :  80%|████████  | 40/50 [00:22<00:05,  1.68it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9955. T_Loss: 4.3310. Mask: 0.9406. :  82%|████████▏ | 41/50 [00:22<00:04,  2.13it/s]total : 1000  current step :  739
total : 1000  current step :  740
total : 1000  current step :  741
Train Iter: 742/1000. LR: 0.0464. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9949. T_Loss: 4.3323. Mask: 0.9408. :  82%|████████▏ | 41/50 [00:23<00:04,  2.13it/s]Train Iter: 742/1000. LR: 0.0464. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9949. T_Loss: 4.3323. Mask: 0.9408. :  84%|████████▍ | 42/50 [00:23<00:05,  1.58it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9944. T_Loss: 4.3287. Mask: 0.9410. :  84%|████████▍ | 42/50 [00:24<00:05,  1.58it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9944. T_Loss: 4.3287. Mask: 0.9410. :  86%|████████▌ | 43/50 [00:24<00:03,  1.80it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9943. T_Loss: 4.3236. Mask: 0.9414. :  86%|████████▌ | 43/50 [00:24<00:03,  1.80it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9943. T_Loss: 4.3236. Mask: 0.9414. :  88%|████████▊ | 44/50 [00:24<00:02,  2.13it/s]total : 1000  current step :  742
total : 1000  current step :  743
total : 1000  current step :  744
Train Iter: 745/1000. LR: 0.0466. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9943. T_Loss: 4.3224. Mask: 0.9414. :  88%|████████▊ | 44/50 [00:25<00:02,  2.13it/s]Train Iter: 745/1000. LR: 0.0466. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9943. T_Loss: 4.3224. Mask: 0.9414. :  90%|█████████ | 45/50 [00:25<00:03,  1.60it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9946. T_Loss: 4.3269. Mask: 0.9411. :  90%|█████████ | 45/50 [00:25<00:03,  1.60it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9946. T_Loss: 4.3269. Mask: 0.9411. :  92%|█████████▏| 46/50 [00:25<00:02,  1.78it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9941. T_Loss: 4.3268. Mask: 0.9412. :  92%|█████████▏| 46/50 [00:26<00:02,  1.78it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9941. T_Loss: 4.3268. Mask: 0.9412. :  94%|█████████▍| 47/50 [00:26<00:01,  2.26it/s]total : 1000  current step :  745
total : 1000  current step :  746
total : 1000  current step :  747
Train Iter: 748/1000. LR: 0.0468. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9937. T_Loss: 4.3272. Mask: 0.9412. :  94%|█████████▍| 47/50 [00:27<00:01,  2.26it/s]Train Iter: 748/1000. LR: 0.0468. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9937. T_Loss: 4.3272. Mask: 0.9412. :  96%|█████████▌| 48/50 [00:27<00:01,  1.58it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9928. T_Loss: 4.3251. Mask: 0.9417. :  96%|█████████▌| 48/50 [00:27<00:01,  1.58it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9928. T_Loss: 4.3251. Mask: 0.9417. :  98%|█████████▊| 49/50 [00:27<00:00,  1.85it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9926. T_Loss: 4.3228. Mask: 0.9418. :  98%|█████████▊| 49/50 [00:27<00:00,  1.85it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9926. T_Loss: 4.3228. Mask: 0.9418. : 100%|██████████| 50/50 [00:27<00:00,  2.24it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9926. T_Loss: 4.3228. Mask: 0.9418. : 100%|██████████| 50/50 [00:27<00:00,  1.81it/s]
total : 1000  current step :  748
total : 1000  current step :  749
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.2874. top1: 68.36. top5: 99.61. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.2874. top1: 68.36. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.2230. top1: 71.48. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.2230. top1: 71.48. top5: 99.61. :  25%|██▌       | 2/8 [00:00<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.2197. top1: 71.22. top5: 99.48. :  25%|██▌       | 2/8 [00:01<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.2197. top1: 71.22. top5: 99.48. :  38%|███▊      | 3/8 [00:01<00:01,  3.03it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.2270. top1: 70.31. top5: 99.51. :  38%|███▊      | 3/8 [00:01<00:01,  3.03it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.2270. top1: 70.31. top5: 99.51. :  50%|█████     | 4/8 [00:01<00:01,  3.35it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1899. top1: 73.05. top5: 99.45. :  50%|█████     | 4/8 [00:01<00:01,  3.35it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1899. top1: 73.05. top5: 99.45. :  62%|██████▎   | 5/8 [00:01<00:00,  3.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1548. top1: 75.07. top5: 99.54. :  62%|██████▎   | 5/8 [00:02<00:00,  3.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1548. top1: 75.07. top5: 99.54. :  75%|███████▌  | 6/8 [00:02<00:00,  3.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1376. top1: 76.51. top5: 99.44. :  75%|███████▌  | 6/8 [00:02<00:00,  3.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1376. top1: 76.51. top5: 99.44. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1319. top1: 76.80. top5: 99.50. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1319. top1: 76.80. top5: 99.50. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1319. top1: 76.80. top5: 99.50. : 100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
total : 1000  current step :  750
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 0.68s. Batch: 0.84s. S_Loss: 1.0194. T_Loss: 5.1078. Mask: 0.9727. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 0.68s. Batch: 0.84s. S_Loss: 1.0194. T_Loss: 5.1078. Mask: 0.9727. :   2%|▏         | 1/50 [00:00<00:41,  1.19it/s]Train Iter: 752/1000. LR: 0.0470. Data: 0.40s. Batch: 0.57s. S_Loss: 1.0172. T_Loss: 4.8933. Mask: 0.9570. :   2%|▏         | 1/50 [00:01<00:41,  1.19it/s]Train Iter: 752/1000. LR: 0.0470. Data: 0.40s. Batch: 0.57s. S_Loss: 1.0172. T_Loss: 4.8933. Mask: 0.9570. :   4%|▍         | 2/50 [00:01<00:25,  1.90it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.29s. Batch: 0.54s. S_Loss: 0.9977. T_Loss: 4.7467. Mask: 0.9492. :   4%|▍         | 2/50 [00:01<00:25,  1.90it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.29s. Batch: 0.54s. S_Loss: 0.9977. T_Loss: 4.7467. Mask: 0.9492. :   6%|▌         | 3/50 [00:01<00:23,  2.01it/s]total : 1000  current step :  751
total : 1000  current step :  752
total : 1000  current step :  753
Train Iter: 754/1000. LR: 0.0471. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9963. T_Loss: 4.6825. Mask: 0.9492. :   6%|▌         | 3/50 [00:02<00:23,  2.01it/s]Train Iter: 754/1000. LR: 0.0471. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9963. T_Loss: 4.6825. Mask: 0.9492. :   8%|▊         | 4/50 [00:02<00:28,  1.63it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9993. T_Loss: 4.6715. Mask: 0.9484. :   8%|▊         | 4/50 [00:02<00:28,  1.63it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9993. T_Loss: 4.6715. Mask: 0.9484. :  10%|█         | 5/50 [00:02<00:22,  1.98it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.26s. Batch: 0.49s. S_Loss: 1.0026. T_Loss: 4.7231. Mask: 0.9466. :  10%|█         | 5/50 [00:02<00:22,  1.98it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.26s. Batch: 0.49s. S_Loss: 1.0026. T_Loss: 4.7231. Mask: 0.9466. :  12%|█▏        | 6/50 [00:02<00:18,  2.44it/s]total : 1000  current step :  754
total : 1000  current step :  755
total : 1000  current step :  756
Train Iter: 757/1000. LR: 0.0473. Data: 0.32s. Batch: 0.54s. S_Loss: 1.0021. T_Loss: 4.6768. Mask: 0.9442. :  12%|█▏        | 6/50 [00:03<00:18,  2.44it/s]Train Iter: 757/1000. LR: 0.0473. Data: 0.32s. Batch: 0.54s. S_Loss: 1.0021. T_Loss: 4.6768. Mask: 0.9442. :  14%|█▍        | 7/50 [00:03<00:23,  1.83it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9988. T_Loss: 4.6118. Mask: 0.9429. :  14%|█▍        | 7/50 [00:04<00:23,  1.83it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9988. T_Loss: 4.6118. Mask: 0.9429. :  16%|█▌        | 8/50 [00:04<00:20,  2.03it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.26s. Batch: 0.49s. S_Loss: 1.0008. T_Loss: 4.6227. Mask: 0.9444. :  16%|█▌        | 8/50 [00:04<00:20,  2.03it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.26s. Batch: 0.49s. S_Loss: 1.0008. T_Loss: 4.6227. Mask: 0.9444. :  18%|█▊        | 9/50 [00:04<00:17,  2.30it/s]total : 1000  current step :  757
total : 1000  current step :  758
total : 1000  current step :  759
Train Iter: 760/1000. LR: 0.0475. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0045. T_Loss: 4.5977. Mask: 0.9414. :  18%|█▊        | 9/50 [00:05<00:17,  2.30it/s]Train Iter: 760/1000. LR: 0.0475. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0045. T_Loss: 4.5977. Mask: 0.9414. :  20%|██        | 10/50 [00:05<00:29,  1.38it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.35s. Batch: 0.58s. S_Loss: 1.0072. T_Loss: 4.6174. Mask: 0.9421. :  20%|██        | 10/50 [00:06<00:29,  1.38it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.35s. Batch: 0.58s. S_Loss: 1.0072. T_Loss: 4.6174. Mask: 0.9421. :  22%|██▏       | 11/50 [00:06<00:26,  1.45it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.34s. Batch: 0.57s. S_Loss: 1.0117. T_Loss: 4.6186. Mask: 0.9424. :  22%|██▏       | 11/50 [00:06<00:26,  1.45it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.34s. Batch: 0.57s. S_Loss: 1.0117. T_Loss: 4.6186. Mask: 0.9424. :  24%|██▍       | 12/50 [00:06<00:23,  1.62it/s]total : 1000  current step :  760
total : 1000  current step :  761
total : 1000  current step :  762
Train Iter: 763/1000. LR: 0.0477. Data: 0.37s. Batch: 0.61s. S_Loss: 1.0137. T_Loss: 4.6051. Mask: 0.9414. :  24%|██▍       | 12/50 [00:07<00:23,  1.62it/s]Train Iter: 763/1000. LR: 0.0477. Data: 0.37s. Batch: 0.61s. S_Loss: 1.0137. T_Loss: 4.6051. Mask: 0.9414. :  26%|██▌       | 13/50 [00:07<00:28,  1.31it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.35s. Batch: 0.59s. S_Loss: 1.0152. T_Loss: 4.6140. Mask: 0.9417. :  26%|██▌       | 13/50 [00:08<00:28,  1.31it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.35s. Batch: 0.59s. S_Loss: 1.0152. T_Loss: 4.6140. Mask: 0.9417. :  28%|██▊       | 14/50 [00:08<00:22,  1.64it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0130. T_Loss: 4.5859. Mask: 0.9417. :  28%|██▊       | 14/50 [00:08<00:22,  1.64it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0130. T_Loss: 4.5859. Mask: 0.9417. :  30%|███       | 15/50 [00:08<00:18,  1.90it/s]total : 1000  current step :  763
total : 1000  current step :  764
total : 1000  current step :  765
Train Iter: 766/1000. LR: 0.0479. Data: 0.36s. Batch: 0.60s. S_Loss: 1.0104. T_Loss: 4.5510. Mask: 0.9419. :  30%|███       | 15/50 [00:09<00:18,  1.90it/s]Train Iter: 766/1000. LR: 0.0479. Data: 0.36s. Batch: 0.60s. S_Loss: 1.0104. T_Loss: 4.5510. Mask: 0.9419. :  32%|███▏      | 16/50 [00:09<00:23,  1.44it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.34s. Batch: 0.59s. S_Loss: 1.0104. T_Loss: 4.5392. Mask: 0.9416. :  32%|███▏      | 16/50 [00:10<00:23,  1.44it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.34s. Batch: 0.59s. S_Loss: 1.0104. T_Loss: 4.5392. Mask: 0.9416. :  34%|███▍      | 17/50 [00:10<00:19,  1.70it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.32s. Batch: 0.56s. S_Loss: 1.0090. T_Loss: 4.5193. Mask: 0.9421. :  34%|███▍      | 17/50 [00:10<00:19,  1.70it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.32s. Batch: 0.56s. S_Loss: 1.0090. T_Loss: 4.5193. Mask: 0.9421. :  36%|███▌      | 18/50 [00:10<00:14,  2.15it/s]total : 1000  current step :  766
total : 1000  current step :  767
total : 1000  current step :  768
Train Iter: 769/1000. LR: 0.0481. Data: 0.35s. Batch: 0.59s. S_Loss: 1.0097. T_Loss: 4.5070. Mask: 0.9431. :  36%|███▌      | 18/50 [00:11<00:14,  2.15it/s]Train Iter: 769/1000. LR: 0.0481. Data: 0.35s. Batch: 0.59s. S_Loss: 1.0097. T_Loss: 4.5070. Mask: 0.9431. :  38%|███▊      | 19/50 [00:11<00:20,  1.53it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0094. T_Loss: 4.4809. Mask: 0.9434. :  38%|███▊      | 19/50 [00:11<00:20,  1.53it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0094. T_Loss: 4.4809. Mask: 0.9434. :  40%|████      | 20/50 [00:11<00:15,  1.95it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.32s. Batch: 0.56s. S_Loss: 1.0090. T_Loss: 4.4629. Mask: 0.9431. :  40%|████      | 20/50 [00:11<00:15,  1.95it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.32s. Batch: 0.56s. S_Loss: 1.0090. T_Loss: 4.4629. Mask: 0.9431. :  42%|████▏     | 21/50 [00:11<00:13,  2.17it/s]total : 1000  current step :  769
total : 1000  current step :  770
total : 1000  current step :  771
Train Iter: 772/1000. LR: 0.0483. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0082. T_Loss: 4.4620. Mask: 0.9435. :  42%|████▏     | 21/50 [00:12<00:13,  2.17it/s]Train Iter: 772/1000. LR: 0.0483. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0082. T_Loss: 4.4620. Mask: 0.9435. :  44%|████▍     | 22/50 [00:12<00:16,  1.68it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.33s. Batch: 0.56s. S_Loss: 1.0074. T_Loss: 4.4569. Mask: 0.9433. :  44%|████▍     | 22/50 [00:13<00:16,  1.68it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.33s. Batch: 0.56s. S_Loss: 1.0074. T_Loss: 4.4569. Mask: 0.9433. :  46%|████▌     | 23/50 [00:13<00:13,  1.94it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.32s. Batch: 0.56s. S_Loss: 1.0059. T_Loss: 4.4340. Mask: 0.9419. :  46%|████▌     | 23/50 [00:13<00:13,  1.94it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.32s. Batch: 0.56s. S_Loss: 1.0059. T_Loss: 4.4340. Mask: 0.9419. :  48%|████▊     | 24/50 [00:13<00:13,  1.92it/s]total : 1000  current step :  772
total : 1000  current step :  773
total : 1000  current step :  774
Train Iter: 775/1000. LR: 0.0484. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0058. T_Loss: 4.4365. Mask: 0.9416. :  48%|████▊     | 24/50 [00:14<00:13,  1.92it/s]Train Iter: 775/1000. LR: 0.0484. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0058. T_Loss: 4.4365. Mask: 0.9416. :  50%|█████     | 25/50 [00:14<00:16,  1.55it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0062. T_Loss: 4.4197. Mask: 0.9411. :  50%|█████     | 25/50 [00:14<00:16,  1.55it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0062. T_Loss: 4.4197. Mask: 0.9411. :  52%|█████▏    | 26/50 [00:14<00:13,  1.82it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.33s. Batch: 0.56s. S_Loss: 1.0045. T_Loss: 4.4023. Mask: 0.9407. :  52%|█████▏    | 26/50 [00:15<00:13,  1.82it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.33s. Batch: 0.56s. S_Loss: 1.0045. T_Loss: 4.4023. Mask: 0.9407. :  54%|█████▍    | 27/50 [00:15<00:12,  1.90it/s]total : 1000  current step :  775
total : 1000  current step :  776
total : 1000  current step :  777
Train Iter: 778/1000. LR: 0.0486. Data: 0.35s. Batch: 0.58s. S_Loss: 1.0039. T_Loss: 4.3965. Mask: 0.9415. :  54%|█████▍    | 27/50 [00:16<00:12,  1.90it/s]Train Iter: 778/1000. LR: 0.0486. Data: 0.35s. Batch: 0.58s. S_Loss: 1.0039. T_Loss: 4.3965. Mask: 0.9415. :  56%|█████▌    | 28/50 [00:16<00:15,  1.46it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0028. T_Loss: 4.3889. Mask: 0.9418. :  56%|█████▌    | 28/50 [00:16<00:15,  1.46it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0028. T_Loss: 4.3889. Mask: 0.9418. :  58%|█████▊    | 29/50 [00:16<00:12,  1.64it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0016. T_Loss: 4.3780. Mask: 0.9409. :  58%|█████▊    | 29/50 [00:17<00:12,  1.64it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0016. T_Loss: 4.3780. Mask: 0.9409. :  60%|██████    | 30/50 [00:17<00:11,  1.82it/s]total : 1000  current step :  778
total : 1000  current step :  779
total : 1000  current step :  780
Train Iter: 781/1000. LR: 0.0488. Data: 0.35s. Batch: 0.59s. S_Loss: 1.0019. T_Loss: 4.3783. Mask: 0.9413. :  60%|██████    | 30/50 [00:18<00:11,  1.82it/s]Train Iter: 781/1000. LR: 0.0488. Data: 0.35s. Batch: 0.59s. S_Loss: 1.0019. T_Loss: 4.3783. Mask: 0.9413. :  62%|██████▏   | 31/50 [00:18<00:13,  1.45it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0028. T_Loss: 4.3833. Mask: 0.9409. :  62%|██████▏   | 31/50 [00:18<00:13,  1.45it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.34s. Batch: 0.58s. S_Loss: 1.0028. T_Loss: 4.3833. Mask: 0.9409. :  64%|██████▍   | 32/50 [00:18<00:10,  1.71it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0020. T_Loss: 4.3781. Mask: 0.9412. :  64%|██████▍   | 32/50 [00:18<00:10,  1.71it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.33s. Batch: 0.57s. S_Loss: 1.0020. T_Loss: 4.3781. Mask: 0.9412. :  66%|██████▌   | 33/50 [00:18<00:08,  1.93it/s]total : 1000  current step :  781
total : 1000  current step :  782
total : 1000  current step :  783
Train Iter: 784/1000. LR: 0.0490. Data: 0.35s. Batch: 0.58s. S_Loss: 1.0004. T_Loss: 4.3767. Mask: 0.9418. :  66%|██████▌   | 33/50 [00:19<00:08,  1.93it/s]Train Iter: 784/1000. LR: 0.0490. Data: 0.35s. Batch: 0.58s. S_Loss: 1.0004. T_Loss: 4.3767. Mask: 0.9418. :  68%|██████▊   | 34/50 [00:19<00:10,  1.55it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.34s. Batch: 0.57s. S_Loss: 0.9999. T_Loss: 4.3860. Mask: 0.9419. :  68%|██████▊   | 34/50 [00:20<00:10,  1.55it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.34s. Batch: 0.57s. S_Loss: 0.9999. T_Loss: 4.3860. Mask: 0.9419. :  70%|███████   | 35/50 [00:20<00:07,  1.93it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.34s. Batch: 0.57s. S_Loss: 0.9989. T_Loss: 4.3913. Mask: 0.9425. :  70%|███████   | 35/50 [00:20<00:07,  1.93it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.34s. Batch: 0.57s. S_Loss: 0.9989. T_Loss: 4.3913. Mask: 0.9425. :  72%|███████▏  | 36/50 [00:20<00:07,  1.97it/s]total : 1000  current step :  784
total : 1000  current step :  785
total : 1000  current step :  786
Train Iter: 787/1000. LR: 0.0492. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9981. T_Loss: 4.3897. Mask: 0.9432. :  72%|███████▏  | 36/50 [00:21<00:07,  1.97it/s]Train Iter: 787/1000. LR: 0.0492. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9981. T_Loss: 4.3897. Mask: 0.9432. :  74%|███████▍  | 37/50 [00:21<00:08,  1.49it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9977. T_Loss: 4.3871. Mask: 0.9427. :  74%|███████▍  | 37/50 [00:22<00:08,  1.49it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9977. T_Loss: 4.3871. Mask: 0.9427. :  76%|███████▌  | 38/50 [00:22<00:07,  1.51it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9968. T_Loss: 4.3908. Mask: 0.9428. :  76%|███████▌  | 38/50 [00:22<00:07,  1.51it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9968. T_Loss: 4.3908. Mask: 0.9428. :  78%|███████▊  | 39/50 [00:22<00:05,  1.87it/s]total : 1000  current step :  787
total : 1000  current step :  788
total : 1000  current step :  789
Train Iter: 790/1000. LR: 0.0494. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9964. T_Loss: 4.3961. Mask: 0.9423. :  78%|███████▊  | 39/50 [00:23<00:05,  1.87it/s]Train Iter: 790/1000. LR: 0.0494. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9964. T_Loss: 4.3961. Mask: 0.9423. :  80%|████████  | 40/50 [00:23<00:06,  1.58it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9969. T_Loss: 4.4032. Mask: 0.9425. :  80%|████████  | 40/50 [00:23<00:06,  1.58it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9969. T_Loss: 4.4032. Mask: 0.9425. :  82%|████████▏ | 41/50 [00:23<00:04,  1.83it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.33s. Batch: 0.57s. S_Loss: 0.9972. T_Loss: 4.4099. Mask: 0.9421. :  82%|████████▏ | 41/50 [00:24<00:04,  1.83it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.33s. Batch: 0.57s. S_Loss: 0.9972. T_Loss: 4.4099. Mask: 0.9421. :  84%|████████▍ | 42/50 [00:24<00:04,  1.90it/s]total : 1000  current step :  790
total : 1000  current step :  791
total : 1000  current step :  792
Train Iter: 793/1000. LR: 0.0496. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9967. T_Loss: 4.4069. Mask: 0.9419. :  84%|████████▍ | 42/50 [00:25<00:04,  1.90it/s]Train Iter: 793/1000. LR: 0.0496. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9967. T_Loss: 4.4069. Mask: 0.9419. :  86%|████████▌ | 43/50 [00:25<00:05,  1.39it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9966. T_Loss: 4.4159. Mask: 0.9415. :  86%|████████▌ | 43/50 [00:25<00:05,  1.39it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9966. T_Loss: 4.4159. Mask: 0.9415. :  88%|████████▊ | 44/50 [00:25<00:03,  1.63it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9967. T_Loss: 4.4161. Mask: 0.9412. :  88%|████████▊ | 44/50 [00:26<00:03,  1.63it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9967. T_Loss: 4.4161. Mask: 0.9412. :  90%|█████████ | 45/50 [00:26<00:02,  1.71it/s]total : 1000  current step :  793
total : 1000  current step :  794
total : 1000  current step :  795
Train Iter: 796/1000. LR: 0.0498. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9965. T_Loss: 4.4236. Mask: 0.9418. :  90%|█████████ | 45/50 [00:27<00:02,  1.71it/s]Train Iter: 796/1000. LR: 0.0498. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9965. T_Loss: 4.4236. Mask: 0.9418. :  92%|█████████▏| 46/50 [00:27<00:02,  1.45it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9971. T_Loss: 4.4289. Mask: 0.9420. :  92%|█████████▏| 46/50 [00:27<00:02,  1.45it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9971. T_Loss: 4.4289. Mask: 0.9420. :  94%|█████████▍| 47/50 [00:27<00:01,  1.67it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9966. T_Loss: 4.4226. Mask: 0.9415. :  94%|█████████▍| 47/50 [00:28<00:01,  1.67it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9966. T_Loss: 4.4226. Mask: 0.9415. :  96%|█████████▌| 48/50 [00:28<00:01,  1.68it/s]total : 1000  current step :  796
total : 1000  current step :  797
total : 1000  current step :  798
Train Iter: 799/1000. LR: 0.0499. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9969. T_Loss: 4.4215. Mask: 0.9412. :  96%|█████████▌| 48/50 [00:29<00:01,  1.68it/s]Train Iter: 799/1000. LR: 0.0499. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9969. T_Loss: 4.4215. Mask: 0.9412. :  98%|█████████▊| 49/50 [00:29<00:00,  1.42it/s]total : 1000  current step :  799
Train Iter: 800/1000. LR: 0.0500. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9966. T_Loss: 4.4201. Mask: 0.9413. :  98%|█████████▊| 49/50 [00:29<00:00,  1.42it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9966. T_Loss: 4.4201. Mask: 0.9413. : 100%|██████████| 50/50 [00:29<00:00,  1.48it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9966. T_Loss: 4.4201. Mask: 0.9413. : 100%|██████████| 50/50 [00:29<00:00,  1.68it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.2956. top1: 68.75. top5: 99.61. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.2956. top1: 68.75. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.2340. top1: 71.29. top5: 99.41. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.2340. top1: 71.29. top5: 99.41. :  25%|██▌       | 2/8 [00:00<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.2310. top1: 70.96. top5: 99.35. :  25%|██▌       | 2/8 [00:01<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.2310. top1: 70.96. top5: 99.35. :  38%|███▊      | 3/8 [00:01<00:01,  3.25it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2391. top1: 70.12. top5: 99.32. :  38%|███▊      | 3/8 [00:01<00:01,  3.25it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2391. top1: 70.12. top5: 99.32. :  50%|█████     | 4/8 [00:01<00:01,  3.57it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.1951. top1: 73.20. top5: 99.22. :  50%|█████     | 4/8 [00:01<00:01,  3.57it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.1951. top1: 73.20. top5: 99.22. :  62%|██████▎   | 5/8 [00:01<00:00,  3.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1559. top1: 75.26. top5: 99.35. :  62%|██████▎   | 5/8 [00:01<00:00,  3.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1559. top1: 75.26. top5: 99.35. :  75%|███████▌  | 6/8 [00:01<00:00,  3.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1352. top1: 76.79. top5: 99.27. :  75%|███████▌  | 6/8 [00:01<00:00,  3.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1352. top1: 76.79. top5: 99.27. :  88%|████████▊ | 7/8 [00:01<00:00,  4.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1275. top1: 77.15. top5: 99.35. :  88%|████████▊ | 7/8 [00:02<00:00,  4.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1275. top1: 77.15. top5: 99.35. : 100%|██████████| 8/8 [00:02<00:00,  4.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1275. top1: 77.15. top5: 99.35. : 100%|██████████| 8/8 [00:02<00:00,  3.53it/s]
total : 1000  current step :  800
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.01s. Batch: 0.25s. S_Loss: 1.0191. T_Loss: 4.7892. Mask: 0.9727. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.01s. Batch: 0.25s. S_Loss: 1.0191. T_Loss: 4.7892. Mask: 0.9727. :   2%|▏         | 1/50 [00:00<00:12,  4.02it/s]total : 1000  current step :  801
Train Iter: 802/1000. LR: 0.0500. Data: 0.37s. Batch: 0.68s. S_Loss: 1.0034. T_Loss: 4.5375. Mask: 0.9551. :   2%|▏         | 1/50 [00:01<00:12,  4.02it/s]Train Iter: 802/1000. LR: 0.0500. Data: 0.37s. Batch: 0.68s. S_Loss: 1.0034. T_Loss: 4.5375. Mask: 0.9551. :   4%|▍         | 2/50 [00:01<00:36,  1.31it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.25s. Batch: 0.55s. S_Loss: 1.0099. T_Loss: 4.5878. Mask: 0.9518. :   4%|▍         | 2/50 [00:01<00:36,  1.31it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.25s. Batch: 0.55s. S_Loss: 1.0099. T_Loss: 4.5878. Mask: 0.9518. :   6%|▌         | 3/50 [00:01<00:25,  1.83it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.19s. Batch: 0.49s. S_Loss: 0.9939. T_Loss: 4.4330. Mask: 0.9482. :   6%|▌         | 3/50 [00:01<00:25,  1.83it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.19s. Batch: 0.49s. S_Loss: 0.9939. T_Loss: 4.4330. Mask: 0.9482. :   8%|▊         | 4/50 [00:01<00:20,  2.20it/s]total : 1000  current step :  802
total : 1000  current step :  803
total : 1000  current step :  804
Train Iter: 805/1000. LR: 0.0499. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9943. T_Loss: 4.4041. Mask: 0.9477. :   8%|▊         | 4/50 [00:02<00:20,  2.20it/s]Train Iter: 805/1000. LR: 0.0499. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9943. T_Loss: 4.4041. Mask: 0.9477. :  10%|█         | 5/50 [00:02<00:28,  1.57it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.26s. Batch: 0.54s. S_Loss: 0.9941. T_Loss: 4.4204. Mask: 0.9505. :  10%|█         | 5/50 [00:03<00:28,  1.57it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.26s. Batch: 0.54s. S_Loss: 0.9941. T_Loss: 4.4204. Mask: 0.9505. :  12%|█▏        | 6/50 [00:03<00:23,  1.87it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.24s. Batch: 0.53s. S_Loss: 0.9946. T_Loss: 4.4177. Mask: 0.9459. :  12%|█▏        | 6/50 [00:03<00:23,  1.87it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.24s. Batch: 0.53s. S_Loss: 0.9946. T_Loss: 4.4177. Mask: 0.9459. :  14%|█▍        | 7/50 [00:03<00:21,  1.99it/s]total : 1000  current step :  805
total : 1000  current step :  806
total : 1000  current step :  807
Train Iter: 808/1000. LR: 0.0498. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9903. T_Loss: 4.4213. Mask: 0.9478. :  14%|█▍        | 7/50 [00:04<00:21,  1.99it/s]Train Iter: 808/1000. LR: 0.0498. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9903. T_Loss: 4.4213. Mask: 0.9478. :  16%|█▌        | 8/50 [00:04<00:26,  1.59it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.28s. Batch: 0.56s. S_Loss: 0.9853. T_Loss: 4.3839. Mask: 0.9501. :  16%|█▌        | 8/50 [00:05<00:26,  1.59it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.28s. Batch: 0.56s. S_Loss: 0.9853. T_Loss: 4.3839. Mask: 0.9501. :  18%|█▊        | 9/50 [00:05<00:23,  1.74it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.26s. Batch: 0.54s. S_Loss: 0.9912. T_Loss: 4.3626. Mask: 0.9473. :  18%|█▊        | 9/50 [00:05<00:23,  1.74it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.26s. Batch: 0.54s. S_Loss: 0.9912. T_Loss: 4.3626. Mask: 0.9473. :  20%|██        | 10/50 [00:05<00:20,  1.94it/s]total : 1000  current step :  808
total : 1000  current step :  809
total : 1000  current step :  810
Train Iter: 811/1000. LR: 0.0496. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9922. T_Loss: 4.3578. Mask: 0.9457. :  20%|██        | 10/50 [00:06<00:20,  1.94it/s]Train Iter: 811/1000. LR: 0.0496. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9922. T_Loss: 4.3578. Mask: 0.9457. :  22%|██▏       | 11/50 [00:06<00:27,  1.44it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9926. T_Loss: 4.3493. Mask: 0.9450. :  22%|██▏       | 11/50 [00:06<00:27,  1.44it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9926. T_Loss: 4.3493. Mask: 0.9450. :  24%|██▍       | 12/50 [00:06<00:23,  1.64it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.27s. Batch: 0.56s. S_Loss: 0.9912. T_Loss: 4.3292. Mask: 0.9456. :  24%|██▍       | 12/50 [00:07<00:23,  1.64it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.27s. Batch: 0.56s. S_Loss: 0.9912. T_Loss: 4.3292. Mask: 0.9456. :  26%|██▌       | 13/50 [00:07<00:19,  1.88it/s]total : 1000  current step :  811
total : 1000  current step :  812
total : 1000  current step :  813
Train Iter: 814/1000. LR: 0.0494. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9888. T_Loss: 4.2932. Mask: 0.9461. :  26%|██▌       | 13/50 [00:08<00:19,  1.88it/s]Train Iter: 814/1000. LR: 0.0494. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9888. T_Loss: 4.2932. Mask: 0.9461. :  28%|██▊       | 14/50 [00:08<00:23,  1.55it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9883. T_Loss: 4.2787. Mask: 0.9464. :  28%|██▊       | 14/50 [00:08<00:23,  1.55it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9883. T_Loss: 4.2787. Mask: 0.9464. :  30%|███       | 15/50 [00:08<00:20,  1.68it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9874. T_Loss: 4.2991. Mask: 0.9482. :  30%|███       | 15/50 [00:09<00:20,  1.68it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9874. T_Loss: 4.2991. Mask: 0.9482. :  32%|███▏      | 16/50 [00:09<00:20,  1.65it/s]total : 1000  current step :  814
total : 1000  current step :  815
total : 1000  current step :  816
Train Iter: 817/1000. LR: 0.0491. Data: 0.32s. Batch: 0.61s. S_Loss: 0.9871. T_Loss: 4.2867. Mask: 0.9478. :  32%|███▏      | 16/50 [00:10<00:20,  1.65it/s]Train Iter: 817/1000. LR: 0.0491. Data: 0.32s. Batch: 0.61s. S_Loss: 0.9871. T_Loss: 4.2867. Mask: 0.9478. :  34%|███▍      | 17/50 [00:10<00:24,  1.34it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9886. T_Loss: 4.2824. Mask: 0.9475. :  34%|███▍      | 17/50 [00:10<00:24,  1.34it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9886. T_Loss: 4.2824. Mask: 0.9475. :  36%|███▌      | 18/50 [00:10<00:19,  1.62it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9882. T_Loss: 4.2956. Mask: 0.9480. :  36%|███▌      | 18/50 [00:11<00:19,  1.62it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9882. T_Loss: 4.2956. Mask: 0.9480. :  38%|███▊      | 19/50 [00:11<00:17,  1.82it/s]total : 1000  current step :  817
total : 1000  current step :  818
total : 1000  current step :  819
Train Iter: 820/1000. LR: 0.0488. Data: 0.31s. Batch: 0.61s. S_Loss: 0.9886. T_Loss: 4.2986. Mask: 0.9486. :  38%|███▊      | 19/50 [00:12<00:17,  1.82it/s]Train Iter: 820/1000. LR: 0.0488. Data: 0.31s. Batch: 0.61s. S_Loss: 0.9886. T_Loss: 4.2986. Mask: 0.9486. :  40%|████      | 20/50 [00:12<00:21,  1.41it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.30s. Batch: 0.60s. S_Loss: 0.9890. T_Loss: 4.3005. Mask: 0.9485. :  40%|████      | 20/50 [00:12<00:21,  1.41it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.30s. Batch: 0.60s. S_Loss: 0.9890. T_Loss: 4.3005. Mask: 0.9485. :  42%|████▏     | 21/50 [00:12<00:17,  1.66it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9893. T_Loss: 4.2945. Mask: 0.9480. :  42%|████▏     | 21/50 [00:12<00:17,  1.66it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9893. T_Loss: 4.2945. Mask: 0.9480. :  44%|████▍     | 22/50 [00:12<00:14,  1.93it/s]total : 1000  current step :  820
total : 1000  current step :  821
total : 1000  current step :  822
Train Iter: 823/1000. LR: 0.0484. Data: 0.30s. Batch: 0.61s. S_Loss: 0.9865. T_Loss: 4.2728. Mask: 0.9474. :  44%|████▍     | 22/50 [00:13<00:14,  1.93it/s]Train Iter: 823/1000. LR: 0.0484. Data: 0.30s. Batch: 0.61s. S_Loss: 0.9865. T_Loss: 4.2728. Mask: 0.9474. :  46%|████▌     | 23/50 [00:13<00:18,  1.44it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.29s. Batch: 0.59s. S_Loss: 0.9863. T_Loss: 4.2679. Mask: 0.9468. :  46%|████▌     | 23/50 [00:14<00:18,  1.44it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.29s. Batch: 0.59s. S_Loss: 0.9863. T_Loss: 4.2679. Mask: 0.9468. :  48%|████▊     | 24/50 [00:14<00:14,  1.80it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.28s. Batch: 0.58s. S_Loss: 0.9904. T_Loss: 4.2627. Mask: 0.9452. :  48%|████▊     | 24/50 [00:14<00:14,  1.80it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.28s. Batch: 0.58s. S_Loss: 0.9904. T_Loss: 4.2627. Mask: 0.9452. :  50%|█████     | 25/50 [00:14<00:12,  2.06it/s]total : 1000  current step :  823
total : 1000  current step :  824
total : 1000  current step :  825
Train Iter: 826/1000. LR: 0.0479. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9913. T_Loss: 4.2559. Mask: 0.9449. :  50%|█████     | 25/50 [00:15<00:12,  2.06it/s]Train Iter: 826/1000. LR: 0.0479. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9913. T_Loss: 4.2559. Mask: 0.9449. :  52%|█████▏    | 26/50 [00:15<00:14,  1.61it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9911. T_Loss: 4.2444. Mask: 0.9452. :  52%|█████▏    | 26/50 [00:15<00:14,  1.61it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9911. T_Loss: 4.2444. Mask: 0.9452. :  54%|█████▍    | 27/50 [00:15<00:12,  1.90it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9919. T_Loss: 4.2410. Mask: 0.9453. :  54%|█████▍    | 27/50 [00:16<00:12,  1.90it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9919. T_Loss: 4.2410. Mask: 0.9453. :  56%|█████▌    | 28/50 [00:16<00:10,  2.19it/s]total : 1000  current step :  826
total : 1000  current step :  827
total : 1000  current step :  828
Train Iter: 829/1000. LR: 0.0475. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9918. T_Loss: 4.2293. Mask: 0.9453. :  56%|█████▌    | 28/50 [00:17<00:10,  2.19it/s]Train Iter: 829/1000. LR: 0.0475. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9918. T_Loss: 4.2293. Mask: 0.9453. :  58%|█████▊    | 29/50 [00:17<00:12,  1.64it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9917. T_Loss: 4.2262. Mask: 0.9458. :  58%|█████▊    | 29/50 [00:17<00:12,  1.64it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9917. T_Loss: 4.2262. Mask: 0.9458. :  60%|██████    | 30/50 [00:17<00:12,  1.63it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9933. T_Loss: 4.2283. Mask: 0.9458. :  60%|██████    | 30/50 [00:17<00:12,  1.63it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9933. T_Loss: 4.2283. Mask: 0.9458. :  62%|██████▏   | 31/50 [00:17<00:09,  1.97it/s]total : 1000  current step :  829
total : 1000  current step :  830
total : 1000  current step :  831
Train Iter: 832/1000. LR: 0.0469. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9948. T_Loss: 4.2375. Mask: 0.9462. :  62%|██████▏   | 31/50 [00:18<00:09,  1.97it/s]Train Iter: 832/1000. LR: 0.0469. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9948. T_Loss: 4.2375. Mask: 0.9462. :  64%|██████▍   | 32/50 [00:18<00:10,  1.66it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9965. T_Loss: 4.2504. Mask: 0.9467. :  64%|██████▍   | 32/50 [00:19<00:10,  1.66it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9965. T_Loss: 4.2504. Mask: 0.9467. :  66%|██████▌   | 33/50 [00:19<00:09,  1.79it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9967. T_Loss: 4.2562. Mask: 0.9467. :  66%|██████▌   | 33/50 [00:19<00:09,  1.79it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9967. T_Loss: 4.2562. Mask: 0.9467. :  68%|██████▊   | 34/50 [00:19<00:08,  1.99it/s]total : 1000  current step :  832
total : 1000  current step :  833
total : 1000  current step :  834
Train Iter: 835/1000. LR: 0.0463. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9974. T_Loss: 4.2513. Mask: 0.9463. :  68%|██████▊   | 34/50 [00:20<00:08,  1.99it/s]Train Iter: 835/1000. LR: 0.0463. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9974. T_Loss: 4.2513. Mask: 0.9463. :  70%|███████   | 35/50 [00:20<00:09,  1.64it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9961. T_Loss: 4.2532. Mask: 0.9466. :  70%|███████   | 35/50 [00:20<00:09,  1.64it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9961. T_Loss: 4.2532. Mask: 0.9466. :  72%|███████▏  | 36/50 [00:20<00:07,  1.83it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9956. T_Loss: 4.2472. Mask: 0.9464. :  72%|███████▏  | 36/50 [00:21<00:07,  1.83it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9956. T_Loss: 4.2472. Mask: 0.9464. :  74%|███████▍  | 37/50 [00:21<00:06,  1.87it/s]total : 1000  current step :  835
total : 1000  current step :  836
total : 1000  current step :  837
Train Iter: 838/1000. LR: 0.0457. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9959. T_Loss: 4.2444. Mask: 0.9466. :  74%|███████▍  | 37/50 [00:22<00:06,  1.87it/s]Train Iter: 838/1000. LR: 0.0457. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9959. T_Loss: 4.2444. Mask: 0.9466. :  76%|███████▌  | 38/50 [00:22<00:07,  1.54it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9958. T_Loss: 4.2472. Mask: 0.9468. :  76%|███████▌  | 38/50 [00:22<00:07,  1.54it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9958. T_Loss: 4.2472. Mask: 0.9468. :  78%|███████▊  | 39/50 [00:22<00:05,  1.85it/s]total : 1000  current step :  838
total : 1000  current step :  839
Train Iter: 840/1000. LR: 0.0452. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9951. T_Loss: 4.2458. Mask: 0.9471. :  78%|███████▊  | 39/50 [00:22<00:05,  1.85it/s]Train Iter: 840/1000. LR: 0.0452. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9951. T_Loss: 4.2458. Mask: 0.9471. :  80%|████████  | 40/50 [00:22<00:05,  1.99it/s]total : 1000  current step :  840
Train Iter: 841/1000. LR: 0.0450. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9959. T_Loss: 4.2543. Mask: 0.9469. :  80%|████████  | 40/50 [00:23<00:05,  1.99it/s]Train Iter: 841/1000. LR: 0.0450. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9959. T_Loss: 4.2543. Mask: 0.9469. :  82%|████████▏ | 41/50 [00:23<00:05,  1.59it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9966. T_Loss: 4.2548. Mask: 0.9471. :  82%|████████▏ | 41/50 [00:24<00:05,  1.59it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9966. T_Loss: 4.2548. Mask: 0.9471. :  84%|████████▍ | 42/50 [00:24<00:03,  2.03it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9973. T_Loss: 4.2701. Mask: 0.9473. :  84%|████████▍ | 42/50 [00:24<00:03,  2.03it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9973. T_Loss: 4.2701. Mask: 0.9473. :  86%|████████▌ | 43/50 [00:24<00:03,  2.02it/s]total : 1000  current step :  841
total : 1000  current step :  842
total : 1000  current step :  843
Train Iter: 844/1000. LR: 0.0443. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9984. T_Loss: 4.2821. Mask: 0.9474. :  86%|████████▌ | 43/50 [00:25<00:03,  2.02it/s]Train Iter: 844/1000. LR: 0.0443. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9984. T_Loss: 4.2821. Mask: 0.9474. :  88%|████████▊ | 44/50 [00:25<00:03,  1.50it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9988. T_Loss: 4.2829. Mask: 0.9473. :  88%|████████▊ | 44/50 [00:25<00:03,  1.50it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9988. T_Loss: 4.2829. Mask: 0.9473. :  90%|█████████ | 45/50 [00:25<00:02,  1.81it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9980. T_Loss: 4.2861. Mask: 0.9478. :  90%|█████████ | 45/50 [00:26<00:02,  1.81it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9980. T_Loss: 4.2861. Mask: 0.9478. :  92%|█████████▏| 46/50 [00:26<00:01,  2.02it/s]total : 1000  current step :  844
total : 1000  current step :  845
total : 1000  current step :  846
Train Iter: 847/1000. LR: 0.0435. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9972. T_Loss: 4.2896. Mask: 0.9480. :  92%|█████████▏| 46/50 [00:27<00:01,  2.02it/s]Train Iter: 847/1000. LR: 0.0435. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9972. T_Loss: 4.2896. Mask: 0.9480. :  94%|█████████▍| 47/50 [00:27<00:01,  1.74it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9970. T_Loss: 4.2864. Mask: 0.9478. :  94%|█████████▍| 47/50 [00:27<00:01,  1.74it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9970. T_Loss: 4.2864. Mask: 0.9478. :  96%|█████████▌| 48/50 [00:27<00:00,  2.04it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.28s. Batch: 0.56s. S_Loss: 0.9970. T_Loss: 4.2819. Mask: 0.9476. :  96%|█████████▌| 48/50 [00:27<00:00,  2.04it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.28s. Batch: 0.56s. S_Loss: 0.9970. T_Loss: 4.2819. Mask: 0.9476. :  98%|█████████▊| 49/50 [00:27<00:00,  2.28it/s]total : 1000  current step :  847
total : 1000  current step :  848
total : 1000  current step :  849
Train Iter: 850/1000. LR: 0.0427. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9964. T_Loss: 4.2777. Mask: 0.9470. :  98%|█████████▊| 49/50 [00:28<00:00,  2.28it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9964. T_Loss: 4.2777. Mask: 0.9470. : 100%|██████████| 50/50 [00:28<00:00,  1.91it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9964. T_Loss: 4.2777. Mask: 0.9470. : 100%|██████████| 50/50 [00:28<00:00,  1.76it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.2381. top1: 71.09. top5: 99.61. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.2381. top1: 71.09. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:02,  2.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.1849. top1: 73.63. top5: 99.41. :  12%|█▎        | 1/8 [00:00<00:02,  2.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.1849. top1: 73.63. top5: 99.41. :  25%|██▌       | 2/8 [00:00<00:01,  3.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1839. top1: 73.70. top5: 99.35. :  25%|██▌       | 2/8 [00:00<00:01,  3.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1839. top1: 73.70. top5: 99.35. :  38%|███▊      | 3/8 [00:00<00:01,  4.04it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1944. top1: 72.27. top5: 99.32. :  38%|███▊      | 3/8 [00:01<00:01,  4.04it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1944. top1: 72.27. top5: 99.32. :  50%|█████     | 4/8 [00:01<00:00,  4.30it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1651. top1: 74.45. top5: 99.22. :  50%|█████     | 4/8 [00:01<00:00,  4.30it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1651. top1: 74.45. top5: 99.22. :  62%|██████▎   | 5/8 [00:01<00:00,  4.52it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1370. top1: 76.37. top5: 99.35. :  62%|██████▎   | 5/8 [00:01<00:00,  4.52it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1370. top1: 76.37. top5: 99.35. :  75%|███████▌  | 6/8 [00:01<00:00,  4.59it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.1231. top1: 77.40. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  4.59it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.1231. top1: 77.40. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  4.61it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.1195. top1: 77.55. top5: 99.30. :  88%|████████▊ | 7/8 [00:01<00:00,  4.61it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.1195. top1: 77.55. top5: 99.30. : 100%|██████████| 8/8 [00:01<00:00,  5.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.1195. top1: 77.55. top5: 99.30. : 100%|██████████| 8/8 [00:01<00:00,  4.21it/s]
total : 1000  current step :  850
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.44s. S_Loss: 0.9836. T_Loss: 4.1690. Mask: 0.9492. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.44s. S_Loss: 0.9836. T_Loss: 4.1690. Mask: 0.9492. :   2%|▏         | 1/50 [00:00<00:21,  2.27it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.40s. S_Loss: 0.9748. T_Loss: 4.0874. Mask: 0.9512. :   2%|▏         | 1/50 [00:00<00:21,  2.27it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.40s. S_Loss: 0.9748. T_Loss: 4.0874. Mask: 0.9512. :   4%|▍         | 2/50 [00:00<00:18,  2.53it/s]total : 1000  current step :  851
total : 1000  current step :  852
Train Iter: 853/1000. LR: 0.0418. Data: 0.21s. Batch: 0.56s. S_Loss: 0.9824. T_Loss: 4.2108. Mask: 0.9544. :   4%|▍         | 2/50 [00:01<00:18,  2.53it/s]Train Iter: 853/1000. LR: 0.0418. Data: 0.21s. Batch: 0.56s. S_Loss: 0.9824. T_Loss: 4.2108. Mask: 0.9544. :   6%|▌         | 3/50 [00:01<00:28,  1.63it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.17s. Batch: 0.50s. S_Loss: 0.9749. T_Loss: 4.1714. Mask: 0.9590. :   6%|▌         | 3/50 [00:01<00:28,  1.63it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.17s. Batch: 0.50s. S_Loss: 0.9749. T_Loss: 4.1714. Mask: 0.9590. :   8%|▊         | 4/50 [00:01<00:22,  2.01it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.14s. Batch: 0.45s. S_Loss: 0.9750. T_Loss: 4.1808. Mask: 0.9586. :   8%|▊         | 4/50 [00:02<00:22,  2.01it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.14s. Batch: 0.45s. S_Loss: 0.9750. T_Loss: 4.1808. Mask: 0.9586. :  10%|█         | 5/50 [00:02<00:18,  2.42it/s]total : 1000  current step :  853
total : 1000  current step :  854
total : 1000  current step :  855
Train Iter: 856/1000. LR: 0.0409. Data: 0.20s. Batch: 0.51s. S_Loss: 0.9791. T_Loss: 4.2435. Mask: 0.9583. :  10%|█         | 5/50 [00:03<00:18,  2.42it/s]Train Iter: 856/1000. LR: 0.0409. Data: 0.20s. Batch: 0.51s. S_Loss: 0.9791. T_Loss: 4.2435. Mask: 0.9583. :  12%|█▏        | 6/50 [00:03<00:23,  1.85it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.17s. Batch: 0.47s. S_Loss: 0.9789. T_Loss: 4.2189. Mask: 0.9565. :  12%|█▏        | 6/50 [00:03<00:23,  1.85it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.17s. Batch: 0.47s. S_Loss: 0.9789. T_Loss: 4.2189. Mask: 0.9565. :  14%|█▍        | 7/50 [00:03<00:19,  2.21it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.15s. Batch: 0.46s. S_Loss: 0.9835. T_Loss: 4.2488. Mask: 0.9551. :  14%|█▍        | 7/50 [00:03<00:19,  2.21it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.15s. Batch: 0.46s. S_Loss: 0.9835. T_Loss: 4.2488. Mask: 0.9551. :  16%|█▌        | 8/50 [00:03<00:18,  2.28it/s]total : 1000  current step :  856
total : 1000  current step :  857
total : 1000  current step :  858
Train Iter: 859/1000. LR: 0.0400. Data: 0.19s. Batch: 0.50s. S_Loss: 0.9878. T_Loss: 4.2646. Mask: 0.9562. :  16%|█▌        | 8/50 [00:04<00:18,  2.28it/s]Train Iter: 859/1000. LR: 0.0400. Data: 0.19s. Batch: 0.50s. S_Loss: 0.9878. T_Loss: 4.2646. Mask: 0.9562. :  18%|█▊        | 9/50 [00:04<00:21,  1.87it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.17s. Batch: 0.47s. S_Loss: 0.9846. T_Loss: 4.2517. Mask: 0.9563. :  18%|█▊        | 9/50 [00:04<00:21,  1.87it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.17s. Batch: 0.47s. S_Loss: 0.9846. T_Loss: 4.2517. Mask: 0.9563. :  20%|██        | 10/50 [00:04<00:17,  2.28it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.16s. Batch: 0.46s. S_Loss: 0.9843. T_Loss: 4.2574. Mask: 0.9570. :  20%|██        | 10/50 [00:05<00:17,  2.28it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.16s. Batch: 0.46s. S_Loss: 0.9843. T_Loss: 4.2574. Mask: 0.9570. :  22%|██▏       | 11/50 [00:05<00:15,  2.47it/s]total : 1000  current step :  859
total : 1000  current step :  860
total : 1000  current step :  861
Train Iter: 862/1000. LR: 0.0391. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9831. T_Loss: 4.2406. Mask: 0.9557. :  22%|██▏       | 11/50 [00:05<00:15,  2.47it/s]Train Iter: 862/1000. LR: 0.0391. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9831. T_Loss: 4.2406. Mask: 0.9557. :  24%|██▍       | 12/50 [00:05<00:21,  1.78it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.18s. Batch: 0.48s. S_Loss: 0.9833. T_Loss: 4.2376. Mask: 0.9546. :  24%|██▍       | 12/50 [00:06<00:21,  1.78it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.18s. Batch: 0.48s. S_Loss: 0.9833. T_Loss: 4.2376. Mask: 0.9546. :  26%|██▌       | 13/50 [00:06<00:17,  2.13it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.17s. Batch: 0.46s. S_Loss: 0.9838. T_Loss: 4.2390. Mask: 0.9537. :  26%|██▌       | 13/50 [00:06<00:17,  2.13it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.17s. Batch: 0.46s. S_Loss: 0.9838. T_Loss: 4.2390. Mask: 0.9537. :  28%|██▊       | 14/50 [00:06<00:15,  2.36it/s]total : 1000  current step :  862
total : 1000  current step :  863
total : 1000  current step :  864
Train Iter: 865/1000. LR: 0.0381. Data: 0.19s. Batch: 0.49s. S_Loss: 0.9798. T_Loss: 4.2243. Mask: 0.9539. :  28%|██▊       | 14/50 [00:07<00:15,  2.36it/s]Train Iter: 865/1000. LR: 0.0381. Data: 0.19s. Batch: 0.49s. S_Loss: 0.9798. T_Loss: 4.2243. Mask: 0.9539. :  30%|███       | 15/50 [00:07<00:19,  1.76it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.18s. Batch: 0.48s. S_Loss: 0.9801. T_Loss: 4.2277. Mask: 0.9517. :  30%|███       | 15/50 [00:07<00:19,  1.76it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.18s. Batch: 0.48s. S_Loss: 0.9801. T_Loss: 4.2277. Mask: 0.9517. :  32%|███▏      | 16/50 [00:07<00:16,  2.12it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.17s. Batch: 0.46s. S_Loss: 0.9807. T_Loss: 4.2288. Mask: 0.9513. :  32%|███▏      | 16/50 [00:07<00:16,  2.12it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.17s. Batch: 0.46s. S_Loss: 0.9807. T_Loss: 4.2288. Mask: 0.9513. :  34%|███▍      | 17/50 [00:07<00:12,  2.61it/s]total : 1000  current step :  865
total : 1000  current step :  866
total : 1000  current step :  867
Train Iter: 868/1000. LR: 0.0370. Data: 0.19s. Batch: 0.48s. S_Loss: 0.9818. T_Loss: 4.2255. Mask: 0.9510. :  34%|███▍      | 17/50 [00:08<00:12,  2.61it/s]Train Iter: 868/1000. LR: 0.0370. Data: 0.19s. Batch: 0.48s. S_Loss: 0.9818. T_Loss: 4.2255. Mask: 0.9510. :  36%|███▌      | 18/50 [00:08<00:15,  2.02it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.18s. Batch: 0.47s. S_Loss: 0.9823. T_Loss: 4.2330. Mask: 0.9515. :  36%|███▌      | 18/50 [00:08<00:15,  2.02it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.18s. Batch: 0.47s. S_Loss: 0.9823. T_Loss: 4.2330. Mask: 0.9515. :  38%|███▊      | 19/50 [00:08<00:13,  2.25it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.17s. Batch: 0.46s. S_Loss: 0.9836. T_Loss: 4.2463. Mask: 0.9510. :  38%|███▊      | 19/50 [00:09<00:13,  2.25it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.17s. Batch: 0.46s. S_Loss: 0.9836. T_Loss: 4.2463. Mask: 0.9510. :  40%|████      | 20/50 [00:09<00:11,  2.60it/s]total : 1000  current step :  868
total : 1000  current step :  869
total : 1000  current step :  870
Train Iter: 871/1000. LR: 0.0360. Data: 0.19s. Batch: 0.48s. S_Loss: 0.9834. T_Loss: 4.2320. Mask: 0.9505. :  40%|████      | 20/50 [00:10<00:11,  2.60it/s]Train Iter: 871/1000. LR: 0.0360. Data: 0.19s. Batch: 0.48s. S_Loss: 0.9834. T_Loss: 4.2320. Mask: 0.9505. :  42%|████▏     | 21/50 [00:10<00:15,  1.90it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.18s. Batch: 0.48s. S_Loss: 0.9854. T_Loss: 4.2367. Mask: 0.9489. :  42%|████▏     | 21/50 [00:10<00:15,  1.90it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.18s. Batch: 0.48s. S_Loss: 0.9854. T_Loss: 4.2367. Mask: 0.9489. :  44%|████▍     | 22/50 [00:10<00:14,  1.93it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.18s. Batch: 0.47s. S_Loss: 0.9852. T_Loss: 4.2380. Mask: 0.9490. :  44%|████▍     | 22/50 [00:10<00:14,  1.93it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.18s. Batch: 0.47s. S_Loss: 0.9852. T_Loss: 4.2380. Mask: 0.9490. :  46%|████▌     | 23/50 [00:10<00:11,  2.35it/s]total : 1000  current step :  871
total : 1000  current step :  872
total : 1000  current step :  873
Train Iter: 874/1000. LR: 0.0349. Data: 0.19s. Batch: 0.48s. S_Loss: 0.9840. T_Loss: 4.2371. Mask: 0.9492. :  46%|████▌     | 23/50 [00:11<00:11,  2.35it/s]Train Iter: 874/1000. LR: 0.0349. Data: 0.19s. Batch: 0.48s. S_Loss: 0.9840. T_Loss: 4.2371. Mask: 0.9492. :  48%|████▊     | 24/50 [00:11<00:13,  1.88it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.18s. Batch: 0.47s. S_Loss: 0.9844. T_Loss: 4.2419. Mask: 0.9487. :  48%|████▊     | 24/50 [00:11<00:13,  1.88it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.18s. Batch: 0.47s. S_Loss: 0.9844. T_Loss: 4.2419. Mask: 0.9487. :  50%|█████     | 25/50 [00:11<00:11,  2.14it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.18s. Batch: 0.46s. S_Loss: 0.9843. T_Loss: 4.2454. Mask: 0.9495. :  50%|█████     | 25/50 [00:12<00:11,  2.14it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.18s. Batch: 0.46s. S_Loss: 0.9843. T_Loss: 4.2454. Mask: 0.9495. :  52%|█████▏    | 26/50 [00:12<00:09,  2.48it/s]total : 1000  current step :  874
total : 1000  current step :  875
total : 1000  current step :  876
Train Iter: 877/1000. LR: 0.0338. Data: 0.19s. Batch: 0.48s. S_Loss: 0.9836. T_Loss: 4.2461. Mask: 0.9492. :  52%|█████▏    | 26/50 [00:12<00:09,  2.48it/s]Train Iter: 877/1000. LR: 0.0338. Data: 0.19s. Batch: 0.48s. S_Loss: 0.9836. T_Loss: 4.2461. Mask: 0.9492. :  54%|█████▍    | 27/50 [00:12<00:11,  1.93it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.19s. Batch: 0.47s. S_Loss: 0.9832. T_Loss: 4.2468. Mask: 0.9495. :  54%|█████▍    | 27/50 [00:13<00:11,  1.93it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.19s. Batch: 0.47s. S_Loss: 0.9832. T_Loss: 4.2468. Mask: 0.9495. :  56%|█████▌    | 28/50 [00:13<00:09,  2.27it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.18s. Batch: 0.46s. S_Loss: 0.9820. T_Loss: 4.2486. Mask: 0.9494. :  56%|█████▌    | 28/50 [00:13<00:09,  2.27it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.18s. Batch: 0.46s. S_Loss: 0.9820. T_Loss: 4.2486. Mask: 0.9494. :  58%|█████▊    | 29/50 [00:13<00:08,  2.39it/s]total : 1000  current step :  877
total : 1000  current step :  878
total : 1000  current step :  879
Train Iter: 880/1000. LR: 0.0327. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9812. T_Loss: 4.2471. Mask: 0.9492. :  58%|█████▊    | 29/50 [00:14<00:08,  2.39it/s]Train Iter: 880/1000. LR: 0.0327. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9812. T_Loss: 4.2471. Mask: 0.9492. :  60%|██████    | 30/50 [00:14<00:12,  1.62it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9799. T_Loss: 4.2506. Mask: 0.9502. :  60%|██████    | 30/50 [00:15<00:12,  1.62it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9799. T_Loss: 4.2506. Mask: 0.9502. :  62%|██████▏   | 31/50 [00:15<00:10,  1.79it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9791. T_Loss: 4.2450. Mask: 0.9502. :  62%|██████▏   | 31/50 [00:15<00:10,  1.79it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9791. T_Loss: 4.2450. Mask: 0.9502. :  64%|██████▍   | 32/50 [00:15<00:09,  1.98it/s]total : 1000  current step :  880
total : 1000  current step :  881
total : 1000  current step :  882
Train Iter: 883/1000. LR: 0.0316. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9804. T_Loss: 4.2401. Mask: 0.9498. :  64%|██████▍   | 32/50 [00:16<00:09,  1.98it/s]Train Iter: 883/1000. LR: 0.0316. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9804. T_Loss: 4.2401. Mask: 0.9498. :  66%|██████▌   | 33/50 [00:16<00:10,  1.55it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9806. T_Loss: 4.2365. Mask: 0.9496. :  66%|██████▌   | 33/50 [00:16<00:10,  1.55it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9806. T_Loss: 4.2365. Mask: 0.9496. :  68%|██████▊   | 34/50 [00:16<00:08,  1.79it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9811. T_Loss: 4.2407. Mask: 0.9496. :  68%|██████▊   | 34/50 [00:16<00:08,  1.79it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9811. T_Loss: 4.2407. Mask: 0.9496. :  70%|███████   | 35/50 [00:16<00:06,  2.17it/s]total : 1000  current step :  883
total : 1000  current step :  884
total : 1000  current step :  885
Train Iter: 886/1000. LR: 0.0305. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9811. T_Loss: 4.2432. Mask: 0.9499. :  70%|███████   | 35/50 [00:17<00:06,  2.17it/s]Train Iter: 886/1000. LR: 0.0305. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9811. T_Loss: 4.2432. Mask: 0.9499. :  72%|███████▏  | 36/50 [00:17<00:08,  1.72it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9807. T_Loss: 4.2478. Mask: 0.9503. :  72%|███████▏  | 36/50 [00:18<00:08,  1.72it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9807. T_Loss: 4.2478. Mask: 0.9503. :  74%|███████▍  | 37/50 [00:18<00:06,  2.04it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9817. T_Loss: 4.2605. Mask: 0.9503. :  74%|███████▍  | 37/50 [00:18<00:06,  2.04it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9817. T_Loss: 4.2605. Mask: 0.9503. :  76%|███████▌  | 38/50 [00:18<00:04,  2.43it/s]total : 1000  current step :  886
total : 1000  current step :  887
total : 1000  current step :  888
Train Iter: 889/1000. LR: 0.0293. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9819. T_Loss: 4.2557. Mask: 0.9502. :  76%|███████▌  | 38/50 [00:19<00:04,  2.43it/s]Train Iter: 889/1000. LR: 0.0293. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9819. T_Loss: 4.2557. Mask: 0.9502. :  78%|███████▊  | 39/50 [00:19<00:06,  1.70it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9822. T_Loss: 4.2583. Mask: 0.9502. :  78%|███████▊  | 39/50 [00:19<00:06,  1.70it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9822. T_Loss: 4.2583. Mask: 0.9502. :  80%|████████  | 40/50 [00:19<00:04,  2.04it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9827. T_Loss: 4.2624. Mask: 0.9503. :  80%|████████  | 40/50 [00:19<00:04,  2.04it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9827. T_Loss: 4.2624. Mask: 0.9503. :  82%|████████▏ | 41/50 [00:19<00:03,  2.39it/s]total : 1000  current step :  889
total : 1000  current step :  890
total : 1000  current step :  891
Train Iter: 892/1000. LR: 0.0281. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9823. T_Loss: 4.2584. Mask: 0.9503. :  82%|████████▏ | 41/50 [00:20<00:03,  2.39it/s]Train Iter: 892/1000. LR: 0.0281. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9823. T_Loss: 4.2584. Mask: 0.9503. :  84%|████████▍ | 42/50 [00:20<00:04,  1.78it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9827. T_Loss: 4.2628. Mask: 0.9500. :  84%|████████▍ | 42/50 [00:21<00:04,  1.78it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9827. T_Loss: 4.2628. Mask: 0.9500. :  86%|████████▌ | 43/50 [00:21<00:03,  1.97it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9831. T_Loss: 4.2707. Mask: 0.9504. :  86%|████████▌ | 43/50 [00:21<00:03,  1.97it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9831. T_Loss: 4.2707. Mask: 0.9504. :  88%|████████▊ | 44/50 [00:21<00:02,  2.17it/s]total : 1000  current step :  892
total : 1000  current step :  893
total : 1000  current step :  894
Train Iter: 895/1000. LR: 0.0270. Data: 0.21s. Batch: 0.50s. S_Loss: 0.9825. T_Loss: 4.2733. Mask: 0.9503. :  88%|████████▊ | 44/50 [00:22<00:02,  2.17it/s]Train Iter: 895/1000. LR: 0.0270. Data: 0.21s. Batch: 0.50s. S_Loss: 0.9825. T_Loss: 4.2733. Mask: 0.9503. :  90%|█████████ | 45/50 [00:22<00:02,  1.67it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9821. T_Loss: 4.2680. Mask: 0.9503. :  90%|█████████ | 45/50 [00:22<00:02,  1.67it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9821. T_Loss: 4.2680. Mask: 0.9503. :  92%|█████████▏| 46/50 [00:22<00:02,  1.89it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9815. T_Loss: 4.2628. Mask: 0.9503. :  92%|█████████▏| 46/50 [00:23<00:02,  1.89it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9815. T_Loss: 4.2628. Mask: 0.9503. :  94%|█████████▍| 47/50 [00:23<00:01,  2.13it/s]total : 1000  current step :  895
total : 1000  current step :  896
total : 1000  current step :  897
Train Iter: 898/1000. LR: 0.0258. Data: 0.21s. Batch: 0.50s. S_Loss: 0.9814. T_Loss: 4.2583. Mask: 0.9495. :  94%|█████████▍| 47/50 [00:23<00:01,  2.13it/s]Train Iter: 898/1000. LR: 0.0258. Data: 0.21s. Batch: 0.50s. S_Loss: 0.9814. T_Loss: 4.2583. Mask: 0.9495. :  96%|█████████▌| 48/50 [00:23<00:01,  1.73it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9815. T_Loss: 4.2594. Mask: 0.9496. :  96%|█████████▌| 48/50 [00:24<00:01,  1.73it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9815. T_Loss: 4.2594. Mask: 0.9496. :  98%|█████████▊| 49/50 [00:24<00:00,  1.91it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9818. T_Loss: 4.2637. Mask: 0.9492. :  98%|█████████▊| 49/50 [00:24<00:00,  1.91it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9818. T_Loss: 4.2637. Mask: 0.9492. : 100%|██████████| 50/50 [00:24<00:00,  2.30it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.21s. Batch: 0.49s. S_Loss: 0.9818. T_Loss: 4.2637. Mask: 0.9492. : 100%|██████████| 50/50 [00:24<00:00,  2.04it/s]
total : 1000  current step :  898
total : 1000  current step :  899
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.1872. top1: 72.27. top5: 99.61. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.1872. top1: 72.27. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:03,  2.23it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.1408. top1: 75.39. top5: 99.41. :  12%|█▎        | 1/8 [00:00<00:03,  2.23it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.1408. top1: 75.39. top5: 99.41. :  25%|██▌       | 2/8 [00:00<00:02,  2.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1404. top1: 75.91. top5: 99.35. :  25%|██▌       | 2/8 [00:00<00:02,  2.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1404. top1: 75.91. top5: 99.35. :  38%|███▊      | 3/8 [00:00<00:01,  3.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1529. top1: 74.41. top5: 99.41. :  38%|███▊      | 3/8 [00:01<00:01,  3.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1529. top1: 74.41. top5: 99.41. :  50%|█████     | 4/8 [00:01<00:00,  4.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1363. top1: 75.94. top5: 99.30. :  50%|█████     | 4/8 [00:01<00:00,  4.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1363. top1: 75.94. top5: 99.30. :  62%|██████▎   | 5/8 [00:01<00:00,  4.27it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1168. top1: 77.60. top5: 99.41. :  62%|██████▎   | 5/8 [00:01<00:00,  4.27it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1168. top1: 77.60. top5: 99.41. :  75%|███████▌  | 6/8 [00:01<00:00,  4.46it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1091. top1: 78.29. top5: 99.39. :  75%|███████▌  | 6/8 [00:01<00:00,  4.46it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.1091. top1: 78.29. top5: 99.39. :  88%|████████▊ | 7/8 [00:01<00:00,  4.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1090. top1: 78.30. top5: 99.45. :  88%|████████▊ | 7/8 [00:01<00:00,  4.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1090. top1: 78.30. top5: 99.45. : 100%|██████████| 8/8 [00:01<00:00,  4.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.1090. top1: 78.30. top5: 99.45. : 100%|██████████| 8/8 [00:02<00:00,  3.94it/s]
total : 1000  current step :  900
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 0.64s. Batch: 0.84s. S_Loss: 0.9575. T_Loss: 4.0704. Mask: 0.9180. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 0.64s. Batch: 0.84s. S_Loss: 0.9575. T_Loss: 4.0704. Mask: 0.9180. :   2%|▏         | 1/50 [00:00<00:41,  1.19it/s]Train Iter: 902/1000. LR: 0.0242. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9376. T_Loss: 4.0572. Mask: 0.9316. :   2%|▏         | 1/50 [00:01<00:41,  1.19it/s]Train Iter: 902/1000. LR: 0.0242. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9376. T_Loss: 4.0572. Mask: 0.9316. :   4%|▍         | 2/50 [00:01<00:26,  1.78it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.28s. Batch: 0.48s. S_Loss: 0.9499. T_Loss: 4.1726. Mask: 0.9414. :   4%|▍         | 2/50 [00:01<00:26,  1.78it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.28s. Batch: 0.48s. S_Loss: 0.9499. T_Loss: 4.1726. Mask: 0.9414. :   6%|▌         | 3/50 [00:01<00:19,  2.45it/s]total : 1000  current step :  901
total : 1000  current step :  902
total : 1000  current step :  903
Train Iter: 904/1000. LR: 0.0234. Data: 0.36s. Batch: 0.57s. S_Loss: 0.9620. T_Loss: 4.2003. Mask: 0.9385. :   6%|▌         | 3/50 [00:02<00:19,  2.45it/s]Train Iter: 904/1000. LR: 0.0234. Data: 0.36s. Batch: 0.57s. S_Loss: 0.9620. T_Loss: 4.2003. Mask: 0.9385. :   8%|▊         | 4/50 [00:02<00:26,  1.72it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.30s. Batch: 0.51s. S_Loss: 0.9699. T_Loss: 4.2390. Mask: 0.9414. :   8%|▊         | 4/50 [00:02<00:26,  1.72it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.30s. Batch: 0.51s. S_Loss: 0.9699. T_Loss: 4.2390. Mask: 0.9414. :  10%|█         | 5/50 [00:02<00:21,  2.14it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.26s. Batch: 0.48s. S_Loss: 0.9719. T_Loss: 4.2189. Mask: 0.9395. :  10%|█         | 5/50 [00:02<00:21,  2.14it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.26s. Batch: 0.48s. S_Loss: 0.9719. T_Loss: 4.2189. Mask: 0.9395. :  12%|█▏        | 6/50 [00:02<00:18,  2.35it/s]total : 1000  current step :  904
total : 1000  current step :  905
total : 1000  current step :  906
Train Iter: 907/1000. LR: 0.0223. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9702. T_Loss: 4.2037. Mask: 0.9436. :  12%|█▏        | 6/50 [00:03<00:18,  2.35it/s]Train Iter: 907/1000. LR: 0.0223. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9702. T_Loss: 4.2037. Mask: 0.9436. :  14%|█▍        | 7/50 [00:03<00:25,  1.68it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9740. T_Loss: 4.2352. Mask: 0.9448. :  14%|█▍        | 7/50 [00:04<00:25,  1.68it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9740. T_Loss: 4.2352. Mask: 0.9448. :  16%|█▌        | 8/50 [00:04<00:21,  1.98it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.24s. Batch: 0.51s. S_Loss: 0.9738. T_Loss: 4.2682. Mask: 0.9453. :  16%|█▌        | 8/50 [00:04<00:21,  1.98it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.24s. Batch: 0.51s. S_Loss: 0.9738. T_Loss: 4.2682. Mask: 0.9453. :  18%|█▊        | 9/50 [00:04<00:20,  1.99it/s]total : 1000  current step :  907
total : 1000  current step :  908
total : 1000  current step :  909
Train Iter: 910/1000. LR: 0.0211. Data: 0.28s. Batch: 0.55s. S_Loss: 0.9745. T_Loss: 4.2807. Mask: 0.9484. :  18%|█▊        | 9/50 [00:05<00:20,  1.99it/s]Train Iter: 910/1000. LR: 0.0211. Data: 0.28s. Batch: 0.55s. S_Loss: 0.9745. T_Loss: 4.2807. Mask: 0.9484. :  20%|██        | 10/50 [00:05<00:24,  1.62it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9704. T_Loss: 4.2506. Mask: 0.9489. :  20%|██        | 10/50 [00:05<00:24,  1.62it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.26s. Batch: 0.53s. S_Loss: 0.9704. T_Loss: 4.2506. Mask: 0.9489. :  22%|██▏       | 11/50 [00:05<00:20,  1.94it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.24s. Batch: 0.51s. S_Loss: 0.9688. T_Loss: 4.2198. Mask: 0.9495. :  22%|██▏       | 11/50 [00:06<00:20,  1.94it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.24s. Batch: 0.51s. S_Loss: 0.9688. T_Loss: 4.2198. Mask: 0.9495. :  24%|██▍       | 12/50 [00:06<00:16,  2.26it/s]total : 1000  current step :  910
total : 1000  current step :  911
total : 1000  current step :  912
Train Iter: 913/1000. LR: 0.0199. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9651. T_Loss: 4.1917. Mask: 0.9510. :  24%|██▍       | 12/50 [00:06<00:16,  2.26it/s]Train Iter: 913/1000. LR: 0.0199. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9651. T_Loss: 4.1917. Mask: 0.9510. :  26%|██▌       | 13/50 [00:06<00:20,  1.79it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9657. T_Loss: 4.1994. Mask: 0.9517. :  26%|██▌       | 13/50 [00:07<00:20,  1.79it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9657. T_Loss: 4.1994. Mask: 0.9517. :  28%|██▊       | 14/50 [00:07<00:18,  1.94it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9659. T_Loss: 4.1830. Mask: 0.9510. :  28%|██▊       | 14/50 [00:07<00:18,  1.94it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9659. T_Loss: 4.1830. Mask: 0.9510. :  30%|███       | 15/50 [00:07<00:15,  2.29it/s]total : 1000  current step :  913
total : 1000  current step :  914
total : 1000  current step :  915
Train Iter: 916/1000. LR: 0.0188. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9658. T_Loss: 4.1886. Mask: 0.9517. :  30%|███       | 15/50 [00:08<00:15,  2.29it/s]Train Iter: 916/1000. LR: 0.0188. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9658. T_Loss: 4.1886. Mask: 0.9517. :  32%|███▏      | 16/50 [00:08<00:18,  1.87it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9648. T_Loss: 4.1845. Mask: 0.9517. :  32%|███▏      | 16/50 [00:08<00:18,  1.87it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9648. T_Loss: 4.1845. Mask: 0.9517. :  34%|███▍      | 17/50 [00:08<00:15,  2.14it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9652. T_Loss: 4.1678. Mask: 0.9507. :  34%|███▍      | 17/50 [00:08<00:15,  2.14it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9652. T_Loss: 4.1678. Mask: 0.9507. :  36%|███▌      | 18/50 [00:08<00:13,  2.38it/s]total : 1000  current step :  916
total : 1000  current step :  917
total : 1000  current step :  918
Train Iter: 919/1000. LR: 0.0176. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9658. T_Loss: 4.1700. Mask: 0.9509. :  36%|███▌      | 18/50 [00:09<00:13,  2.38it/s]Train Iter: 919/1000. LR: 0.0176. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9658. T_Loss: 4.1700. Mask: 0.9509. :  38%|███▊      | 19/50 [00:09<00:17,  1.77it/s]total : 1000  current step :  919
Train Iter: 920/1000. LR: 0.0173. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9655. T_Loss: 4.1730. Mask: 0.9512. :  38%|███▊      | 19/50 [00:10<00:17,  1.77it/s]Train Iter: 920/1000. LR: 0.0173. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9655. T_Loss: 4.1730. Mask: 0.9512. :  40%|████      | 20/50 [00:10<00:15,  1.98it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9654. T_Loss: 4.1713. Mask: 0.9516. :  40%|████      | 20/50 [00:11<00:15,  1.98it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9654. T_Loss: 4.1713. Mask: 0.9516. :  42%|████▏     | 21/50 [00:11<00:17,  1.71it/s]total : 1000  current step :  920
total : 1000  current step :  921
Train Iter: 922/1000. LR: 0.0165. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9665. T_Loss: 4.1561. Mask: 0.9496. :  42%|████▏     | 21/50 [00:11<00:17,  1.71it/s]Train Iter: 922/1000. LR: 0.0165. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9665. T_Loss: 4.1561. Mask: 0.9496. :  44%|████▍     | 22/50 [00:11<00:18,  1.49it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9646. T_Loss: 4.1413. Mask: 0.9499. :  44%|████▍     | 22/50 [00:12<00:18,  1.49it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9646. T_Loss: 4.1413. Mask: 0.9499. :  46%|████▌     | 23/50 [00:12<00:15,  1.74it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9647. T_Loss: 4.1590. Mask: 0.9502. :  46%|████▌     | 23/50 [00:12<00:15,  1.74it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9647. T_Loss: 4.1590. Mask: 0.9502. :  48%|████▊     | 24/50 [00:12<00:12,  2.10it/s]total : 1000  current step :  922
total : 1000  current step :  923
total : 1000  current step :  924
Train Iter: 925/1000. LR: 0.0154. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9663. T_Loss: 4.1719. Mask: 0.9508. :  48%|████▊     | 24/50 [00:13<00:12,  2.10it/s]Train Iter: 925/1000. LR: 0.0154. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9663. T_Loss: 4.1719. Mask: 0.9508. :  50%|█████     | 25/50 [00:13<00:14,  1.71it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9682. T_Loss: 4.1950. Mask: 0.9504. :  50%|█████     | 25/50 [00:13<00:14,  1.71it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9682. T_Loss: 4.1950. Mask: 0.9504. :  52%|█████▏    | 26/50 [00:13<00:12,  1.95it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9695. T_Loss: 4.1957. Mask: 0.9498. :  52%|█████▏    | 26/50 [00:14<00:12,  1.95it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9695. T_Loss: 4.1957. Mask: 0.9498. :  54%|█████▍    | 27/50 [00:14<00:10,  2.10it/s]total : 1000  current step :  925
total : 1000  current step :  926
total : 1000  current step :  927
Train Iter: 928/1000. LR: 0.0144. Data: 0.29s. Batch: 0.54s. S_Loss: 0.9693. T_Loss: 4.1916. Mask: 0.9495. :  54%|█████▍    | 27/50 [00:15<00:10,  2.10it/s]Train Iter: 928/1000. LR: 0.0144. Data: 0.29s. Batch: 0.54s. S_Loss: 0.9693. T_Loss: 4.1916. Mask: 0.9495. :  56%|█████▌    | 28/50 [00:15<00:14,  1.49it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9695. T_Loss: 4.1871. Mask: 0.9496. :  56%|█████▌    | 28/50 [00:15<00:14,  1.49it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9695. T_Loss: 4.1871. Mask: 0.9496. :  58%|█████▊    | 29/50 [00:15<00:11,  1.81it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9692. T_Loss: 4.1843. Mask: 0.9499. :  58%|█████▊    | 29/50 [00:15<00:11,  1.81it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9692. T_Loss: 4.1843. Mask: 0.9499. :  60%|██████    | 30/50 [00:15<00:08,  2.25it/s]total : 1000  current step :  928
total : 1000  current step :  929
total : 1000  current step :  930
Train Iter: 931/1000. LR: 0.0133. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9691. T_Loss: 4.1751. Mask: 0.9495. :  60%|██████    | 30/50 [00:16<00:08,  2.25it/s]Train Iter: 931/1000. LR: 0.0133. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9691. T_Loss: 4.1751. Mask: 0.9495. :  62%|██████▏   | 31/50 [00:16<00:10,  1.81it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9679. T_Loss: 4.1658. Mask: 0.9493. :  62%|██████▏   | 31/50 [00:16<00:10,  1.81it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9679. T_Loss: 4.1658. Mask: 0.9493. :  64%|██████▍   | 32/50 [00:16<00:09,  1.92it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9673. T_Loss: 4.1709. Mask: 0.9503. :  64%|██████▍   | 32/50 [00:17<00:09,  1.92it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9673. T_Loss: 4.1709. Mask: 0.9503. :  66%|██████▌   | 33/50 [00:17<00:07,  2.17it/s]total : 1000  current step :  931
total : 1000  current step :  932
total : 1000  current step :  933
Train Iter: 934/1000. LR: 0.0123. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9670. T_Loss: 4.1729. Mask: 0.9503. :  66%|██████▌   | 33/50 [00:18<00:07,  2.17it/s]Train Iter: 934/1000. LR: 0.0123. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9670. T_Loss: 4.1729. Mask: 0.9503. :  68%|██████▊   | 34/50 [00:18<00:09,  1.75it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9656. T_Loss: 4.1595. Mask: 0.9501. :  68%|██████▊   | 34/50 [00:18<00:09,  1.75it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9656. T_Loss: 4.1595. Mask: 0.9501. :  70%|███████   | 35/50 [00:18<00:07,  1.91it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9653. T_Loss: 4.1562. Mask: 0.9505. :  70%|███████   | 35/50 [00:18<00:07,  1.91it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9653. T_Loss: 4.1562. Mask: 0.9505. :  72%|███████▏  | 36/50 [00:18<00:06,  2.14it/s]total : 1000  current step :  934
total : 1000  current step :  935
total : 1000  current step :  936
Train Iter: 937/1000. LR: 0.0113. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9654. T_Loss: 4.1531. Mask: 0.9507. :  72%|███████▏  | 36/50 [00:19<00:06,  2.14it/s]Train Iter: 937/1000. LR: 0.0113. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9654. T_Loss: 4.1531. Mask: 0.9507. :  74%|███████▍  | 37/50 [00:19<00:07,  1.69it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9653. T_Loss: 4.1483. Mask: 0.9507. :  74%|███████▍  | 37/50 [00:19<00:07,  1.69it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9653. T_Loss: 4.1483. Mask: 0.9507. :  76%|███████▌  | 38/50 [00:19<00:06,  1.99it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9665. T_Loss: 4.1528. Mask: 0.9506. :  76%|███████▌  | 38/50 [00:20<00:06,  1.99it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9665. T_Loss: 4.1528. Mask: 0.9506. :  78%|███████▊  | 39/50 [00:20<00:05,  2.14it/s]total : 1000  current step :  937
total : 1000  current step :  938
total : 1000  current step :  939
Train Iter: 940/1000. LR: 0.0103. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9657. T_Loss: 4.1493. Mask: 0.9511. :  78%|███████▊  | 39/50 [00:21<00:05,  2.14it/s]Train Iter: 940/1000. LR: 0.0103. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9657. T_Loss: 4.1493. Mask: 0.9511. :  80%|████████  | 40/50 [00:21<00:05,  1.76it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9646. T_Loss: 4.1404. Mask: 0.9507. :  80%|████████  | 40/50 [00:21<00:05,  1.76it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9646. T_Loss: 4.1404. Mask: 0.9507. :  82%|████████▏ | 41/50 [00:21<00:04,  1.85it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9643. T_Loss: 4.1405. Mask: 0.9510. :  82%|████████▏ | 41/50 [00:22<00:04,  1.85it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9643. T_Loss: 4.1405. Mask: 0.9510. :  84%|████████▍ | 42/50 [00:22<00:04,  1.91it/s]total : 1000  current step :  940
total : 1000  current step :  941
total : 1000  current step :  942
Train Iter: 943/1000. LR: 0.0094. Data: 0.27s. Batch: 0.54s. S_Loss: 0.9649. T_Loss: 4.1461. Mask: 0.9507. :  84%|████████▍ | 42/50 [00:23<00:04,  1.91it/s]Train Iter: 943/1000. LR: 0.0094. Data: 0.27s. Batch: 0.54s. S_Loss: 0.9649. T_Loss: 4.1461. Mask: 0.9507. :  86%|████████▌ | 43/50 [00:23<00:04,  1.49it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9650. T_Loss: 4.1409. Mask: 0.9505. :  86%|████████▌ | 43/50 [00:23<00:04,  1.49it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9650. T_Loss: 4.1409. Mask: 0.9505. :  88%|████████▊ | 44/50 [00:23<00:03,  1.86it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9647. T_Loss: 4.1379. Mask: 0.9504. :  88%|████████▊ | 44/50 [00:23<00:03,  1.86it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9647. T_Loss: 4.1379. Mask: 0.9504. :  90%|█████████ | 45/50 [00:23<00:02,  2.11it/s]total : 1000  current step :  943
total : 1000  current step :  944
total : 1000  current step :  945
Train Iter: 946/1000. LR: 0.0085. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9659. T_Loss: 4.1497. Mask: 0.9501. :  90%|█████████ | 45/50 [00:24<00:02,  2.11it/s]Train Iter: 946/1000. LR: 0.0085. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9659. T_Loss: 4.1497. Mask: 0.9501. :  92%|█████████▏| 46/50 [00:24<00:02,  1.60it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9647. T_Loss: 4.1451. Mask: 0.9502. :  92%|█████████▏| 46/50 [00:24<00:02,  1.60it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9647. T_Loss: 4.1451. Mask: 0.9502. :  94%|█████████▍| 47/50 [00:24<00:01,  1.89it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9644. T_Loss: 4.1414. Mask: 0.9499. :  94%|█████████▍| 47/50 [00:25<00:01,  1.89it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9644. T_Loss: 4.1414. Mask: 0.9499. :  96%|█████████▌| 48/50 [00:25<00:00,  2.21it/s]total : 1000  current step :  946
total : 1000  current step :  947
total : 1000  current step :  948
Train Iter: 949/1000. LR: 0.0076. Data: 0.28s. Batch: 0.54s. S_Loss: 0.9645. T_Loss: 4.1381. Mask: 0.9503. :  96%|█████████▌| 48/50 [00:26<00:00,  2.21it/s]Train Iter: 949/1000. LR: 0.0076. Data: 0.28s. Batch: 0.54s. S_Loss: 0.9645. T_Loss: 4.1381. Mask: 0.9503. :  98%|█████████▊| 49/50 [00:26<00:00,  1.44it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9647. T_Loss: 4.1329. Mask: 0.9501. :  98%|█████████▊| 49/50 [00:26<00:00,  1.44it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9647. T_Loss: 4.1329. Mask: 0.9501. : 100%|██████████| 50/50 [00:26<00:00,  1.84it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9647. T_Loss: 4.1329. Mask: 0.9501. : 100%|██████████| 50/50 [00:26<00:00,  1.87it/s]
total : 1000  current step :  949
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 1.1220. top1: 75.78. top5: 99.61. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 1.1220. top1: 75.78. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0818. top1: 78.91. top5: 99.41. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0818. top1: 78.91. top5: 99.41. :  25%|██▌       | 2/8 [00:00<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0803. top1: 79.43. top5: 99.35. :  25%|██▌       | 2/8 [00:01<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0803. top1: 79.43. top5: 99.35. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0949. top1: 78.32. top5: 99.41. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0949. top1: 78.32. top5: 99.41. :  50%|█████     | 4/8 [00:01<00:01,  3.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0956. top1: 78.91. top5: 99.30. :  50%|█████     | 4/8 [00:01<00:01,  3.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0956. top1: 78.91. top5: 99.30. :  62%|██████▎   | 5/8 [00:01<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0875. top1: 79.75. top5: 99.35. :  62%|██████▎   | 5/8 [00:01<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0875. top1: 79.75. top5: 99.35. :  75%|███████▌  | 6/8 [00:01<00:00,  3.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0883. top1: 80.02. top5: 99.39. :  75%|███████▌  | 6/8 [00:02<00:00,  3.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0883. top1: 80.02. top5: 99.39. :  88%|████████▊ | 7/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0938. top1: 79.70. top5: 99.40. :  88%|████████▊ | 7/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0938. top1: 79.70. top5: 99.40. : 100%|██████████| 8/8 [00:02<00:00,  3.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0938. top1: 79.70. top5: 99.40. : 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]
total : 1000  current step :  950
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.34s. S_Loss: 0.9623. T_Loss: 4.3492. Mask: 0.9570. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.34s. S_Loss: 0.9623. T_Loss: 4.3492. Mask: 0.9570. :   2%|▏         | 1/50 [00:00<00:16,  2.96it/s]total : 1000  current step :  951
Train Iter: 952/1000. LR: 0.0068. Data: 0.36s. Batch: 0.67s. S_Loss: 0.9860. T_Loss: 4.2826. Mask: 0.9297. :   2%|▏         | 1/50 [00:01<00:16,  2.96it/s]Train Iter: 952/1000. LR: 0.0068. Data: 0.36s. Batch: 0.67s. S_Loss: 0.9860. T_Loss: 4.2826. Mask: 0.9297. :   4%|▍         | 2/50 [00:01<00:35,  1.36it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.25s. Batch: 0.57s. S_Loss: 0.9690. T_Loss: 4.1923. Mask: 0.9401. :   4%|▍         | 2/50 [00:01<00:35,  1.36it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.25s. Batch: 0.57s. S_Loss: 0.9690. T_Loss: 4.1923. Mask: 0.9401. :   6%|▌         | 3/50 [00:01<00:26,  1.75it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.19s. Batch: 0.51s. S_Loss: 0.9618. T_Loss: 4.1887. Mask: 0.9434. :   6%|▌         | 3/50 [00:02<00:26,  1.75it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.19s. Batch: 0.51s. S_Loss: 0.9618. T_Loss: 4.1887. Mask: 0.9434. :   8%|▊         | 4/50 [00:02<00:21,  2.12it/s]total : 1000  current step :  952
total : 1000  current step :  953
total : 1000  current step :  954
Train Iter: 955/1000. LR: 0.0060. Data: 0.32s. Batch: 0.61s. S_Loss: 0.9585. T_Loss: 4.2030. Mask: 0.9453. :   8%|▊         | 4/50 [00:03<00:21,  2.12it/s]Train Iter: 955/1000. LR: 0.0060. Data: 0.32s. Batch: 0.61s. S_Loss: 0.9585. T_Loss: 4.2030. Mask: 0.9453. :  10%|█         | 5/50 [00:03<00:30,  1.49it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.28s. Batch: 0.60s. S_Loss: 0.9621. T_Loss: 4.1781. Mask: 0.9447. :  10%|█         | 5/50 [00:03<00:30,  1.49it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.28s. Batch: 0.60s. S_Loss: 0.9621. T_Loss: 4.1781. Mask: 0.9447. :  12%|█▏        | 6/50 [00:03<00:27,  1.58it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.24s. Batch: 0.57s. S_Loss: 0.9627. T_Loss: 4.1307. Mask: 0.9442. :  12%|█▏        | 6/50 [00:03<00:27,  1.58it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.24s. Batch: 0.57s. S_Loss: 0.9627. T_Loss: 4.1307. Mask: 0.9442. :  14%|█▍        | 7/50 [00:03<00:23,  1.85it/s]total : 1000  current step :  955
total : 1000  current step :  956
total : 1000  current step :  957
Train Iter: 958/1000. LR: 0.0052. Data: 0.30s. Batch: 0.62s. S_Loss: 0.9622. T_Loss: 4.1660. Mask: 0.9473. :  14%|█▍        | 7/50 [00:04<00:23,  1.85it/s]Train Iter: 958/1000. LR: 0.0052. Data: 0.30s. Batch: 0.62s. S_Loss: 0.9622. T_Loss: 4.1660. Mask: 0.9473. :  16%|█▌        | 8/50 [00:04<00:28,  1.46it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.26s. Batch: 0.59s. S_Loss: 0.9614. T_Loss: 4.1538. Mask: 0.9484. :  16%|█▌        | 8/50 [00:05<00:28,  1.46it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.26s. Batch: 0.59s. S_Loss: 0.9614. T_Loss: 4.1538. Mask: 0.9484. :  18%|█▊        | 9/50 [00:05<00:23,  1.72it/s]total : 1000  current step :  958
total : 1000  current step :  959
Train Iter: 960/1000. LR: 0.0048. Data: 0.26s. Batch: 0.58s. S_Loss: 0.9612. T_Loss: 4.1485. Mask: 0.9492. :  18%|█▊        | 9/50 [00:05<00:23,  1.72it/s]Train Iter: 960/1000. LR: 0.0048. Data: 0.26s. Batch: 0.58s. S_Loss: 0.9612. T_Loss: 4.1485. Mask: 0.9492. :  20%|██        | 10/50 [00:05<00:21,  1.84it/s]total : 1000  current step :  960
Train Iter: 961/1000. LR: 0.0045. Data: 0.31s. Batch: 0.61s. S_Loss: 0.9563. T_Loss: 4.1344. Mask: 0.9510. :  20%|██        | 10/50 [00:06<00:21,  1.84it/s]Train Iter: 961/1000. LR: 0.0045. Data: 0.31s. Batch: 0.61s. S_Loss: 0.9563. T_Loss: 4.1344. Mask: 0.9510. :  22%|██▏       | 11/50 [00:06<00:25,  1.52it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.30s. Batch: 0.60s. S_Loss: 0.9593. T_Loss: 4.1417. Mask: 0.9512. :  22%|██▏       | 11/50 [00:07<00:25,  1.52it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.30s. Batch: 0.60s. S_Loss: 0.9593. T_Loss: 4.1417. Mask: 0.9512. :  24%|██▍       | 12/50 [00:07<00:23,  1.62it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.29s. Batch: 0.59s. S_Loss: 0.9612. T_Loss: 4.1539. Mask: 0.9510. :  24%|██▍       | 12/50 [00:07<00:23,  1.62it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.29s. Batch: 0.59s. S_Loss: 0.9612. T_Loss: 4.1539. Mask: 0.9510. :  26%|██▌       | 13/50 [00:07<00:21,  1.76it/s]total : 1000  current step :  961
total : 1000  current step :  962
total : 1000  current step :  963
Train Iter: 964/1000. LR: 0.0039. Data: 0.32s. Batch: 0.61s. S_Loss: 0.9600. T_Loss: 4.1622. Mask: 0.9526. :  26%|██▌       | 13/50 [00:08<00:21,  1.76it/s]Train Iter: 964/1000. LR: 0.0039. Data: 0.32s. Batch: 0.61s. S_Loss: 0.9600. T_Loss: 4.1622. Mask: 0.9526. :  28%|██▊       | 14/50 [00:08<00:23,  1.51it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.30s. Batch: 0.60s. S_Loss: 0.9592. T_Loss: 4.1631. Mask: 0.9521. :  28%|██▊       | 14/50 [00:08<00:23,  1.51it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.30s. Batch: 0.60s. S_Loss: 0.9592. T_Loss: 4.1631. Mask: 0.9521. :  30%|███       | 15/50 [00:08<00:20,  1.73it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9583. T_Loss: 4.1656. Mask: 0.9521. :  30%|███       | 15/50 [00:09<00:20,  1.73it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9583. T_Loss: 4.1656. Mask: 0.9521. :  32%|███▏      | 16/50 [00:09<00:17,  1.92it/s]total : 1000  current step :  964
total : 1000  current step :  965
total : 1000  current step :  966
Train Iter: 967/1000. LR: 0.0033. Data: 0.32s. Batch: 0.60s. S_Loss: 0.9576. T_Loss: 4.1576. Mask: 0.9522. :  32%|███▏      | 16/50 [00:10<00:17,  1.92it/s]Train Iter: 967/1000. LR: 0.0033. Data: 0.32s. Batch: 0.60s. S_Loss: 0.9576. T_Loss: 4.1576. Mask: 0.9522. :  34%|███▍      | 17/50 [00:10<00:20,  1.62it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9601. T_Loss: 4.1693. Mask: 0.9516. :  34%|███▍      | 17/50 [00:10<00:20,  1.62it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9601. T_Loss: 4.1693. Mask: 0.9516. :  36%|███▌      | 18/50 [00:10<00:16,  1.93it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9602. T_Loss: 4.1850. Mask: 0.9515. :  36%|███▌      | 18/50 [00:10<00:16,  1.93it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9602. T_Loss: 4.1850. Mask: 0.9515. :  38%|███▊      | 19/50 [00:10<00:15,  2.06it/s]total : 1000  current step :  967
total : 1000  current step :  968
total : 1000  current step :  969
Train Iter: 970/1000. LR: 0.0027. Data: 0.31s. Batch: 0.60s. S_Loss: 0.9598. T_Loss: 4.1921. Mask: 0.9521. :  38%|███▊      | 19/50 [00:11<00:15,  2.06it/s]Train Iter: 970/1000. LR: 0.0027. Data: 0.31s. Batch: 0.60s. S_Loss: 0.9598. T_Loss: 4.1921. Mask: 0.9521. :  40%|████      | 20/50 [00:11<00:20,  1.48it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9597. T_Loss: 4.1927. Mask: 0.9513. :  40%|████      | 20/50 [00:12<00:20,  1.48it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9597. T_Loss: 4.1927. Mask: 0.9513. :  42%|████▏     | 21/50 [00:12<00:15,  1.82it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.28s. Batch: 0.57s. S_Loss: 0.9605. T_Loss: 4.1960. Mask: 0.9508. :  42%|████▏     | 21/50 [00:12<00:15,  1.82it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.28s. Batch: 0.57s. S_Loss: 0.9605. T_Loss: 4.1960. Mask: 0.9508. :  44%|████▍     | 22/50 [00:12<00:12,  2.22it/s]total : 1000  current step :  970
total : 1000  current step :  971
total : 1000  current step :  972
Train Iter: 973/1000. LR: 0.0022. Data: 0.31s. Batch: 0.59s. S_Loss: 0.9607. T_Loss: 4.2101. Mask: 0.9518. :  44%|████▍     | 22/50 [00:13<00:12,  2.22it/s]Train Iter: 973/1000. LR: 0.0022. Data: 0.31s. Batch: 0.59s. S_Loss: 0.9607. T_Loss: 4.2101. Mask: 0.9518. :  46%|████▌     | 23/50 [00:13<00:17,  1.52it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9603. T_Loss: 4.2046. Mask: 0.9525. :  46%|████▌     | 23/50 [00:13<00:17,  1.52it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9603. T_Loss: 4.2046. Mask: 0.9525. :  48%|████▊     | 24/50 [00:13<00:13,  1.87it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.29s. Batch: 0.56s. S_Loss: 0.9600. T_Loss: 4.2109. Mask: 0.9530. :  48%|████▊     | 24/50 [00:14<00:13,  1.87it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.29s. Batch: 0.56s. S_Loss: 0.9600. T_Loss: 4.2109. Mask: 0.9530. :  50%|█████     | 25/50 [00:14<00:11,  2.21it/s]total : 1000  current step :  973
total : 1000  current step :  974
total : 1000  current step :  975
Train Iter: 976/1000. LR: 0.0018. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9576. T_Loss: 4.1977. Mask: 0.9533. :  50%|█████     | 25/50 [00:15<00:11,  2.21it/s]Train Iter: 976/1000. LR: 0.0018. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9576. T_Loss: 4.1977. Mask: 0.9533. :  52%|█████▏    | 26/50 [00:15<00:15,  1.59it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9568. T_Loss: 4.1958. Mask: 0.9534. :  52%|█████▏    | 26/50 [00:15<00:15,  1.59it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9568. T_Loss: 4.1958. Mask: 0.9534. :  54%|█████▍    | 27/50 [00:15<00:14,  1.59it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9568. T_Loss: 4.1955. Mask: 0.9530. :  54%|█████▍    | 27/50 [00:15<00:14,  1.59it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9568. T_Loss: 4.1955. Mask: 0.9530. :  56%|█████▌    | 28/50 [00:15<00:10,  2.00it/s]total : 1000  current step :  976
total : 1000  current step :  977
total : 1000  current step :  978
Train Iter: 979/1000. LR: 0.0013. Data: 0.31s. Batch: 0.59s. S_Loss: 0.9578. T_Loss: 4.1956. Mask: 0.9522. :  56%|█████▌    | 28/50 [00:17<00:10,  2.00it/s]Train Iter: 979/1000. LR: 0.0013. Data: 0.31s. Batch: 0.59s. S_Loss: 0.9578. T_Loss: 4.1956. Mask: 0.9522. :  58%|█████▊    | 29/50 [00:17<00:13,  1.52it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9584. T_Loss: 4.1951. Mask: 0.9517. :  58%|█████▊    | 29/50 [00:17<00:13,  1.52it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9584. T_Loss: 4.1951. Mask: 0.9517. :  60%|██████    | 30/50 [00:17<00:11,  1.82it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9575. T_Loss: 4.1878. Mask: 0.9514. :  60%|██████    | 30/50 [00:17<00:11,  1.82it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9575. T_Loss: 4.1878. Mask: 0.9514. :  62%|██████▏   | 31/50 [00:17<00:09,  1.97it/s]total : 1000  current step :  979
total : 1000  current step :  980
total : 1000  current step :  981
Train Iter: 982/1000. LR: 0.0010. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9574. T_Loss: 4.1846. Mask: 0.9513. :  62%|██████▏   | 31/50 [00:18<00:09,  1.97it/s]Train Iter: 982/1000. LR: 0.0010. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9574. T_Loss: 4.1846. Mask: 0.9513. :  64%|██████▍   | 32/50 [00:18<00:11,  1.54it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9582. T_Loss: 4.1858. Mask: 0.9511. :  64%|██████▍   | 32/50 [00:18<00:11,  1.54it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9582. T_Loss: 4.1858. Mask: 0.9511. :  66%|██████▌   | 33/50 [00:18<00:08,  1.92it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9587. T_Loss: 4.1894. Mask: 0.9512. :  66%|██████▌   | 33/50 [00:19<00:08,  1.92it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9587. T_Loss: 4.1894. Mask: 0.9512. :  68%|██████▊   | 34/50 [00:19<00:09,  1.75it/s]total : 1000  current step :  982
total : 1000  current step :  983
total : 1000  current step :  984
Train Iter: 985/1000. LR: 0.0007. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9593. T_Loss: 4.1961. Mask: 0.9516. :  68%|██████▊   | 34/50 [00:20<00:09,  1.75it/s]Train Iter: 985/1000. LR: 0.0007. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9593. T_Loss: 4.1961. Mask: 0.9516. :  70%|███████   | 35/50 [00:20<00:09,  1.51it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9593. T_Loss: 4.2022. Mask: 0.9515. :  70%|███████   | 35/50 [00:20<00:09,  1.51it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9593. T_Loss: 4.2022. Mask: 0.9515. :  72%|███████▏  | 36/50 [00:20<00:08,  1.72it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9589. T_Loss: 4.2019. Mask: 0.9515. :  72%|███████▏  | 36/50 [00:21<00:08,  1.72it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9589. T_Loss: 4.2019. Mask: 0.9515. :  74%|███████▍  | 37/50 [00:21<00:06,  1.99it/s]total : 1000  current step :  985
total : 1000  current step :  986
total : 1000  current step :  987
Train Iter: 988/1000. LR: 0.0004. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9581. T_Loss: 4.2011. Mask: 0.9518. :  74%|███████▍  | 37/50 [00:22<00:06,  1.99it/s]Train Iter: 988/1000. LR: 0.0004. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9581. T_Loss: 4.2011. Mask: 0.9518. :  76%|███████▌  | 38/50 [00:22<00:07,  1.67it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9578. T_Loss: 4.1955. Mask: 0.9512. :  76%|███████▌  | 38/50 [00:22<00:07,  1.67it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9578. T_Loss: 4.1955. Mask: 0.9512. :  78%|███████▊  | 39/50 [00:22<00:05,  2.04it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9574. T_Loss: 4.1929. Mask: 0.9520. :  78%|███████▊  | 39/50 [00:22<00:05,  2.04it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9574. T_Loss: 4.1929. Mask: 0.9520. :  80%|████████  | 40/50 [00:22<00:04,  2.10it/s]total : 1000  current step :  988
total : 1000  current step :  989
total : 1000  current step :  990
Train Iter: 991/1000. LR: 0.0002. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9577. T_Loss: 4.1913. Mask: 0.9521. :  80%|████████  | 40/50 [00:23<00:04,  2.10it/s]Train Iter: 991/1000. LR: 0.0002. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9577. T_Loss: 4.1913. Mask: 0.9521. :  82%|████████▏ | 41/50 [00:23<00:05,  1.55it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9579. T_Loss: 4.1890. Mask: 0.9523. :  82%|████████▏ | 41/50 [00:23<00:05,  1.55it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9579. T_Loss: 4.1890. Mask: 0.9523. :  84%|████████▍ | 42/50 [00:23<00:04,  1.99it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9588. T_Loss: 4.1929. Mask: 0.9522. :  84%|████████▍ | 42/50 [00:24<00:04,  1.99it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9588. T_Loss: 4.1929. Mask: 0.9522. :  86%|████████▌ | 43/50 [00:24<00:02,  2.39it/s]total : 1000  current step :  991
total : 1000  current step :  992
total : 1000  current step :  993
Train Iter: 994/1000. LR: 0.0001. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9585. T_Loss: 4.1823. Mask: 0.9520. :  86%|████████▌ | 43/50 [00:25<00:02,  2.39it/s]Train Iter: 994/1000. LR: 0.0001. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9585. T_Loss: 4.1823. Mask: 0.9520. :  88%|████████▊ | 44/50 [00:25<00:03,  1.81it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9587. T_Loss: 4.1849. Mask: 0.9517. :  88%|████████▊ | 44/50 [00:25<00:03,  1.81it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9587. T_Loss: 4.1849. Mask: 0.9517. :  90%|█████████ | 45/50 [00:25<00:02,  2.16it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9598. T_Loss: 4.1924. Mask: 0.9519. :  90%|█████████ | 45/50 [00:25<00:02,  2.16it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9598. T_Loss: 4.1924. Mask: 0.9519. :  92%|█████████▏| 46/50 [00:25<00:01,  2.62it/s]total : 1000  current step :  994
total : 1000  current step :  995
total : 1000  current step :  996
Train Iter: 997/1000. LR: 0.0000. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9597. T_Loss: 4.1946. Mask: 0.9520. :  92%|█████████▏| 46/50 [00:26<00:01,  2.62it/s]Train Iter: 997/1000. LR: 0.0000. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9597. T_Loss: 4.1946. Mask: 0.9520. :  94%|█████████▍| 47/50 [00:26<00:01,  1.81it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9597. T_Loss: 4.1958. Mask: 0.9521. :  94%|█████████▍| 47/50 [00:26<00:01,  1.81it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9597. T_Loss: 4.1958. Mask: 0.9521. :  96%|█████████▌| 48/50 [00:26<00:00,  2.08it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9608. T_Loss: 4.2038. Mask: 0.9523. :  96%|█████████▌| 48/50 [00:26<00:00,  2.08it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9608. T_Loss: 4.2038. Mask: 0.9523. :  98%|█████████▊| 49/50 [00:26<00:00,  2.43it/s]total : 1000  current step :  997
total : 1000  current step :  998
total : 1000  current step :  999
Train Iter: 1000/1000. LR: 0.0000. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9620. T_Loss: 4.2122. Mask: 0.9526. :  98%|█████████▊| 49/50 [00:27<00:00,  2.43it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9620. T_Loss: 4.2122. Mask: 0.9526. : 100%|██████████| 50/50 [00:27<00:00,  1.70it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9620. T_Loss: 4.2122. Mask: 0.9526. : 100%|██████████| 50/50 [00:27<00:00,  1.79it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.0604. top1: 78.12. top5: 99.61. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.0604. top1: 78.12. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0267. top1: 81.64. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0267. top1: 81.64. top5: 99.61. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0245. top1: 82.29. top5: 99.48. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0245. top1: 82.29. top5: 99.48. :  38%|███▊      | 3/8 [00:00<00:01,  3.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0407. top1: 81.05. top5: 99.51. :  38%|███▊      | 3/8 [00:01<00:01,  3.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0407. top1: 81.05. top5: 99.51. :  50%|█████     | 4/8 [00:01<00:01,  3.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0588. top1: 80.94. top5: 99.38. :  50%|█████     | 4/8 [00:01<00:01,  3.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0588. top1: 80.94. top5: 99.38. :  62%|██████▎   | 5/8 [00:01<00:00,  3.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0621. top1: 81.18. top5: 99.41. :  62%|██████▎   | 5/8 [00:01<00:00,  3.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0621. top1: 81.18. top5: 99.41. :  75%|███████▌  | 6/8 [00:01<00:00,  4.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0718. top1: 81.08. top5: 99.44. :  75%|███████▌  | 6/8 [00:01<00:00,  4.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0718. top1: 81.08. top5: 99.44. :  88%|████████▊ | 7/8 [00:01<00:00,  4.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0829. top1: 80.40. top5: 99.45. :  88%|████████▊ | 7/8 [00:02<00:00,  4.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0829. top1: 80.40. top5: 99.45. : 100%|██████████| 8/8 [00:02<00:00,  4.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0829. top1: 80.40. top5: 99.45. : 100%|██████████| 8/8 [00:02<00:00,  3.82it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.47s. Batch: 0.54s. Loss: 0.9271. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.47s. Batch: 0.54s. Loss: 0.9271. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch:  1/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9467. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch:  1/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9467. :  67%|██████▋   | 2/3 [00:00<00:00,  2.52it/s]Finetune Epoch:  1/70. Data: 0.77s. Batch: 0.83s. Loss: 0.9363. :  67%|██████▋   | 2/3 [00:01<00:00,  2.52it/s]Finetune Epoch:  1/70. Data: 0.77s. Batch: 0.83s. Loss: 0.9363. : 100%|██████████| 3/3 [00:01<00:00,  2.92it/s]Finetune Epoch:  1/70. Data: 0.77s. Batch: 0.83s. Loss: 0.9363. : 100%|██████████| 3/3 [00:01<00:00,  2.44it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.7961. top1: 94.92. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.7961. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.7886. top1: 95.12. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.7886. top1: 95.12. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.79it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7893. top1: 95.31. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.79it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7893. top1: 95.31. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8116. top1: 94.34. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8116. top1: 94.34. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9193. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.76it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9193. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9834. top1: 84.51. top5: 99.54. :  62%|██████▎   | 5/8 [00:01<00:00,  4.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9834. top1: 84.51. top5: 99.54. :  75%|███████▌  | 6/8 [00:01<00:00,  4.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0387. top1: 81.19. top5: 99.16. :  75%|███████▌  | 6/8 [00:01<00:00,  4.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0387. top1: 81.19. top5: 99.16. :  88%|████████▊ | 7/8 [00:01<00:00,  4.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0805. top1: 79.05. top5: 98.90. :  88%|████████▊ | 7/8 [00:02<00:00,  4.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0805. top1: 79.05. top5: 98.90. : 100%|██████████| 8/8 [00:02<00:00,  4.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0805. top1: 79.05. top5: 98.90. : 100%|██████████| 8/8 [00:02<00:00,  3.76it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.49s. Batch: 0.57s. Loss: 0.9793. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.49s. Batch: 0.57s. Loss: 0.9793. :  33%|███▎      | 1/3 [00:00<00:01,  1.76it/s]Finetune Epoch:  2/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9474. :  33%|███▎      | 1/3 [00:00<00:01,  1.76it/s]Finetune Epoch:  2/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9474. :  67%|██████▋   | 2/3 [00:00<00:00,  2.64it/s]Finetune Epoch:  2/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9411. :  67%|██████▋   | 2/3 [00:01<00:00,  2.64it/s]Finetune Epoch:  2/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9411. : 100%|██████████| 3/3 [00:01<00:00,  3.14it/s]Finetune Epoch:  2/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9411. : 100%|██████████| 3/3 [00:01<00:00,  2.59it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.7964. top1: 94.92. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.7964. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.19it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7889. top1: 95.12. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.19it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7889. top1: 95.12. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.96it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7896. top1: 95.31. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.96it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7896. top1: 95.31. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8119. top1: 94.34. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8119. top1: 94.34. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9192. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.97it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9192. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9832. top1: 84.51. top5: 99.54. :  62%|██████▎   | 5/8 [00:01<00:00,  4.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9832. top1: 84.51. top5: 99.54. :  75%|███████▌  | 6/8 [00:01<00:00,  4.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0384. top1: 81.19. top5: 99.16. :  75%|███████▌  | 6/8 [00:01<00:00,  4.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0384. top1: 81.19. top5: 99.16. :  88%|████████▊ | 7/8 [00:01<00:00,  4.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0801. top1: 79.05. top5: 98.90. :  88%|████████▊ | 7/8 [00:01<00:00,  4.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0801. top1: 79.05. top5: 98.90. : 100%|██████████| 8/8 [00:01<00:00,  4.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0801. top1: 79.05. top5: 98.90. : 100%|██████████| 8/8 [00:02<00:00,  3.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.50s. Batch: 0.58s. Loss: 0.9030. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.50s. Batch: 0.58s. Loss: 0.9030. :  33%|███▎      | 1/3 [00:00<00:01,  1.72it/s]Finetune Epoch:  3/70. Data: 0.63s. Batch: 0.71s. Loss: 0.9490. :  33%|███▎      | 1/3 [00:00<00:01,  1.72it/s]Finetune Epoch:  3/70. Data: 0.63s. Batch: 0.71s. Loss: 0.9490. :  67%|██████▋   | 2/3 [00:00<00:00,  2.56it/s]Finetune Epoch:  3/70. Data: 0.75s. Batch: 0.83s. Loss: 0.9314. :  67%|██████▋   | 2/3 [00:01<00:00,  2.56it/s]Finetune Epoch:  3/70. Data: 0.75s. Batch: 0.83s. Loss: 0.9314. : 100%|██████████| 3/3 [00:01<00:00,  3.18it/s]Finetune Epoch:  3/70. Data: 0.75s. Batch: 0.83s. Loss: 0.9314. : 100%|██████████| 3/3 [00:01<00:00,  2.59it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.7967. top1: 94.53. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.7967. top1: 94.53. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.10it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7891. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.10it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7891. top1: 94.92. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7898. top1: 95.18. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7898. top1: 95.18. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.66it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8121. top1: 94.24. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.66it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8121. top1: 94.24. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9191. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.90it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9191. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9829. top1: 84.51. top5: 99.54. :  62%|██████▎   | 5/8 [00:01<00:00,  4.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9829. top1: 84.51. top5: 99.54. :  75%|███████▌  | 6/8 [00:01<00:00,  4.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0379. top1: 81.19. top5: 99.16. :  75%|███████▌  | 6/8 [00:01<00:00,  4.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0379. top1: 81.19. top5: 99.16. :  88%|████████▊ | 7/8 [00:01<00:00,  3.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0796. top1: 79.10. top5: 98.90. :  88%|████████▊ | 7/8 [00:02<00:00,  3.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0796. top1: 79.10. top5: 98.90. : 100%|██████████| 8/8 [00:02<00:00,  4.19it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0796. top1: 79.10. top5: 98.90. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.56s. Batch: 0.62s. Loss: 0.9289. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.56s. Batch: 0.62s. Loss: 0.9289. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch:  4/70. Data: 0.70s. Batch: 0.75s. Loss: 0.9336. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch:  4/70. Data: 0.70s. Batch: 0.75s. Loss: 0.9336. :  67%|██████▋   | 2/3 [00:00<00:00,  2.44it/s]Finetune Epoch:  4/70. Data: 0.84s. Batch: 0.88s. Loss: 0.9393. :  67%|██████▋   | 2/3 [00:01<00:00,  2.44it/s]Finetune Epoch:  4/70. Data: 0.84s. Batch: 0.88s. Loss: 0.9393. : 100%|██████████| 3/3 [00:01<00:00,  2.89it/s]Finetune Epoch:  4/70. Data: 0.84s. Batch: 0.88s. Loss: 0.9393. : 100%|██████████| 3/3 [00:01<00:00,  2.39it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.7969. top1: 94.53. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.7969. top1: 94.53. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.38it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7893. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.38it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7893. top1: 94.92. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.7900. top1: 95.18. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.7900. top1: 95.18. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8123. top1: 94.24. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8123. top1: 94.24. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9191. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.99it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9191. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9827. top1: 84.57. top5: 99.54. :  62%|██████▎   | 5/8 [00:01<00:00,  4.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9827. top1: 84.57. top5: 99.54. :  75%|███████▌  | 6/8 [00:01<00:00,  4.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0376. top1: 81.31. top5: 99.16. :  75%|███████▌  | 6/8 [00:01<00:00,  4.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0376. top1: 81.31. top5: 99.16. :  88%|████████▊ | 7/8 [00:01<00:00,  4.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0792. top1: 79.20. top5: 98.90. :  88%|████████▊ | 7/8 [00:01<00:00,  4.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0792. top1: 79.20. top5: 98.90. : 100%|██████████| 8/8 [00:01<00:00,  4.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0792. top1: 79.20. top5: 98.90. : 100%|██████████| 8/8 [00:02<00:00,  3.79it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.54s. Batch: 0.62s. Loss: 0.8944. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.54s. Batch: 0.62s. Loss: 0.8944. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch:  5/70. Data: 0.67s. Batch: 0.74s. Loss: 0.9279. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch:  5/70. Data: 0.67s. Batch: 0.74s. Loss: 0.9279. :  67%|██████▋   | 2/3 [00:00<00:00,  2.50it/s]Finetune Epoch:  5/70. Data: 0.80s. Batch: 0.86s. Loss: 0.9324. :  67%|██████▋   | 2/3 [00:01<00:00,  2.50it/s]Finetune Epoch:  5/70. Data: 0.80s. Batch: 0.86s. Loss: 0.9324. : 100%|██████████| 3/3 [00:01<00:00,  3.03it/s]Finetune Epoch:  5/70. Data: 0.80s. Batch: 0.86s. Loss: 0.9324. : 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.7972. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.7972. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.01it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7895. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.01it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7895. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7902. top1: 95.05. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7902. top1: 95.05. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.45it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8124. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.45it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8124. top1: 94.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.82it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9190. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.82it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9190. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9825. top1: 84.57. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9825. top1: 84.57. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0373. top1: 81.31. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  4.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0373. top1: 81.31. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0787. top1: 79.20. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0787. top1: 79.20. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0787. top1: 79.20. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.56it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.56s. Batch: 0.61s. Loss: 0.9958. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.56s. Batch: 0.61s. Loss: 0.9958. :  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]Finetune Epoch:  6/70. Data: 0.74s. Batch: 0.78s. Loss: 0.9594. :  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]Finetune Epoch:  6/70. Data: 0.74s. Batch: 0.78s. Loss: 0.9594. :  67%|██████▋   | 2/3 [00:00<00:00,  2.19it/s]Finetune Epoch:  6/70. Data: 0.92s. Batch: 0.96s. Loss: 0.9382. :  67%|██████▋   | 2/3 [00:01<00:00,  2.19it/s]Finetune Epoch:  6/70. Data: 0.92s. Batch: 0.96s. Loss: 0.9382. : 100%|██████████| 3/3 [00:01<00:00,  2.41it/s]Finetune Epoch:  6/70. Data: 0.92s. Batch: 0.96s. Loss: 0.9382. : 100%|██████████| 3/3 [00:01<00:00,  2.11it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.7974. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.7974. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7898. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7898. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7905. top1: 95.05. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7905. top1: 95.05. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.51it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8126. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.51it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8126. top1: 94.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9189. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.89it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9189. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9822. top1: 84.57. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  4.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9822. top1: 84.57. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0369. top1: 81.31. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  4.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0369. top1: 81.31. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  4.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0783. top1: 79.20. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0783. top1: 79.20. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0783. top1: 79.20. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.49s. Batch: 0.54s. Loss: 0.9241. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.49s. Batch: 0.54s. Loss: 0.9241. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch:  7/70. Data: 0.62s. Batch: 0.67s. Loss: 0.9334. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch:  7/70. Data: 0.62s. Batch: 0.67s. Loss: 0.9334. :  67%|██████▋   | 2/3 [00:00<00:00,  2.66it/s]Finetune Epoch:  7/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9223. :  67%|██████▋   | 2/3 [00:01<00:00,  2.66it/s]Finetune Epoch:  7/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9223. : 100%|██████████| 3/3 [00:01<00:00,  2.93it/s]Finetune Epoch:  7/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9223. : 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.7977. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.7977. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7900. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7900. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.88it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7906. top1: 95.05. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.88it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7906. top1: 95.05. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.37it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8128. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.37it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8128. top1: 94.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.70it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9189. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.70it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9189. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9820. top1: 84.64. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9820. top1: 84.64. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0365. top1: 81.36. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  4.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0365. top1: 81.36. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  4.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0778. top1: 79.25. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0778. top1: 79.25. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0778. top1: 79.25. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.67it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.56s. Batch: 0.62s. Loss: 0.9209. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.56s. Batch: 0.62s. Loss: 0.9209. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch:  8/70. Data: 0.73s. Batch: 0.78s. Loss: 0.9395. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch:  8/70. Data: 0.73s. Batch: 0.78s. Loss: 0.9395. :  67%|██████▋   | 2/3 [00:00<00:00,  2.23it/s]Finetune Epoch:  8/70. Data: 0.87s. Batch: 0.93s. Loss: 0.9447. :  67%|██████▋   | 2/3 [00:01<00:00,  2.23it/s]Finetune Epoch:  8/70. Data: 0.87s. Batch: 0.93s. Loss: 0.9447. : 100%|██████████| 3/3 [00:01<00:00,  2.70it/s]Finetune Epoch:  8/70. Data: 0.87s. Batch: 0.93s. Loss: 0.9447. : 100%|██████████| 3/3 [00:01<00:00,  2.26it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.7979. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.7979. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.00it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7902. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.00it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7902. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7909. top1: 95.05. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7909. top1: 95.05. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8130. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8130. top1: 94.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.66it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9188. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.66it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9188. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9818. top1: 84.64. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9818. top1: 84.64. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0362. top1: 81.36. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  3.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0362. top1: 81.36. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  4.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0774. top1: 79.25. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0774. top1: 79.25. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0774. top1: 79.25. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.62it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.44s. Batch: 0.50s. Loss: 0.9007. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.44s. Batch: 0.50s. Loss: 0.9007. :  33%|███▎      | 1/3 [00:00<00:01,  1.99it/s]Finetune Epoch:  9/70. Data: 0.56s. Batch: 0.62s. Loss: 0.9270. :  33%|███▎      | 1/3 [00:00<00:01,  1.99it/s]Finetune Epoch:  9/70. Data: 0.56s. Batch: 0.62s. Loss: 0.9270. :  67%|██████▋   | 2/3 [00:00<00:00,  2.91it/s]Finetune Epoch:  9/70. Data: 0.70s. Batch: 0.76s. Loss: 0.9378. :  67%|██████▋   | 2/3 [00:01<00:00,  2.91it/s]Finetune Epoch:  9/70. Data: 0.70s. Batch: 0.76s. Loss: 0.9378. : 100%|██████████| 3/3 [00:01<00:00,  3.09it/s]Finetune Epoch:  9/70. Data: 0.70s. Batch: 0.76s. Loss: 0.9378. : 100%|██████████| 3/3 [00:01<00:00,  2.50it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.7982. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.7982. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.11it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7904. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.11it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7904. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7911. top1: 95.05. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7911. top1: 95.05. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.12it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8132. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.12it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8132. top1: 94.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.48it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9187. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.48it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9187. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.79it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9816. top1: 84.70. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.79it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9816. top1: 84.70. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0358. top1: 81.42. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  3.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0358. top1: 81.42. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  4.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0770. top1: 79.30. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0770. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0770. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.49it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.65s. Batch: 0.72s. Loss: 0.9442. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.65s. Batch: 0.72s. Loss: 0.9442. :  33%|███▎      | 1/3 [00:00<00:01,  1.39it/s]Finetune Epoch: 10/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9447. :  33%|███▎      | 1/3 [00:00<00:01,  1.39it/s]Finetune Epoch: 10/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9447. :  67%|██████▋   | 2/3 [00:00<00:00,  2.27it/s]Finetune Epoch: 10/70. Data: 0.92s. Batch: 0.97s. Loss: 0.9321. :  67%|██████▋   | 2/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 10/70. Data: 0.92s. Batch: 0.97s. Loss: 0.9321. : 100%|██████████| 3/3 [00:01<00:00,  2.79it/s]Finetune Epoch: 10/70. Data: 0.92s. Batch: 0.97s. Loss: 0.9321. : 100%|██████████| 3/3 [00:01<00:00,  2.26it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.7984. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.7984. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.11it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7906. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.11it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7906. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7913. top1: 95.05. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7913. top1: 95.05. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8134. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8134. top1: 94.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.93it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9187. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.93it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9187. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9814. top1: 84.70. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  4.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9814. top1: 84.70. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0355. top1: 81.42. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  4.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0355. top1: 81.42. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  4.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0766. top1: 79.30. top5: 98.95. :  88%|████████▊ | 7/8 [00:01<00:00,  4.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0766. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:01<00:00,  4.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0766. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.82it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.53s. Batch: 0.58s. Loss: 0.9153. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.53s. Batch: 0.58s. Loss: 0.9153. :  33%|███▎      | 1/3 [00:00<00:01,  1.72it/s]Finetune Epoch: 11/70. Data: 0.67s. Batch: 0.72s. Loss: 0.9127. :  33%|███▎      | 1/3 [00:00<00:01,  1.72it/s]Finetune Epoch: 11/70. Data: 0.67s. Batch: 0.72s. Loss: 0.9127. :  67%|██████▋   | 2/3 [00:00<00:00,  2.46it/s]Finetune Epoch: 11/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9373. :  67%|██████▋   | 2/3 [00:01<00:00,  2.46it/s]Finetune Epoch: 11/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9373. : 100%|██████████| 3/3 [00:01<00:00,  2.99it/s]Finetune Epoch: 11/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9373. : 100%|██████████| 3/3 [00:01<00:00,  2.35it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.7987. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.7987. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.7908. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.7908. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.75it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7915. top1: 95.05. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.75it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7915. top1: 95.05. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8136. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8136. top1: 94.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9186. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.76it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9186. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9811. top1: 84.70. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9811. top1: 84.70. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0352. top1: 81.42. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  4.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0352. top1: 81.42. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  4.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0762. top1: 79.30. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0762. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.57it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0762. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.54s. Batch: 0.62s. Loss: 0.9049. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.54s. Batch: 0.62s. Loss: 0.9049. :  33%|███▎      | 1/3 [00:00<00:01,  1.60it/s]Finetune Epoch: 12/70. Data: 0.69s. Batch: 0.76s. Loss: 0.9159. :  33%|███▎      | 1/3 [00:00<00:01,  1.60it/s]Finetune Epoch: 12/70. Data: 0.69s. Batch: 0.76s. Loss: 0.9159. :  67%|██████▋   | 2/3 [00:00<00:00,  2.38it/s]Finetune Epoch: 12/70. Data: 0.82s. Batch: 0.89s. Loss: 0.9233. :  67%|██████▋   | 2/3 [00:01<00:00,  2.38it/s]Finetune Epoch: 12/70. Data: 0.82s. Batch: 0.89s. Loss: 0.9233. : 100%|██████████| 3/3 [00:01<00:00,  2.95it/s]Finetune Epoch: 12/70. Data: 0.82s. Batch: 0.89s. Loss: 0.9233. : 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.7990. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.7990. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.33it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7911. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.33it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7911. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7918. top1: 95.05. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7918. top1: 95.05. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.20it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8138. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.20it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8138. top1: 94.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.30it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9185. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.30it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9185. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.34it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9809. top1: 84.77. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.34it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9809. top1: 84.77. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0348. top1: 81.47. top5: 99.22. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0348. top1: 81.47. top5: 99.22. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0757. top1: 79.35. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0757. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.67it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0757. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.19it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.59s. Batch: 0.65s. Loss: 0.9488. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.59s. Batch: 0.65s. Loss: 0.9488. :  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]Finetune Epoch: 13/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9506. :  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]Finetune Epoch: 13/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9506. :  67%|██████▋   | 2/3 [00:00<00:00,  2.15it/s]Finetune Epoch: 13/70. Data: 0.94s. Batch: 1.00s. Loss: 0.9359. :  67%|██████▋   | 2/3 [00:01<00:00,  2.15it/s]Finetune Epoch: 13/70. Data: 0.94s. Batch: 1.00s. Loss: 0.9359. : 100%|██████████| 3/3 [00:01<00:00,  2.37it/s]Finetune Epoch: 13/70. Data: 0.94s. Batch: 1.00s. Loss: 0.9359. : 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.7992. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.7992. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.7913. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.7913. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7920. top1: 95.05. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7920. top1: 95.05. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8140. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8140. top1: 94.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.16it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9185. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.16it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9185. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.22it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9807. top1: 84.77. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.22it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9807. top1: 84.77. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.27it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0345. top1: 81.47. top5: 99.22. :  75%|███████▌  | 6/8 [00:02<00:00,  3.27it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0345. top1: 81.47. top5: 99.22. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0753. top1: 79.35. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0753. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.73it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0753. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.60s. Batch: 0.67s. Loss: 0.9119. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.60s. Batch: 0.67s. Loss: 0.9119. :  33%|███▎      | 1/3 [00:00<00:01,  1.48it/s]Finetune Epoch: 14/70. Data: 0.75s. Batch: 0.82s. Loss: 0.9218. :  33%|███▎      | 1/3 [00:00<00:01,  1.48it/s]Finetune Epoch: 14/70. Data: 0.75s. Batch: 0.82s. Loss: 0.9218. :  67%|██████▋   | 2/3 [00:00<00:00,  2.22it/s]Finetune Epoch: 14/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9318. :  67%|██████▋   | 2/3 [00:01<00:00,  2.22it/s]Finetune Epoch: 14/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9318. : 100%|██████████| 3/3 [00:01<00:00,  2.79it/s]Finetune Epoch: 14/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9318. : 100%|██████████| 3/3 [00:01<00:00,  2.14it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.7995. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.7995. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.59it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.7915. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.59it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.7915. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.7922. top1: 94.79. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.7922. top1: 94.79. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8142. top1: 93.95. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8142. top1: 93.95. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.66it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9184. top1: 88.75. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  2.66it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9184. top1: 88.75. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9804. top1: 84.64. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:01,  2.86it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9804. top1: 84.64. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0341. top1: 81.36. top5: 99.22. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0341. top1: 81.36. top5: 99.22. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0748. top1: 79.25. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0748. top1: 79.25. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0748. top1: 79.25. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.64s. Batch: 0.70s. Loss: 0.9430. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.64s. Batch: 0.70s. Loss: 0.9430. :  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]Finetune Epoch: 15/70. Data: 0.81s. Batch: 0.87s. Loss: 0.9450. :  33%|███▎      | 1/3 [00:01<00:01,  1.43it/s]Finetune Epoch: 15/70. Data: 0.81s. Batch: 0.87s. Loss: 0.9450. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 15/70. Data: 0.98s. Batch: 1.04s. Loss: 0.9437. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 15/70. Data: 0.98s. Batch: 1.04s. Loss: 0.9437. : 100%|██████████| 3/3 [00:01<00:00,  2.39it/s]Finetune Epoch: 15/70. Data: 0.98s. Batch: 1.04s. Loss: 0.9437. : 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.7998. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.7998. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.85it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.7918. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.85it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.7918. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.80it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7924. top1: 94.79. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.80it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7924. top1: 94.79. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8144. top1: 93.95. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8144. top1: 93.95. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9184. top1: 88.75. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.77it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9184. top1: 88.75. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.94it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9802. top1: 84.64. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.94it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9802. top1: 84.64. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0337. top1: 81.36. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  4.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0337. top1: 81.36. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  3.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0744. top1: 79.25. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0744. top1: 79.25. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0744. top1: 79.25. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.28it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9268. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9268. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 16/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9251. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 16/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9251. :  67%|██████▋   | 2/3 [00:00<00:00,  2.32it/s]Finetune Epoch: 16/70. Data: 0.89s. Batch: 0.94s. Loss: 0.9342. :  67%|██████▋   | 2/3 [00:01<00:00,  2.32it/s]Finetune Epoch: 16/70. Data: 0.89s. Batch: 0.94s. Loss: 0.9342. : 100%|██████████| 3/3 [00:01<00:00,  2.86it/s]Finetune Epoch: 16/70. Data: 0.89s. Batch: 0.94s. Loss: 0.9342. : 100%|██████████| 3/3 [00:01<00:00,  2.32it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8000. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8000. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.07it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7920. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.07it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7920. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.81it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7926. top1: 94.79. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.81it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7926. top1: 94.79. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.34it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8146. top1: 93.95. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.34it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8146. top1: 93.95. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9183. top1: 88.75. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.77it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9183. top1: 88.75. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9800. top1: 84.64. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9800. top1: 84.64. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.69it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0334. top1: 81.42. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  3.69it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0334. top1: 81.42. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  3.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0740. top1: 79.30. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0740. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.91it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0740. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.49s. Batch: 0.55s. Loss: 0.9579. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.49s. Batch: 0.55s. Loss: 0.9579. :  33%|███▎      | 1/3 [00:00<00:01,  1.81it/s]Finetune Epoch: 17/70. Data: 0.62s. Batch: 0.68s. Loss: 0.9375. :  33%|███▎      | 1/3 [00:00<00:01,  1.81it/s]Finetune Epoch: 17/70. Data: 0.62s. Batch: 0.68s. Loss: 0.9375. :  67%|██████▋   | 2/3 [00:00<00:00,  2.66it/s]Finetune Epoch: 17/70. Data: 0.80s. Batch: 0.86s. Loss: 0.9361. :  67%|██████▋   | 2/3 [00:01<00:00,  2.66it/s]Finetune Epoch: 17/70. Data: 0.80s. Batch: 0.86s. Loss: 0.9361. : 100%|██████████| 3/3 [00:01<00:00,  2.55it/s]Finetune Epoch: 17/70. Data: 0.80s. Batch: 0.86s. Loss: 0.9361. : 100%|██████████| 3/3 [00:01<00:00,  2.24it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8002. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8002. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.7921. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.7921. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7928. top1: 94.79. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7928. top1: 94.79. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8147. top1: 93.95. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8147. top1: 93.95. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.55it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9183. top1: 88.75. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.55it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9183. top1: 88.75. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9798. top1: 84.70. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9798. top1: 84.70. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0331. top1: 81.47. top5: 99.22. :  75%|███████▌  | 6/8 [00:02<00:00,  3.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0331. top1: 81.47. top5: 99.22. :  88%|████████▊ | 7/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0736. top1: 79.35. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0736. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0736. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9625. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9625. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch: 18/70. Data: 0.78s. Batch: 0.85s. Loss: 0.9415. :  33%|███▎      | 1/3 [00:01<00:01,  1.44it/s]Finetune Epoch: 18/70. Data: 0.78s. Batch: 0.85s. Loss: 0.9415. :  67%|██████▋   | 2/3 [00:01<00:00,  2.15it/s]Finetune Epoch: 18/70. Data: 0.93s. Batch: 1.00s. Loss: 0.9319. :  67%|██████▋   | 2/3 [00:01<00:00,  2.15it/s]Finetune Epoch: 18/70. Data: 0.93s. Batch: 1.00s. Loss: 0.9319. : 100%|██████████| 3/3 [00:01<00:00,  2.55it/s]Finetune Epoch: 18/70. Data: 0.93s. Batch: 1.00s. Loss: 0.9319. : 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8004. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8004. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.00it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7923. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.00it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7923. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7930. top1: 94.79. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:01,  3.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7930. top1: 94.79. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8149. top1: 93.95. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8149. top1: 93.95. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9182. top1: 88.75. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9182. top1: 88.75. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9796. top1: 84.70. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9796. top1: 84.70. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0329. top1: 81.47. top5: 99.22. :  75%|███████▌  | 6/8 [00:02<00:00,  3.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0329. top1: 81.47. top5: 99.22. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0733. top1: 79.35. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0733. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0733. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  2.94it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.51s. Batch: 0.57s. Loss: 0.9192. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.51s. Batch: 0.57s. Loss: 0.9192. :  33%|███▎      | 1/3 [00:00<00:01,  1.76it/s]Finetune Epoch: 19/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9276. :  33%|███▎      | 1/3 [00:00<00:01,  1.76it/s]Finetune Epoch: 19/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9276. :  67%|██████▋   | 2/3 [00:00<00:00,  2.29it/s]Finetune Epoch: 19/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9348. :  67%|██████▋   | 2/3 [00:01<00:00,  2.29it/s]Finetune Epoch: 19/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9348. : 100%|██████████| 3/3 [00:01<00:00,  2.84it/s]Finetune Epoch: 19/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9348. : 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8007. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8007. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.7925. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.7925. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.7932. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.7932. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8150. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8150. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.03it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9181. top1: 88.59. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.03it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9181. top1: 88.59. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9794. top1: 84.57. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:00,  3.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9794. top1: 84.57. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.10it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0325. top1: 81.42. top5: 99.22. :  75%|███████▌  | 6/8 [00:02<00:00,  3.10it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0325. top1: 81.42. top5: 99.22. :  88%|████████▊ | 7/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0729. top1: 79.30. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0729. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0729. top1: 79.30. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.50s. Batch: 0.55s. Loss: 0.9158. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.50s. Batch: 0.55s. Loss: 0.9158. :  33%|███▎      | 1/3 [00:00<00:01,  1.81it/s]Finetune Epoch: 20/70. Data: 0.63s. Batch: 0.68s. Loss: 0.9363. :  33%|███▎      | 1/3 [00:00<00:01,  1.81it/s]Finetune Epoch: 20/70. Data: 0.63s. Batch: 0.68s. Loss: 0.9363. :  67%|██████▋   | 2/3 [00:00<00:00,  2.63it/s]Finetune Epoch: 20/70. Data: 0.76s. Batch: 0.81s. Loss: 0.9360. :  67%|██████▋   | 2/3 [00:01<00:00,  2.63it/s]Finetune Epoch: 20/70. Data: 0.76s. Batch: 0.81s. Loss: 0.9360. : 100%|██████████| 3/3 [00:01<00:00,  3.06it/s]Finetune Epoch: 20/70. Data: 0.76s. Batch: 0.81s. Loss: 0.9360. : 100%|██████████| 3/3 [00:01<00:00,  2.56it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8010. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8010. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.16it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7928. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.16it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7928. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.08it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7935. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.08it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7935. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8153. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8153. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9181. top1: 88.59. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.97it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9181. top1: 88.59. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9792. top1: 84.57. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  4.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9792. top1: 84.57. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.34it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0322. top1: 81.47. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  4.34it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0322. top1: 81.47. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  4.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0724. top1: 79.35. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0724. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0724. top1: 79.35. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.65it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.47s. Batch: 0.54s. Loss: 0.9474. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.47s. Batch: 0.54s. Loss: 0.9474. :  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s]Finetune Epoch: 21/70. Data: 0.60s. Batch: 0.66s. Loss: 0.9407. :  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s]Finetune Epoch: 21/70. Data: 0.60s. Batch: 0.66s. Loss: 0.9407. :  67%|██████▋   | 2/3 [00:00<00:00,  2.75it/s]Finetune Epoch: 21/70. Data: 0.74s. Batch: 0.79s. Loss: 0.9374. :  67%|██████▋   | 2/3 [00:01<00:00,  2.75it/s]Finetune Epoch: 21/70. Data: 0.74s. Batch: 0.79s. Loss: 0.9374. : 100%|██████████| 3/3 [00:01<00:00,  3.12it/s]Finetune Epoch: 21/70. Data: 0.74s. Batch: 0.79s. Loss: 0.9374. : 100%|██████████| 3/3 [00:01<00:00,  2.62it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8013. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8013. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.7930. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.7930. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.59it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7937. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.59it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7937. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8155. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8155. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9180. top1: 88.59. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9180. top1: 88.59. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.18it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9789. top1: 84.57. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.18it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9789. top1: 84.57. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0318. top1: 81.53. top5: 99.22. :  75%|███████▌  | 6/8 [00:02<00:00,  3.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0318. top1: 81.53. top5: 99.22. :  88%|████████▊ | 7/8 [00:02<00:00,  3.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0720. top1: 79.40. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0720. top1: 79.40. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0720. top1: 79.40. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.25it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.51s. Batch: 0.56s. Loss: 0.9874. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.51s. Batch: 0.56s. Loss: 0.9874. :  33%|███▎      | 1/3 [00:00<00:01,  1.78it/s]Finetune Epoch: 22/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9649. :  33%|███▎      | 1/3 [00:00<00:01,  1.78it/s]Finetune Epoch: 22/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9649. :  67%|██████▋   | 2/3 [00:00<00:00,  2.29it/s]Finetune Epoch: 22/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9451. :  67%|██████▋   | 2/3 [00:01<00:00,  2.29it/s]Finetune Epoch: 22/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9451. : 100%|██████████| 3/3 [00:01<00:00,  2.86it/s]Finetune Epoch: 22/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9451. : 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8015. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8015. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.17it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7932. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.17it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7932. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7939. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7939. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.36it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8157. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.36it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8157. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.71it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9180. top1: 88.59. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.71it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9180. top1: 88.59. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9788. top1: 84.57. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9788. top1: 84.57. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.79it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0315. top1: 81.53. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  3.79it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0315. top1: 81.53. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0716. top1: 79.40. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0716. top1: 79.40. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0716. top1: 79.40. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.51it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.50s. Batch: 0.55s. Loss: 1.0128. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.50s. Batch: 0.55s. Loss: 1.0128. :  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s]Finetune Epoch: 23/70. Data: 0.63s. Batch: 0.67s. Loss: 0.9521. :  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s]Finetune Epoch: 23/70. Data: 0.63s. Batch: 0.67s. Loss: 0.9521. :  67%|██████▋   | 2/3 [00:00<00:00,  2.67it/s]Finetune Epoch: 23/70. Data: 0.76s. Batch: 0.81s. Loss: 0.9382. :  67%|██████▋   | 2/3 [00:01<00:00,  2.67it/s]Finetune Epoch: 23/70. Data: 0.76s. Batch: 0.81s. Loss: 0.9382. : 100%|██████████| 3/3 [00:01<00:00,  2.98it/s]Finetune Epoch: 23/70. Data: 0.76s. Batch: 0.81s. Loss: 0.9382. : 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8017. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8017. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.01it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7934. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.01it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7934. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7941. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7941. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.52it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8158. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.52it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8158. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.48it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9179. top1: 88.59. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.48it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9179. top1: 88.59. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9786. top1: 84.57. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9786. top1: 84.57. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.79it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0313. top1: 81.53. top5: 99.22. :  75%|███████▌  | 6/8 [00:01<00:00,  3.79it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0313. top1: 81.53. top5: 99.22. :  88%|████████▊ | 7/8 [00:01<00:00,  3.91it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0713. top1: 79.40. top5: 98.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.91it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0713. top1: 79.40. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  4.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0713. top1: 79.40. top5: 98.95. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.55s. Batch: 0.60s. Loss: 0.9222. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.55s. Batch: 0.60s. Loss: 0.9222. :  33%|███▎      | 1/3 [00:00<00:01,  1.68it/s]Finetune Epoch: 24/70. Data: 0.68s. Batch: 0.73s. Loss: 0.9329. :  33%|███▎      | 1/3 [00:00<00:01,  1.68it/s]Finetune Epoch: 24/70. Data: 0.68s. Batch: 0.73s. Loss: 0.9329. :  67%|██████▋   | 2/3 [00:00<00:00,  2.47it/s]Finetune Epoch: 24/70. Data: 0.81s. Batch: 0.86s. Loss: 0.9347. :  67%|██████▋   | 2/3 [00:01<00:00,  2.47it/s]Finetune Epoch: 24/70. Data: 0.81s. Batch: 0.86s. Loss: 0.9347. : 100%|██████████| 3/3 [00:01<00:00,  2.94it/s]Finetune Epoch: 24/70. Data: 0.81s. Batch: 0.86s. Loss: 0.9347. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8019. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8019. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7936. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7936. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7943. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7943. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8160. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8160. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9179. top1: 88.59. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.94it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9179. top1: 88.59. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9784. top1: 84.57. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  4.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9784. top1: 84.57. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.19it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0309. top1: 81.53. top5: 99.27. :  75%|███████▌  | 6/8 [00:01<00:00,  4.19it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0309. top1: 81.53. top5: 99.27. :  88%|████████▊ | 7/8 [00:01<00:00,  4.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0709. top1: 79.40. top5: 99.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0709. top1: 79.40. top5: 99.00. : 100%|██████████| 8/8 [00:01<00:00,  4.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0709. top1: 79.40. top5: 99.00. : 100%|██████████| 8/8 [00:02<00:00,  3.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.52s. Batch: 0.59s. Loss: 0.8996. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.52s. Batch: 0.59s. Loss: 0.8996. :  33%|███▎      | 1/3 [00:00<00:01,  1.69it/s]Finetune Epoch: 25/70. Data: 0.65s. Batch: 0.72s. Loss: 0.9317. :  33%|███▎      | 1/3 [00:00<00:01,  1.69it/s]Finetune Epoch: 25/70. Data: 0.65s. Batch: 0.72s. Loss: 0.9317. :  67%|██████▋   | 2/3 [00:00<00:00,  2.56it/s]Finetune Epoch: 25/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9397. :  67%|██████▋   | 2/3 [00:01<00:00,  2.56it/s]Finetune Epoch: 25/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9397. : 100%|██████████| 3/3 [00:01<00:00,  3.11it/s]Finetune Epoch: 25/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9397. : 100%|██████████| 3/3 [00:01<00:00,  2.54it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8023. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8023. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7939. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7939. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.72it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7946. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.72it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7946. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8162. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8162. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.73it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9178. top1: 88.67. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.73it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9178. top1: 88.67. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9781. top1: 84.64. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9781. top1: 84.64. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0305. top1: 81.64. top5: 99.27. :  75%|███████▌  | 6/8 [00:01<00:00,  4.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0305. top1: 81.64. top5: 99.27. :  88%|████████▊ | 7/8 [00:01<00:00,  4.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0704. top1: 79.50. top5: 99.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0704. top1: 79.50. top5: 99.00. : 100%|██████████| 8/8 [00:02<00:00,  4.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0704. top1: 79.50. top5: 99.00. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.51s. Batch: 0.55s. Loss: 0.8684. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.51s. Batch: 0.55s. Loss: 0.8684. :  33%|███▎      | 1/3 [00:00<00:01,  1.81it/s]Finetune Epoch: 26/70. Data: 0.64s. Batch: 0.69s. Loss: 0.9353. :  33%|███▎      | 1/3 [00:00<00:01,  1.81it/s]Finetune Epoch: 26/70. Data: 0.64s. Batch: 0.69s. Loss: 0.9353. :  67%|██████▋   | 2/3 [00:00<00:00,  2.58it/s]Finetune Epoch: 26/70. Data: 0.78s. Batch: 0.83s. Loss: 0.9371. :  67%|██████▋   | 2/3 [00:01<00:00,  2.58it/s]Finetune Epoch: 26/70. Data: 0.78s. Batch: 0.83s. Loss: 0.9371. : 100%|██████████| 3/3 [00:01<00:00,  2.90it/s]Finetune Epoch: 26/70. Data: 0.78s. Batch: 0.83s. Loss: 0.9371. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8025. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8025. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.10it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7941. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.10it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7941. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.99it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7948. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.99it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7948. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8164. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8164. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9178. top1: 88.67. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.90it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9178. top1: 88.67. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.12it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9779. top1: 84.70. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  4.12it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9779. top1: 84.70. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0302. top1: 81.75. top5: 99.27. :  75%|███████▌  | 6/8 [00:01<00:00,  4.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0302. top1: 81.75. top5: 99.27. :  88%|████████▊ | 7/8 [00:01<00:00,  4.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0700. top1: 79.60. top5: 99.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0700. top1: 79.60. top5: 99.00. : 100%|██████████| 8/8 [00:01<00:00,  4.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0700. top1: 79.60. top5: 99.00. : 100%|██████████| 8/8 [00:02<00:00,  3.85it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.47s. Batch: 0.53s. Loss: 0.8953. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.47s. Batch: 0.53s. Loss: 0.8953. :  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s]Finetune Epoch: 27/70. Data: 0.59s. Batch: 0.65s. Loss: 0.9445. :  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s]Finetune Epoch: 27/70. Data: 0.59s. Batch: 0.65s. Loss: 0.9445. :  67%|██████▋   | 2/3 [00:00<00:00,  2.79it/s]Finetune Epoch: 27/70. Data: 0.72s. Batch: 0.77s. Loss: 0.9379. :  67%|██████▋   | 2/3 [00:01<00:00,  2.79it/s]Finetune Epoch: 27/70. Data: 0.72s. Batch: 0.77s. Loss: 0.9379. : 100%|██████████| 3/3 [00:01<00:00,  3.27it/s]Finetune Epoch: 27/70. Data: 0.72s. Batch: 0.77s. Loss: 0.9379. : 100%|██████████| 3/3 [00:01<00:00,  2.68it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8029. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8029. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7944. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7944. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7951. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7951. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8167. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8167. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9177. top1: 88.67. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.81it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9177. top1: 88.67. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9777. top1: 84.77. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9777. top1: 84.77. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.10it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0298. top1: 81.86. top5: 99.27. :  75%|███████▌  | 6/8 [00:01<00:00,  4.10it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0298. top1: 81.86. top5: 99.27. :  88%|████████▊ | 7/8 [00:01<00:00,  4.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0695. top1: 79.70. top5: 99.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0695. top1: 79.70. top5: 99.00. : 100%|██████████| 8/8 [00:02<00:00,  4.57it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0695. top1: 79.70. top5: 99.00. : 100%|██████████| 8/8 [00:02<00:00,  3.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.56s. Batch: 0.61s. Loss: 0.9699. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.56s. Batch: 0.61s. Loss: 0.9699. :  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]Finetune Epoch: 28/70. Data: 0.69s. Batch: 0.74s. Loss: 0.9381. :  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]Finetune Epoch: 28/70. Data: 0.69s. Batch: 0.74s. Loss: 0.9381. :  67%|██████▋   | 2/3 [00:00<00:00,  2.43it/s]Finetune Epoch: 28/70. Data: 0.82s. Batch: 0.87s. Loss: 0.9328. :  67%|██████▋   | 2/3 [00:01<00:00,  2.43it/s]Finetune Epoch: 28/70. Data: 0.82s. Batch: 0.87s. Loss: 0.9328. : 100%|██████████| 3/3 [00:01<00:00,  2.98it/s]Finetune Epoch: 28/70. Data: 0.82s. Batch: 0.87s. Loss: 0.9328. : 100%|██████████| 3/3 [00:01<00:00,  2.43it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8031. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8031. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.03it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7946. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.03it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7946. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7953. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7953. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.34it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8169. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.34it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8169. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.59it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9177. top1: 88.67. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.59it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9177. top1: 88.67. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.89it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9775. top1: 84.77. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.89it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9775. top1: 84.77. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  4.10it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0295. top1: 81.92. top5: 99.27. :  75%|███████▌  | 6/8 [00:01<00:00,  4.10it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0295. top1: 81.92. top5: 99.27. :  88%|████████▊ | 7/8 [00:01<00:00,  4.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0691. top1: 79.75. top5: 99.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0691. top1: 79.75. top5: 99.00. : 100%|██████████| 8/8 [00:02<00:00,  4.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0691. top1: 79.75. top5: 99.00. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.50s. Batch: 0.57s. Loss: 0.9504. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.50s. Batch: 0.57s. Loss: 0.9504. :  33%|███▎      | 1/3 [00:00<00:01,  1.75it/s]Finetune Epoch: 29/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9466. :  33%|███▎      | 1/3 [00:00<00:01,  1.75it/s]Finetune Epoch: 29/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9466. :  67%|██████▋   | 2/3 [00:00<00:00,  2.35it/s]Finetune Epoch: 29/70. Data: 0.81s. Batch: 0.86s. Loss: 0.9491. :  67%|██████▋   | 2/3 [00:01<00:00,  2.35it/s]Finetune Epoch: 29/70. Data: 0.81s. Batch: 0.86s. Loss: 0.9491. : 100%|██████████| 3/3 [00:01<00:00,  2.95it/s]Finetune Epoch: 29/70. Data: 0.81s. Batch: 0.86s. Loss: 0.9491. : 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8034. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8034. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.09it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7948. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.09it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7948. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7955. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7955. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8171. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8171. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9176. top1: 88.67. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.79it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9176. top1: 88.67. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.03it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9773. top1: 84.77. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.03it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9773. top1: 84.77. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.20it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0292. top1: 81.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.20it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0292. top1: 81.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0687. top1: 79.75. top5: 99.05. :  88%|████████▊ | 7/8 [00:02<00:00,  4.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0687. top1: 79.75. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  4.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0687. top1: 79.75. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  3.62it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.47s. Batch: 0.52s. Loss: 0.9185. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.47s. Batch: 0.52s. Loss: 0.9185. :  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s]Finetune Epoch: 30/70. Data: 0.62s. Batch: 0.66s. Loss: 0.9190. :  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s]Finetune Epoch: 30/70. Data: 0.62s. Batch: 0.66s. Loss: 0.9190. :  67%|██████▋   | 2/3 [00:00<00:00,  2.63it/s]Finetune Epoch: 30/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9279. :  67%|██████▋   | 2/3 [00:01<00:00,  2.63it/s]Finetune Epoch: 30/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9279. : 100%|██████████| 3/3 [00:01<00:00,  3.09it/s]Finetune Epoch: 30/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9279. : 100%|██████████| 3/3 [00:01<00:00,  2.60it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8036. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8036. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7950. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7950. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.89it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7957. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.89it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7957. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.28it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8172. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.28it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8172. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.57it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9175. top1: 88.67. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.57it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9175. top1: 88.67. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.74it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9771. top1: 84.77. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.74it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9771. top1: 84.77. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0288. top1: 81.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  3.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0288. top1: 81.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  3.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0683. top1: 79.75. top5: 99.05. :  88%|████████▊ | 7/8 [00:02<00:00,  3.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0683. top1: 79.75. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  4.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0683. top1: 79.75. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  3.59it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.54s. Batch: 0.61s. Loss: 0.9342. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.54s. Batch: 0.61s. Loss: 0.9342. :  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]Finetune Epoch: 31/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9401. :  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]Finetune Epoch: 31/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9401. :  67%|██████▋   | 2/3 [00:00<00:00,  2.53it/s]Finetune Epoch: 31/70. Data: 0.81s. Batch: 0.87s. Loss: 0.9353. :  67%|██████▋   | 2/3 [00:01<00:00,  2.53it/s]Finetune Epoch: 31/70. Data: 0.81s. Batch: 0.87s. Loss: 0.9353. : 100%|██████████| 3/3 [00:01<00:00,  2.89it/s]Finetune Epoch: 31/70. Data: 0.81s. Batch: 0.87s. Loss: 0.9353. : 100%|██████████| 3/3 [00:01<00:00,  2.42it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8039. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8039. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7952. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7952. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7959. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7959. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8174. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8174. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9175. top1: 88.67. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.83it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9175. top1: 88.67. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9769. top1: 84.83. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9769. top1: 84.83. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0286. top1: 81.98. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0286. top1: 81.98. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0679. top1: 79.80. top5: 99.05. :  88%|████████▊ | 7/8 [00:02<00:00,  4.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0679. top1: 79.80. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  4.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0679. top1: 79.80. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  3.79it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.46s. Batch: 0.52s. Loss: 0.9393. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.46s. Batch: 0.52s. Loss: 0.9393. :  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s]Finetune Epoch: 32/70. Data: 0.59s. Batch: 0.65s. Loss: 0.9182. :  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s]Finetune Epoch: 32/70. Data: 0.59s. Batch: 0.65s. Loss: 0.9182. :  67%|██████▋   | 2/3 [00:00<00:00,  2.76it/s]Finetune Epoch: 32/70. Data: 0.71s. Batch: 0.77s. Loss: 0.9329. :  67%|██████▋   | 2/3 [00:01<00:00,  2.76it/s]Finetune Epoch: 32/70. Data: 0.71s. Batch: 0.77s. Loss: 0.9329. : 100%|██████████| 3/3 [00:01<00:00,  3.27it/s]Finetune Epoch: 32/70. Data: 0.71s. Batch: 0.77s. Loss: 0.9329. : 100%|██████████| 3/3 [00:01<00:00,  2.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8041. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8041. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.20it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7954. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.20it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7954. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.7961. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.7961. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.79it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8176. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.79it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8176. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9175. top1: 88.67. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.01it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9175. top1: 88.67. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9767. top1: 84.83. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9767. top1: 84.83. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0283. top1: 81.98. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0283. top1: 81.98. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0676. top1: 79.90. top5: 99.05. :  88%|████████▊ | 7/8 [00:02<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0676. top1: 79.90. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  4.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0676. top1: 79.90. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  3.81it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.46s. Batch: 0.51s. Loss: 0.9148. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.46s. Batch: 0.51s. Loss: 0.9148. :  33%|███▎      | 1/3 [00:00<00:01,  1.97it/s]Finetune Epoch: 33/70. Data: 0.63s. Batch: 0.67s. Loss: 0.9092. :  33%|███▎      | 1/3 [00:00<00:01,  1.97it/s]Finetune Epoch: 33/70. Data: 0.63s. Batch: 0.67s. Loss: 0.9092. :  67%|██████▋   | 2/3 [00:00<00:00,  2.47it/s]Finetune Epoch: 33/70. Data: 0.77s. Batch: 0.82s. Loss: 0.9252. :  67%|██████▋   | 2/3 [00:01<00:00,  2.47it/s]Finetune Epoch: 33/70. Data: 0.77s. Batch: 0.82s. Loss: 0.9252. : 100%|██████████| 3/3 [00:01<00:00,  2.91it/s]Finetune Epoch: 33/70. Data: 0.77s. Batch: 0.82s. Loss: 0.9252. : 100%|██████████| 3/3 [00:01<00:00,  2.49it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8043. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8043. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7956. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7956. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7963. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7963. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8178. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8178. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9174. top1: 88.75. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.81it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9174. top1: 88.75. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9765. top1: 84.90. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9765. top1: 84.90. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0280. top1: 82.03. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0280. top1: 82.03. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0672. top1: 79.95. top5: 99.05. :  88%|████████▊ | 7/8 [00:02<00:00,  4.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0672. top1: 79.95. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  4.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0672. top1: 79.95. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  3.73it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.50s. Batch: 0.55s. Loss: 0.9374. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.50s. Batch: 0.55s. Loss: 0.9374. :  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s]Finetune Epoch: 34/70. Data: 0.63s. Batch: 0.68s. Loss: 0.9235. :  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s]Finetune Epoch: 34/70. Data: 0.63s. Batch: 0.68s. Loss: 0.9235. :  67%|██████▋   | 2/3 [00:00<00:00,  2.67it/s]Finetune Epoch: 34/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9223. :  67%|██████▋   | 2/3 [00:01<00:00,  2.67it/s]Finetune Epoch: 34/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9223. : 100%|██████████| 3/3 [00:01<00:00,  2.61it/s]Finetune Epoch: 34/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9223. : 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8046. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8046. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.24it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7958. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.24it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7958. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.7966. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.7966. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8180. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8180. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9174. top1: 88.75. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.06it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9174. top1: 88.75. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9763. top1: 84.90. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9763. top1: 84.90. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0277. top1: 82.03. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0277. top1: 82.03. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0668. top1: 79.95. top5: 99.05. :  88%|████████▊ | 7/8 [00:01<00:00,  4.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0668. top1: 79.95. top5: 99.05. : 100%|██████████| 8/8 [00:01<00:00,  4.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0668. top1: 79.95. top5: 99.05. : 100%|██████████| 8/8 [00:01<00:00,  4.01it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.43s. Batch: 0.49s. Loss: 0.9226. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.43s. Batch: 0.49s. Loss: 0.9226. :  33%|███▎      | 1/3 [00:00<00:00,  2.03it/s]Finetune Epoch: 35/70. Data: 0.58s. Batch: 0.63s. Loss: 0.9196. :  33%|███▎      | 1/3 [00:00<00:00,  2.03it/s]Finetune Epoch: 35/70. Data: 0.58s. Batch: 0.63s. Loss: 0.9196. :  67%|██████▋   | 2/3 [00:00<00:00,  2.70it/s]Finetune Epoch: 35/70. Data: 0.72s. Batch: 0.77s. Loss: 0.9308. :  67%|██████▋   | 2/3 [00:01<00:00,  2.70it/s]Finetune Epoch: 35/70. Data: 0.72s. Batch: 0.77s. Loss: 0.9308. : 100%|██████████| 3/3 [00:01<00:00,  3.10it/s]Finetune Epoch: 35/70. Data: 0.72s. Batch: 0.77s. Loss: 0.9308. : 100%|██████████| 3/3 [00:01<00:00,  2.63it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8049. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8049. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7961. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7961. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.90it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7968. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.90it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7968. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.32it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8182. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.32it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8182. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.67it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9173. top1: 88.83. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.67it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9173. top1: 88.83. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9761. top1: 85.09. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9761. top1: 85.09. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.10it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0273. top1: 82.20. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.10it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0273. top1: 82.20. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0664. top1: 80.15. top5: 99.05. :  88%|████████▊ | 7/8 [00:02<00:00,  4.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0664. top1: 80.15. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  4.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0664. top1: 80.15. top5: 99.05. : 100%|██████████| 8/8 [00:02<00:00,  3.66it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.45s. Batch: 0.50s. Loss: 0.9311. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.45s. Batch: 0.50s. Loss: 0.9311. :  33%|███▎      | 1/3 [00:00<00:01,  2.00it/s]Finetune Epoch: 36/70. Data: 0.58s. Batch: 0.63s. Loss: 0.9366. :  33%|███▎      | 1/3 [00:00<00:01,  2.00it/s]Finetune Epoch: 36/70. Data: 0.58s. Batch: 0.63s. Loss: 0.9366. :  67%|██████▋   | 2/3 [00:00<00:00,  2.78it/s]Finetune Epoch: 36/70. Data: 0.71s. Batch: 0.75s. Loss: 0.9318. :  67%|██████▋   | 2/3 [00:00<00:00,  2.78it/s]Finetune Epoch: 36/70. Data: 0.71s. Batch: 0.75s. Loss: 0.9318. : 100%|██████████| 3/3 [00:01<00:00,  3.29it/s]Finetune Epoch: 36/70. Data: 0.71s. Batch: 0.75s. Loss: 0.9318. : 100%|██████████| 3/3 [00:01<00:00,  2.74it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8052. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8052. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.97it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7963. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.97it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7963. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7971. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.7971. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.51it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8184. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.51it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8184. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9173. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.90it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9173. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9759. top1: 85.22. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9759. top1: 85.22. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0270. top1: 82.31. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0270. top1: 82.31. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0660. top1: 80.25. top5: 99.10. :  88%|████████▊ | 7/8 [00:02<00:00,  4.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0660. top1: 80.25. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  4.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0660. top1: 80.25. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  3.81it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.56s. Batch: 0.62s. Loss: 0.9237. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.56s. Batch: 0.62s. Loss: 0.9237. :  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s]Finetune Epoch: 37/70. Data: 0.70s. Batch: 0.75s. Loss: 0.9085. :  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s]Finetune Epoch: 37/70. Data: 0.70s. Batch: 0.75s. Loss: 0.9085. :  67%|██████▋   | 2/3 [00:00<00:00,  2.47it/s]Finetune Epoch: 37/70. Data: 0.83s. Batch: 0.88s. Loss: 0.9297. :  67%|██████▋   | 2/3 [00:01<00:00,  2.47it/s]Finetune Epoch: 37/70. Data: 0.83s. Batch: 0.88s. Loss: 0.9297. : 100%|██████████| 3/3 [00:01<00:00,  2.97it/s]Finetune Epoch: 37/70. Data: 0.83s. Batch: 0.88s. Loss: 0.9297. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8055. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8055. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.17it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7966. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.17it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7966. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7973. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7973. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8186. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8186. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9172. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.95it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9172. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.12it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9757. top1: 85.22. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.12it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9757. top1: 85.22. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0267. top1: 82.31. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0267. top1: 82.31. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0656. top1: 80.30. top5: 99.10. :  88%|████████▊ | 7/8 [00:01<00:00,  4.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0656. top1: 80.30. top5: 99.10. : 100%|██████████| 8/8 [00:01<00:00,  4.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0656. top1: 80.30. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  3.92it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.47s. Batch: 0.54s. Loss: 0.9507. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.47s. Batch: 0.54s. Loss: 0.9507. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch: 38/70. Data: 0.60s. Batch: 0.66s. Loss: 0.9328. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch: 38/70. Data: 0.60s. Batch: 0.66s. Loss: 0.9328. :  67%|██████▋   | 2/3 [00:00<00:00,  2.72it/s]Finetune Epoch: 38/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9285. :  67%|██████▋   | 2/3 [00:01<00:00,  2.72it/s]Finetune Epoch: 38/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9285. : 100%|██████████| 3/3 [00:01<00:00,  3.05it/s]Finetune Epoch: 38/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9285. : 100%|██████████| 3/3 [00:01<00:00,  2.56it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8057. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8057. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.7968. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.7968. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.65it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7975. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.65it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7975. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.24it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8188. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.24it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8188. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9172. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.61it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9172. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9755. top1: 85.22. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9755. top1: 85.22. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0264. top1: 82.31. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0264. top1: 82.31. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  3.90it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0652. top1: 80.35. top5: 99.10. :  88%|████████▊ | 7/8 [00:02<00:00,  3.90it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0652. top1: 80.35. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  4.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0652. top1: 80.35. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  3.50it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.52s. Batch: 0.57s. Loss: 0.9540. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.52s. Batch: 0.57s. Loss: 0.9540. :  33%|███▎      | 1/3 [00:00<00:01,  1.76it/s]Finetune Epoch: 39/70. Data: 0.65s. Batch: 0.70s. Loss: 0.9465. :  33%|███▎      | 1/3 [00:00<00:01,  1.76it/s]Finetune Epoch: 39/70. Data: 0.65s. Batch: 0.70s. Loss: 0.9465. :  67%|██████▋   | 2/3 [00:00<00:00,  2.56it/s]Finetune Epoch: 39/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9346. :  67%|██████▋   | 2/3 [00:01<00:00,  2.56it/s]Finetune Epoch: 39/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9346. : 100%|██████████| 3/3 [00:01<00:00,  2.82it/s]Finetune Epoch: 39/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9346. : 100%|██████████| 3/3 [00:01<00:00,  2.41it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8060. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8060. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.14it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7970. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.14it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7970. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7977. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7977. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8190. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8190. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9172. top1: 88.83. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.94it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9172. top1: 88.83. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.15it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9753. top1: 85.16. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.15it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9753. top1: 85.16. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0261. top1: 82.25. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0261. top1: 82.25. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.29it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0648. top1: 80.35. top5: 99.10. :  88%|████████▊ | 7/8 [00:02<00:00,  4.29it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0648. top1: 80.35. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  4.29it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0648. top1: 80.35. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  3.63it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 40/70. Data: 0.53s. Batch: 0.61s. Loss: 0.9355. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 40/70. Data: 0.53s. Batch: 0.61s. Loss: 0.9355. :  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]Finetune Epoch: 40/70. Data: 0.67s. Batch: 0.72s. Loss: 0.9330. :  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]Finetune Epoch: 40/70. Data: 0.67s. Batch: 0.72s. Loss: 0.9330. :  67%|██████▋   | 2/3 [00:00<00:00,  2.57it/s]Finetune Epoch: 40/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9328. :  67%|██████▋   | 2/3 [00:01<00:00,  2.57it/s]Finetune Epoch: 40/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9328. : 100%|██████████| 3/3 [00:01<00:00,  3.03it/s]Finetune Epoch: 40/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9328. : 100%|██████████| 3/3 [00:01<00:00,  2.50it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8063. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8063. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7972. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7972. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.75it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7980. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.75it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7980. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8192. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8192. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9171. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.76it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9171. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9752. top1: 85.29. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9752. top1: 85.29. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0258. top1: 82.37. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0258. top1: 82.37. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0645. top1: 80.45. top5: 99.10. :  88%|████████▊ | 7/8 [00:02<00:00,  4.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0645. top1: 80.45. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  4.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0645. top1: 80.45. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.49s. Batch: 0.56s. Loss: 0.9526. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.49s. Batch: 0.56s. Loss: 0.9526. :  33%|███▎      | 1/3 [00:00<00:01,  1.78it/s]Finetune Epoch: 41/70. Data: 0.62s. Batch: 0.69s. Loss: 0.9371. :  33%|███▎      | 1/3 [00:00<00:01,  1.78it/s]Finetune Epoch: 41/70. Data: 0.62s. Batch: 0.69s. Loss: 0.9371. :  67%|██████▋   | 2/3 [00:00<00:00,  2.62it/s]Finetune Epoch: 41/70. Data: 0.75s. Batch: 0.81s. Loss: 0.9352. :  67%|██████▋   | 2/3 [00:01<00:00,  2.62it/s]Finetune Epoch: 41/70. Data: 0.75s. Batch: 0.81s. Loss: 0.9352. : 100%|██████████| 3/3 [00:01<00:00,  3.15it/s]Finetune Epoch: 41/70. Data: 0.75s. Batch: 0.81s. Loss: 0.9352. : 100%|██████████| 3/3 [00:01<00:00,  2.59it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8065. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8065. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.81it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7974. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.81it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7974. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.78it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7982. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.78it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7982. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.25it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8194. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.25it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8194. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.60it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9171. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.60it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9171. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.83it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9750. top1: 85.29. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.83it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9750. top1: 85.29. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0255. top1: 82.37. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  3.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0255. top1: 82.37. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0641. top1: 80.45. top5: 99.10. :  88%|████████▊ | 7/8 [00:02<00:00,  4.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0641. top1: 80.45. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  4.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0641. top1: 80.45. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  3.54it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.46s. Batch: 0.51s. Loss: 0.9925. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.46s. Batch: 0.51s. Loss: 0.9925. :  33%|███▎      | 1/3 [00:00<00:01,  1.95it/s]Finetune Epoch: 42/70. Data: 0.61s. Batch: 0.66s. Loss: 0.9561. :  33%|███▎      | 1/3 [00:00<00:01,  1.95it/s]Finetune Epoch: 42/70. Data: 0.61s. Batch: 0.66s. Loss: 0.9561. :  67%|██████▋   | 2/3 [00:00<00:00,  2.64it/s]Finetune Epoch: 42/70. Data: 0.74s. Batch: 0.79s. Loss: 0.9439. :  67%|██████▋   | 2/3 [00:01<00:00,  2.64it/s]Finetune Epoch: 42/70. Data: 0.74s. Batch: 0.79s. Loss: 0.9439. : 100%|██████████| 3/3 [00:01<00:00,  3.05it/s]Finetune Epoch: 42/70. Data: 0.74s. Batch: 0.79s. Loss: 0.9439. : 100%|██████████| 3/3 [00:01<00:00,  2.57it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8067. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8067. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.12it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7977. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.12it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7977. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7984. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7984. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8196. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8196. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9170. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.79it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9170. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.91it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9748. top1: 85.29. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.91it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9748. top1: 85.29. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0252. top1: 82.42. top5: 99.33. :  75%|███████▌  | 6/8 [00:02<00:00,  3.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0252. top1: 82.42. top5: 99.33. :  88%|████████▊ | 7/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0638. top1: 80.50. top5: 99.10. :  88%|████████▊ | 7/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0638. top1: 80.50. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0638. top1: 80.50. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  3.29it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.54s. Batch: 0.60s. Loss: 0.9184. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.54s. Batch: 0.60s. Loss: 0.9184. :  33%|███▎      | 1/3 [00:00<00:01,  1.68it/s]Finetune Epoch: 43/70. Data: 0.67s. Batch: 0.72s. Loss: 0.9444. :  33%|███▎      | 1/3 [00:00<00:01,  1.68it/s]Finetune Epoch: 43/70. Data: 0.67s. Batch: 0.72s. Loss: 0.9444. :  67%|██████▋   | 2/3 [00:00<00:00,  2.52it/s]Finetune Epoch: 43/70. Data: 0.80s. Batch: 0.86s. Loss: 0.9337. :  67%|██████▋   | 2/3 [00:01<00:00,  2.52it/s]Finetune Epoch: 43/70. Data: 0.80s. Batch: 0.86s. Loss: 0.9337. : 100%|██████████| 3/3 [00:01<00:00,  2.96it/s]Finetune Epoch: 43/70. Data: 0.80s. Batch: 0.86s. Loss: 0.9337. : 100%|██████████| 3/3 [00:01<00:00,  2.46it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8070. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8070. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.7979. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.7979. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.43it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7986. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.43it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7986. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8198. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8198. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.38it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9170. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.38it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9170. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.74it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9746. top1: 85.29. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.74it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9746. top1: 85.29. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0249. top1: 82.42. top5: 99.33. :  75%|███████▌  | 6/8 [00:02<00:00,  3.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0249. top1: 82.42. top5: 99.33. :  88%|████████▊ | 7/8 [00:02<00:00,  4.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0634. top1: 80.50. top5: 99.10. :  88%|████████▊ | 7/8 [00:02<00:00,  4.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0634. top1: 80.50. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  4.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0634. top1: 80.50. top5: 99.10. : 100%|██████████| 8/8 [00:02<00:00,  3.43it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9049. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9049. :  33%|███▎      | 1/3 [00:00<00:01,  1.40it/s]Finetune Epoch: 44/70. Data: 0.79s. Batch: 0.84s. Loss: 0.9360. :  33%|███▎      | 1/3 [00:00<00:01,  1.40it/s]Finetune Epoch: 44/70. Data: 0.79s. Batch: 0.84s. Loss: 0.9360. :  67%|██████▋   | 2/3 [00:00<00:00,  2.23it/s]Finetune Epoch: 44/70. Data: 0.93s. Batch: 0.98s. Loss: 0.9374. :  67%|██████▋   | 2/3 [00:01<00:00,  2.23it/s]Finetune Epoch: 44/70. Data: 0.93s. Batch: 0.98s. Loss: 0.9374. : 100%|██████████| 3/3 [00:01<00:00,  2.70it/s]Finetune Epoch: 44/70. Data: 0.93s. Batch: 0.98s. Loss: 0.9374. : 100%|██████████| 3/3 [00:01<00:00,  2.22it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8072. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8072. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.08it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7981. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.08it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7981. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7988. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.7988. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8200. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8200. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.62it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9170. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.62it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9170. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.56it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9744. top1: 85.29. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.56it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9744. top1: 85.29. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0246. top1: 82.42. top5: 99.33. :  75%|███████▌  | 6/8 [00:02<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0246. top1: 82.42. top5: 99.33. :  88%|████████▊ | 7/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0630. top1: 80.50. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0630. top1: 80.50. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0630. top1: 80.50. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.26it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.54s. Batch: 0.59s. Loss: 0.9711. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.54s. Batch: 0.59s. Loss: 0.9711. :  33%|███▎      | 1/3 [00:00<00:01,  1.70it/s]Finetune Epoch: 45/70. Data: 0.68s. Batch: 0.73s. Loss: 0.9389. :  33%|███▎      | 1/3 [00:00<00:01,  1.70it/s]Finetune Epoch: 45/70. Data: 0.68s. Batch: 0.73s. Loss: 0.9389. :  67%|██████▋   | 2/3 [00:00<00:00,  2.45it/s]Finetune Epoch: 45/70. Data: 0.81s. Batch: 0.86s. Loss: 0.9365. :  67%|██████▋   | 2/3 [00:01<00:00,  2.45it/s]Finetune Epoch: 45/70. Data: 0.81s. Batch: 0.86s. Loss: 0.9365. : 100%|██████████| 3/3 [00:01<00:00,  2.95it/s]Finetune Epoch: 45/70. Data: 0.81s. Batch: 0.86s. Loss: 0.9365. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8075. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8075. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7983. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7983. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.77it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7990. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.77it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.7990. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.40it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8201. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.40it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8201. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9169. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.80it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9169. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9742. top1: 85.35. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9742. top1: 85.35. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0243. top1: 82.48. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  3.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0243. top1: 82.48. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0626. top1: 80.55. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0626. top1: 80.55. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0626. top1: 80.55. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.38it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.57s. Batch: 0.64s. Loss: 0.9376. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.57s. Batch: 0.64s. Loss: 0.9376. :  33%|███▎      | 1/3 [00:00<00:01,  1.56it/s]Finetune Epoch: 46/70. Data: 0.72s. Batch: 0.78s. Loss: 0.9141. :  33%|███▎      | 1/3 [00:00<00:01,  1.56it/s]Finetune Epoch: 46/70. Data: 0.72s. Batch: 0.78s. Loss: 0.9141. :  67%|██████▋   | 2/3 [00:00<00:00,  2.35it/s]Finetune Epoch: 46/70. Data: 0.85s. Batch: 0.91s. Loss: 0.9308. :  67%|██████▋   | 2/3 [00:01<00:00,  2.35it/s]Finetune Epoch: 46/70. Data: 0.85s. Batch: 0.91s. Loss: 0.9308. : 100%|██████████| 3/3 [00:01<00:00,  2.85it/s]Finetune Epoch: 46/70. Data: 0.85s. Batch: 0.91s. Loss: 0.9308. : 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8078. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8078. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7985. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.7985. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.78it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7992. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.78it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7992. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.33it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8203. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.33it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8203. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.15it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9169. top1: 89.06. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.15it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9169. top1: 89.06. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9741. top1: 85.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9741. top1: 85.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.57it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0240. top1: 82.53. top5: 99.33. :  75%|███████▌  | 6/8 [00:02<00:00,  3.57it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0240. top1: 82.53. top5: 99.33. :  88%|████████▊ | 7/8 [00:02<00:00,  3.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0623. top1: 80.60. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  3.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0623. top1: 80.60. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0623. top1: 80.60. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.34it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.52s. Batch: 0.58s. Loss: 0.9079. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.52s. Batch: 0.58s. Loss: 0.9079. :  33%|███▎      | 1/3 [00:00<00:01,  1.72it/s]Finetune Epoch: 47/70. Data: 0.71s. Batch: 0.76s. Loss: 0.9431. :  33%|███▎      | 1/3 [00:00<00:01,  1.72it/s]Finetune Epoch: 47/70. Data: 0.71s. Batch: 0.76s. Loss: 0.9431. :  67%|██████▋   | 2/3 [00:00<00:00,  2.23it/s]Finetune Epoch: 47/70. Data: 0.89s. Batch: 0.94s. Loss: 0.9451. :  67%|██████▋   | 2/3 [00:01<00:00,  2.23it/s]Finetune Epoch: 47/70. Data: 0.89s. Batch: 0.94s. Loss: 0.9451. : 100%|██████████| 3/3 [00:01<00:00,  2.42it/s]Finetune Epoch: 47/70. Data: 0.89s. Batch: 0.94s. Loss: 0.9451. : 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8080. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8080. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7987. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7987. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7994. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.7994. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.52it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8205. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.52it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8205. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.59it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9168. top1: 89.06. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.59it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9168. top1: 89.06. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.50it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9739. top1: 85.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.50it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9739. top1: 85.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.38it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0238. top1: 82.53. top5: 99.33. :  75%|███████▌  | 6/8 [00:02<00:00,  3.38it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0238. top1: 82.53. top5: 99.33. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0620. top1: 80.60. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0620. top1: 80.60. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0620. top1: 80.60. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.25it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.56s. Batch: 0.64s. Loss: 0.9275. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.56s. Batch: 0.64s. Loss: 0.9275. :  33%|███▎      | 1/3 [00:00<00:01,  1.57it/s]Finetune Epoch: 48/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9298. :  33%|███▎      | 1/3 [00:00<00:01,  1.57it/s]Finetune Epoch: 48/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9298. :  67%|██████▋   | 2/3 [00:00<00:00,  2.22it/s]Finetune Epoch: 48/70. Data: 0.90s. Batch: 0.97s. Loss: 0.9399. :  67%|██████▋   | 2/3 [00:01<00:00,  2.22it/s]Finetune Epoch: 48/70. Data: 0.90s. Batch: 0.97s. Loss: 0.9399. : 100%|██████████| 3/3 [00:01<00:00,  2.48it/s]Finetune Epoch: 48/70. Data: 0.90s. Batch: 0.97s. Loss: 0.9399. : 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8082. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8082. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.7989. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.7989. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.7996. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.7996. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8207. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8207. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9168. top1: 89.14. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9168. top1: 89.14. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9737. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9737. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.77it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0235. top1: 82.65. top5: 99.33. :  75%|███████▌  | 6/8 [00:02<00:00,  3.77it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0235. top1: 82.65. top5: 99.33. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0616. top1: 80.70. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0616. top1: 80.70. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0616. top1: 80.70. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.22it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.65s. Batch: 0.70s. Loss: 0.9659. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.65s. Batch: 0.70s. Loss: 0.9659. :  33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]Finetune Epoch: 49/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9473. :  33%|███▎      | 1/3 [00:01<00:01,  1.42it/s]Finetune Epoch: 49/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9473. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 49/70. Data: 0.99s. Batch: 1.05s. Loss: 0.9282. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 49/70. Data: 0.99s. Batch: 1.05s. Loss: 0.9282. : 100%|██████████| 3/3 [00:01<00:00,  2.36it/s]Finetune Epoch: 49/70. Data: 0.99s. Batch: 1.05s. Loss: 0.9282. : 100%|██████████| 3/3 [00:01<00:00,  2.00it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8084. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8084. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.08it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7991. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.08it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7991. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.87it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7998. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.87it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.7998. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8208. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8208. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.27it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9168. top1: 89.14. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.27it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9168. top1: 89.14. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.28it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9736. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.28it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9736. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0232. top1: 82.65. top5: 99.33. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0232. top1: 82.65. top5: 99.33. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0613. top1: 80.70. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0613. top1: 80.70. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0613. top1: 80.70. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.59s. Batch: 0.64s. Loss: 0.9205. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.59s. Batch: 0.64s. Loss: 0.9205. :  33%|███▎      | 1/3 [00:00<00:01,  1.57it/s]Finetune Epoch: 50/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9180. :  33%|███▎      | 1/3 [00:00<00:01,  1.57it/s]Finetune Epoch: 50/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9180. :  67%|██████▋   | 2/3 [00:00<00:00,  2.20it/s]Finetune Epoch: 50/70. Data: 0.92s. Batch: 0.97s. Loss: 0.9319. :  67%|██████▋   | 2/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 50/70. Data: 0.92s. Batch: 0.97s. Loss: 0.9319. : 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]Finetune Epoch: 50/70. Data: 0.92s. Batch: 0.97s. Loss: 0.9319. : 100%|██████████| 3/3 [00:01<00:00,  2.12it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8087. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8087. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.7993. top1: 93.95. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.96it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.7993. top1: 93.95. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8001. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8001. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.03it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8210. top1: 93.46. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.03it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8210. top1: 93.46. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9167. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9167. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9734. top1: 85.35. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9734. top1: 85.35. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0229. top1: 82.59. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  3.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0229. top1: 82.59. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0609. top1: 80.65. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0609. top1: 80.65. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0609. top1: 80.65. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.49s. Batch: 0.54s. Loss: 0.9155. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.49s. Batch: 0.54s. Loss: 0.9155. :  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s]Finetune Epoch: 51/70. Data: 0.65s. Batch: 0.71s. Loss: 0.9348. :  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s]Finetune Epoch: 51/70. Data: 0.65s. Batch: 0.71s. Loss: 0.9348. :  67%|██████▋   | 2/3 [00:00<00:00,  2.34it/s]Finetune Epoch: 51/70. Data: 0.79s. Batch: 0.85s. Loss: 0.9280. :  67%|██████▋   | 2/3 [00:01<00:00,  2.34it/s]Finetune Epoch: 51/70. Data: 0.79s. Batch: 0.85s. Loss: 0.9280. : 100%|██████████| 3/3 [00:01<00:00,  2.97it/s]Finetune Epoch: 51/70. Data: 0.79s. Batch: 0.85s. Loss: 0.9280. : 100%|██████████| 3/3 [00:01<00:00,  2.41it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8090. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8090. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.01it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7996. top1: 93.95. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.01it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.7996. top1: 93.95. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8003. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8003. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8212. top1: 93.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8212. top1: 93.36. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9167. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.79it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9167. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9732. top1: 85.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9732. top1: 85.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0226. top1: 82.65. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0226. top1: 82.65. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0605. top1: 80.70. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0605. top1: 80.70. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0605. top1: 80.70. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.53s. Batch: 0.60s. Loss: 0.9491. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.53s. Batch: 0.60s. Loss: 0.9491. :  33%|███▎      | 1/3 [00:00<00:01,  1.68it/s]Finetune Epoch: 52/70. Data: 0.70s. Batch: 0.78s. Loss: 0.9415. :  33%|███▎      | 1/3 [00:00<00:01,  1.68it/s]Finetune Epoch: 52/70. Data: 0.70s. Batch: 0.78s. Loss: 0.9415. :  67%|██████▋   | 2/3 [00:00<00:00,  2.18it/s]Finetune Epoch: 52/70. Data: 0.85s. Batch: 0.92s. Loss: 0.9346. :  67%|██████▋   | 2/3 [00:01<00:00,  2.18it/s]Finetune Epoch: 52/70. Data: 0.85s. Batch: 0.92s. Loss: 0.9346. : 100%|██████████| 3/3 [00:01<00:00,  2.79it/s]Finetune Epoch: 52/70. Data: 0.85s. Batch: 0.92s. Loss: 0.9346. : 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8092. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8092. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.7998. top1: 93.95. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.7998. top1: 93.95. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8005. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8005. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.14it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8214. top1: 93.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.14it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8214. top1: 93.36. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.56it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9167. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.56it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9167. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.75it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9730. top1: 85.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.75it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9730. top1: 85.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0224. top1: 82.70. top5: 99.33. :  75%|███████▌  | 6/8 [00:02<00:00,  3.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0224. top1: 82.70. top5: 99.33. :  88%|████████▊ | 7/8 [00:02<00:00,  4.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0602. top1: 80.75. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0602. top1: 80.75. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0602. top1: 80.75. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.39it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.52s. Batch: 0.57s. Loss: 0.9799. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.52s. Batch: 0.57s. Loss: 0.9799. :  33%|███▎      | 1/3 [00:00<00:01,  1.75it/s]Finetune Epoch: 53/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9504. :  33%|███▎      | 1/3 [00:00<00:01,  1.75it/s]Finetune Epoch: 53/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9504. :  67%|██████▋   | 2/3 [00:00<00:00,  2.52it/s]Finetune Epoch: 53/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9360. :  67%|██████▋   | 2/3 [00:01<00:00,  2.52it/s]Finetune Epoch: 53/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9360. : 100%|██████████| 3/3 [00:01<00:00,  2.84it/s]Finetune Epoch: 53/70. Data: 0.80s. Batch: 0.85s. Loss: 0.9360. : 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8095. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8095. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.05it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8000. top1: 93.95. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.05it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8000. top1: 93.95. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.93it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8007. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.93it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8007. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8216. top1: 93.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8216. top1: 93.36. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.68it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9166. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.68it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9166. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9729. top1: 85.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9729. top1: 85.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0221. top1: 82.70. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  3.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0221. top1: 82.70. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0598. top1: 80.75. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0598. top1: 80.75. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.51it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0598. top1: 80.75. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.67it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.48s. Batch: 0.54s. Loss: 0.8939. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.48s. Batch: 0.54s. Loss: 0.8939. :  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s]Finetune Epoch: 54/70. Data: 0.60s. Batch: 0.66s. Loss: 0.9242. :  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s]Finetune Epoch: 54/70. Data: 0.60s. Batch: 0.66s. Loss: 0.9242. :  67%|██████▋   | 2/3 [00:00<00:00,  2.78it/s]Finetune Epoch: 54/70. Data: 0.72s. Batch: 0.78s. Loss: 0.9206. :  67%|██████▋   | 2/3 [00:01<00:00,  2.78it/s]Finetune Epoch: 54/70. Data: 0.72s. Batch: 0.78s. Loss: 0.9206. : 100%|██████████| 3/3 [00:01<00:00,  3.26it/s]Finetune Epoch: 54/70. Data: 0.72s. Batch: 0.78s. Loss: 0.9206. : 100%|██████████| 3/3 [00:01<00:00,  2.57it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8097. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8097. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.97it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8002. top1: 93.95. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.97it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8002. top1: 93.95. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.99it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8010. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.99it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8010. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8218. top1: 93.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8218. top1: 93.36. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9166. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.75it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9166. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9727. top1: 85.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9727. top1: 85.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0218. top1: 82.76. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0218. top1: 82.76. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0594. top1: 80.80. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0594. top1: 80.80. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0594. top1: 80.80. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.77it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.54s. Batch: 0.60s. Loss: 0.9322. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.54s. Batch: 0.60s. Loss: 0.9322. :  33%|███▎      | 1/3 [00:00<00:01,  1.66it/s]Finetune Epoch: 55/70. Data: 0.66s. Batch: 0.72s. Loss: 0.9401. :  33%|███▎      | 1/3 [00:00<00:01,  1.66it/s]Finetune Epoch: 55/70. Data: 0.66s. Batch: 0.72s. Loss: 0.9401. :  67%|██████▋   | 2/3 [00:00<00:00,  2.55it/s]Finetune Epoch: 55/70. Data: 0.79s. Batch: 0.85s. Loss: 0.9414. :  67%|██████▋   | 2/3 [00:01<00:00,  2.55it/s]Finetune Epoch: 55/70. Data: 0.79s. Batch: 0.85s. Loss: 0.9414. : 100%|██████████| 3/3 [00:01<00:00,  2.98it/s]Finetune Epoch: 55/70. Data: 0.79s. Batch: 0.85s. Loss: 0.9414. : 100%|██████████| 3/3 [00:01<00:00,  2.46it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8100. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8100. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8005. top1: 93.95. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8005. top1: 93.95. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8012. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8012. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.43it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8221. top1: 93.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.43it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8221. top1: 93.36. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9166. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.78it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9166. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9725. top1: 85.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9725. top1: 85.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0215. top1: 82.81. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0215. top1: 82.81. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0591. top1: 80.85. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0591. top1: 80.85. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.62it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0591. top1: 80.85. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.50s. Batch: 0.55s. Loss: 0.9052. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.50s. Batch: 0.55s. Loss: 0.9052. :  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s]Finetune Epoch: 56/70. Data: 0.62s. Batch: 0.67s. Loss: 0.9389. :  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s]Finetune Epoch: 56/70. Data: 0.62s. Batch: 0.67s. Loss: 0.9389. :  67%|██████▋   | 2/3 [00:00<00:00,  2.70it/s]Finetune Epoch: 56/70. Data: 0.74s. Batch: 0.79s. Loss: 0.9267. :  67%|██████▋   | 2/3 [00:01<00:00,  2.70it/s]Finetune Epoch: 56/70. Data: 0.74s. Batch: 0.79s. Loss: 0.9267. : 100%|██████████| 3/3 [00:01<00:00,  3.23it/s]Finetune Epoch: 56/70. Data: 0.74s. Batch: 0.79s. Loss: 0.9267. : 100%|██████████| 3/3 [00:01<00:00,  2.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8103. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8103. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.04it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8007. top1: 93.95. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.04it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8007. top1: 93.95. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8014. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8014. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8222. top1: 93.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8222. top1: 93.36. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.82it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9165. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.82it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9165. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.93it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9723. top1: 85.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.93it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9723. top1: 85.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.18it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0212. top1: 82.81. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.18it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0212. top1: 82.81. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0587. top1: 80.85. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0587. top1: 80.85. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.73it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0587. top1: 80.85. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.81it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.47s. Batch: 0.52s. Loss: 0.9426. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.47s. Batch: 0.52s. Loss: 0.9426. :  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s]Finetune Epoch: 57/70. Data: 0.60s. Batch: 0.65s. Loss: 0.9484. :  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s]Finetune Epoch: 57/70. Data: 0.60s. Batch: 0.65s. Loss: 0.9484. :  67%|██████▋   | 2/3 [00:00<00:00,  2.77it/s]Finetune Epoch: 57/70. Data: 0.72s. Batch: 0.77s. Loss: 0.9319. :  67%|██████▋   | 2/3 [00:01<00:00,  2.77it/s]Finetune Epoch: 57/70. Data: 0.72s. Batch: 0.77s. Loss: 0.9319. : 100%|██████████| 3/3 [00:01<00:00,  3.24it/s]Finetune Epoch: 57/70. Data: 0.72s. Batch: 0.77s. Loss: 0.9319. : 100%|██████████| 3/3 [00:01<00:00,  2.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8105. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8105. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8009. top1: 93.95. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8009. top1: 93.95. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8016. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8016. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8224. top1: 93.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8224. top1: 93.36. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9165. top1: 88.98. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.99it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9165. top1: 88.98. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9722. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9722. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0209. top1: 82.87. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0209. top1: 82.87. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.13it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0584. top1: 80.90. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.13it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0584. top1: 80.90. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0584. top1: 80.90. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.77it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.47s. Batch: 0.53s. Loss: 0.8925. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.47s. Batch: 0.53s. Loss: 0.8925. :  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s]Finetune Epoch: 58/70. Data: 0.61s. Batch: 0.66s. Loss: 0.9073. :  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s]Finetune Epoch: 58/70. Data: 0.61s. Batch: 0.66s. Loss: 0.9073. :  67%|██████▋   | 2/3 [00:00<00:00,  2.67it/s]Finetune Epoch: 58/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9420. :  67%|██████▋   | 2/3 [00:01<00:00,  2.67it/s]Finetune Epoch: 58/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9420. : 100%|██████████| 3/3 [00:01<00:00,  3.04it/s]Finetune Epoch: 58/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9420. : 100%|██████████| 3/3 [00:01<00:00,  2.52it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8108. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8108. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.91it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8011. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.91it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8011. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.89it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8018. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.89it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8018. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.51it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8226. top1: 93.26. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.51it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8226. top1: 93.26. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.82it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9165. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.82it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9165. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9720. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9720. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.19it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0207. top1: 82.87. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.19it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0207. top1: 82.87. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0581. top1: 80.90. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0581. top1: 80.90. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0581. top1: 80.90. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.47s. Batch: 0.54s. Loss: 0.9890. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.47s. Batch: 0.54s. Loss: 0.9890. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch: 59/70. Data: 0.60s. Batch: 0.67s. Loss: 0.9537. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch: 59/70. Data: 0.60s. Batch: 0.67s. Loss: 0.9537. :  67%|██████▋   | 2/3 [00:00<00:00,  2.65it/s]Finetune Epoch: 59/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9319. :  67%|██████▋   | 2/3 [00:01<00:00,  2.65it/s]Finetune Epoch: 59/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9319. : 100%|██████████| 3/3 [00:01<00:00,  3.18it/s]Finetune Epoch: 59/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9319. : 100%|██████████| 3/3 [00:01<00:00,  2.57it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8109. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8109. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.05it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8013. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.05it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8013. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8020. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8020. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.61it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8228. top1: 93.26. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.61it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8228. top1: 93.26. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9164. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.99it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9164. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9718. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9718. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0204. top1: 82.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0204. top1: 82.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0578. top1: 80.95. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0578. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0578. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.50s. Batch: 0.55s. Loss: 0.9586. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.50s. Batch: 0.55s. Loss: 0.9586. :  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s]Finetune Epoch: 60/70. Data: 0.62s. Batch: 0.67s. Loss: 0.9309. :  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s]Finetune Epoch: 60/70. Data: 0.62s. Batch: 0.67s. Loss: 0.9309. :  67%|██████▋   | 2/3 [00:00<00:00,  2.68it/s]Finetune Epoch: 60/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9346. :  67%|██████▋   | 2/3 [00:01<00:00,  2.68it/s]Finetune Epoch: 60/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9346. : 100%|██████████| 3/3 [00:01<00:00,  3.18it/s]Finetune Epoch: 60/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9346. : 100%|██████████| 3/3 [00:01<00:00,  2.63it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8112. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8112. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.09it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8015. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.09it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8015. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.11it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8022. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.11it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8022. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.45it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8229. top1: 93.26. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.45it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8229. top1: 93.26. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9164. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9164. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9717. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9717. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0201. top1: 82.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0201. top1: 82.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0574. top1: 80.95. top5: 99.15. :  88%|████████▊ | 7/8 [00:02<00:00,  4.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0574. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  4.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0574. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.80it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.43s. Batch: 0.50s. Loss: 0.9337. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.43s. Batch: 0.50s. Loss: 0.9337. :  33%|███▎      | 1/3 [00:00<00:01,  1.99it/s]Finetune Epoch: 61/70. Data: 0.55s. Batch: 0.62s. Loss: 0.9221. :  33%|███▎      | 1/3 [00:00<00:01,  1.99it/s]Finetune Epoch: 61/70. Data: 0.55s. Batch: 0.62s. Loss: 0.9221. :  67%|██████▋   | 2/3 [00:00<00:00,  2.86it/s]Finetune Epoch: 61/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9310. :  67%|██████▋   | 2/3 [00:00<00:00,  2.86it/s]Finetune Epoch: 61/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9310. : 100%|██████████| 3/3 [00:00<00:00,  3.51it/s]Finetune Epoch: 61/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9310. : 100%|██████████| 3/3 [00:01<00:00,  2.82it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8115. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8115. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.19it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8017. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.19it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8017. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8025. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8025. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.75it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8232. top1: 93.26. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.75it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8232. top1: 93.26. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.17it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9164. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.17it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9164. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.39it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9715. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.39it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9715. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.52it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0199. top1: 82.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.52it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0199. top1: 82.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0570. top1: 80.95. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0570. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0570. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.02it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.56s. Batch: 0.62s. Loss: 0.8994. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.56s. Batch: 0.62s. Loss: 0.8994. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch: 62/70. Data: 0.69s. Batch: 0.76s. Loss: 0.9268. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch: 62/70. Data: 0.69s. Batch: 0.76s. Loss: 0.9268. :  67%|██████▋   | 2/3 [00:00<00:00,  2.37it/s]Finetune Epoch: 62/70. Data: 0.82s. Batch: 0.89s. Loss: 0.9273. :  67%|██████▋   | 2/3 [00:01<00:00,  2.37it/s]Finetune Epoch: 62/70. Data: 0.82s. Batch: 0.89s. Loss: 0.9273. : 100%|██████████| 3/3 [00:01<00:00,  2.95it/s]Finetune Epoch: 62/70. Data: 0.82s. Batch: 0.89s. Loss: 0.9273. : 100%|██████████| 3/3 [00:01<00:00,  2.42it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8117. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8117. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8019. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8019. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8026. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8026. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8233. top1: 93.26. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8233. top1: 93.26. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.19it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9164. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.19it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9164. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.33it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9714. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.33it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9714. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.52it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0196. top1: 82.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.52it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0196. top1: 82.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0567. top1: 80.95. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0567. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.76it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0567. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.05it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.53s. Batch: 0.59s. Loss: 0.9716. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.53s. Batch: 0.59s. Loss: 0.9716. :  33%|███▎      | 1/3 [00:00<00:01,  1.70it/s]Finetune Epoch: 63/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9518. :  33%|███▎      | 1/3 [00:00<00:01,  1.70it/s]Finetune Epoch: 63/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9518. :  67%|██████▋   | 2/3 [00:00<00:00,  2.63it/s]Finetune Epoch: 63/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9303. :  67%|██████▋   | 2/3 [00:01<00:00,  2.63it/s]Finetune Epoch: 63/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9303. : 100%|██████████| 3/3 [00:01<00:00,  3.01it/s]Finetune Epoch: 63/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9303. : 100%|██████████| 3/3 [00:01<00:00,  2.51it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8120. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8120. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8021. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8021. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.40it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8029. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.40it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8029. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.96it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8235. top1: 93.26. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.96it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8235. top1: 93.26. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.23it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9163. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.23it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9163. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9712. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9712. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.51it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0194. top1: 82.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.51it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0194. top1: 82.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0564. top1: 80.95. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0564. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0564. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.13it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.47s. Batch: 0.52s. Loss: 0.8889. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.47s. Batch: 0.52s. Loss: 0.8889. :  33%|███▎      | 1/3 [00:00<00:01,  1.92it/s]Finetune Epoch: 64/70. Data: 0.61s. Batch: 0.66s. Loss: 0.9121. :  33%|███▎      | 1/3 [00:00<00:01,  1.92it/s]Finetune Epoch: 64/70. Data: 0.61s. Batch: 0.66s. Loss: 0.9121. :  67%|██████▋   | 2/3 [00:00<00:00,  2.61it/s]Finetune Epoch: 64/70. Data: 0.74s. Batch: 0.80s. Loss: 0.9178. :  67%|██████▋   | 2/3 [00:01<00:00,  2.61it/s]Finetune Epoch: 64/70. Data: 0.74s. Batch: 0.80s. Loss: 0.9178. : 100%|██████████| 3/3 [00:01<00:00,  3.08it/s]Finetune Epoch: 64/70. Data: 0.74s. Batch: 0.80s. Loss: 0.9178. : 100%|██████████| 3/3 [00:01<00:00,  2.56it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8122. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8122. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.02it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8023. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.02it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8023. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8031. top1: 94.14. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8031. top1: 94.14. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8237. top1: 93.26. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8237. top1: 93.26. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.70it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9163. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.70it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9163. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.03it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9711. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.03it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9711. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0191. top1: 82.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0191. top1: 82.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0561. top1: 80.95. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0561. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.25s. Loss: 1.0561. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:02<00:00,  3.84it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.46s. Batch: 0.51s. Loss: 0.9384. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.46s. Batch: 0.51s. Loss: 0.9384. :  33%|███▎      | 1/3 [00:00<00:01,  1.97it/s]Finetune Epoch: 65/70. Data: 0.58s. Batch: 0.63s. Loss: 0.9367. :  33%|███▎      | 1/3 [00:00<00:01,  1.97it/s]Finetune Epoch: 65/70. Data: 0.58s. Batch: 0.63s. Loss: 0.9367. :  67%|██████▋   | 2/3 [00:00<00:00,  2.87it/s]Finetune Epoch: 65/70. Data: 0.70s. Batch: 0.75s. Loss: 0.9379. :  67%|██████▋   | 2/3 [00:00<00:00,  2.87it/s]Finetune Epoch: 65/70. Data: 0.70s. Batch: 0.75s. Loss: 0.9379. : 100%|██████████| 3/3 [00:00<00:00,  3.35it/s]Finetune Epoch: 65/70. Data: 0.70s. Batch: 0.75s. Loss: 0.9379. : 100%|██████████| 3/3 [00:01<00:00,  2.78it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8124. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8124. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.31it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8024. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.31it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8024. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8032. top1: 94.01. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8032. top1: 94.01. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.80it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8238. top1: 93.16. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.80it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8238. top1: 93.16. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.20it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9163. top1: 88.83. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.20it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9163. top1: 88.83. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.49it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9709. top1: 85.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.49it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9709. top1: 85.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.71it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0189. top1: 82.87. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.71it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0189. top1: 82.87. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0558. top1: 80.90. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0558. top1: 80.90. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  5.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0558. top1: 80.90. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.18it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.44s. Batch: 0.50s. Loss: 0.9324. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.44s. Batch: 0.50s. Loss: 0.9324. :  33%|███▎      | 1/3 [00:00<00:00,  2.01it/s]Finetune Epoch: 66/70. Data: 0.56s. Batch: 0.61s. Loss: 0.9600. :  33%|███▎      | 1/3 [00:00<00:00,  2.01it/s]Finetune Epoch: 66/70. Data: 0.56s. Batch: 0.61s. Loss: 0.9600. :  67%|██████▋   | 2/3 [00:00<00:00,  2.96it/s]Finetune Epoch: 66/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9416. :  67%|██████▋   | 2/3 [00:00<00:00,  2.96it/s]Finetune Epoch: 66/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9416. : 100%|██████████| 3/3 [00:00<00:00,  3.29it/s]Finetune Epoch: 66/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9416. : 100%|██████████| 3/3 [00:01<00:00,  2.78it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8126. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8126. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.35it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8026. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.35it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8026. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.40it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8034. top1: 94.01. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.40it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8034. top1: 94.01. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8240. top1: 93.16. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8240. top1: 93.16. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.05it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9162. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.05it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9162. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9708. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9708. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0187. top1: 82.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0187. top1: 82.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0556. top1: 80.95. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0556. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0556. top1: 80.95. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.06it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.41s. Batch: 0.48s. Loss: 0.8957. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.41s. Batch: 0.48s. Loss: 0.8957. :  33%|███▎      | 1/3 [00:00<00:00,  2.09it/s]Finetune Epoch: 67/70. Data: 0.53s. Batch: 0.60s. Loss: 0.9376. :  33%|███▎      | 1/3 [00:00<00:00,  2.09it/s]Finetune Epoch: 67/70. Data: 0.53s. Batch: 0.60s. Loss: 0.9376. :  67%|██████▋   | 2/3 [00:00<00:00,  2.90it/s]Finetune Epoch: 67/70. Data: 0.66s. Batch: 0.73s. Loss: 0.9306. :  67%|██████▋   | 2/3 [00:00<00:00,  2.90it/s]Finetune Epoch: 67/70. Data: 0.66s. Batch: 0.73s. Loss: 0.9306. : 100%|██████████| 3/3 [00:00<00:00,  3.35it/s]Finetune Epoch: 67/70. Data: 0.66s. Batch: 0.73s. Loss: 0.9306. : 100%|██████████| 3/3 [00:01<00:00,  2.80it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8128. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8128. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.60it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8028. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.60it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8028. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.62it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8036. top1: 94.01. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.62it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8036. top1: 94.01. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  4.15it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.8242. top1: 93.16. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  4.15it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.8242. top1: 93.16. top5: 100.00. :  50%|█████     | 4/8 [00:00<00:00,  4.35it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9162. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.35it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9162. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.23s. Loss: 0.9706. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.23s. Loss: 0.9706. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.71it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0184. top1: 82.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.71it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0184. top1: 82.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.22s. Loss: 1.0552. top1: 81.00. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.22s. Loss: 1.0552. top1: 81.00. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  5.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.22s. Loss: 1.0552. top1: 81.00. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.31it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 68/70. Data: 0.42s. Batch: 0.47s. Loss: 0.9177. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 68/70. Data: 0.42s. Batch: 0.47s. Loss: 0.9177. :  33%|███▎      | 1/3 [00:00<00:00,  2.12it/s]Finetune Epoch: 68/70. Data: 0.54s. Batch: 0.59s. Loss: 0.9324. :  33%|███▎      | 1/3 [00:00<00:00,  2.12it/s]Finetune Epoch: 68/70. Data: 0.54s. Batch: 0.59s. Loss: 0.9324. :  67%|██████▋   | 2/3 [00:00<00:00,  2.98it/s]Finetune Epoch: 68/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9264. :  67%|██████▋   | 2/3 [00:00<00:00,  2.98it/s]Finetune Epoch: 68/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9264. : 100%|██████████| 3/3 [00:00<00:00,  3.49it/s]Finetune Epoch: 68/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9264. : 100%|██████████| 3/3 [00:01<00:00,  2.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8131. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8131. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8030. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8030. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8038. top1: 94.01. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8038. top1: 94.01. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.8243. top1: 93.16. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.8243. top1: 93.16. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.27it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9162. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.27it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9162. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.53it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9705. top1: 85.48. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.53it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9705. top1: 85.48. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0182. top1: 82.92. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0182. top1: 82.92. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.22s. Loss: 1.0549. top1: 81.00. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.22s. Loss: 1.0549. top1: 81.00. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  5.13it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.22s. Loss: 1.0549. top1: 81.00. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.23it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.40s. Batch: 0.44s. Loss: 0.9315. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.40s. Batch: 0.44s. Loss: 0.9315. :  33%|███▎      | 1/3 [00:00<00:00,  2.25it/s]Finetune Epoch: 69/70. Data: 0.52s. Batch: 0.57s. Loss: 0.9340. :  33%|███▎      | 1/3 [00:00<00:00,  2.25it/s]Finetune Epoch: 69/70. Data: 0.52s. Batch: 0.57s. Loss: 0.9340. :  67%|██████▋   | 2/3 [00:00<00:00,  3.05it/s]Finetune Epoch: 69/70. Data: 0.65s. Batch: 0.69s. Loss: 0.9317. :  67%|██████▋   | 2/3 [00:00<00:00,  3.05it/s]Finetune Epoch: 69/70. Data: 0.65s. Batch: 0.69s. Loss: 0.9317. : 100%|██████████| 3/3 [00:00<00:00,  3.41it/s]Finetune Epoch: 69/70. Data: 0.65s. Batch: 0.69s. Loss: 0.9317. : 100%|██████████| 3/3 [00:01<00:00,  2.89it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8133. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8133. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8033. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8033. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8040. top1: 94.01. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8040. top1: 94.01. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.66it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8245. top1: 93.16. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.66it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8245. top1: 93.16. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9162. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.06it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9162. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9703. top1: 85.55. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9703. top1: 85.55. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.55it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0179. top1: 82.98. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.55it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0179. top1: 82.98. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.62it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0546. top1: 81.10. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.62it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0546. top1: 81.10. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.23s. Loss: 1.0546. top1: 81.10. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.10it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.42s. Batch: 0.47s. Loss: 0.9681. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.42s. Batch: 0.47s. Loss: 0.9681. :  33%|███▎      | 1/3 [00:00<00:00,  2.14it/s]Finetune Epoch: 70/70. Data: 0.53s. Batch: 0.59s. Loss: 0.9360. :  33%|███▎      | 1/3 [00:00<00:00,  2.14it/s]Finetune Epoch: 70/70. Data: 0.53s. Batch: 0.59s. Loss: 0.9360. :  67%|██████▋   | 2/3 [00:00<00:00,  3.00it/s]Finetune Epoch: 70/70. Data: 0.66s. Batch: 0.72s. Loss: 0.9302. :  67%|██████▋   | 2/3 [00:00<00:00,  3.00it/s]Finetune Epoch: 70/70. Data: 0.66s. Batch: 0.72s. Loss: 0.9302. : 100%|██████████| 3/3 [00:00<00:00,  3.25it/s]Finetune Epoch: 70/70. Data: 0.66s. Batch: 0.72s. Loss: 0.9302. : 100%|██████████| 3/3 [00:01<00:00,  2.76it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8136. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8136. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.49it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8035. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:02,  2.49it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8035. top1: 93.75. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8042. top1: 94.01. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8042. top1: 94.01. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8247. top1: 93.16. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8247. top1: 93.16. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:00,  4.10it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9161. top1: 88.91. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:00,  4.10it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9161. top1: 88.91. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9702. top1: 85.55. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  4.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9702. top1: 85.55. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  4.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0176. top1: 83.04. top5: 99.33. :  75%|███████▌  | 6/8 [00:01<00:00,  4.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0176. top1: 83.04. top5: 99.33. :  88%|████████▊ | 7/8 [00:01<00:00,  4.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0542. top1: 81.15. top5: 99.15. :  88%|████████▊ | 7/8 [00:01<00:00,  4.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0542. top1: 81.15. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 1.0542. top1: 81.15. top5: 99.15. : 100%|██████████| 8/8 [00:01<00:00,  4.05it/s]
total 9984 correct 7083 accuracy 70.94350961538461
[INFO] main.py:349 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 2 -> 10
[INFO] main.py:357 > [2-3] Start to train under online
[INFO] main.py:372 > Train over streamed data once
batch_size : 128 stream_batch_size : 64 memory_batch_size : 42 pseudo_stream_size 64
num_stuff 156
[INFO] rainbow_memory.py:120 > Streamed samples: 800
[INFO] rainbow_memory.py:121 > In-memory samples: 0
[INFO] rainbow_memory.py:122 > Pseudo samples: 9984
[INFO] rainbow_memory.py:128 > Train samples: 10784
[INFO] rainbow_memory.py:129 > Test samples: 2000
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
stream torch.Size([64, 3, 32, 32]) pseudo torch.Size([64, 3, 32, 32])
last_idx 11
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
pseudo shape torch.Size([64, 3, 32, 32])
final_idx 156
task0/train/loss 0.6374712432645688 0
task0/test/loss 0.5677731456235051 0
task0/test/acc 0.7565 0
task0/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 1/1 | train_loss 0.6375 | train_acc 0.6660 | test_loss 0.5678 | test_acc 0.7565 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[WARNING] finetune.py:736 > Fill the unused slots by breaking the equilibrium.
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
deer    277
dog     223
Name: klass, dtype: int64
[INFO] main.py:388 > Train over memory
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42 pseudo_stream_size 42
num_stuff 0
[INFO] rainbow_memory.py:120 > Streamed samples: 0
[INFO] rainbow_memory.py:121 > In-memory samples: 500
[INFO] rainbow_memory.py:122 > Pseudo samples: 0
[INFO] rainbow_memory.py:128 > Train samples: 500
[INFO] rainbow_memory.py:129 > Test samples: 2000
last_idx 11
final_idx 0
task0/train/loss 0.6047728881239891 0
task0/test/loss 0.517121858894825 0
task0/test/acc 0.744 0
task0/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 1/256 | train_loss 0.6048 | train_acc 0.7300 | test_loss 0.5171 | test_acc 0.7440 | lr 0.0050
last_idx 11
final_idx 0
task0/train/loss 6.0742997626463575 1
task0/test/loss 3679.031373529137 1
task0/test/acc 0.5 1
task0/train/lr 0.05 1
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 2/256 | train_loss 6.0743 | train_acc 0.4820 | test_loss 3679.0314 | test_acc 0.5000 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 3.278062085310618 2
task0/test/loss 199.57786648042256 2
task0/test/acc 0.553 2
task0/train/lr 0.05 2
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 3/256 | train_loss 3.2781 | train_acc 0.4540 | test_loss 199.5779 | test_acc 0.5530 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 1.2988323817650478 3
task0/test/loss 4.497497143628805 3
task0/test/acc 0.5 3
task0/train/lr 0.02525 3
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 4/256 | train_loss 1.2988 | train_acc 0.5200 | test_loss 4.4975 | test_acc 0.5000 | lr 0.0253
last_idx 11
final_idx 0
task0/train/loss 2.271151622136434 4
task0/test/loss 1.9776181552721106 4
task0/test/acc 0.548 4
task0/train/lr 0.05 4
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 5/256 | train_loss 2.2712 | train_acc 0.5020 | test_loss 1.9776 | test_acc 0.5480 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 1.1429530382156372 5
task0/test/loss 1.1471985092629557 5
task0/test/acc 0.5 5
task0/train/lr 0.04275089283436705 5
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 6/256 | train_loss 1.1430 | train_acc 0.5140 | test_loss 1.1472 | test_acc 0.5000 | lr 0.0428
last_idx 11
final_idx 0
task0/train/loss 0.9843345483144125 6
task0/test/loss 0.8372596320898636 6
task0/test/acc 0.5005 6
task0/train/lr 0.02525 6
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 7/256 | train_loss 0.9843 | train_acc 0.5620 | test_loss 0.8373 | test_acc 0.5005 | lr 0.0253
last_idx 11
final_idx 0
task0/train/loss 0.8001319766044617 7
task0/test/loss 0.7451081625793291 7
task0/test/acc 0.5095 7
task0/train/lr 0.00774910716563295 7
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 8/256 | train_loss 0.8001 | train_acc 0.5100 | test_loss 0.7451 | test_acc 0.5095 | lr 0.0077
last_idx 11
final_idx 0
task0/train/loss 1.4954725752274196 8
task0/test/loss 0.9258983536906864 8
task0/test/acc 0.551 8
task0/train/lr 0.05 8
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 9/256 | train_loss 1.4955 | train_acc 0.5020 | test_loss 0.9259 | test_acc 0.5510 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 1.150717760125796 9
task0/test/loss 0.8983595585045607 9
task0/test/acc 0.501 9
task0/train/lr 0.04811601842965435 9
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 10/256 | train_loss 1.1507 | train_acc 0.5000 | test_loss 0.8984 | test_acc 0.5010 | lr 0.0481
last_idx 11
final_idx 0
task0/train/loss 1.06669386724631 10
task0/test/loss 0.8278055346530416 10
task0/test/acc 0.5 10
task0/train/lr 0.04275089283436705 10
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 11/256 | train_loss 1.0667 | train_acc 0.5300 | test_loss 0.8278 | test_acc 0.5000 | lr 0.0428
last_idx 11
final_idx 0
task0/train/loss 0.8790049999952316 11
task0/test/loss 0.682649729044541 11
task0/test/acc 0.565 11
task0/train/lr 0.03472141495103598 11
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 12/256 | train_loss 0.8790 | train_acc 0.5240 | test_loss 0.6826 | test_acc 0.5650 | lr 0.0347
last_idx 11
final_idx 0
task0/train/loss 0.7740298906962076 12
task0/test/loss 0.742731652829958 12
task0/test/acc 0.5495 12
task0/train/lr 0.02525 12
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 13/256 | train_loss 0.7740 | train_acc 0.5400 | test_loss 0.7427 | test_acc 0.5495 | lr 0.0253
last_idx 11
final_idx 0
task0/train/loss 0.7523669898509979 13
task0/test/loss 0.7640616841938185 13
task0/test/acc 0.5275 13
task0/train/lr 0.01577858504896403 13
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 14/256 | train_loss 0.7524 | train_acc 0.5680 | test_loss 0.7641 | test_acc 0.5275 | lr 0.0158
last_idx 11
final_idx 0
task0/train/loss 0.730907678604126 14
task0/test/loss 0.7959798567968867 14
task0/test/acc 0.515 14
task0/train/lr 0.00774910716563295 14
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 15/256 | train_loss 0.7309 | train_acc 0.5560 | test_loss 0.7960 | test_acc 0.5150 | lr 0.0077
last_idx 11
final_idx 0
task0/train/loss 0.7124715745449066 15
task0/test/loss 0.6774251713700916 15
task0/test/acc 0.558 15
task0/train/lr 0.0023839815703456534 15
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 16/256 | train_loss 0.7125 | train_acc 0.5980 | test_loss 0.6774 | test_acc 0.5580 | lr 0.0024
last_idx 11
final_idx 0
task0/train/loss 0.8678571532169977 16
task0/test/loss 0.8166053683861442 16
task0/test/acc 0.6495 16
task0/train/lr 0.05 16
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 17/256 | train_loss 0.8679 | train_acc 0.5800 | test_loss 0.8166 | test_acc 0.6495 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 0.9587687402963638 17
task0/test/loss 0.6814158889262573 17
task0/test/acc 0.59 17
task0/train/lr 0.049524435689979954 17
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 18/256 | train_loss 0.9588 | train_acc 0.5500 | test_loss 0.6814 | test_acc 0.5900 | lr 0.0495
last_idx 11
final_idx 0
task0/train/loss 0.8045061777035395 18
task0/test/loss 0.9180374161704726 18
task0/test/acc 0.5 18
task0/train/lr 0.04811601842965435 18
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 19/256 | train_loss 0.8045 | train_acc 0.5300 | test_loss 0.9180 | test_acc 0.5000 | lr 0.0481
last_idx 11
final_idx 0
task0/train/loss 0.838535209496816 19
task0/test/loss 0.6625831075336622 19
task0/test/acc 0.602 19
task0/train/lr 0.045828872904488 19
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 20/256 | train_loss 0.8385 | train_acc 0.5340 | test_loss 0.6626 | test_acc 0.6020 | lr 0.0458
last_idx 11
final_idx 0
task0/train/loss 0.7289919902880987 20
task0/test/loss 0.7675747213804204 20
task0/test/acc 0.5255 20
task0/train/lr 0.04275089283436705 20
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 21/256 | train_loss 0.7290 | train_acc 0.5980 | test_loss 0.7676 | test_acc 0.5255 | lr 0.0428
last_idx 11
final_idx 0
task0/train/loss 0.7182151277860006 21
task0/test/loss 0.6932942348977794 21
task0/test/acc 0.6675 21
task0/train/lr 0.039000363267235154 21
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 22/256 | train_loss 0.7182 | train_acc 0.5740 | test_loss 0.6933 | test_acc 0.6675 | lr 0.0390
last_idx 11
final_idx 0
task0/train/loss 0.7586190849542618 22
task0/test/loss 0.7376343659732653 22
task0/test/acc 0.5225 22
task0/train/lr 0.03472141495103598 22
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 23/256 | train_loss 0.7586 | train_acc 0.5340 | test_loss 0.7376 | test_acc 0.5225 | lr 0.0347
last_idx 11
final_idx 0
task0/train/loss 0.6688288350900015 23
task0/test/loss 0.7626516955054324 23
task0/test/acc 0.5415 23
task0/train/lr 0.03007848546989918 23
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 24/256 | train_loss 0.6688 | train_acc 0.6100 | test_loss 0.7627 | test_acc 0.5415 | lr 0.0301
last_idx 11
final_idx 0
task0/train/loss 0.6697872479756674 24
task0/test/loss 0.7208050703224929 24
task0/test/acc 0.597 24
task0/train/lr 0.02525 24
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 25/256 | train_loss 0.6698 | train_acc 0.6000 | test_loss 0.7208 | test_acc 0.5970 | lr 0.0253
last_idx 11
final_idx 0
task0/train/loss 0.7072311490774155 25
task0/test/loss 0.6545289340226547 25
task0/test/acc 0.6365 25
task0/train/lr 0.02042151453010083 25
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 26/256 | train_loss 0.7072 | train_acc 0.5920 | test_loss 0.6545 | test_acc 0.6365 | lr 0.0204
last_idx 11
final_idx 0
task0/train/loss 0.6700829962889353 26
task0/test/loss 0.7066005261048026 26
task0/test/acc 0.579 26
task0/train/lr 0.01577858504896403 26
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 27/256 | train_loss 0.6701 | train_acc 0.5880 | test_loss 0.7066 | test_acc 0.5790 | lr 0.0158
last_idx 11
final_idx 0
task0/train/loss 0.6927236169576645 27
task0/test/loss 0.6976471384582312 27
task0/test/acc 0.5795 27
task0/train/lr 0.011499636732764853 27
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 28/256 | train_loss 0.6927 | train_acc 0.6080 | test_loss 0.6976 | test_acc 0.5795 | lr 0.0115
last_idx 11
final_idx 0
task0/train/loss 0.6794234613577524 28
task0/test/loss 0.6161062302796737 28
task0/test/acc 0.6575 28
task0/train/lr 0.00774910716563295 28
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 29/256 | train_loss 0.6794 | train_acc 0.6160 | test_loss 0.6161 | test_acc 0.6575 | lr 0.0077
last_idx 11
final_idx 0
task0/train/loss 0.6572455714146296 29
task0/test/loss 0.6488500632669615 29
task0/test/acc 0.622 29
task0/train/lr 0.004671127095512003 29
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 30/256 | train_loss 0.6572 | train_acc 0.5800 | test_loss 0.6489 | test_acc 0.6220 | lr 0.0047
last_idx 11
final_idx 0
task0/train/loss 0.6442148586114248 30
task0/test/loss 0.6865296551714772 30
task0/test/acc 0.594 30
task0/train/lr 0.0023839815703456534 30
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 31/256 | train_loss 0.6442 | train_acc 0.6460 | test_loss 0.6865 | test_acc 0.5940 | lr 0.0024
last_idx 11
final_idx 0
task0/train/loss 0.6487177610397339 31
task0/test/loss 0.6831468087823495 31
task0/test/acc 0.598 31
task0/train/lr 0.0009755643100200469 31
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 32/256 | train_loss 0.6487 | train_acc 0.6280 | test_loss 0.6831 | test_acc 0.5980 | lr 0.0010
last_idx 11
final_idx 0
task0/train/loss 0.7418459604183832 32
task0/test/loss 0.8608184942732686 32
task0/test/acc 0.519 32
task0/train/lr 0.05 32
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 33/256 | train_loss 0.7418 | train_acc 0.6360 | test_loss 0.8608 | test_acc 0.5190 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 0.6933148801326752 33
task0/test/loss 0.732133545305418 33
task0/test/acc 0.6175 33
task0/train/lr 0.049880821985136874 33
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 34/256 | train_loss 0.6933 | train_acc 0.6200 | test_loss 0.7321 | test_acc 0.6175 | lr 0.0499
last_idx 11
final_idx 0
task0/train/loss 0.6756772051254908 34
task0/test/loss 0.6329848481261212 34
task0/test/acc 0.7125 34
task0/train/lr 0.049524435689979954 34
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 35/256 | train_loss 0.6757 | train_acc 0.6260 | test_loss 0.6330 | test_acc 0.7125 | lr 0.0495
last_idx 11
final_idx 0
task0/train/loss 0.7306775897741318 35
task0/test/loss 1.0456225610459629 35
task0/test/acc 0.5065 35
task0/train/lr 0.048934273309372174 35
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 36/256 | train_loss 0.7307 | train_acc 0.6140 | test_loss 1.0456 | test_acc 0.5065 | lr 0.0489
last_idx 11
final_idx 0
task0/train/loss 0.753970871369044 36
task0/test/loss 0.6807197432803072 36
task0/test/acc 0.645 36
task0/train/lr 0.04811601842965435 36
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 37/256 | train_loss 0.7540 | train_acc 0.6120 | test_loss 0.6807 | test_acc 0.6450 | lr 0.0481
last_idx 11
final_idx 0
task0/train/loss 0.7110370695590973 37
task0/test/loss 0.5909934575143068 37
task0/test/acc 0.701 37
task0/train/lr 0.04707755129262179 37
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 38/256 | train_loss 0.7110 | train_acc 0.6280 | test_loss 0.5910 | test_acc 0.7010 | lr 0.0471
last_idx 11
final_idx 0
task0/train/loss 0.6567176729440689 38
task0/test/loss 0.7099772040610728 38
task0/test/acc 0.6445 38
task0/train/lr 0.045828872904488 38
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 39/256 | train_loss 0.6567 | train_acc 0.6660 | test_loss 0.7100 | test_acc 0.6445 | lr 0.0458
last_idx 11
final_idx 0
task0/train/loss 0.6655645171801249 39
task0/test/loss 0.5496072134245997 39
task0/test/acc 0.752 39
task0/train/lr 0.04438200872072774 39
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 40/256 | train_loss 0.6656 | train_acc 0.6500 | test_loss 0.5496 | test_acc 0.7520 | lr 0.0444
last_idx 11
final_idx 0
task0/train/loss 0.6382633298635483 40
task0/test/loss 0.7565322563700054 40
task0/test/acc 0.61 40
task0/train/lr 0.04275089283436705 40
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 41/256 | train_loss 0.6383 | train_acc 0.6280 | test_loss 0.7565 | test_acc 0.6100 | lr 0.0428
last_idx 11
final_idx 0
task0/train/loss 0.6504499614238739 41
task0/test/loss 0.6406808236370916 41
task0/test/acc 0.691 41
task0/train/lr 0.040951233783050225 41
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 42/256 | train_loss 0.6504 | train_acc 0.6340 | test_loss 0.6407 | test_acc 0.6910 | lr 0.0410
last_idx 11
final_idx 0
task0/train/loss 0.6454664766788483 42
task0/test/loss 0.7258298834380896 42
task0/test/acc 0.6485 42
task0/train/lr 0.039000363267235154 42
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 43/256 | train_loss 0.6455 | train_acc 0.6540 | test_loss 0.7258 | test_acc 0.6485 | lr 0.0390
last_idx 11
final_idx 0
task0/train/loss 0.6770796279112498 43
task0/test/loss 0.5400189867486125 43
task0/test/acc 0.739 43
task0/train/lr 0.03691706923644345 43
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 44/256 | train_loss 0.6771 | train_acc 0.6160 | test_loss 0.5400 | test_acc 0.7390 | lr 0.0369
last_idx 11
final_idx 0
task0/train/loss 0.6203597585360209 44
task0/test/loss 0.7111177091365275 44
task0/test/acc 0.604 44
task0/train/lr 0.03472141495103598 44
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 45/256 | train_loss 0.6204 | train_acc 0.6860 | test_loss 0.7111 | test_acc 0.6040 | lr 0.0347
last_idx 11
final_idx 0
task0/train/loss 0.6518984486659368 45
task0/test/loss 0.57121752170117 45
task0/test/acc 0.7015 45
task0/train/lr 0.03243454576204794 45
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 46/256 | train_loss 0.6519 | train_acc 0.6640 | test_loss 0.5712 | test_acc 0.7015 | lr 0.0324
last_idx 11
final_idx 0
task0/train/loss 0.6522575716177622 46
task0/test/loss 0.5652177333831787 46
task0/test/acc 0.7105 46
task0/train/lr 0.03007848546989918 46
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 47/256 | train_loss 0.6523 | train_acc 0.5980 | test_loss 0.5652 | test_acc 0.7105 | lr 0.0301
last_idx 11
final_idx 0
task0/train/loss 0.6459436515967051 47
task0/test/loss 0.6364831134029056 47
task0/test/acc 0.6525 47
task0/train/lr 0.027675924223156633 47
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 48/256 | train_loss 0.6459 | train_acc 0.6300 | test_loss 0.6365 | test_acc 0.6525 | lr 0.0277
last_idx 11
final_idx 0
task0/train/loss 0.6227584977944692 48
task0/test/loss 0.6068960279226303 48
task0/test/acc 0.715 48
task0/train/lr 0.02525 48
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 49/256 | train_loss 0.6228 | train_acc 0.6760 | test_loss 0.6069 | test_acc 0.7150 | lr 0.0253
last_idx 11
final_idx 0
task0/train/loss 0.6243180831273397 49
task0/test/loss 0.8352725353577862 49
task0/test/acc 0.5895 49
task0/train/lr 0.022824075776843374 49
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 50/256 | train_loss 0.6243 | train_acc 0.6760 | test_loss 0.8353 | test_acc 0.5895 | lr 0.0228
last_idx 11
final_idx 0
task0/train/loss 0.6065497249364853 50
task0/test/loss 0.6822266377832579 50
task0/test/acc 0.6535 50
task0/train/lr 0.02042151453010083 50
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 51/256 | train_loss 0.6065 | train_acc 0.6720 | test_loss 0.6822 | test_acc 0.6535 | lr 0.0204
last_idx 11
final_idx 0
task0/train/loss 0.5893107826511065 51
task0/test/loss 0.7642378335737664 51
task0/test/acc 0.616 51
task0/train/lr 0.018065454237952062 51
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 52/256 | train_loss 0.5893 | train_acc 0.6680 | test_loss 0.7642 | test_acc 0.6160 | lr 0.0181
last_idx 11
final_idx 0
task0/train/loss 0.5948903436462084 52
task0/test/loss 0.6500434256766153 52
task0/test/acc 0.679 52
task0/train/lr 0.01577858504896403 52
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 53/256 | train_loss 0.5949 | train_acc 0.7160 | test_loss 0.6500 | test_acc 0.6790 | lr 0.0158
last_idx 11
final_idx 0
task0/train/loss 0.618353304763635 53
task0/test/loss 0.5718198195099831 53
task0/test/acc 0.701 53
task0/train/lr 0.013582930763556558 53
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 54/256 | train_loss 0.6184 | train_acc 0.6680 | test_loss 0.5718 | test_acc 0.7010 | lr 0.0136
last_idx 11
final_idx 0
task0/train/loss 0.5578656370441119 54
task0/test/loss 0.6037498262265454 54
task0/test/acc 0.691 54
task0/train/lr 0.011499636732764853 54
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 55/256 | train_loss 0.5579 | train_acc 0.7060 | test_loss 0.6037 | test_acc 0.6910 | lr 0.0115
last_idx 11
final_idx 0
task0/train/loss 0.5755842054883639 55
task0/test/loss 0.6522579303254252 55
task0/test/acc 0.681 55
task0/train/lr 0.009548766216949778 55
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 56/256 | train_loss 0.5756 | train_acc 0.6660 | test_loss 0.6523 | test_acc 0.6810 | lr 0.0095
last_idx 11
final_idx 0
task0/train/loss 0.5810448328653971 56
task0/test/loss 0.5882971247901088 56
task0/test/acc 0.7045 56
task0/train/lr 0.00774910716563295 56
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 57/256 | train_loss 0.5810 | train_acc 0.6840 | test_loss 0.5883 | test_acc 0.7045 | lr 0.0077
last_idx 11
final_idx 0
task0/train/loss 0.5638391127189001 57
task0/test/loss 0.6226216272815414 57
task0/test/acc 0.688 57
task0/train/lr 0.0061179912792722595 57
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 58/256 | train_loss 0.5638 | train_acc 0.6980 | test_loss 0.6226 | test_acc 0.6880 | lr 0.0061
last_idx 11
final_idx 0
task0/train/loss 0.579550730685393 58
task0/test/loss 0.5654311267578084 58
task0/test/acc 0.713 58
task0/train/lr 0.004671127095512003 58
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 59/256 | train_loss 0.5796 | train_acc 0.7340 | test_loss 0.5654 | test_acc 0.7130 | lr 0.0047
last_idx 11
final_idx 0
task0/train/loss 0.5684914688269297 59
task0/test/loss 0.6023681798706884 59
task0/test/acc 0.7 59
task0/train/lr 0.0034224487073782153 59
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 60/256 | train_loss 0.5685 | train_acc 0.6980 | test_loss 0.6024 | test_acc 0.7000 | lr 0.0034
last_idx 11
final_idx 0
task0/train/loss 0.5842959756652514 60
task0/test/loss 0.6053723806272382 60
task0/test/acc 0.697 60
task0/train/lr 0.0023839815703456534 60
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 61/256 | train_loss 0.5843 | train_acc 0.7080 | test_loss 0.6054 | test_acc 0.6970 | lr 0.0024
last_idx 11
final_idx 0
task0/train/loss 0.5935213342308998 61
task0/test/loss 0.5792376742414806 61
task0/test/acc 0.7085 61
task0/train/lr 0.0015657266906278318 61
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 62/256 | train_loss 0.5935 | train_acc 0.6980 | test_loss 0.5792 | test_acc 0.7085 | lr 0.0016
last_idx 11
final_idx 0
task0/train/loss 0.5475579177339872 62
task0/test/loss 0.6070085948576098 62
task0/test/acc 0.6905 62
task0/train/lr 0.0009755643100200469 62
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 63/256 | train_loss 0.5476 | train_acc 0.7080 | test_loss 0.6070 | test_acc 0.6905 | lr 0.0010
last_idx 11
final_idx 0
task0/train/loss 0.5343350271383921 63
task0/test/loss 0.6287630193907282 63
task0/test/acc 0.686 63
task0/train/lr 0.0006191780148631288 63
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 64/256 | train_loss 0.5343 | train_acc 0.7260 | test_loss 0.6288 | test_acc 0.6860 | lr 0.0006
last_idx 11
final_idx 0
task0/train/loss 0.5950935607155164 64
task0/test/loss 0.8156225473984428 64
task0/test/acc 0.6105 64
task0/train/lr 0.05 64
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 65/256 | train_loss 0.5951 | train_acc 0.7000 | test_loss 0.8156 | test_acc 0.6105 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 0.6532410929600397 65
task0/test/loss 0.6651321183080259 65
task0/test/acc 0.644 65
task0/train/lr 0.04997018754107802 65
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 66/256 | train_loss 0.6532 | train_acc 0.6680 | test_loss 0.6651 | test_acc 0.6440 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 0.640918917953968 66
task0/test/loss 0.5777096690043159 66
task0/test/acc 0.742 66
task0/train/lr 0.049880821985136874 66
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 67/256 | train_loss 0.6409 | train_acc 0.6740 | test_loss 0.5777 | test_acc 0.7420 | lr 0.0499
last_idx 11
final_idx 0
task0/train/loss 0.621045840283235 67
task0/test/loss 0.588745683107687 67
task0/test/acc 0.6955 67
task0/train/lr 0.04973211862162834 67
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 68/256 | train_loss 0.6210 | train_acc 0.6780 | test_loss 0.5887 | test_acc 0.6955 | lr 0.0497
last_idx 11
final_idx 0
task0/train/loss 0.6561431487401327 68
task0/test/loss 0.590160172270692 68
task0/test/acc 0.6745 68
task0/train/lr 0.049524435689979954 68
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 69/256 | train_loss 0.6561 | train_acc 0.6940 | test_loss 0.5902 | test_acc 0.6745 | lr 0.0495
last_idx 11
final_idx 0
task0/train/loss 0.6000923464695612 69
task0/test/loss 0.7895244782709557 69
task0/test/acc 0.6175 69
task0/train/lr 0.04925827351656497 69
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 70/256 | train_loss 0.6001 | train_acc 0.6880 | test_loss 0.7895 | test_acc 0.6175 | lr 0.0493
last_idx 11
final_idx 0
task0/train/loss 0.6170972064137459 70
task0/test/loss 0.7319210333668668 70
task0/test/acc 0.619 70
task0/train/lr 0.048934273309372174 70
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 71/256 | train_loss 0.6171 | train_acc 0.6800 | test_loss 0.7319 | test_acc 0.6190 | lr 0.0489
last_idx 11
final_idx 0
task0/train/loss 0.6323257436354955 71
task0/test/loss 0.5295266709897829 71
task0/test/acc 0.744 71
task0/train/lr 0.048553215613279764 71
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 72/256 | train_loss 0.6323 | train_acc 0.6660 | test_loss 0.5295 | test_acc 0.7440 | lr 0.0486
last_idx 11
final_idx 0
task0/train/loss 0.621872012813886 72
task0/test/loss 0.5542621418185856 72
task0/test/acc 0.738 72
task0/train/lr 0.04811601842965435 72
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 73/256 | train_loss 0.6219 | train_acc 0.7040 | test_loss 0.5543 | test_acc 0.7380 | lr 0.0481
last_idx 11
final_idx 0
task0/train/loss 0.5803327783942223 73
task0/test/loss 0.6282357068165488 73
task0/test/acc 0.71 73
task0/train/lr 0.047623735004805226 73
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 74/256 | train_loss 0.5803 | train_acc 0.7060 | test_loss 0.6282 | test_acc 0.7100 | lr 0.0476
last_idx 11
final_idx 0
task0/train/loss 0.6395961989959081 74
task0/test/loss 0.6951849434686743 74
task0/test/acc 0.6755 74
task0/train/lr 0.04707755129262179 74
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 75/256 | train_loss 0.6396 | train_acc 0.6520 | test_loss 0.6952 | test_acc 0.6755 | lr 0.0471
last_idx 11
final_idx 0
task0/train/loss 0.6357429698109627 75
task0/test/loss 0.5769269621890524 75
task0/test/acc 0.7005 75
task0/train/lr 0.046478783097506735 75
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 76/256 | train_loss 0.6357 | train_acc 0.6840 | test_loss 0.5769 | test_acc 0.7005 | lr 0.0465
last_idx 11
final_idx 0
task0/train/loss 0.6029200752576193 76
task0/test/loss 0.5889509197162546 76
task0/test/acc 0.717 76
task0/train/lr 0.045828872904488 76
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 77/256 | train_loss 0.6029 | train_acc 0.6720 | test_loss 0.5890 | test_acc 0.7170 | lr 0.0458
last_idx 11
final_idx 0
task0/train/loss 0.6061187262336413 77
task0/test/loss 0.5348775995814282 77
task0/test/acc 0.736 77
task0/train/lr 0.04512938640414596 77
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 78/256 | train_loss 0.6061 | train_acc 0.6780 | test_loss 0.5349 | test_acc 0.7360 | lr 0.0451
last_idx 11
final_idx 0
task0/train/loss 0.6088030685981115 78
task0/test/loss 0.6275095570346584 78
task0/test/acc 0.6815 78
task0/train/lr 0.04438200872072774 78
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 79/256 | train_loss 0.6088 | train_acc 0.6820 | test_loss 0.6275 | test_acc 0.6815 | lr 0.0444
last_idx 11
final_idx 0
task0/train/loss 0.6108980824549993 79
task0/test/loss 0.6762369608749514 79
task0/test/acc 0.633 79
task0/train/lr 0.043588540352535246 79
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 80/256 | train_loss 0.6109 | train_acc 0.7040 | test_loss 0.6762 | test_acc 0.6330 | lr 0.0436
last_idx 11
final_idx 0
task0/train/loss 0.6172421624263128 80
task0/test/loss 0.5738367629439934 80
task0/test/acc 0.702 80
task0/train/lr 0.04275089283436705 80
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 81/256 | train_loss 0.6172 | train_acc 0.6900 | test_loss 0.5738 | test_acc 0.7020 | lr 0.0428
last_idx 11
final_idx 0
task0/train/loss 0.5572936882575353 81
task0/test/loss 0.6128423414800478 81
task0/test/acc 0.717 81
task0/train/lr 0.04187108413246371 81
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 82/256 | train_loss 0.5573 | train_acc 0.7240 | test_loss 0.6128 | test_acc 0.7170 | lr 0.0419
last_idx 11
final_idx 0
task0/train/loss 0.5991348773241043 82
task0/test/loss 0.5565682798624039 82
task0/test/acc 0.7165 82
task0/train/lr 0.040951233783050225 82
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 83/256 | train_loss 0.5991 | train_acc 0.6880 | test_loss 0.5566 | test_acc 0.7165 | lr 0.0410
last_idx 11
final_idx 0
task0/train/loss 0.6251720314224561 83
task0/test/loss 0.838163323700428 83
task0/test/acc 0.603 83
task0/train/lr 0.03999355778618773 83
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 84/256 | train_loss 0.6252 | train_acc 0.6740 | test_loss 0.8382 | test_acc 0.6030 | lr 0.0400
last_idx 11
final_idx 0
task0/train/loss 0.6704244911670685 84
task0/test/loss 0.5818402734787568 84
task0/test/acc 0.707 84
task0/train/lr 0.039000363267235154 84
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 85/256 | train_loss 0.6704 | train_acc 0.6440 | test_loss 0.5818 | test_acc 0.7070 | lr 0.0390
last_idx 11
final_idx 0
task0/train/loss 0.6016297787427902 85
task0/test/loss 0.6700279149024383 85
task0/test/acc 0.663 85
task0/train/lr 0.03797404291878224 85
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 86/256 | train_loss 0.6016 | train_acc 0.6920 | test_loss 0.6700 | test_acc 0.6630 | lr 0.0380
last_idx 11
final_idx 0
task0/train/loss 0.5662042697270712 86
task0/test/loss 0.7279154909369738 86
task0/test/acc 0.672 86
task0/train/lr 0.03691706923644345 86
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 87/256 | train_loss 0.5662 | train_acc 0.7180 | test_loss 0.7279 | test_acc 0.6720 | lr 0.0369
last_idx 11
final_idx 0
task0/train/loss 0.598143587509791 87
task0/test/loss 0.5591041870091272 87
task0/test/acc 0.7385 87
task0/train/lr 0.03583198856239948 87
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 88/256 | train_loss 0.5981 | train_acc 0.6960 | test_loss 0.5591 | test_acc 0.7385 | lr 0.0358
last_idx 11
final_idx 0
task0/train/loss 0.6108925864100456 88
task0/test/loss 0.6109074946978817 88
task0/test/acc 0.6825 88
task0/train/lr 0.03472141495103598 88
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 89/256 | train_loss 0.6109 | train_acc 0.6900 | test_loss 0.6109 | test_acc 0.6825 | lr 0.0347
last_idx 11
final_idx 0
task0/train/loss 0.5820257440209389 89
task0/test/loss 0.6631948157497074 89
task0/test/acc 0.6785 89
task0/train/lr 0.03358802387145745 89
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 90/256 | train_loss 0.5820 | train_acc 0.7160 | test_loss 0.6632 | test_acc 0.6785 | lr 0.0336
last_idx 11
final_idx 0
task0/train/loss 0.6393755823373795 90
task0/test/loss 0.7148699208122232 90
task0/test/acc 0.6535 90
task0/train/lr 0.03243454576204794 90
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 91/256 | train_loss 0.6394 | train_acc 0.6940 | test_loss 0.7149 | test_acc 0.6535 | lr 0.0324
last_idx 11
final_idx 0
task0/train/loss 0.5769277935226759 91
task0/test/loss 0.5804913056933362 91
task0/test/acc 0.724 91
task0/train/lr 0.03126375945260579 91
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 92/256 | train_loss 0.5769 | train_acc 0.7260 | test_loss 0.5805 | test_acc 0.7240 | lr 0.0313
last_idx 11
final_idx 0
task0/train/loss 0.5546517074108124 92
task0/test/loss 0.6832282308327116 92
task0/test/acc 0.688 92
task0/train/lr 0.03007848546989918 92
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 93/256 | train_loss 0.5547 | train_acc 0.7500 | test_loss 0.6832 | test_acc 0.6880 | lr 0.0301
last_idx 11
final_idx 0
task0/train/loss 0.5641265213489532 93
task0/test/loss 0.6270474169565283 93
task0/test/acc 0.687 93
task0/train/lr 0.028881579242770204 93
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 94/256 | train_loss 0.5641 | train_acc 0.6960 | test_loss 0.6270 | test_acc 0.6870 | lr 0.0289
last_idx 11
final_idx 0
task0/train/loss 0.5711118305722872 94
task0/test/loss 0.701519527675017 94
task0/test/acc 0.6715 94
task0/train/lr 0.027675924223156633 94
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 95/256 | train_loss 0.5711 | train_acc 0.7280 | test_loss 0.7015 | test_acc 0.6715 | lr 0.0277
last_idx 11
final_idx 0
task0/train/loss 0.5672445421417555 95
task0/test/loss 0.5907061080569806 95
task0/test/acc 0.71 95
task0/train/lr 0.0264644249396036 95
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 96/256 | train_loss 0.5672 | train_acc 0.7000 | test_loss 0.5907 | test_acc 0.7100 | lr 0.0265
last_idx 11
final_idx 0
task0/train/loss 0.5773014525572459 96
task0/test/loss 0.6438932807549186 96
task0/test/acc 0.6935 96
task0/train/lr 0.02525 96
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 97/256 | train_loss 0.5773 | train_acc 0.6860 | test_loss 0.6439 | test_acc 0.6935 | lr 0.0253
last_idx 11
final_idx 0
task0/train/loss 0.5468169202407202 97
task0/test/loss 0.5257694598125375 97
task0/test/acc 0.7385 97
task0/train/lr 0.024035575060396407 97
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 98/256 | train_loss 0.5468 | train_acc 0.7100 | test_loss 0.5258 | test_acc 0.7385 | lr 0.0240
last_idx 11
final_idx 0
task0/train/loss 0.5233817398548126 98
task0/test/loss 0.6942712505874427 98
task0/test/acc 0.675 98
task0/train/lr 0.022824075776843374 98
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 99/256 | train_loss 0.5234 | train_acc 0.7500 | test_loss 0.6943 | test_acc 0.6750 | lr 0.0228
last_idx 11
final_idx 0
task0/train/loss 0.5097551320989927 99
task0/test/loss 0.607363150657519 99
task0/test/acc 0.728 99
task0/train/lr 0.0216184207572298 99
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 100/256 | train_loss 0.5098 | train_acc 0.7600 | test_loss 0.6074 | test_acc 0.7280 | lr 0.0216
last_idx 11
final_idx 0
task0/train/loss 0.5487497126062711 100
task0/test/loss 0.5207627703962119 100
task0/test/acc 0.755 100
task0/train/lr 0.02042151453010083 100
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 101/256 | train_loss 0.5487 | train_acc 0.7440 | test_loss 0.5208 | test_acc 0.7550 | lr 0.0204
last_idx 11
final_idx 0
task0/train/loss 0.5316222310066223 101
task0/test/loss 0.6049511068217133 101
task0/test/acc 0.72 101
task0/train/lr 0.019236240547394222 101
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 102/256 | train_loss 0.5316 | train_acc 0.7200 | test_loss 0.6050 | test_acc 0.7200 | lr 0.0192
last_idx 11
final_idx 0
task0/train/loss 0.568107028802236 102
task0/test/loss 0.6163328139354354 102
task0/test/acc 0.701 102
task0/train/lr 0.018065454237952062 102
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 103/256 | train_loss 0.5681 | train_acc 0.7120 | test_loss 0.6163 | test_acc 0.7010 | lr 0.0181
last_idx 11
final_idx 0
task0/train/loss 0.554439644018809 103
task0/test/loss 0.591428779065609 103
task0/test/acc 0.709 103
task0/train/lr 0.016911976128542557 103
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 104/256 | train_loss 0.5544 | train_acc 0.7340 | test_loss 0.5914 | test_acc 0.7090 | lr 0.0169
last_idx 11
final_idx 0
task0/train/loss 0.5526363030076027 104
task0/test/loss 0.5357447567841281 104
task0/test/acc 0.74 104
task0/train/lr 0.01577858504896403 104
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 105/256 | train_loss 0.5526 | train_acc 0.7340 | test_loss 0.5357 | test_acc 0.7400 | lr 0.0158
last_idx 11
final_idx 0
task0/train/loss 0.5581093778212866 105
task0/test/loss 0.5484194457530975 105
task0/test/acc 0.7355 105
task0/train/lr 0.014668011437600525 105
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 106/256 | train_loss 0.5581 | train_acc 0.7560 | test_loss 0.5484 | test_acc 0.7355 | lr 0.0147
last_idx 11
final_idx 0
task0/train/loss 0.5543127780159315 106
task0/test/loss 0.5532231518755788 106
task0/test/acc 0.733 106
task0/train/lr 0.013582930763556558 106
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 107/256 | train_loss 0.5543 | train_acc 0.7040 | test_loss 0.5532 | test_acc 0.7330 | lr 0.0136
last_idx 11
final_idx 0
task0/train/loss 0.6152973622083664 107
task0/test/loss 0.5278758669029111 107
task0/test/acc 0.7485 107
task0/train/lr 0.012525957081217764 107
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 108/256 | train_loss 0.6153 | train_acc 0.6480 | test_loss 0.5279 | test_acc 0.7485 | lr 0.0125
last_idx 11
final_idx 0
task0/train/loss 0.5714887976646423 108
task0/test/loss 0.549932814810587 108
task0/test/acc 0.7295 108
task0/train/lr 0.011499636732764853 108
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 109/256 | train_loss 0.5715 | train_acc 0.6900 | test_loss 0.5499 | test_acc 0.7295 | lr 0.0115
last_idx 11
final_idx 0
task0/train/loss 0.5378895203272501 109
task0/test/loss 0.5496609311388887 109
task0/test/acc 0.7385 109
task0/train/lr 0.010506442213812275 109
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 110/256 | train_loss 0.5379 | train_acc 0.6880 | test_loss 0.5497 | test_acc 0.7385 | lr 0.0105
last_idx 11
final_idx 0
task0/train/loss 0.5502101828654608 110
task0/test/loss 0.6132797363983549 110
task0/test/acc 0.711 110
task0/train/lr 0.009548766216949778 110
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 111/256 | train_loss 0.5502 | train_acc 0.7380 | test_loss 0.6133 | test_acc 0.7110 | lr 0.0095
last_idx 11
final_idx 0
task0/train/loss 0.5828286285201708 111
task0/test/loss 0.6060949421447256 111
task0/test/acc 0.724 111
task0/train/lr 0.008628915867536294 111
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 112/256 | train_loss 0.5828 | train_acc 0.6580 | test_loss 0.6061 | test_acc 0.7240 | lr 0.0086
last_idx 11
final_idx 0
task0/train/loss 0.5334294786055883 112
task0/test/loss 0.5418067205211391 112
task0/test/acc 0.739 112
task0/train/lr 0.00774910716563295 112
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 113/256 | train_loss 0.5334 | train_acc 0.7220 | test_loss 0.5418 | test_acc 0.7390 | lr 0.0077
last_idx 11
final_idx 0
task0/train/loss 0.48540092756350833 113
task0/test/loss 0.5910508010698401 113
task0/test/acc 0.7295 113
task0/train/lr 0.006911459647464768 113
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 114/256 | train_loss 0.4854 | train_acc 0.7620 | test_loss 0.5911 | test_acc 0.7295 | lr 0.0069
last_idx 11
final_idx 0
task0/train/loss 0.5244531035423279 114
task0/test/loss 0.5562139023905215 114
task0/test/acc 0.731 114
task0/train/lr 0.0061179912792722595 114
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 115/256 | train_loss 0.5245 | train_acc 0.7280 | test_loss 0.5562 | test_acc 0.7310 | lr 0.0061
last_idx 11
final_idx 0
task0/train/loss 0.5571840951840082 115
task0/test/loss 0.5597550000833429 115
task0/test/acc 0.733 115
task0/train/lr 0.005370613595854041 115
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 116/256 | train_loss 0.5572 | train_acc 0.7300 | test_loss 0.5598 | test_acc 0.7330 | lr 0.0054
last_idx 11
final_idx 0
task0/train/loss 0.5505459407965342 116
task0/test/loss 0.5263879959997924 116
task0/test/acc 0.752 116
task0/train/lr 0.004671127095512003 116
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 117/256 | train_loss 0.5505 | train_acc 0.7420 | test_loss 0.5264 | test_acc 0.7520 | lr 0.0047
last_idx 11
final_idx 0
task0/train/loss 0.5051829988757769 117
task0/test/loss 0.5688332863475966 117
task0/test/acc 0.7335 117
task0/train/lr 0.004021216902493268 117
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 118/256 | train_loss 0.5052 | train_acc 0.7680 | test_loss 0.5688 | test_acc 0.7335 | lr 0.0040
last_idx 11
final_idx 0
task0/train/loss 0.5052609841028849 118
task0/test/loss 0.560147179533606 118
task0/test/acc 0.7385 118
task0/train/lr 0.0034224487073782153 118
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 119/256 | train_loss 0.5053 | train_acc 0.7420 | test_loss 0.5601 | test_acc 0.7385 | lr 0.0034
last_idx 11
final_idx 0
task0/train/loss 0.551202138264974 119
task0/test/loss 0.5433229340807252 119
task0/test/acc 0.747 119
task0/train/lr 0.0028762649951947776 119
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 120/256 | train_loss 0.5512 | train_acc 0.7160 | test_loss 0.5433 | test_acc 0.7470 | lr 0.0029
last_idx 11
final_idx 0
task0/train/loss 0.5605380088090897 120
task0/test/loss 0.5167241459307463 120
task0/test/acc 0.762 120
task0/train/lr 0.0023839815703456534 120
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 121/256 | train_loss 0.5605 | train_acc 0.7280 | test_loss 0.5167 | test_acc 0.7620 | lr 0.0024
last_idx 11
final_idx 0
task0/train/loss 0.5265109911561012 121
task0/test/loss 0.5598583866072737 121
task0/test/acc 0.7365 121
task0/train/lr 0.0019467843867202379 121
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 122/256 | train_loss 0.5265 | train_acc 0.7200 | test_loss 0.5599 | test_acc 0.7365 | lr 0.0019
last_idx 11
final_idx 0
task0/train/loss 0.43260276814301807 122
task0/test/loss 0.594124279430379 122
task0/test/acc 0.7285 122
task0/train/lr 0.0015657266906278318 122
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 123/256 | train_loss 0.4326 | train_acc 0.8100 | test_loss 0.5941 | test_acc 0.7285 | lr 0.0016
last_idx 11
final_idx 0
task0/train/loss 0.484094575047493 123
task0/test/loss 0.5764396875772787 123
task0/test/acc 0.739 123
task0/train/lr 0.0012417264834350366 123
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 124/256 | train_loss 0.4841 | train_acc 0.7440 | test_loss 0.5764 | test_acc 0.7390 | lr 0.0012
last_idx 11
final_idx 0
task0/train/loss 0.5036835297942162 124
task0/test/loss 0.5560983809763971 124
task0/test/acc 0.7445 124
task0/train/lr 0.0009755643100200469 124
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 125/256 | train_loss 0.5037 | train_acc 0.7360 | test_loss 0.5561 | test_acc 0.7445 | lr 0.0010
last_idx 11
final_idx 0
task0/train/loss 0.5346125438809395 125
task0/test/loss 0.5607372383060663 125
task0/test/acc 0.74 125
task0/train/lr 0.0007678813783716699 125
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 126/256 | train_loss 0.5346 | train_acc 0.7300 | test_loss 0.5607 | test_acc 0.7400 | lr 0.0008
last_idx 11
final_idx 0
task0/train/loss 0.4937713195880254 126
task0/test/loss 0.5638024672541929 126
task0/test/acc 0.7415 126
task0/train/lr 0.0006191780148631288 126
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 127/256 | train_loss 0.4938 | train_acc 0.7620 | test_loss 0.5638 | test_acc 0.7415 | lr 0.0006
last_idx 11
final_idx 0
task0/train/loss 0.5264212886492411 127
task0/test/loss 0.5660058792842471 127
task0/test/acc 0.739 127
task0/train/lr 0.0005298124589219829 127
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 128/256 | train_loss 0.5264 | train_acc 0.6880 | test_loss 0.5660 | test_acc 0.7390 | lr 0.0005
last_idx 11
final_idx 0
task0/train/loss 0.6109847202897072 128
task0/test/loss 0.8087410580204881 128
task0/test/acc 0.6355 128
task0/train/lr 0.05 128
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 129/256 | train_loss 0.6110 | train_acc 0.6740 | test_loss 0.8087 | test_acc 0.6355 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 0.5900973652799925 129
task0/test/loss 0.553192934912184 129
task0/test/acc 0.7275 129
task0/train/lr 0.04999254576273106 129
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 130/256 | train_loss 0.5901 | train_acc 0.6980 | test_loss 0.5532 | test_acc 0.7275 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 0.5647986556092898 130
task0/test/loss 0.5995856617455897 130
task0/test/acc 0.6985 130
task0/train/lr 0.04997018754107802 130
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 131/256 | train_loss 0.5648 | train_acc 0.7260 | test_loss 0.5996 | test_acc 0.6985 | lr 0.0500
last_idx 11
final_idx 0
task0/train/loss 0.5832475125789642 131
task0/test/loss 0.5431248205511466 131
task0/test/acc 0.7235 131
task0/train/lr 0.04993293880279759 131
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 132/256 | train_loss 0.5832 | train_acc 0.7200 | test_loss 0.5431 | test_acc 0.7235 | lr 0.0499
last_idx 11
final_idx 0
task0/train/loss 0.5520384808381399 132
task0/test/loss 0.5322004900030468 132
task0/test/acc 0.7375 132
task0/train/lr 0.049880821985136874 132
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 133/256 | train_loss 0.5520 | train_acc 0.7200 | test_loss 0.5322 | test_acc 0.7375 | lr 0.0499
last_idx 11
final_idx 0
task0/train/loss 0.624392511943976 133
task0/test/loss 0.6637874381697696 133
task0/test/acc 0.6395 133
task0/train/lr 0.04981386848131808 133
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 134/256 | train_loss 0.6244 | train_acc 0.6920 | test_loss 0.6638 | test_acc 0.6395 | lr 0.0498
last_idx 11
final_idx 0
task0/train/loss 0.5719636678695679 134
task0/test/loss 0.579282156151274 134
task0/test/acc 0.714 134
task0/train/lr 0.04973211862162834 134
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 135/256 | train_loss 0.5720 | train_acc 0.7100 | test_loss 0.5793 | test_acc 0.7140 | lr 0.0497
last_idx 11
final_idx 0
task0/train/loss 0.5744443734486898 135
task0/test/loss 0.5854271672990011 135
task0/test/acc 0.708 135
task0/train/lr 0.04963562164912629 135
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 136/256 | train_loss 0.5744 | train_acc 0.7100 | test_loss 0.5854 | test_acc 0.7080 | lr 0.0496
last_idx 11
final_idx 0
task0/train/loss 0.5208266898989677 136
task0/test/loss 0.6933698926282965 136
task0/test/acc 0.704 136
task0/train/lr 0.049524435689979954 136
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 137/256 | train_loss 0.5208 | train_acc 0.7360 | test_loss 0.6934 | test_acc 0.7040 | lr 0.0495
last_idx 11
final_idx 0
task0/train/loss 0.5406868383288383 137
task0/test/loss 0.5430540892740955 137
task0/test/acc 0.7605 137
task0/train/lr 0.04939862771845358 137
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 138/256 | train_loss 0.5407 | train_acc 0.7600 | test_loss 0.5431 | test_acc 0.7605 | lr 0.0494
last_idx 11
final_idx 0
task0/train/loss 0.563921776910623 138
task0/test/loss 0.49093371824077936 138
task0/test/acc 0.7585 138
task0/train/lr 0.04925827351656497 138
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 139/256 | train_loss 0.5639 | train_acc 0.7040 | test_loss 0.4909 | test_acc 0.7585 | lr 0.0493
last_idx 11
final_idx 0
task0/train/loss 0.5817501122752825 139
task0/test/loss 0.4984010820803435 139
task0/test/acc 0.7695 139
task0/train/lr 0.04910345762843714 139
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 140/256 | train_loss 0.5818 | train_acc 0.7100 | test_loss 0.4984 | test_acc 0.7695 | lr 0.0491
last_idx 11
final_idx 0
task0/train/loss 0.5550762390096983 140
task0/test/loss 0.5291000355197035 140
task0/test/acc 0.7495 140
task0/train/lr 0.048934273309372174 140
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 141/256 | train_loss 0.5551 | train_acc 0.7240 | test_loss 0.5291 | test_acc 0.7495 | lr 0.0489
last_idx 11
final_idx 0
task0/train/loss 0.5613503058751425 141
task0/test/loss 0.7723270210882892 141
task0/test/acc 0.6615 141
task0/train/lr 0.04875082246967766 141
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 142/256 | train_loss 0.5614 | train_acc 0.7420 | test_loss 0.7723 | test_acc 0.6615 | lr 0.0488
last_idx 11
final_idx 0
task0/train/loss 0.5991546536485354 142
task0/test/loss 0.5124999142211416 142
task0/test/acc 0.763 142
task0/train/lr 0.048553215613279764 142
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 143/256 | train_loss 0.5992 | train_acc 0.7080 | test_loss 0.5125 | test_acc 0.7630 | lr 0.0486
last_idx 11
final_idx 0
task0/train/loss 0.5127396757404009 143
task0/test/loss 0.660203033329352 143
task0/test/acc 0.725 143
task0/train/lr 0.04834157177115979 143
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 144/256 | train_loss 0.5127 | train_acc 0.7340 | test_loss 0.6602 | test_acc 0.7250 | lr 0.0483
last_idx 11
final_idx 0
task0/train/loss 0.5119757999976476 144
task0/test/loss 0.642993767624316 144
task0/test/acc 0.7215 144
task0/train/lr 0.04811601842965435 144
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 145/256 | train_loss 0.5120 | train_acc 0.7520 | test_loss 0.6430 | test_acc 0.7215 | lr 0.0481
last_idx 11
final_idx 0
task0/train/loss 0.5832528099417686 145
task0/test/loss 0.7265892968229626 145
task0/test/acc 0.656 145
task0/train/lr 0.047876691453662384 145
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 146/256 | train_loss 0.5833 | train_acc 0.6960 | test_loss 0.7266 | test_acc 0.6560 | lr 0.0479
last_idx 11
final_idx 0
task0/train/loss 0.5415598601102829 146
task0/test/loss 0.5547924304138059 146
task0/test/acc 0.7335 146
task0/train/lr 0.047623735004805226 146
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 147/256 | train_loss 0.5416 | train_acc 0.7420 | test_loss 0.5548 | test_acc 0.7335 | lr 0.0476
last_idx 11
final_idx 0
task0/train/loss 0.5228567471106847 147
task0/test/loss 0.5989283169417278 147
task0/test/acc 0.7315 147
task0/train/lr 0.047357301454589 147
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 148/256 | train_loss 0.5229 | train_acc 0.7540 | test_loss 0.5989 | test_acc 0.7315 | lr 0.0474
last_idx 11
final_idx 0
task0/train/loss 0.5667080283164978 148
task0/test/loss 0.6338788947981336 148
task0/test/acc 0.7125 148
task0/train/lr 0.04707755129262179 148
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 149/256 | train_loss 0.5667 | train_acc 0.7360 | test_loss 0.6339 | test_acc 0.7125 | lr 0.0471
last_idx 11
final_idx 0
task0/train/loss 0.581498441596826 149
task0/test/loss 0.5156759425349857 149
task0/test/acc 0.758 149
task0/train/lr 0.04678465302994061 149
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 150/256 | train_loss 0.5815 | train_acc 0.6920 | test_loss 0.5157 | test_acc 0.7580 | lr 0.0468
last_idx 11
final_idx 0
task0/train/loss 0.532041996717453 150
task0/test/loss 0.6950662174950475 150
task0/test/acc 0.7305 150
task0/train/lr 0.046478783097506735 150
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 151/256 | train_loss 0.5320 | train_acc 0.7320 | test_loss 0.6951 | test_acc 0.7305 | lr 0.0465
last_idx 11
final_idx 0
task0/train/loss 0.5811779523889223 151
task0/test/loss 0.5302886402477389 151
task0/test/acc 0.751 151
task0/train/lr 0.04616012573993025 151
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 152/256 | train_loss 0.5812 | train_acc 0.6940 | test_loss 0.5303 | test_acc 0.7510 | lr 0.0462
last_idx 11
final_idx 0
task0/train/loss 0.501540961364905 152
task0/test/loss 0.5059809286309325 152
task0/test/acc 0.7755 152
task0/train/lr 0.045828872904488 152
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 153/256 | train_loss 0.5015 | train_acc 0.7640 | test_loss 0.5060 | test_acc 0.7755 | lr 0.0458
last_idx 11
final_idx 0
task0/train/loss 0.5789794102311134 153
task0/test/loss 0.7248984983433848 153
task0/test/acc 0.6615 153
task0/train/lr 0.0454852241255017 153
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 154/256 | train_loss 0.5790 | train_acc 0.7120 | test_loss 0.7249 | test_acc 0.6615 | lr 0.0455
last_idx 11
final_idx 0
task0/train/loss 0.5498240813612938 154
task0/test/loss 0.8418413831487946 154
task0/test/acc 0.66 154
task0/train/lr 0.04512938640414596 154
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 155/256 | train_loss 0.5498 | train_acc 0.7240 | test_loss 0.8418 | test_acc 0.6600 | lr 0.0451
last_idx 11
final_idx 0
task0/train/loss 0.5011028399070104 155
task0/test/loss 0.6668475500267484 155
task0/test/acc 0.724 155
task0/train/lr 0.04476157408375851 155
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 156/256 | train_loss 0.5011 | train_acc 0.7740 | test_loss 0.6668 | test_acc 0.7240 | lr 0.0448
last_idx 11
final_idx 0
task0/train/loss 0.5187124162912369 156
task0/test/loss 0.6921098727894865 156
task0/test/acc 0.697 156
task0/train/lr 0.04438200872072774 156
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 157/256 | train_loss 0.5187 | train_acc 0.7460 | test_loss 0.6921 | test_acc 0.6970 | lr 0.0444
last_idx 11
final_idx 0
task0/train/loss 0.5629878689845403 157
task0/test/loss 0.5170146249558615 157
task0/test/acc 0.7525 157
task0/train/lr 0.0439909189510355 157
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 158/256 | train_loss 0.5630 | train_acc 0.7240 | test_loss 0.5170 | test_acc 0.7525 | lr 0.0440
last_idx 11
final_idx 0
task0/train/loss 0.5798562665780386 158
task0/test/loss 0.5283994140184444 158
task0/test/acc 0.766 158
task0/train/lr 0.043588540352535246 158
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 159/256 | train_loss 0.5799 | train_acc 0.7000 | test_loss 0.5284 | test_acc 0.7660 | lr 0.0436
last_idx 11
final_idx 0
task0/train/loss 0.5437016561627388 159
task0/test/loss 0.4972803505218547 159
task0/test/acc 0.7595 159
task0/train/lr 0.043175115303048815 159
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 160/256 | train_loss 0.5437 | train_acc 0.7500 | test_loss 0.4973 | test_acc 0.7595 | lr 0.0432
last_idx 11
final_idx 0
task0/train/loss 0.5435564344127973 160
task0/test/loss 0.5650847727513831 160
task0/test/acc 0.7375 160
task0/train/lr 0.04275089283436705 160
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 161/256 | train_loss 0.5436 | train_acc 0.7420 | test_loss 0.5651 | test_acc 0.7375 | lr 0.0428
last_idx 11
final_idx 0
task0/train/loss 0.5466652289032936 161
task0/test/loss 0.6508713395375273 161
task0/test/acc 0.704 161
task0/train/lr 0.042316128482242414 161
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 162/256 | train_loss 0.5467 | train_acc 0.7340 | test_loss 0.6509 | test_acc 0.7040 | lr 0.0423
last_idx 11
final_idx 0
task0/train/loss 0.514160285393397 162
task0/test/loss 0.4999081531296606 162
task0/test/acc 0.7715 162
task0/train/lr 0.04187108413246371 162
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 163/256 | train_loss 0.5142 | train_acc 0.7500 | test_loss 0.4999 | test_acc 0.7715 | lr 0.0419
last_idx 11
final_idx 0
task0/train/loss 0.5859121680259705 163
task0/test/loss 0.48397192326576816 163
task0/test/acc 0.7685 163
task0/train/lr 0.04141602786310598 163
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 164/256 | train_loss 0.5859 | train_acc 0.6840 | test_loss 0.4840 | test_acc 0.7685 | lr 0.0414
last_idx 11
final_idx 0
task0/train/loss 0.5172147651513418 164
task0/test/loss 0.6490147698508657 164
task0/test/acc 0.7075 164
task0/train/lr 0.040951233783050225 164
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 165/256 | train_loss 0.5172 | train_acc 0.7600 | test_loss 0.6490 | test_acc 0.7075 | lr 0.0410
last_idx 11
final_idx 0
task0/train/loss 0.5409110287825266 165
task0/test/loss 0.7423302390329216 165
task0/test/acc 0.6655 165
task0/train/lr 0.040476981866870515 165
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 166/256 | train_loss 0.5409 | train_acc 0.7380 | test_loss 0.7423 | test_acc 0.6655 | lr 0.0405
last_idx 11
final_idx 0
task0/train/loss 0.5900088449319204 166
task0/test/loss 0.5474439470664315 166
task0/test/acc 0.7305 166
task0/train/lr 0.03999355778618773 166
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 167/256 | train_loss 0.5900 | train_acc 0.6980 | test_loss 0.5474 | test_acc 0.7305 | lr 0.0400
last_idx 11
final_idx 0
task0/train/loss 0.5431684777140617 167
task0/test/loss 0.5233810573168423 167
task0/test/acc 0.764 167
task0/train/lr 0.039501252737591676 167
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 168/256 | train_loss 0.5432 | train_acc 0.7360 | test_loss 0.5234 | test_acc 0.7640 | lr 0.0395
last_idx 11
final_idx 0
task0/train/loss 0.537322387099266 168
task0/test/loss 0.5126286758028943 168
task0/test/acc 0.7685 168
task0/train/lr 0.039000363267235154 168
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 169/256 | train_loss 0.5373 | train_acc 0.7420 | test_loss 0.5126 | test_acc 0.7685 | lr 0.0390
last_idx 11
final_idx 0
task0/train/loss 0.5640924299756686 169
task0/test/loss 0.4952224823443786 169
task0/test/acc 0.7675 169
task0/train/lr 0.03849119109220566 169
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 170/256 | train_loss 0.5641 | train_acc 0.7340 | test_loss 0.4952 | test_acc 0.7675 | lr 0.0385
last_idx 11
final_idx 0
task0/train/loss 0.5259263664484024 170
task0/test/loss 0.5958410989007225 170
task0/test/acc 0.729 170
task0/train/lr 0.03797404291878224 170
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 171/256 | train_loss 0.5259 | train_acc 0.7640 | test_loss 0.5958 | test_acc 0.7290 | lr 0.0380
last_idx 11
final_idx 0
task0/train/loss 0.5120913907885551 171
task0/test/loss 0.5716364819394506 171
task0/test/acc 0.7475 171
task0/train/lr 0.03744923025768716 171
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 172/256 | train_loss 0.5121 | train_acc 0.7500 | test_loss 0.5716 | test_acc 0.7475 | lr 0.0374
last_idx 11
final_idx 0
task0/train/loss 0.46903648724158603 172
task0/test/loss 0.5880423813410427 172
task0/test/acc 0.758 172
task0/train/lr 0.03691706923644345 172
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 173/256 | train_loss 0.4690 | train_acc 0.7840 | test_loss 0.5880 | test_acc 0.7580 | lr 0.0369
last_idx 11
final_idx 0
task0/train/loss 0.554760587712129 173
task0/test/loss 0.5093483121498771 173
task0/test/acc 0.765 173
task0/train/lr 0.03637788040895152 173
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 174/256 | train_loss 0.5548 | train_acc 0.7360 | test_loss 0.5093 | test_acc 0.7650 | lr 0.0364
last_idx 11
final_idx 0
task0/train/loss 0.5394012530644735 174
task0/test/loss 0.575307380893956 174
task0/test/acc 0.7395 174
task0/train/lr 0.03583198856239948 174
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 175/256 | train_loss 0.5394 | train_acc 0.7040 | test_loss 0.5753 | test_acc 0.7395 | lr 0.0358
last_idx 11
final_idx 0
task0/train/loss 0.5197445005178452 175
task0/test/loss 0.6375190502275592 175
task0/test/acc 0.7125 175
task0/train/lr 0.0352797225216235 175
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 176/256 | train_loss 0.5197 | train_acc 0.7560 | test_loss 0.6375 | test_acc 0.7125 | lr 0.0353
last_idx 11
final_idx 0
task0/train/loss 0.49417463690042496 176
task0/test/loss 0.7159792946084685 176
task0/test/acc 0.7045 176
task0/train/lr 0.03472141495103598 176
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 177/256 | train_loss 0.4942 | train_acc 0.7700 | test_loss 0.7160 | test_acc 0.7045 | lr 0.0347
last_idx 11
final_idx 0
task0/train/loss 0.5246453409393629 177
task0/test/loss 0.581756370385056 177
task0/test/acc 0.7455 177
task0/train/lr 0.034157402154240964 177
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 178/256 | train_loss 0.5246 | train_acc 0.7600 | test_loss 0.5818 | test_acc 0.7455 | lr 0.0342
last_idx 11
final_idx 0
task0/train/loss 0.5296235010027885 178
task0/test/loss 0.5051660945881968 178
task0/test/acc 0.7595 178
task0/train/lr 0.03358802387145745 178
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 179/256 | train_loss 0.5296 | train_acc 0.7140 | test_loss 0.5052 | test_acc 0.7595 | lr 0.0336
last_idx 11
final_idx 0
task0/train/loss 0.5155145054062208 179
task0/test/loss 0.6989821423978909 179
task0/test/acc 0.6895 179
task0/train/lr 0.03301362307487257 179
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 180/256 | train_loss 0.5155 | train_acc 0.7380 | test_loss 0.6990 | test_acc 0.6895 | lr 0.0330
last_idx 11
final_idx 0
task0/train/loss 0.5017707869410515 180
task0/test/loss 0.5796099894720576 180
task0/test/acc 0.751 180
task0/train/lr 0.03243454576204794 180
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 181/256 | train_loss 0.5018 | train_acc 0.7600 | test_loss 0.5796 | test_acc 0.7510 | lr 0.0324
last_idx 11
final_idx 0
task0/train/loss 0.5298849393924078 181
task0/test/loss 0.5014394865087841 181
task0/test/acc 0.766 181
task0/train/lr 0.03185114074750374 181
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 182/256 | train_loss 0.5299 | train_acc 0.7200 | test_loss 0.5014 | test_acc 0.7660 | lr 0.0319
last_idx 11
final_idx 0
task0/train/loss 0.5048448815941811 182
task0/test/loss 0.5699757306796053 182
task0/test/acc 0.7315 182
task0/train/lr 0.03126375945260579 182
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 183/256 | train_loss 0.5048 | train_acc 0.7380 | test_loss 0.5700 | test_acc 0.7315 | lr 0.0313
last_idx 11
final_idx 0
task0/train/loss 0.49311351776123047 183
task0/test/loss 0.5405036060382491 183
task0/test/acc 0.7565 183
task0/train/lr 0.030672755693882527 183
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 184/256 | train_loss 0.4931 | train_acc 0.7680 | test_loss 0.5405 | test_acc 0.7565 | lr 0.0307
last_idx 11
final_idx 0
task0/train/loss 0.4837358643611272 184
task0/test/loss 0.473872565380905 184
task0/test/acc 0.7885 184
task0/train/lr 0.03007848546989918 184
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 185/256 | train_loss 0.4837 | train_acc 0.7580 | test_loss 0.4739 | test_acc 0.7885 | lr 0.0301
last_idx 11
final_idx 0
task0/train/loss 0.52407389630874 185
task0/test/loss 0.5295180087180241 185
task0/test/acc 0.772 185
task0/train/lr 0.029481306746817457 185
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 186/256 | train_loss 0.5241 | train_acc 0.7600 | test_loss 0.5295 | test_acc 0.7720 | lr 0.0295
last_idx 11
final_idx 0
task0/train/loss 0.514084555208683 186
task0/test/loss 0.4745846785928892 186
task0/test/acc 0.7865 186
task0/train/lr 0.028881579242770204 186
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 187/256 | train_loss 0.5141 | train_acc 0.7600 | test_loss 0.4746 | test_acc 0.7865 | lr 0.0289
last_idx 11
final_idx 0
task0/train/loss 0.4760078266263008 187
task0/test/loss 0.5832064525912637 187
task0/test/acc 0.7515 187
task0/train/lr 0.028279664211180604 187
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 188/256 | train_loss 0.4760 | train_acc 0.7780 | test_loss 0.5832 | test_acc 0.7515 | lr 0.0283
last_idx 11
final_idx 0
task0/train/loss 0.48987360050280887 188
task0/test/loss 0.5105649516310381 188
task0/test/acc 0.7685 188
task0/train/lr 0.027675924223156633 188
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 189/256 | train_loss 0.4899 | train_acc 0.7500 | test_loss 0.5106 | test_acc 0.7685 | lr 0.0277
last_idx 11
final_idx 0
task0/train/loss 0.525009403626124 189
task0/test/loss 0.7675263259721838 189
task0/test/acc 0.695 189
task0/train/lr 0.027070722949091772 189
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 190/256 | train_loss 0.5250 | train_acc 0.7380 | test_loss 0.7675 | test_acc 0.6950 | lr 0.0271
last_idx 11
final_idx 0
task0/train/loss 0.5013784691691399 190
task0/test/loss 0.581397582007491 190
task0/test/acc 0.747 190
task0/train/lr 0.0264644249396036 190
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 191/256 | train_loss 0.5014 | train_acc 0.7620 | test_loss 0.5814 | test_acc 0.7470 | lr 0.0265
last_idx 11
final_idx 0
task0/train/loss 0.4827658062179883 191
task0/test/loss 0.5113161433002223 191
task0/test/acc 0.7675 191
task0/train/lr 0.02585739540594208 191
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 192/256 | train_loss 0.4828 | train_acc 0.7700 | test_loss 0.5113 | test_acc 0.7675 | lr 0.0259
last_idx 11
final_idx 0
task0/train/loss 0.511461521188418 192
task0/test/loss 0.5619812748678352 192
task0/test/acc 0.742 192
task0/train/lr 0.02525 192
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 193/256 | train_loss 0.5115 | train_acc 0.7500 | test_loss 0.5620 | test_acc 0.7420 | lr 0.0253
last_idx 11
final_idx 0
task0/train/loss 0.5112734685341517 193
task0/test/loss 0.6569602626335361 193
task0/test/acc 0.7145 193
task0/train/lr 0.024642604594057926 193
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 194/256 | train_loss 0.5113 | train_acc 0.7500 | test_loss 0.6570 | test_acc 0.7145 | lr 0.0246
last_idx 11
final_idx 0
task0/train/loss 0.4738701755801837 194
task0/test/loss 0.48387983441352844 194
task0/test/acc 0.774 194
task0/train/lr 0.024035575060396407 194
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 195/256 | train_loss 0.4739 | train_acc 0.7840 | test_loss 0.4839 | test_acc 0.7740 | lr 0.0240
last_idx 11
final_idx 0
task0/train/loss 0.5181421091159185 195
task0/test/loss 0.4383225019859231 195
task0/test/acc 0.797 195
task0/train/lr 0.023429277050908234 195
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 196/256 | train_loss 0.5181 | train_acc 0.7460 | test_loss 0.4383 | test_acc 0.7970 | lr 0.0234
last_idx 11
final_idx 0
task0/train/loss 0.47979780038197833 196
task0/test/loss 0.67577365534785 196
task0/test/acc 0.704 196
task0/train/lr 0.022824075776843374 196
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 197/256 | train_loss 0.4798 | train_acc 0.7500 | test_loss 0.6758 | test_acc 0.7040 | lr 0.0228
last_idx 11
final_idx 0
task0/train/loss 0.4603111495574315 197
task0/test/loss 0.6268805339122596 197
task0/test/acc 0.747 197
task0/train/lr 0.022220335788819403 197
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 198/256 | train_loss 0.4603 | train_acc 0.7720 | test_loss 0.6269 | test_acc 0.7470 | lr 0.0222
last_idx 11
final_idx 0
task0/train/loss 0.4630925605694453 198
task0/test/loss 0.5604558128701604 198
task0/test/acc 0.754 198
task0/train/lr 0.0216184207572298 198
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 199/256 | train_loss 0.4631 | train_acc 0.7880 | test_loss 0.5605 | test_acc 0.7540 | lr 0.0216
last_idx 11
final_idx 0
task0/train/loss 0.5353714674711227 199
task0/test/loss 0.5427362407031266 199
task0/test/acc 0.7515 199
task0/train/lr 0.021018693253182546 199
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 200/256 | train_loss 0.5354 | train_acc 0.7660 | test_loss 0.5427 | test_acc 0.7515 | lr 0.0210
last_idx 11
final_idx 0
task0/train/loss 0.4791773036122322 200
task0/test/loss 0.6761966556634592 200
task0/test/acc 0.7125 200
task0/train/lr 0.02042151453010083 200
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 201/256 | train_loss 0.4792 | train_acc 0.7440 | test_loss 0.6762 | test_acc 0.7125 | lr 0.0204
last_idx 11
final_idx 0
task0/train/loss 0.5096875925858816 201
task0/test/loss 0.615148457415078 201
task0/test/acc 0.72 201
task0/train/lr 0.019827244306117476 201
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 202/256 | train_loss 0.5097 | train_acc 0.7520 | test_loss 0.6151 | test_acc 0.7200 | lr 0.0198
last_idx 11
final_idx 0
task0/train/loss 0.4402247965335846 202
task0/test/loss 0.5968194377973027 202
task0/test/acc 0.7435 202
task0/train/lr 0.019236240547394222 202
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 203/256 | train_loss 0.4402 | train_acc 0.8020 | test_loss 0.5968 | test_acc 0.7435 | lr 0.0192
last_idx 11
final_idx 0
task0/train/loss 0.46221935252348584 203
task0/test/loss 0.5303129093802493 203
task0/test/acc 0.7595 203
task0/train/lr 0.018648859252496267 203
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 204/256 | train_loss 0.4622 | train_acc 0.7800 | test_loss 0.5303 | test_acc 0.7595 | lr 0.0186
last_idx 11
final_idx 0
task0/train/loss 0.5638114735484123 204
task0/test/loss 0.6459321057342965 204
task0/test/acc 0.7075 204
task0/train/lr 0.018065454237952062 204
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 205/256 | train_loss 0.5638 | train_acc 0.7320 | test_loss 0.6459 | test_acc 0.7075 | lr 0.0181
last_idx 11
final_idx 0
task0/train/loss 0.4873288149634997 205
task0/test/loss 0.49881843396502995 205
task0/test/acc 0.7645 205
task0/train/lr 0.017486376925127438 205
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 206/256 | train_loss 0.4873 | train_acc 0.7860 | test_loss 0.4988 | test_acc 0.7645 | lr 0.0175
last_idx 11
final_idx 0
task0/train/loss 0.4905661518375079 206
task0/test/loss 0.5487576864011909 206
task0/test/acc 0.741 206
task0/train/lr 0.016911976128542557 206
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 207/256 | train_loss 0.4906 | train_acc 0.7460 | test_loss 0.5488 | test_acc 0.7410 | lr 0.0169
last_idx 11
final_idx 0
task0/train/loss 0.42061812182267505 207
task0/test/loss 0.5167610331074052 207
task0/test/acc 0.781 207
task0/train/lr 0.016342597845759043 207
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 208/256 | train_loss 0.4206 | train_acc 0.7900 | test_loss 0.5168 | test_acc 0.7810 | lr 0.0163
last_idx 11
final_idx 0
task0/train/loss 0.49621839076280594 208
task0/test/loss 0.4869957091691701 208
task0/test/acc 0.7935 208
task0/train/lr 0.01577858504896403 208
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 209/256 | train_loss 0.4962 | train_acc 0.7540 | test_loss 0.4870 | test_acc 0.7935 | lr 0.0158
last_idx 11
final_idx 0
task0/train/loss 0.4662703176339467 209
task0/test/loss 0.5039985150746678 209
task0/test/acc 0.777 209
task0/train/lr 0.015220277478376504 209
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 210/256 | train_loss 0.4663 | train_acc 0.7320 | test_loss 0.5040 | test_acc 0.7770 | lr 0.0152
last_idx 11
final_idx 0
task0/train/loss 0.4462355251113574 210
task0/test/loss 0.5806213916967744 210
task0/test/acc 0.751 210
task0/train/lr 0.014668011437600525 210
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 211/256 | train_loss 0.4462 | train_acc 0.8020 | test_loss 0.5806 | test_acc 0.7510 | lr 0.0147
last_idx 11
final_idx 0
task0/train/loss 0.4858170623580615 211
task0/test/loss 0.5563119993261669 211
task0/test/acc 0.7495 211
task0/train/lr 0.014122119591048485 211
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 212/256 | train_loss 0.4858 | train_acc 0.7700 | test_loss 0.5563 | test_acc 0.7495 | lr 0.0141
last_idx 11
final_idx 0
task0/train/loss 0.5118778770168623 212
task0/test/loss 0.5618600192601266 212
task0/test/acc 0.765 212
task0/train/lr 0.013582930763556558 212
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 213/256 | train_loss 0.5119 | train_acc 0.7760 | test_loss 0.5619 | test_acc 0.7650 | lr 0.0136
last_idx 11
final_idx 0
task0/train/loss 0.5109202985962232 213
task0/test/loss 0.5240675760028155 213
task0/test/acc 0.761 213
task0/train/lr 0.013050769742312849 213
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 214/256 | train_loss 0.5109 | train_acc 0.7580 | test_loss 0.5241 | test_acc 0.7610 | lr 0.0131
last_idx 11
final_idx 0
task0/train/loss 0.4700447271267573 214
task0/test/loss 0.4879567130752232 214
task0/test/acc 0.7765 214
task0/train/lr 0.012525957081217764 214
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 215/256 | train_loss 0.4700 | train_acc 0.7720 | test_loss 0.4880 | test_acc 0.7765 | lr 0.0125
last_idx 11
final_idx 0
task0/train/loss 0.44910745819409686 215
task0/test/loss 0.6011684522356676 215
task0/test/acc 0.7425 215
task0/train/lr 0.01200880890779435 215
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 216/256 | train_loss 0.4491 | train_acc 0.7700 | test_loss 0.6012 | test_acc 0.7425 | lr 0.0120
last_idx 11
final_idx 0
task0/train/loss 0.4943799798687299 216
task0/test/loss 0.6267848481302676 216
task0/test/acc 0.731 216
task0/train/lr 0.011499636732764853 216
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 217/256 | train_loss 0.4944 | train_acc 0.7780 | test_loss 0.6268 | test_acc 0.7310 | lr 0.0115
last_idx 11
final_idx 0
task0/train/loss 0.5098128343621889 217
task0/test/loss 0.5151341827991216 217
task0/test/acc 0.7695 217
task0/train/lr 0.010998747262408329 217
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 218/256 | train_loss 0.5098 | train_acc 0.7860 | test_loss 0.5151 | test_acc 0.7695 | lr 0.0110
last_idx 11
final_idx 0
task0/train/loss 0.41230007261037827 218
task0/test/loss 0.49018253256445343 218
task0/test/acc 0.7835 218
task0/train/lr 0.010506442213812275 218
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 219/256 | train_loss 0.4123 | train_acc 0.8260 | test_loss 0.4902 | test_acc 0.7835 | lr 0.0105
last_idx 11
final_idx 0
task0/train/loss 0.4942243571082751 219
task0/test/loss 0.4797503702018572 219
task0/test/acc 0.789 219
task0/train/lr 0.01002301813312949 219
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 220/256 | train_loss 0.4942 | train_acc 0.7800 | test_loss 0.4798 | test_acc 0.7890 | lr 0.0100
last_idx 11
final_idx 0
task0/train/loss 0.44109783073266345 220
task0/test/loss 0.5150507839153642 220
task0/test/acc 0.7715 220
task0/train/lr 0.009548766216949778 220
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 221/256 | train_loss 0.4411 | train_acc 0.7760 | test_loss 0.5151 | test_acc 0.7715 | lr 0.0095
last_idx 11
final_idx 0
task0/train/loss 0.48105977723995846 221
task0/test/loss 0.5250711607868257 221
task0/test/acc 0.7675 221
task0/train/lr 0.009083972136894032 221
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 222/256 | train_loss 0.4811 | train_acc 0.7720 | test_loss 0.5251 | test_acc 0.7675 | lr 0.0091
last_idx 11
final_idx 0
task0/train/loss 0.42991914600133896 222
task0/test/loss 0.6201037068891785 222
task0/test/acc 0.741 222
task0/train/lr 0.008628915867536294 222
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 223/256 | train_loss 0.4299 | train_acc 0.8020 | test_loss 0.6201 | test_acc 0.7410 | lr 0.0086
last_idx 11
final_idx 0
task0/train/loss 0.45527589321136475 223
task0/test/loss 0.5383537076413631 223
task0/test/acc 0.76 223
task0/train/lr 0.008183871517757594 223
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 224/256 | train_loss 0.4553 | train_acc 0.8140 | test_loss 0.5384 | test_acc 0.7600 | lr 0.0082
last_idx 11
final_idx 0
task0/train/loss 0.518325666586558 224
task0/test/loss 0.5534400564172993 224
task0/test/acc 0.743 224
task0/train/lr 0.00774910716563295 224
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 225/256 | train_loss 0.5183 | train_acc 0.7380 | test_loss 0.5534 | test_acc 0.7430 | lr 0.0077
last_idx 11
final_idx 0
task0/train/loss 0.4851753239830335 225
task0/test/loss 0.4747110548550668 225
task0/test/acc 0.7845 225
task0/train/lr 0.007324884696951197 225
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 226/256 | train_loss 0.4852 | train_acc 0.7760 | test_loss 0.4747 | test_acc 0.7845 | lr 0.0073
last_idx 11
final_idx 0
task0/train/loss 0.46085957686106366 226
task0/test/loss 0.5130850995040458 226
task0/test/acc 0.7675 226
task0/train/lr 0.006911459647464768 226
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 227/256 | train_loss 0.4609 | train_acc 0.7780 | test_loss 0.5131 | test_acc 0.7675 | lr 0.0069
last_idx 11
final_idx 0
task0/train/loss 0.4278442325691382 227
task0/test/loss 0.5280759145060311 227
task0/test/acc 0.7635 227
task0/train/lr 0.006509081048964508 227
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 228/256 | train_loss 0.4278 | train_acc 0.8040 | test_loss 0.5281 | test_acc 0.7635 | lr 0.0065
last_idx 11
final_idx 0
task0/train/loss 0.4633658453822136 228
task0/test/loss 0.5386465894787208 228
task0/test/acc 0.7575 228
task0/train/lr 0.0061179912792722595 228
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 229/256 | train_loss 0.4634 | train_acc 0.7540 | test_loss 0.5386 | test_acc 0.7575 | lr 0.0061
last_idx 11
final_idx 0
task0/train/loss 0.40239281952381134 229
task0/test/loss 0.5625601490230664 229
task0/test/acc 0.755 229
task0/train/lr 0.005738425916241496 229
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 230/256 | train_loss 0.4024 | train_acc 0.8180 | test_loss 0.5626 | test_acc 0.7550 | lr 0.0057
last_idx 11
final_idx 0
task0/train/loss 0.439062458773454 230
task0/test/loss 0.48316497760622396 230
task0/test/acc 0.7885 230
task0/train/lr 0.005370613595854041 230
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 231/256 | train_loss 0.4391 | train_acc 0.7860 | test_loss 0.4832 | test_acc 0.7885 | lr 0.0054
last_idx 11
final_idx 0
task0/train/loss 0.4439682935674985 231
task0/test/loss 0.45787912801555963 231
task0/test/acc 0.7965 231
task0/train/lr 0.005014775874498306 231
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 232/256 | train_loss 0.4440 | train_acc 0.8020 | test_loss 0.4579 | test_acc 0.7965 | lr 0.0050
last_idx 11
final_idx 0
task0/train/loss 0.46501258512338 232
task0/test/loss 0.46683630816962407 232
task0/test/acc 0.7935 232
task0/train/lr 0.004671127095512003 232
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 233/256 | train_loss 0.4650 | train_acc 0.8240 | test_loss 0.4668 | test_acc 0.7935 | lr 0.0047
last_idx 11
final_idx 0
task0/train/loss 0.48198331892490387 233
task0/test/loss 0.5202867100420205 233
task0/test/acc 0.769 233
task0/train/lr 0.004339874260069749 233
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 234/256 | train_loss 0.4820 | train_acc 0.7960 | test_loss 0.5203 | test_acc 0.7690 | lr 0.0043
last_idx 11
final_idx 0
task0/train/loss 0.41354042043288547 234
task0/test/loss 0.5208666988688967 234
task0/test/acc 0.7715 234
task0/train/lr 0.004021216902493268 234
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 235/256 | train_loss 0.4135 | train_acc 0.8040 | test_loss 0.5209 | test_acc 0.7715 | lr 0.0040
last_idx 11
final_idx 0
task0/train/loss 0.373931913326184 235
task0/test/loss 0.5039723439385062 235
task0/test/acc 0.782 235
task0/train/lr 0.0037153469700593944 235
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 236/256 | train_loss 0.3739 | train_acc 0.8260 | test_loss 0.5040 | test_acc 0.7820 | lr 0.0037
last_idx 11
final_idx 0
task0/train/loss 0.45762234926223755 236
task0/test/loss 0.48239306635830714 236
task0/test/acc 0.7865 236
task0/train/lr 0.0034224487073782153 236
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 237/256 | train_loss 0.4576 | train_acc 0.7860 | test_loss 0.4824 | test_acc 0.7865 | lr 0.0034
last_idx 11
final_idx 0
task0/train/loss 0.49520043407877284 237
task0/test/loss 0.5181642228170581 237
task0/test/acc 0.776 237
task0/train/lr 0.0031426985454109987 237
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 238/256 | train_loss 0.4952 | train_acc 0.7440 | test_loss 0.5182 | test_acc 0.7760 | lr 0.0031
last_idx 11
final_idx 0
task0/train/loss 0.520102230211099 238
task0/test/loss 0.5282551127931346 238
task0/test/acc 0.769 238
task0/train/lr 0.0028762649951947776 238
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 239/256 | train_loss 0.5201 | train_acc 0.7600 | test_loss 0.5283 | test_acc 0.7690 | lr 0.0029
last_idx 11
final_idx 0
task0/train/loss 0.3665820111831029 239
task0/test/loss 0.5678404143647008 239
task0/test/acc 0.7495 239
task0/train/lr 0.0026233085463376153 239
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 240/256 | train_loss 0.3666 | train_acc 0.8500 | test_loss 0.5678 | test_acc 0.7495 | lr 0.0026
last_idx 11
final_idx 0
task0/train/loss 0.4532854308684667 240
task0/test/loss 0.5342543654791687 240
task0/test/acc 0.768 240
task0/train/lr 0.0023839815703456534 240
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 241/256 | train_loss 0.4533 | train_acc 0.8140 | test_loss 0.5343 | test_acc 0.7680 | lr 0.0024
last_idx 11
final_idx 0
task0/train/loss 0.45369838923215866 241
task0/test/loss 0.5011072667396587 241
task0/test/acc 0.7765 241
task0/train/lr 0.0021584282288402137 241
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 242/256 | train_loss 0.4537 | train_acc 0.7800 | test_loss 0.5011 | test_acc 0.7765 | lr 0.0022
last_idx 11
final_idx 0
task0/train/loss 0.4143882244825363 242
task0/test/loss 0.5049527034811352 242
task0/test/acc 0.7745 242
task0/train/lr 0.0019467843867202379 242
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 243/256 | train_loss 0.4144 | train_acc 0.8120 | test_loss 0.5050 | test_acc 0.7745 | lr 0.0019
last_idx 11
final_idx 0
task0/train/loss 0.3783967395623525 243
task0/test/loss 0.5429955050349236 243
task0/test/acc 0.7665 243
task0/train/lr 0.0017491775303223424 243
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 244/256 | train_loss 0.3784 | train_acc 0.8520 | test_loss 0.5430 | test_acc 0.7665 | lr 0.0017
last_idx 11
final_idx 0
task0/train/loss 0.43850110967954 244
task0/test/loss 0.5680659566398548 244
task0/test/acc 0.7605 244
task0/train/lr 0.0015657266906278318 244
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 245/256 | train_loss 0.4385 | train_acc 0.8260 | test_loss 0.5681 | test_acc 0.7605 | lr 0.0016
last_idx 11
final_idx 0
task0/train/loss 0.4481201296051343 245
task0/test/loss 0.5766967718853898 245
task0/test/acc 0.7525 245
task0/train/lr 0.001396542371562864 245
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 246/256 | train_loss 0.4481 | train_acc 0.8140 | test_loss 0.5767 | test_acc 0.7525 | lr 0.0014
last_idx 11
final_idx 0
task0/train/loss 0.4247903774182002 246
task0/test/loss 0.5362484890805639 246
task0/test/acc 0.771 246
task0/train/lr 0.0012417264834350366 246
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 247/256 | train_loss 0.4248 | train_acc 0.8040 | test_loss 0.5362 | test_acc 0.7710 | lr 0.0012
last_idx 11
final_idx 0
task0/train/loss 0.49968724449475604 247
task0/test/loss 0.5387854313720828 247
task0/test/acc 0.768 247
task0/train/lr 0.0011013722815464207 247
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 248/256 | train_loss 0.4997 | train_acc 0.7720 | test_loss 0.5388 | test_acc 0.7680 | lr 0.0011
last_idx 11
final_idx 0
task0/train/loss 0.48647157847881317 248
task0/test/loss 0.5437054214594157 248
task0/test/acc 0.7655 248
task0/train/lr 0.0009755643100200469 248
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 249/256 | train_loss 0.4865 | train_acc 0.7660 | test_loss 0.5437 | test_acc 0.7655 | lr 0.0010
last_idx 11
final_idx 0
task0/train/loss 0.46980379273494083 249
task0/test/loss 0.5440555611706298 249
task0/test/acc 0.7635 249
task0/train/lr 0.0008643783508737047 249
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 250/256 | train_loss 0.4698 | train_acc 0.7700 | test_loss 0.5441 | test_acc 0.7635 | lr 0.0009
last_idx 11
final_idx 0
task0/train/loss 0.41554130365451175 250
task0/test/loss 0.5617023393835711 250
task0/test/acc 0.7545 250
task0/train/lr 0.0007678813783716699 250
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 251/256 | train_loss 0.4155 | train_acc 0.8080 | test_loss 0.5617 | test_acc 0.7545 | lr 0.0008
last_idx 11
final_idx 0
task0/train/loss 0.42925303677717846 251
task0/test/loss 0.5383556809114374 251
task0/test/acc 0.7665 251
task0/train/lr 0.0006861315186819283 251
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 252/256 | train_loss 0.4293 | train_acc 0.7960 | test_loss 0.5384 | test_acc 0.7665 | lr 0.0007
last_idx 11
final_idx 0
task0/train/loss 0.4825793777902921 252
task0/test/loss 0.5305220957683481 252
task0/test/acc 0.772 252
task0/train/lr 0.0006191780148631288 252
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 253/256 | train_loss 0.4826 | train_acc 0.7760 | test_loss 0.5305 | test_acc 0.7720 | lr 0.0006
last_idx 11
final_idx 0
task0/train/loss 0.42946288486321765 253
task0/test/loss 0.5584003131674684 253
task0/test/acc 0.76 253
task0/train/lr 0.0005670611972024174 253
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 254/256 | train_loss 0.4295 | train_acc 0.7980 | test_loss 0.5584 | test_acc 0.7600 | lr 0.0006
last_idx 11
final_idx 0
task0/train/loss 0.43913030127684277 254
task0/test/loss 0.5334390307898107 254
task0/test/acc 0.768 254
task0/train/lr 0.0005298124589219829 254
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 255/256 | train_loss 0.4391 | train_acc 0.8340 | test_loss 0.5334 | test_acc 0.7680 | lr 0.0005
last_idx 11
final_idx 0
task0/train/loss 0.3917406151692073 255
task0/test/loss 0.5313424568461336 255
task0/test/acc 0.7705 255
task0/train/lr 0.0005074542372689448 255
[INFO] rainbow_memory.py:184 > Task 0 | Epoch 256/256 | train_loss 0.3917 | train_acc 0.8200 | test_loss 0.5313 | test_acc 0.7705 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (0)
[INFO] main.py:398 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (0)
[INFO] main.py:405 > [2-5] Report task result
Metrics/TaskAcc 0.797 0

##################################################
# Task 1 iteration
##################################################

[INFO] main.py:316 > [2-1] Prepare a datalist for the current task
total : 1000  current step :  0
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.62s. Batch: 0.79s. S_Loss: 2.3855. T_Loss: 2.4390. Mask: 0.0039. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.62s. Batch: 0.79s. S_Loss: 2.3855. T_Loss: 2.4390. Mask: 0.0039. :   2%|▏         | 1/50 [00:00<00:38,  1.26it/s]Train Iter:   2/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4020. T_Loss: 2.4177. Mask: 0.0039. :   2%|▏         | 1/50 [00:01<00:38,  1.26it/s]Train Iter:   2/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4020. T_Loss: 2.4177. Mask: 0.0039. :   4%|▍         | 2/50 [00:01<00:28,  1.70it/s]Train Iter:   3/1000. LR: 0.0000. Data: 0.38s. Batch: 0.56s. S_Loss: 2.3971. T_Loss: 2.4137. Mask: 0.0026. :   4%|▍         | 2/50 [00:01<00:28,  1.70it/s]Train Iter:   3/1000. LR: 0.0000. Data: 0.38s. Batch: 0.56s. S_Loss: 2.3971. T_Loss: 2.4137. Mask: 0.0026. :   6%|▌         | 3/50 [00:01<00:24,  1.92it/s]total : 1000  current step :  1
total : 1000  current step :  2
total : 1000  current step :  3
Train Iter:   4/1000. LR: 0.0000. Data: 0.50s. Batch: 0.68s. S_Loss: 2.3799. T_Loss: 2.4151. Mask: 0.0020. :   6%|▌         | 3/50 [00:02<00:24,  1.92it/s]Train Iter:   4/1000. LR: 0.0000. Data: 0.50s. Batch: 0.68s. S_Loss: 2.3799. T_Loss: 2.4151. Mask: 0.0020. :   8%|▊         | 4/50 [00:02<00:33,  1.38it/s]Train Iter:   5/1000. LR: 0.0000. Data: 0.42s. Batch: 0.60s. S_Loss: 2.3857. T_Loss: 2.4149. Mask: 0.0016. :   8%|▊         | 4/50 [00:03<00:33,  1.38it/s]Train Iter:   5/1000. LR: 0.0000. Data: 0.42s. Batch: 0.60s. S_Loss: 2.3857. T_Loss: 2.4149. Mask: 0.0016. :  10%|█         | 5/50 [00:03<00:25,  1.74it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.3788. T_Loss: 2.4022. Mask: 0.0013. :  10%|█         | 5/50 [00:03<00:25,  1.74it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.3788. T_Loss: 2.4022. Mask: 0.0013. :  12%|█▏        | 6/50 [00:03<00:21,  2.02it/s]total : 1000  current step :  4
total : 1000  current step :  5
total : 1000  current step :  6
Train Iter:   7/1000. LR: 0.0000. Data: 0.47s. Batch: 0.66s. S_Loss: 2.3734. T_Loss: 2.4081. Mask: 0.0017. :  12%|█▏        | 6/50 [00:04<00:21,  2.02it/s]Train Iter:   7/1000. LR: 0.0000. Data: 0.47s. Batch: 0.66s. S_Loss: 2.3734. T_Loss: 2.4081. Mask: 0.0017. :  14%|█▍        | 7/50 [00:04<00:31,  1.36it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.3739. T_Loss: 2.3943. Mask: 0.0020. :  14%|█▍        | 7/50 [00:04<00:31,  1.36it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.3739. T_Loss: 2.3943. Mask: 0.0020. :  16%|█▌        | 8/50 [00:04<00:25,  1.66it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.3709. T_Loss: 2.3832. Mask: 0.0022. :  16%|█▌        | 8/50 [00:05<00:25,  1.66it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.3709. T_Loss: 2.3832. Mask: 0.0022. :  18%|█▊        | 9/50 [00:05<00:20,  1.98it/s]total : 1000  current step :  7
total : 1000  current step :  8
total : 1000  current step :  9
Train Iter:  10/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.3724. T_Loss: 2.3773. Mask: 0.0027. :  18%|█▊        | 9/50 [00:06<00:20,  1.98it/s]Train Iter:  10/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.3724. T_Loss: 2.3773. Mask: 0.0027. :  20%|██        | 10/50 [00:06<00:27,  1.45it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.3673. T_Loss: 2.3683. Mask: 0.0032. :  20%|██        | 10/50 [00:06<00:27,  1.45it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.3673. T_Loss: 2.3683. Mask: 0.0032. :  22%|██▏       | 11/50 [00:06<00:21,  1.82it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.3598. T_Loss: 2.3560. Mask: 0.0033. :  22%|██▏       | 11/50 [00:06<00:21,  1.82it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.3598. T_Loss: 2.3560. Mask: 0.0033. :  24%|██▍       | 12/50 [00:06<00:18,  2.07it/s]total : 1000  current step :  10
total : 1000  current step :  11
total : 1000  current step :  12
Train Iter:  13/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.3581. T_Loss: 2.3392. Mask: 0.0036. :  24%|██▍       | 12/50 [00:08<00:18,  2.07it/s]Train Iter:  13/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.3581. T_Loss: 2.3392. Mask: 0.0036. :  26%|██▌       | 13/50 [00:08<00:26,  1.42it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.3552. T_Loss: 2.3171. Mask: 0.0036. :  26%|██▌       | 13/50 [00:08<00:26,  1.42it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.3552. T_Loss: 2.3171. Mask: 0.0036. :  28%|██▊       | 14/50 [00:08<00:20,  1.77it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.3506. T_Loss: 2.2987. Mask: 0.0034. :  28%|██▊       | 14/50 [00:08<00:20,  1.77it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.3506. T_Loss: 2.2987. Mask: 0.0034. :  30%|███       | 15/50 [00:08<00:16,  2.13it/s]total : 1000  current step :  13
total : 1000  current step :  14
total : 1000  current step :  15
Train Iter:  16/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.3459. T_Loss: 2.2807. Mask: 0.0042. :  30%|███       | 15/50 [00:09<00:16,  2.13it/s]Train Iter:  16/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.3459. T_Loss: 2.2807. Mask: 0.0042. :  32%|███▏      | 16/50 [00:09<00:22,  1.54it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.3413. T_Loss: 2.2573. Mask: 0.0044. :  32%|███▏      | 16/50 [00:09<00:22,  1.54it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.3413. T_Loss: 2.2573. Mask: 0.0044. :  34%|███▍      | 17/50 [00:09<00:17,  1.85it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.3346. T_Loss: 2.2398. Mask: 0.0065. :  34%|███▍      | 17/50 [00:10<00:17,  1.85it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.3346. T_Loss: 2.2398. Mask: 0.0065. :  36%|███▌      | 18/50 [00:10<00:15,  2.08it/s]total : 1000  current step :  16
total : 1000  current step :  17
total : 1000  current step :  18
Train Iter:  19/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.3297. T_Loss: 2.2160. Mask: 0.0076. :  36%|███▌      | 18/50 [00:11<00:15,  2.08it/s]Train Iter:  19/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.3297. T_Loss: 2.2160. Mask: 0.0076. :  38%|███▊      | 19/50 [00:11<00:20,  1.51it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.40s. Batch: 0.58s. S_Loss: 2.3256. T_Loss: 2.1907. Mask: 0.0090. :  38%|███▊      | 19/50 [00:11<00:20,  1.51it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.40s. Batch: 0.58s. S_Loss: 2.3256. T_Loss: 2.1907. Mask: 0.0090. :  40%|████      | 20/50 [00:11<00:16,  1.79it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.39s. Batch: 0.57s. S_Loss: 2.3209. T_Loss: 2.1672. Mask: 0.0123. :  40%|████      | 20/50 [00:12<00:16,  1.79it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.39s. Batch: 0.57s. S_Loss: 2.3209. T_Loss: 2.1672. Mask: 0.0123. :  42%|████▏     | 21/50 [00:12<00:14,  1.96it/s]total : 1000  current step :  19
total : 1000  current step :  20
total : 1000  current step :  21
Train Iter:  22/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.3175. T_Loss: 2.1451. Mask: 0.0156. :  42%|████▏     | 21/50 [00:13<00:14,  1.96it/s]Train Iter:  22/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.3175. T_Loss: 2.1451. Mask: 0.0156. :  44%|████▍     | 22/50 [00:13<00:19,  1.47it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.40s. Batch: 0.58s. S_Loss: 2.3149. T_Loss: 2.1227. Mask: 0.0194. :  44%|████▍     | 22/50 [00:13<00:19,  1.47it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.40s. Batch: 0.58s. S_Loss: 2.3149. T_Loss: 2.1227. Mask: 0.0194. :  46%|████▌     | 23/50 [00:13<00:15,  1.78it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.39s. Batch: 0.57s. S_Loss: 2.3127. T_Loss: 2.0983. Mask: 0.0239. :  46%|████▌     | 23/50 [00:13<00:15,  1.78it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.39s. Batch: 0.57s. S_Loss: 2.3127. T_Loss: 2.0983. Mask: 0.0239. :  48%|████▊     | 24/50 [00:13<00:12,  2.03it/s]total : 1000  current step :  22
total : 1000  current step :  23
total : 1000  current step :  24
Train Iter:  25/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.3101. T_Loss: 2.0767. Mask: 0.0308. :  48%|████▊     | 24/50 [00:14<00:12,  2.03it/s]Train Iter:  25/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.3101. T_Loss: 2.0767. Mask: 0.0308. :  50%|█████     | 25/50 [00:14<00:16,  1.50it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.3073. T_Loss: 2.0561. Mask: 0.0398. :  50%|█████     | 25/50 [00:15<00:16,  1.50it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.3073. T_Loss: 2.0561. Mask: 0.0398. :  52%|█████▏    | 26/50 [00:15<00:13,  1.78it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.3039. T_Loss: 2.0386. Mask: 0.0522. :  52%|█████▏    | 26/50 [00:15<00:13,  1.78it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.3039. T_Loss: 2.0386. Mask: 0.0522. :  54%|█████▍    | 27/50 [00:15<00:10,  2.14it/s]total : 1000  current step :  25
total : 1000  current step :  26
total : 1000  current step :  27
Train Iter:  28/1000. LR: 0.0000. Data: 0.40s. Batch: 0.58s. S_Loss: 2.3010. T_Loss: 2.0203. Mask: 0.0652. :  54%|█████▍    | 27/50 [00:16<00:10,  2.14it/s]Train Iter:  28/1000. LR: 0.0000. Data: 0.40s. Batch: 0.58s. S_Loss: 2.3010. T_Loss: 2.0203. Mask: 0.0652. :  56%|█████▌    | 28/50 [00:16<00:13,  1.62it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.2973. T_Loss: 2.0040. Mask: 0.0795. :  56%|█████▌    | 28/50 [00:16<00:13,  1.62it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.2973. T_Loss: 2.0040. Mask: 0.0795. :  58%|█████▊    | 29/50 [00:16<00:11,  1.85it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.2955. T_Loss: 1.9916. Mask: 0.0969. :  58%|█████▊    | 29/50 [00:17<00:11,  1.85it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.2955. T_Loss: 1.9916. Mask: 0.0969. :  60%|██████    | 30/50 [00:17<00:09,  2.11it/s]total : 1000  current step :  28
total : 1000  current step :  29
total : 1000  current step :  30
Train Iter:  31/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.2945. T_Loss: 1.9779. Mask: 0.1129. :  60%|██████    | 30/50 [00:18<00:09,  2.11it/s]Train Iter:  31/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.2945. T_Loss: 1.9779. Mask: 0.1129. :  62%|██████▏   | 31/50 [00:18<00:12,  1.55it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.2912. T_Loss: 1.9660. Mask: 0.1300. :  62%|██████▏   | 31/50 [00:18<00:12,  1.55it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.2912. T_Loss: 1.9660. Mask: 0.1300. :  64%|██████▍   | 32/50 [00:18<00:09,  1.95it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2887. T_Loss: 1.9527. Mask: 0.1473. :  64%|██████▍   | 32/50 [00:18<00:09,  1.95it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2887. T_Loss: 1.9527. Mask: 0.1473. :  66%|██████▌   | 33/50 [00:18<00:07,  2.25it/s]total : 1000  current step :  31
total : 1000  current step :  32
total : 1000  current step :  33
Train Iter:  34/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2879. T_Loss: 1.9407. Mask: 0.1635. :  66%|██████▌   | 33/50 [00:19<00:07,  2.25it/s]Train Iter:  34/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2879. T_Loss: 1.9407. Mask: 0.1635. :  68%|██████▊   | 34/50 [00:19<00:09,  1.62it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.2864. T_Loss: 1.9285. Mask: 0.1794. :  68%|██████▊   | 34/50 [00:19<00:09,  1.62it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.2864. T_Loss: 1.9285. Mask: 0.1794. :  70%|███████   | 35/50 [00:19<00:07,  1.90it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2844. T_Loss: 1.9175. Mask: 0.1952. :  70%|███████   | 35/50 [00:20<00:07,  1.90it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2844. T_Loss: 1.9175. Mask: 0.1952. :  72%|███████▏  | 36/50 [00:20<00:06,  2.22it/s]total : 1000  current step :  34
total : 1000  current step :  35
total : 1000  current step :  36
Train Iter:  37/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2816. T_Loss: 1.9075. Mask: 0.2105. :  72%|███████▏  | 36/50 [00:21<00:06,  2.22it/s]Train Iter:  37/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2816. T_Loss: 1.9075. Mask: 0.2105. :  74%|███████▍  | 37/50 [00:21<00:08,  1.48it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.2807. T_Loss: 1.8971. Mask: 0.2257. :  74%|███████▍  | 37/50 [00:21<00:08,  1.48it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.2807. T_Loss: 1.8971. Mask: 0.2257. :  76%|███████▌  | 38/50 [00:21<00:06,  1.79it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2806. T_Loss: 1.8854. Mask: 0.2394. :  76%|███████▌  | 38/50 [00:22<00:06,  1.79it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2806. T_Loss: 1.8854. Mask: 0.2394. :  78%|███████▊  | 39/50 [00:22<00:05,  2.02it/s]total : 1000  current step :  37
total : 1000  current step :  38
total : 1000  current step :  39
Train Iter:  40/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.2788. T_Loss: 1.8756. Mask: 0.2535. :  78%|███████▊  | 39/50 [00:23<00:05,  2.02it/s]Train Iter:  40/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.2788. T_Loss: 1.8756. Mask: 0.2535. :  80%|████████  | 40/50 [00:23<00:07,  1.29it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2772. T_Loss: 1.8669. Mask: 0.2669. :  80%|████████  | 40/50 [00:24<00:07,  1.29it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2772. T_Loss: 1.8669. Mask: 0.2669. :  82%|████████▏ | 41/50 [00:24<00:06,  1.39it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2758. T_Loss: 1.8574. Mask: 0.2799. :  82%|████████▏ | 41/50 [00:24<00:06,  1.39it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2758. T_Loss: 1.8574. Mask: 0.2799. :  84%|████████▍ | 42/50 [00:24<00:05,  1.45it/s]total : 1000  current step :  40
total : 1000  current step :  41
total : 1000  current step :  42
Train Iter:  43/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 2.2754. T_Loss: 1.8498. Mask: 0.2927. :  84%|████████▍ | 42/50 [00:25<00:05,  1.45it/s]Train Iter:  43/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 2.2754. T_Loss: 1.8498. Mask: 0.2927. :  86%|████████▌ | 43/50 [00:25<00:05,  1.24it/s]Train Iter:  44/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2744. T_Loss: 1.8422. Mask: 0.3048. :  86%|████████▌ | 43/50 [00:26<00:05,  1.24it/s]Train Iter:  44/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2744. T_Loss: 1.8422. Mask: 0.3048. :  88%|████████▊ | 44/50 [00:26<00:04,  1.47it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2721. T_Loss: 1.8339. Mask: 0.3160. :  88%|████████▊ | 44/50 [00:26<00:04,  1.47it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2721. T_Loss: 1.8339. Mask: 0.3160. :  90%|█████████ | 45/50 [00:26<00:02,  1.71it/s]total : 1000  current step :  43
total : 1000  current step :  44
total : 1000  current step :  45
Train Iter:  46/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 2.2719. T_Loss: 1.8275. Mask: 0.3271. :  90%|█████████ | 45/50 [00:27<00:02,  1.71it/s]Train Iter:  46/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 2.2719. T_Loss: 1.8275. Mask: 0.3271. :  92%|█████████▏| 46/50 [00:27<00:02,  1.38it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2706. T_Loss: 1.8206. Mask: 0.3383. :  92%|█████████▏| 46/50 [00:27<00:02,  1.38it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2706. T_Loss: 1.8206. Mask: 0.3383. :  94%|█████████▍| 47/50 [00:27<00:01,  1.66it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2701. T_Loss: 1.8144. Mask: 0.3485. :  94%|█████████▍| 47/50 [00:28<00:01,  1.66it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2701. T_Loss: 1.8144. Mask: 0.3485. :  96%|█████████▌| 48/50 [00:28<00:01,  1.90it/s]total : 1000  current step :  46
total : 1000  current step :  47
total : 1000  current step :  48
Train Iter:  49/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.2701. T_Loss: 1.8079. Mask: 0.3583. :  96%|█████████▌| 48/50 [00:29<00:01,  1.90it/s]Train Iter:  49/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.2701. T_Loss: 1.8079. Mask: 0.3583. :  98%|█████████▊| 49/50 [00:29<00:00,  1.40it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2698. T_Loss: 1.8016. Mask: 0.3680. :  98%|█████████▊| 49/50 [00:29<00:00,  1.40it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2698. T_Loss: 1.8016. Mask: 0.3680. : 100%|██████████| 50/50 [00:29<00:00,  1.60it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2698. T_Loss: 1.8016. Mask: 0.3680. : 100%|██████████| 50/50 [00:29<00:00,  1.68it/s]
total : 1000  current step :  49
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 3.2542. top1: 0.00. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 3.2542. top1: 0.00. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.56it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 3.2602. top1: 0.00. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.56it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 3.2602. top1: 0.00. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 3.2947. top1: 0.00. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 3.2947. top1: 0.00. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 3.4320. top1: 0.10. top5: 97.66. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 3.4320. top1: 0.10. top5: 97.66. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 4.4032. top1: 0.08. top5: 78.12. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 4.4032. top1: 0.08. top5: 78.12. :  62%|██████▎   | 5/8 [00:01<00:00,  3.37it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 5.0688. top1: 0.07. top5: 65.17. :  62%|██████▎   | 5/8 [00:01<00:00,  3.37it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 5.0688. top1: 0.07. top5: 65.17. :  75%|███████▌  | 6/8 [00:01<00:00,  3.46it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 5.5214. top1: 0.06. top5: 55.86. :  75%|███████▌  | 6/8 [00:02<00:00,  3.46it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 5.5214. top1: 0.06. top5: 55.86. :  88%|████████▊ | 7/8 [00:02<00:00,  3.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 5.8612. top1: 0.05. top5: 50.05. :  88%|████████▊ | 7/8 [00:02<00:00,  3.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 5.8612. top1: 0.05. top5: 50.05. : 100%|██████████| 8/8 [00:02<00:00,  3.61it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 5.8612. top1: 0.05. top5: 50.05. : 100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
total : 1000  current step :  50
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.00s. Batch: 0.24s. S_Loss: 2.2471. T_Loss: 1.5567. Mask: 0.8594. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.00s. Batch: 0.24s. S_Loss: 2.2471. T_Loss: 1.5567. Mask: 0.8594. :   2%|▏         | 1/50 [00:00<00:11,  4.10it/s]total : 1000  current step :  51
Train Iter:  52/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.2311. T_Loss: 1.5550. Mask: 0.8672. :   2%|▏         | 1/50 [00:01<00:11,  4.10it/s]Train Iter:  52/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.2311. T_Loss: 1.5550. Mask: 0.8672. :   4%|▍         | 2/50 [00:01<00:33,  1.42it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2276. T_Loss: 1.5468. Mask: 0.8646. :   4%|▍         | 2/50 [00:01<00:33,  1.42it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2276. T_Loss: 1.5468. Mask: 0.8646. :   6%|▌         | 3/50 [00:01<00:24,  1.89it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.2359. T_Loss: 1.5434. Mask: 0.8613. :   6%|▌         | 3/50 [00:01<00:24,  1.89it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.27s. Batch: 0.48s. S_Loss: 2.2359. T_Loss: 1.5434. Mask: 0.8613. :   8%|▊         | 4/50 [00:01<00:20,  2.22it/s]total : 1000  current step :  52
total : 1000  current step :  53
total : 1000  current step :  54
Train Iter:  55/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2328. T_Loss: 1.5368. Mask: 0.8680. :   8%|▊         | 4/50 [00:03<00:20,  2.22it/s]Train Iter:  55/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2328. T_Loss: 1.5368. Mask: 0.8680. :  10%|█         | 5/50 [00:03<00:30,  1.47it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.33s. Batch: 0.53s. S_Loss: 2.2339. T_Loss: 1.5373. Mask: 0.8678. :  10%|█         | 5/50 [00:03<00:30,  1.47it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.33s. Batch: 0.53s. S_Loss: 2.2339. T_Loss: 1.5373. Mask: 0.8678. :  12%|█▏        | 6/50 [00:03<00:22,  1.94it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.2338. T_Loss: 1.5259. Mask: 0.8622. :  12%|█▏        | 6/50 [00:03<00:22,  1.94it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.2338. T_Loss: 1.5259. Mask: 0.8622. :  14%|█▍        | 7/50 [00:03<00:19,  2.17it/s]total : 1000  current step :  55
total : 1000  current step :  56
total : 1000  current step :  57
Train Iter:  58/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2405. T_Loss: 1.5212. Mask: 0.8604. :  14%|█▍        | 7/50 [00:04<00:19,  2.17it/s]Train Iter:  58/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2405. T_Loss: 1.5212. Mask: 0.8604. :  16%|█▌        | 8/50 [00:04<00:26,  1.60it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.35s. Batch: 0.54s. S_Loss: 2.2422. T_Loss: 1.5186. Mask: 0.8637. :  16%|█▌        | 8/50 [00:04<00:26,  1.60it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.35s. Batch: 0.54s. S_Loss: 2.2422. T_Loss: 1.5186. Mask: 0.8637. :  18%|█▊        | 9/50 [00:04<00:21,  1.87it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.33s. Batch: 0.52s. S_Loss: 2.2365. T_Loss: 1.5206. Mask: 0.8664. :  18%|█▊        | 9/50 [00:05<00:21,  1.87it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.33s. Batch: 0.52s. S_Loss: 2.2365. T_Loss: 1.5206. Mask: 0.8664. :  20%|██        | 10/50 [00:05<00:18,  2.15it/s]total : 1000  current step :  58
total : 1000  current step :  59
total : 1000  current step :  60
Train Iter:  61/1000. LR: 0.0000. Data: 0.38s. Batch: 0.56s. S_Loss: 2.2394. T_Loss: 1.5238. Mask: 0.8707. :  20%|██        | 10/50 [00:06<00:18,  2.15it/s]Train Iter:  61/1000. LR: 0.0000. Data: 0.38s. Batch: 0.56s. S_Loss: 2.2394. T_Loss: 1.5238. Mask: 0.8707. :  22%|██▏       | 11/50 [00:06<00:25,  1.56it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.36s. Batch: 0.54s. S_Loss: 2.2380. T_Loss: 1.5210. Mask: 0.8730. :  22%|██▏       | 11/50 [00:06<00:25,  1.56it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.36s. Batch: 0.54s. S_Loss: 2.2380. T_Loss: 1.5210. Mask: 0.8730. :  24%|██▍       | 12/50 [00:06<00:20,  1.83it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.35s. Batch: 0.54s. S_Loss: 2.2345. T_Loss: 1.5223. Mask: 0.8780. :  24%|██▍       | 12/50 [00:06<00:20,  1.83it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.35s. Batch: 0.54s. S_Loss: 2.2345. T_Loss: 1.5223. Mask: 0.8780. :  26%|██▌       | 13/50 [00:06<00:18,  1.95it/s]total : 1000  current step :  61
total : 1000  current step :  62
total : 1000  current step :  63
Train Iter:  64/1000. LR: 0.0000. Data: 0.39s. Batch: 0.57s. S_Loss: 2.2351. T_Loss: 1.5226. Mask: 0.8789. :  26%|██▌       | 13/50 [00:08<00:18,  1.95it/s]Train Iter:  64/1000. LR: 0.0000. Data: 0.39s. Batch: 0.57s. S_Loss: 2.2351. T_Loss: 1.5226. Mask: 0.8789. :  28%|██▊       | 14/50 [00:08<00:24,  1.46it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2357. T_Loss: 1.5268. Mask: 0.8810. :  28%|██▊       | 14/50 [00:08<00:24,  1.46it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.2357. T_Loss: 1.5268. Mask: 0.8810. :  30%|███       | 15/50 [00:08<00:20,  1.74it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.36s. Batch: 0.55s. S_Loss: 2.2339. T_Loss: 1.5283. Mask: 0.8813. :  30%|███       | 15/50 [00:08<00:20,  1.74it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.36s. Batch: 0.55s. S_Loss: 2.2339. T_Loss: 1.5283. Mask: 0.8813. :  32%|███▏      | 16/50 [00:08<00:18,  1.85it/s]total : 1000  current step :  64
total : 1000  current step :  65
total : 1000  current step :  66
Train Iter:  67/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.2330. T_Loss: 1.5270. Mask: 0.8814. :  32%|███▏      | 16/50 [00:09<00:18,  1.85it/s]Train Iter:  67/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.2330. T_Loss: 1.5270. Mask: 0.8814. :  34%|███▍      | 17/50 [00:09<00:22,  1.44it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.2338. T_Loss: 1.5311. Mask: 0.8843. :  34%|███▍      | 17/50 [00:10<00:22,  1.44it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.38s. Batch: 0.57s. S_Loss: 2.2338. T_Loss: 1.5311. Mask: 0.8843. :  36%|███▌      | 18/50 [00:10<00:18,  1.69it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.36s. Batch: 0.55s. S_Loss: 2.2338. T_Loss: 1.5301. Mask: 0.8859. :  36%|███▌      | 18/50 [00:10<00:18,  1.69it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.36s. Batch: 0.55s. S_Loss: 2.2338. T_Loss: 1.5301. Mask: 0.8859. :  38%|███▊      | 19/50 [00:10<00:15,  2.00it/s]total : 1000  current step :  67
total : 1000  current step :  68
total : 1000  current step :  69
Train Iter:  70/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2329. T_Loss: 1.5334. Mask: 0.8885. :  38%|███▊      | 19/50 [00:11<00:15,  2.00it/s]Train Iter:  70/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2329. T_Loss: 1.5334. Mask: 0.8885. :  40%|████      | 20/50 [00:11<00:20,  1.50it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.2332. T_Loss: 1.5348. Mask: 0.8893. :  40%|████      | 20/50 [00:11<00:20,  1.50it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.2332. T_Loss: 1.5348. Mask: 0.8893. :  42%|████▏     | 21/50 [00:11<00:16,  1.71it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.36s. Batch: 0.55s. S_Loss: 2.2323. T_Loss: 1.5367. Mask: 0.8910. :  42%|████▏     | 21/50 [00:12<00:16,  1.71it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.36s. Batch: 0.55s. S_Loss: 2.2323. T_Loss: 1.5367. Mask: 0.8910. :  44%|████▍     | 22/50 [00:12<00:13,  2.13it/s]total : 1000  current step :  70
total : 1000  current step :  71
total : 1000  current step :  72
Train Iter:  73/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2323. T_Loss: 1.5390. Mask: 0.8932. :  44%|████▍     | 22/50 [00:13<00:13,  2.13it/s]Train Iter:  73/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2323. T_Loss: 1.5390. Mask: 0.8932. :  46%|████▌     | 23/50 [00:13<00:17,  1.53it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.2326. T_Loss: 1.5404. Mask: 0.8932. :  46%|████▌     | 23/50 [00:13<00:17,  1.53it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.2326. T_Loss: 1.5404. Mask: 0.8932. :  48%|████▊     | 24/50 [00:13<00:15,  1.73it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.2341. T_Loss: 1.5410. Mask: 0.8936. :  48%|████▊     | 24/50 [00:14<00:15,  1.73it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.2341. T_Loss: 1.5410. Mask: 0.8936. :  50%|█████     | 25/50 [00:14<00:12,  1.98it/s]total : 1000  current step :  73
total : 1000  current step :  74
total : 1000  current step :  75
Train Iter:  76/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2340. T_Loss: 1.5446. Mask: 0.8944. :  50%|█████     | 25/50 [00:15<00:12,  1.98it/s]Train Iter:  76/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.2340. T_Loss: 1.5446. Mask: 0.8944. :  52%|█████▏    | 26/50 [00:15<00:16,  1.47it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2339. T_Loss: 1.5451. Mask: 0.8948. :  52%|█████▏    | 26/50 [00:15<00:16,  1.47it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2339. T_Loss: 1.5451. Mask: 0.8948. :  54%|█████▍    | 27/50 [00:15<00:13,  1.73it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.2337. T_Loss: 1.5462. Mask: 0.8958. :  54%|█████▍    | 27/50 [00:15<00:13,  1.73it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.2337. T_Loss: 1.5462. Mask: 0.8958. :  56%|█████▌    | 28/50 [00:15<00:10,  2.10it/s]total : 1000  current step :  76
total : 1000  current step :  77
total : 1000  current step :  78
Train Iter:  79/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.2335. T_Loss: 1.5451. Mask: 0.8956. :  56%|█████▌    | 28/50 [00:16<00:10,  2.10it/s]Train Iter:  79/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.2335. T_Loss: 1.5451. Mask: 0.8956. :  58%|█████▊    | 29/50 [00:16<00:13,  1.61it/s]total : 1000  current step :  79
Train Iter:  80/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2328. T_Loss: 1.5464. Mask: 0.8973. :  58%|█████▊    | 29/50 [00:17<00:13,  1.61it/s]Train Iter:  80/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2328. T_Loss: 1.5464. Mask: 0.8973. :  60%|██████    | 30/50 [00:17<00:12,  1.56it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2315. T_Loss: 1.5465. Mask: 0.8982. :  60%|██████    | 30/50 [00:18<00:12,  1.56it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2315. T_Loss: 1.5465. Mask: 0.8982. :  62%|██████▏   | 31/50 [00:18<00:13,  1.45it/s]total : 1000  current step :  80
total : 1000  current step :  81
Train Iter:  82/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.2311. T_Loss: 1.5479. Mask: 0.8983. :  62%|██████▏   | 31/50 [00:19<00:13,  1.45it/s]Train Iter:  82/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.2311. T_Loss: 1.5479. Mask: 0.8983. :  64%|██████▍   | 32/50 [00:19<00:14,  1.23it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2311. T_Loss: 1.5508. Mask: 0.8990. :  64%|██████▍   | 32/50 [00:19<00:14,  1.23it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2311. T_Loss: 1.5508. Mask: 0.8990. :  66%|██████▌   | 33/50 [00:19<00:11,  1.51it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2314. T_Loss: 1.5522. Mask: 0.8991. :  66%|██████▌   | 33/50 [00:19<00:11,  1.51it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2314. T_Loss: 1.5522. Mask: 0.8991. :  68%|██████▊   | 34/50 [00:19<00:09,  1.78it/s]total : 1000  current step :  82
total : 1000  current step :  83
total : 1000  current step :  84
Train Iter:  85/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.2321. T_Loss: 1.5549. Mask: 0.9007. :  68%|██████▊   | 34/50 [00:20<00:09,  1.78it/s]Train Iter:  85/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.2321. T_Loss: 1.5549. Mask: 0.9007. :  70%|███████   | 35/50 [00:20<00:10,  1.38it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2319. T_Loss: 1.5567. Mask: 0.9016. :  70%|███████   | 35/50 [00:21<00:10,  1.38it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2319. T_Loss: 1.5567. Mask: 0.9016. :  72%|███████▏  | 36/50 [00:21<00:08,  1.59it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2326. T_Loss: 1.5601. Mask: 0.9021. :  72%|███████▏  | 36/50 [00:21<00:08,  1.59it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2326. T_Loss: 1.5601. Mask: 0.9021. :  74%|███████▍  | 37/50 [00:21<00:06,  1.90it/s]total : 1000  current step :  85
total : 1000  current step :  86
total : 1000  current step :  87
Train Iter:  88/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2340. T_Loss: 1.5604. Mask: 0.9031. :  74%|███████▍  | 37/50 [00:22<00:06,  1.90it/s]Train Iter:  88/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2340. T_Loss: 1.5604. Mask: 0.9031. :  76%|███████▌  | 38/50 [00:22<00:08,  1.42it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2344. T_Loss: 1.5616. Mask: 0.9039. :  76%|███████▌  | 38/50 [00:23<00:08,  1.42it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2344. T_Loss: 1.5616. Mask: 0.9039. :  78%|███████▊  | 39/50 [00:23<00:06,  1.64it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2341. T_Loss: 1.5633. Mask: 0.9052. :  78%|███████▊  | 39/50 [00:23<00:06,  1.64it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2341. T_Loss: 1.5633. Mask: 0.9052. :  80%|████████  | 40/50 [00:23<00:04,  2.04it/s]total : 1000  current step :  88
total : 1000  current step :  89
total : 1000  current step :  90
Train Iter:  91/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2346. T_Loss: 1.5674. Mask: 0.9059. :  80%|████████  | 40/50 [00:24<00:04,  2.04it/s]Train Iter:  91/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2346. T_Loss: 1.5674. Mask: 0.9059. :  82%|████████▏ | 41/50 [00:24<00:05,  1.62it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2341. T_Loss: 1.5692. Mask: 0.9061. :  82%|████████▏ | 41/50 [00:24<00:05,  1.62it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2341. T_Loss: 1.5692. Mask: 0.9061. :  84%|████████▍ | 42/50 [00:24<00:04,  1.87it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2344. T_Loss: 1.5696. Mask: 0.9069. :  84%|████████▍ | 42/50 [00:24<00:04,  1.87it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2344. T_Loss: 1.5696. Mask: 0.9069. :  86%|████████▌ | 43/50 [00:24<00:03,  2.14it/s]total : 1000  current step :  91
total : 1000  current step :  92
total : 1000  current step :  93
Train Iter:  94/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2344. T_Loss: 1.5735. Mask: 0.9076. :  86%|████████▌ | 43/50 [00:25<00:03,  2.14it/s]Train Iter:  94/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2344. T_Loss: 1.5735. Mask: 0.9076. :  88%|████████▊ | 44/50 [00:25<00:03,  1.62it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2345. T_Loss: 1.5747. Mask: 0.9083. :  88%|████████▊ | 44/50 [00:26<00:03,  1.62it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2345. T_Loss: 1.5747. Mask: 0.9083. :  90%|█████████ | 45/50 [00:26<00:02,  1.75it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2354. T_Loss: 1.5772. Mask: 0.9083. :  90%|█████████ | 45/50 [00:26<00:02,  1.75it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2354. T_Loss: 1.5772. Mask: 0.9083. :  92%|█████████▏| 46/50 [00:26<00:01,  2.15it/s]total : 1000  current step :  94
total : 1000  current step :  95
total : 1000  current step :  96
Train Iter:  97/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2362. T_Loss: 1.5792. Mask: 0.9089. :  92%|█████████▏| 46/50 [00:27<00:01,  2.15it/s]Train Iter:  97/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2362. T_Loss: 1.5792. Mask: 0.9089. :  94%|█████████▍| 47/50 [00:27<00:01,  1.73it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2366. T_Loss: 1.5823. Mask: 0.9097. :  94%|█████████▍| 47/50 [00:27<00:01,  1.73it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2366. T_Loss: 1.5823. Mask: 0.9097. :  96%|█████████▌| 48/50 [00:27<00:01,  1.98it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2368. T_Loss: 1.5838. Mask: 0.9103. :  96%|█████████▌| 48/50 [00:28<00:01,  1.98it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2368. T_Loss: 1.5838. Mask: 0.9103. :  98%|█████████▊| 49/50 [00:28<00:00,  2.18it/s]total : 1000  current step :  97
total : 1000  current step :  98
total : 1000  current step :  99
Train Iter: 100/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2364. T_Loss: 1.5867. Mask: 0.9109. :  98%|█████████▊| 49/50 [00:29<00:00,  2.18it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2364. T_Loss: 1.5867. Mask: 0.9109. : 100%|██████████| 50/50 [00:29<00:00,  1.55it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2364. T_Loss: 1.5867. Mask: 0.9109. : 100%|██████████| 50/50 [00:29<00:00,  1.71it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 2.0965. top1: 0.00. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 2.0965. top1: 0.00. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 2.0960. top1: 0.39. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 2.0960. top1: 0.39. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.1108. top1: 0.26. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.1108. top1: 0.26. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.03it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.1689. top1: 0.20. top5: 97.75. :  38%|███▊      | 3/8 [00:01<00:01,  3.03it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.1689. top1: 0.20. top5: 97.75. :  50%|█████     | 4/8 [00:01<00:01,  3.31it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.5751. top1: 0.16. top5: 78.67. :  50%|█████     | 4/8 [00:01<00:01,  3.31it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.5751. top1: 0.16. top5: 78.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.8526. top1: 0.13. top5: 65.89. :  62%|██████▎   | 5/8 [00:01<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.8526. top1: 0.13. top5: 65.89. :  75%|███████▌  | 6/8 [00:01<00:00,  3.56it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 3.0406. top1: 0.11. top5: 56.75. :  75%|███████▌  | 6/8 [00:02<00:00,  3.56it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 3.0406. top1: 0.11. top5: 56.75. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 3.1839. top1: 0.10. top5: 50.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 3.1839. top1: 0.10. top5: 50.95. : 100%|██████████| 8/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 3.1839. top1: 0.10. top5: 50.95. : 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]
total : 1000  current step :  100
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.16s. S_Loss: 2.2252. T_Loss: 1.6679. Mask: 0.9336. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.16s. S_Loss: 2.2252. T_Loss: 1.6679. Mask: 0.9336. :   2%|▏         | 1/50 [00:00<00:08,  6.11it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.16s. S_Loss: 2.2338. T_Loss: 1.6755. Mask: 0.9258. :   2%|▏         | 1/50 [00:00<00:08,  6.11it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.16s. S_Loss: 2.2338. T_Loss: 1.6755. Mask: 0.9258. :   4%|▍         | 2/50 [00:00<00:07,  6.42it/s]total : 1000  current step :  101
total : 1000  current step :  102
Train Iter: 103/1000. LR: 0.0000. Data: 0.30s. Batch: 0.48s. S_Loss: 2.2409. T_Loss: 1.6880. Mask: 0.9297. :   4%|▍         | 2/50 [00:01<00:07,  6.42it/s]Train Iter: 103/1000. LR: 0.0000. Data: 0.30s. Batch: 0.48s. S_Loss: 2.2409. T_Loss: 1.6880. Mask: 0.9297. :   6%|▌         | 3/50 [00:01<00:28,  1.65it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.23s. Batch: 0.41s. S_Loss: 2.2424. T_Loss: 1.6769. Mask: 0.9326. :   6%|▌         | 3/50 [00:01<00:28,  1.65it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.23s. Batch: 0.41s. S_Loss: 2.2424. T_Loss: 1.6769. Mask: 0.9326. :   8%|▊         | 4/50 [00:01<00:20,  2.24it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.19s. Batch: 0.37s. S_Loss: 2.2407. T_Loss: 1.7113. Mask: 0.9375. :   8%|▊         | 4/50 [00:01<00:20,  2.24it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.19s. Batch: 0.37s. S_Loss: 2.2407. T_Loss: 1.7113. Mask: 0.9375. :  10%|█         | 5/50 [00:01<00:16,  2.79it/s]total : 1000  current step :  103
total : 1000  current step :  104
total : 1000  current step :  105
Train Iter: 106/1000. LR: 0.0000. Data: 0.32s. Batch: 0.50s. S_Loss: 2.2428. T_Loss: 1.7081. Mask: 0.9355. :  10%|█         | 5/50 [00:03<00:16,  2.79it/s]Train Iter: 106/1000. LR: 0.0000. Data: 0.32s. Batch: 0.50s. S_Loss: 2.2428. T_Loss: 1.7081. Mask: 0.9355. :  12%|█▏        | 6/50 [00:03<00:27,  1.58it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.30s. Batch: 0.48s. S_Loss: 2.2462. T_Loss: 1.7254. Mask: 0.9330. :  12%|█▏        | 6/50 [00:03<00:27,  1.58it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.30s. Batch: 0.48s. S_Loss: 2.2462. T_Loss: 1.7254. Mask: 0.9330. :  14%|█▍        | 7/50 [00:03<00:23,  1.83it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.27s. Batch: 0.46s. S_Loss: 2.2437. T_Loss: 1.7207. Mask: 0.9360. :  14%|█▍        | 7/50 [00:03<00:23,  1.83it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.27s. Batch: 0.46s. S_Loss: 2.2437. T_Loss: 1.7207. Mask: 0.9360. :  16%|█▌        | 8/50 [00:03<00:19,  2.18it/s]total : 1000  current step :  106
total : 1000  current step :  107
total : 1000  current step :  108
Train Iter: 109/1000. LR: 0.0000. Data: 0.34s. Batch: 0.52s. S_Loss: 2.2457. T_Loss: 1.7325. Mask: 0.9388. :  16%|█▌        | 8/50 [00:04<00:19,  2.18it/s]Train Iter: 109/1000. LR: 0.0000. Data: 0.34s. Batch: 0.52s. S_Loss: 2.2457. T_Loss: 1.7325. Mask: 0.9388. :  18%|█▊        | 9/50 [00:04<00:26,  1.54it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.31s. Batch: 0.50s. S_Loss: 2.2486. T_Loss: 1.7447. Mask: 0.9402. :  18%|█▊        | 9/50 [00:05<00:26,  1.54it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.31s. Batch: 0.50s. S_Loss: 2.2486. T_Loss: 1.7447. Mask: 0.9402. :  20%|██        | 10/50 [00:05<00:21,  1.84it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.29s. Batch: 0.49s. S_Loss: 2.2493. T_Loss: 1.7407. Mask: 0.9400. :  20%|██        | 10/50 [00:05<00:21,  1.84it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.29s. Batch: 0.49s. S_Loss: 2.2493. T_Loss: 1.7407. Mask: 0.9400. :  22%|██▏       | 11/50 [00:05<00:18,  2.07it/s]total : 1000  current step :  109
total : 1000  current step :  110
total : 1000  current step :  111
Train Iter: 112/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2492. T_Loss: 1.7419. Mask: 0.9398. :  22%|██▏       | 11/50 [00:06<00:18,  2.07it/s]Train Iter: 112/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2492. T_Loss: 1.7419. Mask: 0.9398. :  24%|██▍       | 12/50 [00:06<00:23,  1.59it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.2468. T_Loss: 1.7454. Mask: 0.9417. :  24%|██▍       | 12/50 [00:06<00:23,  1.59it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.2468. T_Loss: 1.7454. Mask: 0.9417. :  26%|██▌       | 13/50 [00:06<00:19,  1.88it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.28s. Batch: 0.50s. S_Loss: 2.2440. T_Loss: 1.7449. Mask: 0.9425. :  26%|██▌       | 13/50 [00:06<00:19,  1.88it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.28s. Batch: 0.50s. S_Loss: 2.2440. T_Loss: 1.7449. Mask: 0.9425. :  28%|██▊       | 14/50 [00:06<00:16,  2.17it/s]total : 1000  current step :  112
total : 1000  current step :  113
total : 1000  current step :  114
Train Iter: 115/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2415. T_Loss: 1.7497. Mask: 0.9398. :  28%|██▊       | 14/50 [00:08<00:16,  2.17it/s]Train Iter: 115/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2415. T_Loss: 1.7497. Mask: 0.9398. :  30%|███       | 15/50 [00:08<00:23,  1.48it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2405. T_Loss: 1.7495. Mask: 0.9397. :  30%|███       | 15/50 [00:08<00:23,  1.48it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2405. T_Loss: 1.7495. Mask: 0.9397. :  32%|███▏      | 16/50 [00:08<00:19,  1.77it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.28s. Batch: 0.51s. S_Loss: 2.2420. T_Loss: 1.7517. Mask: 0.9393. :  32%|███▏      | 16/50 [00:08<00:19,  1.77it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.28s. Batch: 0.51s. S_Loss: 2.2420. T_Loss: 1.7517. Mask: 0.9393. :  34%|███▍      | 17/50 [00:08<00:14,  2.21it/s]total : 1000  current step :  115
total : 1000  current step :  116
total : 1000  current step :  117
Train Iter: 118/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2426. T_Loss: 1.7529. Mask: 0.9390. :  34%|███▍      | 17/50 [00:09<00:14,  2.21it/s]Train Iter: 118/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2426. T_Loss: 1.7529. Mask: 0.9390. :  36%|███▌      | 18/50 [00:09<00:18,  1.72it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.29s. Batch: 0.51s. S_Loss: 2.2425. T_Loss: 1.7514. Mask: 0.9394. :  36%|███▌      | 18/50 [00:09<00:18,  1.72it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.29s. Batch: 0.51s. S_Loss: 2.2425. T_Loss: 1.7514. Mask: 0.9394. :  38%|███▊      | 19/50 [00:09<00:15,  2.03it/s]total : 1000  current step :  118
total : 1000  current step :  119
Train Iter: 120/1000. LR: 0.0000. Data: 0.29s. Batch: 0.51s. S_Loss: 2.2420. T_Loss: 1.7487. Mask: 0.9387. :  38%|███▊      | 19/50 [00:10<00:15,  2.03it/s]Train Iter: 120/1000. LR: 0.0000. Data: 0.29s. Batch: 0.51s. S_Loss: 2.2420. T_Loss: 1.7487. Mask: 0.9387. :  40%|████      | 20/50 [00:10<00:14,  2.06it/s]total : 1000  current step :  120
Train Iter: 121/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2433. T_Loss: 1.7518. Mask: 0.9388. :  40%|████      | 20/50 [00:11<00:14,  2.06it/s]Train Iter: 121/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2433. T_Loss: 1.7518. Mask: 0.9388. :  42%|████▏     | 21/50 [00:11<00:17,  1.61it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2422. T_Loss: 1.7526. Mask: 0.9395. :  42%|████▏     | 21/50 [00:11<00:17,  1.61it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2422. T_Loss: 1.7526. Mask: 0.9395. :  44%|████▍     | 22/50 [00:11<00:14,  1.96it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.2419. T_Loss: 1.7552. Mask: 0.9397. :  44%|████▍     | 22/50 [00:11<00:14,  1.96it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.2419. T_Loss: 1.7552. Mask: 0.9397. :  46%|████▌     | 23/50 [00:11<00:11,  2.33it/s]total : 1000  current step :  121
total : 1000  current step :  122
total : 1000  current step :  123
Train Iter: 124/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2405. T_Loss: 1.7558. Mask: 0.9395. :  46%|████▌     | 23/50 [00:12<00:11,  2.33it/s]Train Iter: 124/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2405. T_Loss: 1.7558. Mask: 0.9395. :  48%|████▊     | 24/50 [00:12<00:16,  1.60it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2416. T_Loss: 1.7562. Mask: 0.9402. :  48%|████▊     | 24/50 [00:13<00:16,  1.60it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2416. T_Loss: 1.7562. Mask: 0.9402. :  50%|█████     | 25/50 [00:13<00:12,  1.94it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.2417. T_Loss: 1.7595. Mask: 0.9404. :  50%|█████     | 25/50 [00:13<00:12,  1.94it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.2417. T_Loss: 1.7595. Mask: 0.9404. :  52%|█████▏    | 26/50 [00:13<00:10,  2.22it/s]total : 1000  current step :  124
total : 1000  current step :  125
total : 1000  current step :  126
Train Iter: 127/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2415. T_Loss: 1.7608. Mask: 0.9398. :  52%|█████▏    | 26/50 [00:14<00:10,  2.22it/s]Train Iter: 127/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2415. T_Loss: 1.7608. Mask: 0.9398. :  54%|█████▍    | 27/50 [00:14<00:13,  1.65it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2416. T_Loss: 1.7650. Mask: 0.9397. :  54%|█████▍    | 27/50 [00:14<00:13,  1.65it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2416. T_Loss: 1.7650. Mask: 0.9397. :  56%|█████▌    | 28/50 [00:14<00:11,  1.92it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.2412. T_Loss: 1.7681. Mask: 0.9387. :  56%|█████▌    | 28/50 [00:14<00:11,  1.92it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.2412. T_Loss: 1.7681. Mask: 0.9387. :  58%|█████▊    | 29/50 [00:14<00:09,  2.17it/s]total : 1000  current step :  127
total : 1000  current step :  128
total : 1000  current step :  129
Train Iter: 130/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2408. T_Loss: 1.7719. Mask: 0.9395. :  58%|█████▊    | 29/50 [00:15<00:09,  2.17it/s]Train Iter: 130/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2408. T_Loss: 1.7719. Mask: 0.9395. :  60%|██████    | 30/50 [00:15<00:11,  1.68it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2418. T_Loss: 1.7757. Mask: 0.9393. :  60%|██████    | 30/50 [00:16<00:11,  1.68it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2418. T_Loss: 1.7757. Mask: 0.9393. :  62%|██████▏   | 31/50 [00:16<00:09,  1.93it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2418. T_Loss: 1.7779. Mask: 0.9396. :  62%|██████▏   | 31/50 [00:16<00:09,  1.93it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2418. T_Loss: 1.7779. Mask: 0.9396. :  64%|██████▍   | 32/50 [00:16<00:08,  2.14it/s]total : 1000  current step :  130
total : 1000  current step :  131
total : 1000  current step :  132
Train Iter: 133/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2424. T_Loss: 1.7864. Mask: 0.9392. :  64%|██████▍   | 32/50 [00:17<00:08,  2.14it/s]Train Iter: 133/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2424. T_Loss: 1.7864. Mask: 0.9392. :  66%|██████▌   | 33/50 [00:17<00:11,  1.53it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2422. T_Loss: 1.7912. Mask: 0.9389. :  66%|██████▌   | 33/50 [00:17<00:11,  1.53it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2422. T_Loss: 1.7912. Mask: 0.9389. :  68%|██████▊   | 34/50 [00:17<00:08,  1.84it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2417. T_Loss: 1.7932. Mask: 0.9392. :  68%|██████▊   | 34/50 [00:18<00:08,  1.84it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2417. T_Loss: 1.7932. Mask: 0.9392. :  70%|███████   | 35/50 [00:18<00:06,  2.16it/s]total : 1000  current step :  133
total : 1000  current step :  134
total : 1000  current step :  135
Train Iter: 136/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2408. T_Loss: 1.7959. Mask: 0.9379. :  70%|███████   | 35/50 [00:19<00:06,  2.16it/s]Train Iter: 136/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2408. T_Loss: 1.7959. Mask: 0.9379. :  72%|███████▏  | 36/50 [00:19<00:08,  1.70it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2407. T_Loss: 1.7983. Mask: 0.9379. :  72%|███████▏  | 36/50 [00:19<00:08,  1.70it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2407. T_Loss: 1.7983. Mask: 0.9379. :  74%|███████▍  | 37/50 [00:19<00:07,  1.83it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2414. T_Loss: 1.8023. Mask: 0.9373. :  74%|███████▍  | 37/50 [00:19<00:07,  1.83it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2414. T_Loss: 1.8023. Mask: 0.9373. :  76%|███████▌  | 38/50 [00:19<00:05,  2.05it/s]total : 1000  current step :  136
total : 1000  current step :  137
total : 1000  current step :  138
Train Iter: 139/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2424. T_Loss: 1.8039. Mask: 0.9372. :  76%|███████▌  | 38/50 [00:20<00:05,  2.05it/s]Train Iter: 139/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2424. T_Loss: 1.8039. Mask: 0.9372. :  78%|███████▊  | 39/50 [00:20<00:06,  1.63it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2422. T_Loss: 1.8075. Mask: 0.9369. :  78%|███████▊  | 39/50 [00:21<00:06,  1.63it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2422. T_Loss: 1.8075. Mask: 0.9369. :  80%|████████  | 40/50 [00:21<00:05,  1.92it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.29s. Batch: 0.52s. S_Loss: 2.2416. T_Loss: 1.8100. Mask: 0.9380. :  80%|████████  | 40/50 [00:21<00:05,  1.92it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.29s. Batch: 0.52s. S_Loss: 2.2416. T_Loss: 1.8100. Mask: 0.9380. :  82%|████████▏ | 41/50 [00:21<00:03,  2.26it/s]total : 1000  current step :  139
total : 1000  current step :  140
total : 1000  current step :  141
Train Iter: 142/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2420. T_Loss: 1.8103. Mask: 0.9381. :  82%|████████▏ | 41/50 [00:22<00:03,  2.26it/s]Train Iter: 142/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2420. T_Loss: 1.8103. Mask: 0.9381. :  84%|████████▍ | 42/50 [00:22<00:04,  1.62it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2418. T_Loss: 1.8127. Mask: 0.9384. :  84%|████████▍ | 42/50 [00:22<00:04,  1.62it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2418. T_Loss: 1.8127. Mask: 0.9384. :  86%|████████▌ | 43/50 [00:22<00:03,  1.83it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.29s. Batch: 0.52s. S_Loss: 2.2417. T_Loss: 1.8166. Mask: 0.9382. :  86%|████████▌ | 43/50 [00:23<00:03,  1.83it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.29s. Batch: 0.52s. S_Loss: 2.2417. T_Loss: 1.8166. Mask: 0.9382. :  88%|████████▊ | 44/50 [00:23<00:02,  2.09it/s]total : 1000  current step :  142
total : 1000  current step :  143
total : 1000  current step :  144
Train Iter: 145/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2416. T_Loss: 1.8190. Mask: 0.9379. :  88%|████████▊ | 44/50 [00:24<00:02,  2.09it/s]Train Iter: 145/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2416. T_Loss: 1.8190. Mask: 0.9379. :  90%|█████████ | 45/50 [00:24<00:03,  1.46it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2415. T_Loss: 1.8263. Mask: 0.9378. :  90%|█████████ | 45/50 [00:24<00:03,  1.46it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2415. T_Loss: 1.8263. Mask: 0.9378. :  92%|█████████▏| 46/50 [00:24<00:02,  1.80it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2425. T_Loss: 1.8302. Mask: 0.9382. :  92%|█████████▏| 46/50 [00:24<00:02,  1.80it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2425. T_Loss: 1.8302. Mask: 0.9382. :  94%|█████████▍| 47/50 [00:24<00:01,  2.09it/s]total : 1000  current step :  145
total : 1000  current step :  146
total : 1000  current step :  147
Train Iter: 148/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2421. T_Loss: 1.8305. Mask: 0.9386. :  94%|█████████▍| 47/50 [00:25<00:01,  2.09it/s]Train Iter: 148/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2421. T_Loss: 1.8305. Mask: 0.9386. :  96%|█████████▌| 48/50 [00:25<00:01,  1.52it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2428. T_Loss: 1.8317. Mask: 0.9389. :  96%|█████████▌| 48/50 [00:26<00:01,  1.52it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2428. T_Loss: 1.8317. Mask: 0.9389. :  98%|█████████▊| 49/50 [00:26<00:00,  1.81it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2423. T_Loss: 1.8337. Mask: 0.9395. :  98%|█████████▊| 49/50 [00:26<00:00,  1.81it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2423. T_Loss: 1.8337. Mask: 0.9395. : 100%|██████████| 50/50 [00:26<00:00,  2.25it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.30s. Batch: 0.52s. S_Loss: 2.2423. T_Loss: 1.8337. Mask: 0.9395. : 100%|██████████| 50/50 [00:26<00:00,  1.90it/s]
total : 1000  current step :  148
total : 1000  current step :  149
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.9143. top1: 0.00. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.9143. top1: 0.00. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.9121. top1: 0.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.9121. top1: 0.59. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.9173. top1: 0.65. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.9173. top1: 0.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.9498. top1: 0.59. top5: 97.75. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.9498. top1: 0.59. top5: 97.75. :  50%|█████     | 4/8 [00:01<00:01,  3.40it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.1725. top1: 0.47. top5: 79.30. :  50%|█████     | 4/8 [00:01<00:01,  3.40it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.1725. top1: 0.47. top5: 79.30. :  62%|██████▎   | 5/8 [00:01<00:00,  3.76it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.3252. top1: 0.39. top5: 67.12. :  62%|██████▎   | 5/8 [00:01<00:00,  3.76it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.3252. top1: 0.39. top5: 67.12. :  75%|███████▌  | 6/8 [00:01<00:00,  3.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.4288. top1: 0.33. top5: 58.37. :  75%|███████▌  | 6/8 [00:02<00:00,  3.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.4288. top1: 0.33. top5: 58.37. :  88%|████████▊ | 7/8 [00:02<00:00,  4.00it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.5079. top1: 0.30. top5: 52.85. :  88%|████████▊ | 7/8 [00:02<00:00,  4.00it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.5079. top1: 0.30. top5: 52.85. : 100%|██████████| 8/8 [00:02<00:00,  4.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.5079. top1: 0.30. top5: 52.85. : 100%|██████████| 8/8 [00:02<00:00,  3.22it/s]
total : 1000  current step :  150
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 0.72s. Batch: 0.91s. S_Loss: 2.2584. T_Loss: 2.0968. Mask: 0.9453. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 0.72s. Batch: 0.91s. S_Loss: 2.2584. T_Loss: 2.0968. Mask: 0.9453. :   2%|▏         | 1/50 [00:00<00:44,  1.10it/s]Train Iter: 152/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.2553. T_Loss: 2.0902. Mask: 0.9414. :   2%|▏         | 1/50 [00:01<00:44,  1.10it/s]Train Iter: 152/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.2553. T_Loss: 2.0902. Mask: 0.9414. :   4%|▍         | 2/50 [00:01<00:27,  1.77it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2413. T_Loss: 2.0898. Mask: 0.9388. :   4%|▍         | 2/50 [00:01<00:27,  1.77it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2413. T_Loss: 2.0898. Mask: 0.9388. :   6%|▌         | 3/50 [00:01<00:22,  2.08it/s]total : 1000  current step :  151
total : 1000  current step :  152
total : 1000  current step :  153
Train Iter: 154/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.2452. T_Loss: 2.1089. Mask: 0.9404. :   6%|▌         | 3/50 [00:02<00:22,  2.08it/s]Train Iter: 154/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.2452. T_Loss: 2.1089. Mask: 0.9404. :   8%|▊         | 4/50 [00:02<00:31,  1.47it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2496. T_Loss: 2.0685. Mask: 0.9414. :   8%|▊         | 4/50 [00:02<00:31,  1.47it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.2496. T_Loss: 2.0685. Mask: 0.9414. :  10%|█         | 5/50 [00:02<00:25,  1.78it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.2469. T_Loss: 2.0518. Mask: 0.9414. :  10%|█         | 5/50 [00:03<00:25,  1.78it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.2469. T_Loss: 2.0518. Mask: 0.9414. :  12%|█▏        | 6/50 [00:03<00:22,  1.95it/s]total : 1000  current step :  154
total : 1000  current step :  155
total : 1000  current step :  156
Train Iter: 157/1000. LR: 0.0000. Data: 0.42s. Batch: 0.63s. S_Loss: 2.2474. T_Loss: 2.0465. Mask: 0.9431. :  12%|█▏        | 6/50 [00:04<00:22,  1.95it/s]Train Iter: 157/1000. LR: 0.0000. Data: 0.42s. Batch: 0.63s. S_Loss: 2.2474. T_Loss: 2.0465. Mask: 0.9431. :  14%|█▍        | 7/50 [00:04<00:29,  1.47it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2492. T_Loss: 2.0424. Mask: 0.9419. :  14%|█▍        | 7/50 [00:04<00:29,  1.47it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.2492. T_Loss: 2.0424. Mask: 0.9419. :  16%|█▌        | 8/50 [00:04<00:24,  1.74it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2511. T_Loss: 2.0360. Mask: 0.9414. :  16%|█▌        | 8/50 [00:05<00:24,  1.74it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2511. T_Loss: 2.0360. Mask: 0.9414. :  18%|█▊        | 9/50 [00:05<00:21,  1.92it/s]total : 1000  current step :  157
total : 1000  current step :  158
total : 1000  current step :  159
Train Iter: 160/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.2517. T_Loss: 2.0257. Mask: 0.9414. :  18%|█▊        | 9/50 [00:06<00:21,  1.92it/s]Train Iter: 160/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.2517. T_Loss: 2.0257. Mask: 0.9414. :  20%|██        | 10/50 [00:06<00:32,  1.24it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.2501. T_Loss: 2.0264. Mask: 0.9428. :  20%|██        | 10/50 [00:07<00:32,  1.24it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.2501. T_Loss: 2.0264. Mask: 0.9428. :  22%|██▏       | 11/50 [00:07<00:28,  1.36it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.2501. T_Loss: 2.0204. Mask: 0.9427. :  22%|██▏       | 11/50 [00:07<00:28,  1.36it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.2501. T_Loss: 2.0204. Mask: 0.9427. :  24%|██▍       | 12/50 [00:07<00:22,  1.66it/s]total : 1000  current step :  160
total : 1000  current step :  161
total : 1000  current step :  162
Train Iter: 163/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.2512. T_Loss: 2.0216. Mask: 0.9441. :  24%|██▍       | 12/50 [00:08<00:22,  1.66it/s]Train Iter: 163/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.2512. T_Loss: 2.0216. Mask: 0.9441. :  26%|██▌       | 13/50 [00:08<00:27,  1.35it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.2530. T_Loss: 2.0298. Mask: 0.9464. :  26%|██▌       | 13/50 [00:08<00:27,  1.35it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.2530. T_Loss: 2.0298. Mask: 0.9464. :  28%|██▊       | 14/50 [00:08<00:21,  1.64it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2530. T_Loss: 2.0335. Mask: 0.9451. :  28%|██▊       | 14/50 [00:09<00:21,  1.64it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2530. T_Loss: 2.0335. Mask: 0.9451. :  30%|███       | 15/50 [00:09<00:17,  1.98it/s]total : 1000  current step :  163
total : 1000  current step :  164
total : 1000  current step :  165
Train Iter: 166/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.2532. T_Loss: 2.0210. Mask: 0.9443. :  30%|███       | 15/50 [00:10<00:17,  1.98it/s]Train Iter: 166/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.2532. T_Loss: 2.0210. Mask: 0.9443. :  32%|███▏      | 16/50 [00:10<00:21,  1.59it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2514. T_Loss: 2.0328. Mask: 0.9451. :  32%|███▏      | 16/50 [00:10<00:21,  1.59it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2514. T_Loss: 2.0328. Mask: 0.9451. :  34%|███▍      | 17/50 [00:10<00:16,  2.00it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.2513. T_Loss: 2.0418. Mask: 0.9460. :  34%|███▍      | 17/50 [00:10<00:16,  2.00it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.2513. T_Loss: 2.0418. Mask: 0.9460. :  36%|███▌      | 18/50 [00:10<00:15,  2.08it/s]total : 1000  current step :  166
total : 1000  current step :  167
total : 1000  current step :  168
Train Iter: 169/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.2523. T_Loss: 2.0337. Mask: 0.9459. :  36%|███▌      | 18/50 [00:11<00:15,  2.08it/s]Train Iter: 169/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.2523. T_Loss: 2.0337. Mask: 0.9459. :  38%|███▊      | 19/50 [00:11<00:19,  1.59it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2503. T_Loss: 2.0340. Mask: 0.9459. :  38%|███▊      | 19/50 [00:11<00:19,  1.59it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2503. T_Loss: 2.0340. Mask: 0.9459. :  40%|████      | 20/50 [00:11<00:15,  1.91it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2495. T_Loss: 2.0434. Mask: 0.9455. :  40%|████      | 20/50 [00:12<00:15,  1.91it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2495. T_Loss: 2.0434. Mask: 0.9455. :  42%|████▏     | 21/50 [00:12<00:13,  2.08it/s]total : 1000  current step :  169
total : 1000  current step :  170
total : 1000  current step :  171
Train Iter: 172/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.2515. T_Loss: 2.0608. Mask: 0.9451. :  42%|████▏     | 21/50 [00:13<00:13,  2.08it/s]Train Iter: 172/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.2515. T_Loss: 2.0608. Mask: 0.9451. :  44%|████▍     | 22/50 [00:13<00:18,  1.51it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.2520. T_Loss: 2.0630. Mask: 0.9458. :  44%|████▍     | 22/50 [00:13<00:18,  1.51it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.2520. T_Loss: 2.0630. Mask: 0.9458. :  46%|████▌     | 23/50 [00:13<00:15,  1.75it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2519. T_Loss: 2.0692. Mask: 0.9458. :  46%|████▌     | 23/50 [00:13<00:15,  1.75it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2519. T_Loss: 2.0692. Mask: 0.9458. :  48%|████▊     | 24/50 [00:13<00:12,  2.08it/s]total : 1000  current step :  172
total : 1000  current step :  173
total : 1000  current step :  174
Train Iter: 175/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2529. T_Loss: 2.0689. Mask: 0.9455. :  48%|████▊     | 24/50 [00:14<00:12,  2.08it/s]Train Iter: 175/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.2529. T_Loss: 2.0689. Mask: 0.9455. :  50%|█████     | 25/50 [00:14<00:15,  1.58it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2533. T_Loss: 2.0679. Mask: 0.9450. :  50%|█████     | 25/50 [00:15<00:15,  1.58it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2533. T_Loss: 2.0679. Mask: 0.9450. :  52%|█████▏    | 26/50 [00:15<00:12,  1.91it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2529. T_Loss: 2.0692. Mask: 0.9452. :  52%|█████▏    | 26/50 [00:15<00:12,  1.91it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2529. T_Loss: 2.0692. Mask: 0.9452. :  54%|█████▍    | 27/50 [00:15<00:10,  2.19it/s]total : 1000  current step :  175
total : 1000  current step :  176
total : 1000  current step :  177
Train Iter: 178/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2522. T_Loss: 2.0700. Mask: 0.9442. :  54%|█████▍    | 27/50 [00:16<00:10,  2.19it/s]Train Iter: 178/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2522. T_Loss: 2.0700. Mask: 0.9442. :  56%|█████▌    | 28/50 [00:16<00:13,  1.59it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2516. T_Loss: 2.0767. Mask: 0.9445. :  56%|█████▌    | 28/50 [00:16<00:13,  1.59it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.2516. T_Loss: 2.0767. Mask: 0.9445. :  58%|█████▊    | 29/50 [00:16<00:10,  1.91it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2511. T_Loss: 2.0813. Mask: 0.9445. :  58%|█████▊    | 29/50 [00:17<00:10,  1.91it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2511. T_Loss: 2.0813. Mask: 0.9445. :  60%|██████    | 30/50 [00:17<00:09,  2.01it/s]total : 1000  current step :  178
total : 1000  current step :  179
total : 1000  current step :  180
Train Iter: 181/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2518. T_Loss: 2.0860. Mask: 0.9444. :  60%|██████    | 30/50 [00:18<00:09,  2.01it/s]Train Iter: 181/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2518. T_Loss: 2.0860. Mask: 0.9444. :  62%|██████▏   | 31/50 [00:18<00:12,  1.53it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2507. T_Loss: 2.0910. Mask: 0.9448. :  62%|██████▏   | 31/50 [00:18<00:12,  1.53it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2507. T_Loss: 2.0910. Mask: 0.9448. :  64%|██████▍   | 32/50 [00:18<00:10,  1.72it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2501. T_Loss: 2.0965. Mask: 0.9452. :  64%|██████▍   | 32/50 [00:18<00:10,  1.72it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2501. T_Loss: 2.0965. Mask: 0.9452. :  66%|██████▌   | 33/50 [00:18<00:08,  2.08it/s]total : 1000  current step :  181
total : 1000  current step :  182
total : 1000  current step :  183
Train Iter: 184/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2510. T_Loss: 2.0985. Mask: 0.9455. :  66%|██████▌   | 33/50 [00:20<00:08,  2.08it/s]Train Iter: 184/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.2510. T_Loss: 2.0985. Mask: 0.9455. :  68%|██████▊   | 34/50 [00:20<00:10,  1.53it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2522. T_Loss: 2.0991. Mask: 0.9456. :  68%|██████▊   | 34/50 [00:20<00:10,  1.53it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2522. T_Loss: 2.0991. Mask: 0.9456. :  70%|███████   | 35/50 [00:20<00:08,  1.82it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2517. T_Loss: 2.1011. Mask: 0.9456. :  70%|███████   | 35/50 [00:20<00:08,  1.82it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2517. T_Loss: 2.1011. Mask: 0.9456. :  72%|███████▏  | 36/50 [00:20<00:06,  2.22it/s]total : 1000  current step :  184
total : 1000  current step :  185
total : 1000  current step :  186
Train Iter: 187/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2519. T_Loss: 2.1040. Mask: 0.9456. :  72%|███████▏  | 36/50 [00:21<00:06,  2.22it/s]Train Iter: 187/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2519. T_Loss: 2.1040. Mask: 0.9456. :  74%|███████▍  | 37/50 [00:21<00:08,  1.61it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2515. T_Loss: 2.1045. Mask: 0.9458. :  74%|███████▍  | 37/50 [00:21<00:08,  1.61it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2515. T_Loss: 2.1045. Mask: 0.9458. :  76%|███████▌  | 38/50 [00:21<00:06,  1.88it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2504. T_Loss: 2.1027. Mask: 0.9456. :  76%|███████▌  | 38/50 [00:22<00:06,  1.88it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2504. T_Loss: 2.1027. Mask: 0.9456. :  78%|███████▊  | 39/50 [00:22<00:05,  2.03it/s]total : 1000  current step :  187
total : 1000  current step :  188
total : 1000  current step :  189
Train Iter: 190/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2503. T_Loss: 2.1021. Mask: 0.9460. :  78%|███████▊  | 39/50 [00:23<00:05,  2.03it/s]Train Iter: 190/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2503. T_Loss: 2.1021. Mask: 0.9460. :  80%|████████  | 40/50 [00:23<00:06,  1.53it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2505. T_Loss: 2.1047. Mask: 0.9462. :  80%|████████  | 40/50 [00:23<00:06,  1.53it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2505. T_Loss: 2.1047. Mask: 0.9462. :  82%|████████▏ | 41/50 [00:23<00:05,  1.74it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.2496. T_Loss: 2.1057. Mask: 0.9465. :  82%|████████▏ | 41/50 [00:24<00:05,  1.74it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.2496. T_Loss: 2.1057. Mask: 0.9465. :  84%|████████▍ | 42/50 [00:24<00:03,  2.05it/s]total : 1000  current step :  190
total : 1000  current step :  191
total : 1000  current step :  192
Train Iter: 193/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2492. T_Loss: 2.1056. Mask: 0.9459. :  84%|████████▍ | 42/50 [00:25<00:03,  2.05it/s]Train Iter: 193/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2492. T_Loss: 2.1056. Mask: 0.9459. :  86%|████████▌ | 43/50 [00:25<00:04,  1.49it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2493. T_Loss: 2.1076. Mask: 0.9460. :  86%|████████▌ | 43/50 [00:25<00:04,  1.49it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2493. T_Loss: 2.1076. Mask: 0.9460. :  88%|████████▊ | 44/50 [00:25<00:03,  1.79it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.2494. T_Loss: 2.1104. Mask: 0.9463. :  88%|████████▊ | 44/50 [00:25<00:03,  1.79it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.2494. T_Loss: 2.1104. Mask: 0.9463. :  90%|█████████ | 45/50 [00:25<00:02,  2.07it/s]total : 1000  current step :  193
total : 1000  current step :  194
total : 1000  current step :  195
Train Iter: 196/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2497. T_Loss: 2.1156. Mask: 0.9463. :  90%|█████████ | 45/50 [00:26<00:02,  2.07it/s]Train Iter: 196/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2497. T_Loss: 2.1156. Mask: 0.9463. :  92%|█████████▏| 46/50 [00:26<00:02,  1.55it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2493. T_Loss: 2.1191. Mask: 0.9468. :  92%|█████████▏| 46/50 [00:27<00:02,  1.55it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.2493. T_Loss: 2.1191. Mask: 0.9468. :  94%|█████████▍| 47/50 [00:27<00:01,  1.86it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.2491. T_Loss: 2.1248. Mask: 0.9463. :  94%|█████████▍| 47/50 [00:27<00:01,  1.86it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.2491. T_Loss: 2.1248. Mask: 0.9463. :  96%|█████████▌| 48/50 [00:27<00:00,  2.05it/s]total : 1000  current step :  196
total : 1000  current step :  197
total : 1000  current step :  198
Train Iter: 199/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2485. T_Loss: 2.1288. Mask: 0.9460. :  96%|█████████▌| 48/50 [00:28<00:00,  2.05it/s]Train Iter: 199/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.2485. T_Loss: 2.1288. Mask: 0.9460. :  98%|█████████▊| 49/50 [00:28<00:00,  1.61it/s]total : 1000  current step :  199
Train Iter: 200/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2482. T_Loss: 2.1305. Mask: 0.9459. :  98%|█████████▊| 49/50 [00:29<00:00,  1.61it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2482. T_Loss: 2.1305. Mask: 0.9459. : 100%|██████████| 50/50 [00:29<00:00,  1.49it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.2482. T_Loss: 2.1305. Mask: 0.9459. : 100%|██████████| 50/50 [00:29<00:00,  1.72it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.9603. top1: 0.78. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.9603. top1: 0.78. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.9597. top1: 1.76. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.9597. top1: 1.76. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.9609. top1: 1.69. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.39it/s] Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.9609. top1: 1.69. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.97it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.9812. top1: 1.56. top5: 97.75. :  38%|███▊      | 3/8 [00:01<00:01,  2.97it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.9812. top1: 1.56. top5: 97.75. :  50%|█████     | 4/8 [00:01<00:01,  3.42it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.1156. top1: 1.33. top5: 81.72. :  50%|█████     | 4/8 [00:01<00:01,  3.42it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.1156. top1: 1.33. top5: 81.72. :  62%|██████▎   | 5/8 [00:01<00:00,  3.62it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.2084. top1: 1.11. top5: 70.57. :  62%|██████▎   | 5/8 [00:01<00:00,  3.62it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.2084. top1: 1.11. top5: 70.57. :  75%|███████▌  | 6/8 [00:01<00:00,  3.74it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.2707. top1: 0.95. top5: 63.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.74it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.2707. top1: 0.95. top5: 63.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.3192. top1: 0.85. top5: 57.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.3192. top1: 0.85. top5: 57.65. : 100%|██████████| 8/8 [00:02<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.3192. top1: 0.85. top5: 57.65. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
total : 1000  current step :  200
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.01s. Batch: 0.16s. S_Loss: 2.2365. T_Loss: 2.2341. Mask: 0.9648. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.01s. Batch: 0.16s. S_Loss: 2.2365. T_Loss: 2.2341. Mask: 0.9648. :   2%|▏         | 1/50 [00:00<00:07,  6.35it/s]total : 1000  current step :  201
Train Iter: 202/1000. LR: 0.0000. Data: 0.42s. Batch: 0.59s. S_Loss: 2.2480. T_Loss: 2.2971. Mask: 0.9629. :   2%|▏         | 1/50 [00:01<00:07,  6.35it/s]Train Iter: 202/1000. LR: 0.0000. Data: 0.42s. Batch: 0.59s. S_Loss: 2.2480. T_Loss: 2.2971. Mask: 0.9629. :   4%|▍         | 2/50 [00:01<00:31,  1.51it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.31s. Batch: 0.48s. S_Loss: 2.2349. T_Loss: 2.3241. Mask: 0.9583. :   4%|▍         | 2/50 [00:01<00:31,  1.51it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.31s. Batch: 0.48s. S_Loss: 2.2349. T_Loss: 2.3241. Mask: 0.9583. :   6%|▌         | 3/50 [00:01<00:22,  2.06it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.27s. Batch: 0.43s. S_Loss: 2.2458. T_Loss: 2.3156. Mask: 0.9551. :   6%|▌         | 3/50 [00:01<00:22,  2.06it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.27s. Batch: 0.43s. S_Loss: 2.2458. T_Loss: 2.3156. Mask: 0.9551. :   8%|▊         | 4/50 [00:01<00:18,  2.45it/s]total : 1000  current step :  202
total : 1000  current step :  203
total : 1000  current step :  204
Train Iter: 205/1000. LR: 0.0000. Data: 0.37s. Batch: 0.55s. S_Loss: 2.2482. T_Loss: 2.2992. Mask: 0.9555. :   8%|▊         | 4/50 [00:02<00:18,  2.45it/s]Train Iter: 205/1000. LR: 0.0000. Data: 0.37s. Batch: 0.55s. S_Loss: 2.2482. T_Loss: 2.2992. Mask: 0.9555. :  10%|█         | 5/50 [00:02<00:28,  1.60it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.31s. Batch: 0.50s. S_Loss: 2.2514. T_Loss: 2.2726. Mask: 0.9570. :  10%|█         | 5/50 [00:03<00:28,  1.60it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.31s. Batch: 0.50s. S_Loss: 2.2514. T_Loss: 2.2726. Mask: 0.9570. :  12%|█▏        | 6/50 [00:03<00:22,  1.98it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.28s. Batch: 0.47s. S_Loss: 2.2545. T_Loss: 2.2644. Mask: 0.9576. :  12%|█▏        | 6/50 [00:03<00:22,  1.98it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.28s. Batch: 0.47s. S_Loss: 2.2545. T_Loss: 2.2644. Mask: 0.9576. :  14%|█▍        | 7/50 [00:03<00:18,  2.33it/s]total : 1000  current step :  205
total : 1000  current step :  206
total : 1000  current step :  207
Train Iter: 208/1000. LR: 0.0000. Data: 0.34s. Batch: 0.52s. S_Loss: 2.2580. T_Loss: 2.2672. Mask: 0.9575. :  14%|█▍        | 7/50 [00:04<00:18,  2.33it/s]Train Iter: 208/1000. LR: 0.0000. Data: 0.34s. Batch: 0.52s. S_Loss: 2.2580. T_Loss: 2.2672. Mask: 0.9575. :  16%|█▌        | 8/50 [00:04<00:24,  1.71it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.32s. Batch: 0.51s. S_Loss: 2.2571. T_Loss: 2.2543. Mask: 0.9579. :  16%|█▌        | 8/50 [00:04<00:24,  1.71it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.32s. Batch: 0.51s. S_Loss: 2.2571. T_Loss: 2.2543. Mask: 0.9579. :  18%|█▊        | 9/50 [00:04<00:21,  1.87it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.29s. Batch: 0.50s. S_Loss: 2.2518. T_Loss: 2.2632. Mask: 0.9578. :  18%|█▊        | 9/50 [00:04<00:21,  1.87it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.29s. Batch: 0.50s. S_Loss: 2.2518. T_Loss: 2.2632. Mask: 0.9578. :  20%|██        | 10/50 [00:04<00:19,  2.09it/s]total : 1000  current step :  208
total : 1000  current step :  209
total : 1000  current step :  210
Train Iter: 211/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.2501. T_Loss: 2.2905. Mask: 0.9577. :  20%|██        | 10/50 [00:06<00:19,  2.09it/s]Train Iter: 211/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.2501. T_Loss: 2.2905. Mask: 0.9577. :  22%|██▏       | 11/50 [00:06<00:26,  1.48it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2480. T_Loss: 2.2855. Mask: 0.9580. :  22%|██▏       | 11/50 [00:06<00:26,  1.48it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2480. T_Loss: 2.2855. Mask: 0.9580. :  24%|██▍       | 12/50 [00:06<00:21,  1.78it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.2454. T_Loss: 2.2800. Mask: 0.9579. :  24%|██▍       | 12/50 [00:06<00:21,  1.78it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.2454. T_Loss: 2.2800. Mask: 0.9579. :  26%|██▌       | 13/50 [00:06<00:17,  2.12it/s]total : 1000  current step :  211
total : 1000  current step :  212
total : 1000  current step :  213
Train Iter: 214/1000. LR: 0.0000. Data: 0.34s. Batch: 0.56s. S_Loss: 2.2453. T_Loss: 2.2801. Mask: 0.9565. :  26%|██▌       | 13/50 [00:07<00:17,  2.12it/s]Train Iter: 214/1000. LR: 0.0000. Data: 0.34s. Batch: 0.56s. S_Loss: 2.2453. T_Loss: 2.2801. Mask: 0.9565. :  28%|██▊       | 14/50 [00:07<00:24,  1.50it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2457. T_Loss: 2.2882. Mask: 0.9581. :  28%|██▊       | 14/50 [00:08<00:24,  1.50it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2457. T_Loss: 2.2882. Mask: 0.9581. :  30%|███       | 15/50 [00:08<00:18,  1.88it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2460. T_Loss: 2.2848. Mask: 0.9570. :  30%|███       | 15/50 [00:08<00:18,  1.88it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2460. T_Loss: 2.2848. Mask: 0.9570. :  32%|███▏      | 16/50 [00:08<00:15,  2.23it/s]total : 1000  current step :  214
total : 1000  current step :  215
total : 1000  current step :  216
Train Iter: 217/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2462. T_Loss: 2.2902. Mask: 0.9580. :  32%|███▏      | 16/50 [00:09<00:15,  2.23it/s]Train Iter: 217/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2462. T_Loss: 2.2902. Mask: 0.9580. :  34%|███▍      | 17/50 [00:09<00:21,  1.54it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2464. T_Loss: 2.2914. Mask: 0.9562. :  34%|███▍      | 17/50 [00:09<00:21,  1.54it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2464. T_Loss: 2.2914. Mask: 0.9562. :  36%|███▌      | 18/50 [00:09<00:17,  1.88it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2458. T_Loss: 2.3040. Mask: 0.9556. :  36%|███▌      | 18/50 [00:09<00:17,  1.88it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2458. T_Loss: 2.3040. Mask: 0.9556. :  38%|███▊      | 19/50 [00:09<00:13,  2.24it/s]total : 1000  current step :  217
total : 1000  current step :  218
total : 1000  current step :  219
Train Iter: 220/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2454. T_Loss: 2.3079. Mask: 0.9555. :  38%|███▊      | 19/50 [00:11<00:13,  2.24it/s]Train Iter: 220/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2454. T_Loss: 2.3079. Mask: 0.9555. :  40%|████      | 20/50 [00:11<00:19,  1.54it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2432. T_Loss: 2.3150. Mask: 0.9559. :  40%|████      | 20/50 [00:11<00:19,  1.54it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2432. T_Loss: 2.3150. Mask: 0.9559. :  42%|████▏     | 21/50 [00:11<00:14,  1.96it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2443. T_Loss: 2.3289. Mask: 0.9556. :  42%|████▏     | 21/50 [00:11<00:14,  1.96it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2443. T_Loss: 2.3289. Mask: 0.9556. :  44%|████▍     | 22/50 [00:11<00:12,  2.28it/s]total : 1000  current step :  220
total : 1000  current step :  221
total : 1000  current step :  222
Train Iter: 223/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2431. T_Loss: 2.3336. Mask: 0.9557. :  44%|████▍     | 22/50 [00:12<00:12,  2.28it/s]Train Iter: 223/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2431. T_Loss: 2.3336. Mask: 0.9557. :  46%|████▌     | 23/50 [00:12<00:16,  1.62it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2437. T_Loss: 2.3315. Mask: 0.9562. :  46%|████▌     | 23/50 [00:12<00:16,  1.62it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2437. T_Loss: 2.3315. Mask: 0.9562. :  48%|████▊     | 24/50 [00:12<00:13,  1.94it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2442. T_Loss: 2.3364. Mask: 0.9561. :  48%|████▊     | 24/50 [00:13<00:13,  1.94it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2442. T_Loss: 2.3364. Mask: 0.9561. :  50%|█████     | 25/50 [00:13<00:11,  2.21it/s]total : 1000  current step :  223
total : 1000  current step :  224
total : 1000  current step :  225
Train Iter: 226/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2448. T_Loss: 2.3504. Mask: 0.9557. :  50%|█████     | 25/50 [00:14<00:11,  2.21it/s]Train Iter: 226/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2448. T_Loss: 2.3504. Mask: 0.9557. :  52%|█████▏    | 26/50 [00:14<00:15,  1.59it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2447. T_Loss: 2.3587. Mask: 0.9565. :  52%|█████▏    | 26/50 [00:14<00:15,  1.59it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2447. T_Loss: 2.3587. Mask: 0.9565. :  54%|█████▍    | 27/50 [00:14<00:12,  1.91it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2443. T_Loss: 2.3649. Mask: 0.9563. :  54%|█████▍    | 27/50 [00:14<00:12,  1.91it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2443. T_Loss: 2.3649. Mask: 0.9563. :  56%|█████▌    | 28/50 [00:14<00:10,  2.06it/s]total : 1000  current step :  226
total : 1000  current step :  227
total : 1000  current step :  228
Train Iter: 229/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2436. T_Loss: 2.3738. Mask: 0.9560. :  56%|█████▌    | 28/50 [00:15<00:10,  2.06it/s]Train Iter: 229/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2436. T_Loss: 2.3738. Mask: 0.9560. :  58%|█████▊    | 29/50 [00:15<00:14,  1.50it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2445. T_Loss: 2.3752. Mask: 0.9564. :  58%|█████▊    | 29/50 [00:16<00:14,  1.50it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2445. T_Loss: 2.3752. Mask: 0.9564. :  60%|██████    | 30/50 [00:16<00:11,  1.78it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2444. T_Loss: 2.3721. Mask: 0.9564. :  60%|██████    | 30/50 [00:16<00:11,  1.78it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2444. T_Loss: 2.3721. Mask: 0.9564. :  62%|██████▏   | 31/50 [00:16<00:09,  1.98it/s]total : 1000  current step :  229
total : 1000  current step :  230
total : 1000  current step :  231
Train Iter: 232/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2446. T_Loss: 2.3710. Mask: 0.9572. :  62%|██████▏   | 31/50 [00:17<00:09,  1.98it/s]Train Iter: 232/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2446. T_Loss: 2.3710. Mask: 0.9572. :  64%|██████▍   | 32/50 [00:17<00:11,  1.60it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2448. T_Loss: 2.3759. Mask: 0.9573. :  64%|██████▍   | 32/50 [00:17<00:11,  1.60it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2448. T_Loss: 2.3759. Mask: 0.9573. :  66%|██████▌   | 33/50 [00:17<00:09,  1.85it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2445. T_Loss: 2.3772. Mask: 0.9563. :  66%|██████▌   | 33/50 [00:18<00:09,  1.85it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2445. T_Loss: 2.3772. Mask: 0.9563. :  68%|██████▊   | 34/50 [00:18<00:07,  2.18it/s]total : 1000  current step :  232
total : 1000  current step :  233
total : 1000  current step :  234
Train Iter: 235/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.2448. T_Loss: 2.3829. Mask: 0.9564. :  68%|██████▊   | 34/50 [00:19<00:07,  2.18it/s]Train Iter: 235/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.2448. T_Loss: 2.3829. Mask: 0.9564. :  70%|███████   | 35/50 [00:19<00:09,  1.56it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2443. T_Loss: 2.3855. Mask: 0.9564. :  70%|███████   | 35/50 [00:19<00:09,  1.56it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2443. T_Loss: 2.3855. Mask: 0.9564. :  72%|███████▏  | 36/50 [00:19<00:07,  1.86it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2441. T_Loss: 2.3920. Mask: 0.9563. :  72%|███████▏  | 36/50 [00:19<00:07,  1.86it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2441. T_Loss: 2.3920. Mask: 0.9563. :  74%|███████▍  | 37/50 [00:19<00:05,  2.24it/s]total : 1000  current step :  235
total : 1000  current step :  236
total : 1000  current step :  237
Train Iter: 238/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2443. T_Loss: 2.3969. Mask: 0.9566. :  74%|███████▍  | 37/50 [00:20<00:05,  2.24it/s]Train Iter: 238/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2443. T_Loss: 2.3969. Mask: 0.9566. :  76%|███████▌  | 38/50 [00:20<00:07,  1.58it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2441. T_Loss: 2.3977. Mask: 0.9568. :  76%|███████▌  | 38/50 [00:20<00:07,  1.58it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2441. T_Loss: 2.3977. Mask: 0.9568. :  78%|███████▊  | 39/50 [00:20<00:05,  1.94it/s]total : 1000  current step :  238
total : 1000  current step :  239
Train Iter: 240/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2436. T_Loss: 2.3961. Mask: 0.9565. :  78%|███████▊  | 39/50 [00:21<00:05,  1.94it/s]Train Iter: 240/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2436. T_Loss: 2.3961. Mask: 0.9565. :  80%|████████  | 40/50 [00:21<00:05,  1.97it/s]total : 1000  current step :  240
Train Iter: 241/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2441. T_Loss: 2.3946. Mask: 0.9563. :  80%|████████  | 40/50 [00:22<00:05,  1.97it/s]Train Iter: 241/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2441. T_Loss: 2.3946. Mask: 0.9563. :  82%|████████▏ | 41/50 [00:22<00:06,  1.49it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2435. T_Loss: 2.3966. Mask: 0.9563. :  82%|████████▏ | 41/50 [00:22<00:06,  1.49it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2435. T_Loss: 2.3966. Mask: 0.9563. :  84%|████████▍ | 42/50 [00:22<00:04,  1.82it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2434. T_Loss: 2.4024. Mask: 0.9565. :  84%|████████▍ | 42/50 [00:23<00:04,  1.82it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2434. T_Loss: 2.4024. Mask: 0.9565. :  86%|████████▌ | 43/50 [00:23<00:03,  2.03it/s]total : 1000  current step :  241
total : 1000  current step :  242
total : 1000  current step :  243
Train Iter: 244/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2429. T_Loss: 2.4042. Mask: 0.9566. :  86%|████████▌ | 43/50 [00:24<00:03,  2.03it/s]Train Iter: 244/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2429. T_Loss: 2.4042. Mask: 0.9566. :  88%|████████▊ | 44/50 [00:24<00:03,  1.62it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2434. T_Loss: 2.4082. Mask: 0.9570. :  88%|████████▊ | 44/50 [00:24<00:03,  1.62it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2434. T_Loss: 2.4082. Mask: 0.9570. :  90%|█████████ | 45/50 [00:24<00:02,  1.89it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2432. T_Loss: 2.4068. Mask: 0.9570. :  90%|█████████ | 45/50 [00:24<00:02,  1.89it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2432. T_Loss: 2.4068. Mask: 0.9570. :  92%|█████████▏| 46/50 [00:24<00:01,  2.23it/s]total : 1000  current step :  244
total : 1000  current step :  245
total : 1000  current step :  246
Train Iter: 247/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.2430. T_Loss: 2.4057. Mask: 0.9574. :  92%|█████████▏| 46/50 [00:25<00:01,  2.23it/s]Train Iter: 247/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.2430. T_Loss: 2.4057. Mask: 0.9574. :  94%|█████████▍| 47/50 [00:25<00:01,  1.60it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2435. T_Loss: 2.4073. Mask: 0.9574. :  94%|█████████▍| 47/50 [00:25<00:01,  1.60it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2435. T_Loss: 2.4073. Mask: 0.9574. :  96%|█████████▌| 48/50 [00:25<00:01,  1.89it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2443. T_Loss: 2.4106. Mask: 0.9573. :  96%|█████████▌| 48/50 [00:26<00:01,  1.89it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2443. T_Loss: 2.4106. Mask: 0.9573. :  98%|█████████▊| 49/50 [00:26<00:00,  2.31it/s]total : 1000  current step :  247
total : 1000  current step :  248
total : 1000  current step :  249
Train Iter: 250/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2439. T_Loss: 2.4164. Mask: 0.9576. :  98%|█████████▊| 49/50 [00:27<00:00,  2.31it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2439. T_Loss: 2.4164. Mask: 0.9576. : 100%|██████████| 50/50 [00:27<00:00,  1.64it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2439. T_Loss: 2.4164. Mask: 0.9576. : 100%|██████████| 50/50 [00:27<00:00,  1.84it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 2.0324. top1: 1.95. top5: 99.22. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 2.0324. top1: 1.95. top5: 99.22. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 2.0340. top1: 2.54. top5: 99.02. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 2.0340. top1: 2.54. top5: 99.02. :  25%|██▌       | 2/8 [00:00<00:02,  2.53it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.0336. top1: 2.60. top5: 98.57. :  25%|██▌       | 2/8 [00:01<00:02,  2.53it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.0336. top1: 2.60. top5: 98.57. :  38%|███▊      | 3/8 [00:01<00:01,  2.92it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.0477. top1: 2.25. top5: 96.88. :  38%|███▊      | 3/8 [00:01<00:01,  2.92it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.0477. top1: 2.25. top5: 96.88. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.1315. top1: 1.88. top5: 84.61. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.1315. top1: 1.88. top5: 84.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.1897. top1: 1.56. top5: 75.98. :  62%|██████▎   | 5/8 [00:01<00:00,  3.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.1897. top1: 1.56. top5: 75.98. :  75%|███████▌  | 6/8 [00:01<00:00,  3.69it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.2281. top1: 1.45. top5: 70.37. :  75%|███████▌  | 6/8 [00:02<00:00,  3.69it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.2281. top1: 1.45. top5: 70.37. :  88%|████████▊ | 7/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.2592. top1: 1.30. top5: 65.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.2592. top1: 1.30. top5: 65.95. : 100%|██████████| 8/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.2592. top1: 1.30. top5: 65.95. : 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]
total : 1000  current step :  250
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.32s. S_Loss: 2.2324. T_Loss: 2.7520. Mask: 0.9766. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.32s. S_Loss: 2.2324. T_Loss: 2.7520. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:15,  3.11it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.28s. S_Loss: 2.2187. T_Loss: 2.6499. Mask: 0.9609. :   2%|▏         | 1/50 [00:00<00:15,  3.11it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.28s. S_Loss: 2.2187. T_Loss: 2.6499. Mask: 0.9609. :   4%|▍         | 2/50 [00:00<00:13,  3.66it/s]total : 1000  current step :  251
total : 1000  current step :  252
Train Iter: 253/1000. LR: 0.0000. Data: 0.29s. Batch: 0.53s. S_Loss: 2.2178. T_Loss: 2.6341. Mask: 0.9609. :   4%|▍         | 2/50 [00:01<00:13,  3.66it/s]Train Iter: 253/1000. LR: 0.0000. Data: 0.29s. Batch: 0.53s. S_Loss: 2.2178. T_Loss: 2.6341. Mask: 0.9609. :   6%|▌         | 3/50 [00:01<00:28,  1.62it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.25s. Batch: 0.49s. S_Loss: 2.2203. T_Loss: 2.5795. Mask: 0.9609. :   6%|▌         | 3/50 [00:01<00:28,  1.62it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.25s. Batch: 0.49s. S_Loss: 2.2203. T_Loss: 2.5795. Mask: 0.9609. :   8%|▊         | 4/50 [00:01<00:24,  1.89it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.21s. Batch: 0.44s. S_Loss: 2.2238. T_Loss: 2.5848. Mask: 0.9602. :   8%|▊         | 4/50 [00:02<00:24,  1.89it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.21s. Batch: 0.44s. S_Loss: 2.2238. T_Loss: 2.5848. Mask: 0.9602. :  10%|█         | 5/50 [00:02<00:18,  2.37it/s]total : 1000  current step :  253
total : 1000  current step :  254
total : 1000  current step :  255
Train Iter: 256/1000. LR: 0.0000. Data: 0.34s. Batch: 0.56s. S_Loss: 2.2223. T_Loss: 2.5885. Mask: 0.9596. :  10%|█         | 5/50 [00:03<00:18,  2.37it/s]Train Iter: 256/1000. LR: 0.0000. Data: 0.34s. Batch: 0.56s. S_Loss: 2.2223. T_Loss: 2.5885. Mask: 0.9596. :  12%|█▏        | 6/50 [00:03<00:29,  1.50it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2205. T_Loss: 2.5836. Mask: 0.9632. :  12%|█▏        | 6/50 [00:03<00:29,  1.50it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2205. T_Loss: 2.5836. Mask: 0.9632. :  14%|█▍        | 7/50 [00:03<00:24,  1.74it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.29s. Batch: 0.51s. S_Loss: 2.2243. T_Loss: 2.5494. Mask: 0.9585. :  14%|█▍        | 7/50 [00:04<00:24,  1.74it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.29s. Batch: 0.51s. S_Loss: 2.2243. T_Loss: 2.5494. Mask: 0.9585. :  16%|█▌        | 8/50 [00:04<00:21,  1.99it/s]total : 1000  current step :  256
total : 1000  current step :  257
total : 1000  current step :  258
Train Iter: 259/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.2260. T_Loss: 2.5434. Mask: 0.9601. :  16%|█▌        | 8/50 [00:05<00:21,  1.99it/s]Train Iter: 259/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.2260. T_Loss: 2.5434. Mask: 0.9601. :  18%|█▊        | 9/50 [00:05<00:26,  1.53it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2267. T_Loss: 2.5546. Mask: 0.9586. :  18%|█▊        | 9/50 [00:05<00:26,  1.53it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2267. T_Loss: 2.5546. Mask: 0.9586. :  20%|██        | 10/50 [00:05<00:22,  1.77it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2297. T_Loss: 2.5472. Mask: 0.9577. :  20%|██        | 10/50 [00:05<00:22,  1.77it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.31s. Batch: 0.52s. S_Loss: 2.2297. T_Loss: 2.5472. Mask: 0.9577. :  22%|██▏       | 11/50 [00:05<00:19,  2.02it/s]total : 1000  current step :  259
total : 1000  current step :  260
total : 1000  current step :  261
Train Iter: 262/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2305. T_Loss: 2.5585. Mask: 0.9593. :  22%|██▏       | 11/50 [00:06<00:19,  2.02it/s]Train Iter: 262/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.2305. T_Loss: 2.5585. Mask: 0.9593. :  24%|██▍       | 12/50 [00:06<00:25,  1.51it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2291. T_Loss: 2.5744. Mask: 0.9597. :  24%|██▍       | 12/50 [00:07<00:25,  1.51it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.2291. T_Loss: 2.5744. Mask: 0.9597. :  26%|██▌       | 13/50 [00:07<00:20,  1.77it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2295. T_Loss: 2.5836. Mask: 0.9607. :  26%|██▌       | 13/50 [00:07<00:20,  1.77it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.32s. Batch: 0.53s. S_Loss: 2.2295. T_Loss: 2.5836. Mask: 0.9607. :  28%|██▊       | 14/50 [00:07<00:16,  2.12it/s]total : 1000  current step :  262
total : 1000  current step :  263
total : 1000  current step :  264
Train Iter: 265/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.2304. T_Loss: 2.5932. Mask: 0.9602. :  28%|██▊       | 14/50 [00:08<00:16,  2.12it/s]Train Iter: 265/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.2304. T_Loss: 2.5932. Mask: 0.9602. :  30%|███       | 15/50 [00:08<00:22,  1.58it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2323. T_Loss: 2.6001. Mask: 0.9592. :  30%|███       | 15/50 [00:08<00:22,  1.58it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2323. T_Loss: 2.6001. Mask: 0.9592. :  32%|███▏      | 16/50 [00:08<00:17,  1.90it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2302. T_Loss: 2.6033. Mask: 0.9584. :  32%|███▏      | 16/50 [00:08<00:17,  1.90it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2302. T_Loss: 2.6033. Mask: 0.9584. :  34%|███▍      | 17/50 [00:08<00:15,  2.19it/s]total : 1000  current step :  265
total : 1000  current step :  266
total : 1000  current step :  267
Train Iter: 268/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.2309. T_Loss: 2.5976. Mask: 0.9586. :  34%|███▍      | 17/50 [00:09<00:15,  2.19it/s]Train Iter: 268/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.2309. T_Loss: 2.5976. Mask: 0.9586. :  36%|███▌      | 18/50 [00:09<00:19,  1.68it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2312. T_Loss: 2.6032. Mask: 0.9591. :  36%|███▌      | 18/50 [00:10<00:19,  1.68it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.2312. T_Loss: 2.6032. Mask: 0.9591. :  38%|███▊      | 19/50 [00:10<00:17,  1.81it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2312. T_Loss: 2.6022. Mask: 0.9588. :  38%|███▊      | 19/50 [00:10<00:17,  1.81it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2312. T_Loss: 2.6022. Mask: 0.9588. :  40%|████      | 20/50 [00:10<00:14,  2.03it/s]total : 1000  current step :  268
total : 1000  current step :  269
total : 1000  current step :  270
Train Iter: 271/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.2309. T_Loss: 2.6121. Mask: 0.9585. :  40%|████      | 20/50 [00:11<00:14,  2.03it/s]Train Iter: 271/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.2309. T_Loss: 2.6121. Mask: 0.9585. :  42%|████▏     | 21/50 [00:11<00:17,  1.65it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2306. T_Loss: 2.6144. Mask: 0.9586. :  42%|████▏     | 21/50 [00:11<00:17,  1.65it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2306. T_Loss: 2.6144. Mask: 0.9586. :  44%|████▍     | 22/50 [00:11<00:14,  1.93it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2319. T_Loss: 2.6176. Mask: 0.9596. :  44%|████▍     | 22/50 [00:12<00:14,  1.93it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2319. T_Loss: 2.6176. Mask: 0.9596. :  46%|████▌     | 23/50 [00:12<00:12,  2.20it/s]total : 1000  current step :  271
total : 1000  current step :  272
total : 1000  current step :  273
Train Iter: 274/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2336. T_Loss: 2.6341. Mask: 0.9604. :  46%|████▌     | 23/50 [00:13<00:12,  2.20it/s]Train Iter: 274/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2336. T_Loss: 2.6341. Mask: 0.9604. :  48%|████▊     | 24/50 [00:13<00:16,  1.60it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2338. T_Loss: 2.6473. Mask: 0.9597. :  48%|████▊     | 24/50 [00:13<00:16,  1.60it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2338. T_Loss: 2.6473. Mask: 0.9597. :  50%|█████     | 25/50 [00:13<00:13,  1.88it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2352. T_Loss: 2.6586. Mask: 0.9593. :  50%|█████     | 25/50 [00:13<00:13,  1.88it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2352. T_Loss: 2.6586. Mask: 0.9593. :  52%|█████▏    | 26/50 [00:13<00:10,  2.21it/s]total : 1000  current step :  274
total : 1000  current step :  275
total : 1000  current step :  276
Train Iter: 277/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2347. T_Loss: 2.6719. Mask: 0.9588. :  52%|█████▏    | 26/50 [00:14<00:10,  2.21it/s]Train Iter: 277/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2347. T_Loss: 2.6719. Mask: 0.9588. :  54%|█████▍    | 27/50 [00:14<00:13,  1.67it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2346. T_Loss: 2.6742. Mask: 0.9576. :  54%|█████▍    | 27/50 [00:15<00:13,  1.67it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2346. T_Loss: 2.6742. Mask: 0.9576. :  56%|█████▌    | 28/50 [00:15<00:11,  1.92it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2339. T_Loss: 2.6800. Mask: 0.9569. :  56%|█████▌    | 28/50 [00:15<00:11,  1.92it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2339. T_Loss: 2.6800. Mask: 0.9569. :  58%|█████▊    | 29/50 [00:15<00:09,  2.19it/s]total : 1000  current step :  277
total : 1000  current step :  278
total : 1000  current step :  279
Train Iter: 280/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2342. T_Loss: 2.6852. Mask: 0.9566. :  58%|█████▊    | 29/50 [00:16<00:09,  2.19it/s]Train Iter: 280/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2342. T_Loss: 2.6852. Mask: 0.9566. :  60%|██████    | 30/50 [00:16<00:13,  1.48it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2349. T_Loss: 2.6900. Mask: 0.9560. :  60%|██████    | 30/50 [00:17<00:13,  1.48it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2349. T_Loss: 2.6900. Mask: 0.9560. :  62%|██████▏   | 31/50 [00:17<00:12,  1.57it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2363. T_Loss: 2.6885. Mask: 0.9552. :  62%|██████▏   | 31/50 [00:17<00:12,  1.57it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2363. T_Loss: 2.6885. Mask: 0.9552. :  64%|██████▍   | 32/50 [00:17<00:09,  1.92it/s]total : 1000  current step :  280
total : 1000  current step :  281
total : 1000  current step :  282
Train Iter: 283/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2355. T_Loss: 2.6921. Mask: 0.9551. :  64%|██████▍   | 32/50 [00:18<00:09,  1.92it/s]Train Iter: 283/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2355. T_Loss: 2.6921. Mask: 0.9551. :  66%|██████▌   | 33/50 [00:18<00:11,  1.54it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2366. T_Loss: 2.6940. Mask: 0.9557. :  66%|██████▌   | 33/50 [00:18<00:11,  1.54it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2366. T_Loss: 2.6940. Mask: 0.9557. :  68%|██████▊   | 34/50 [00:18<00:08,  1.95it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2371. T_Loss: 2.6958. Mask: 0.9563. :  68%|██████▊   | 34/50 [00:18<00:08,  1.95it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2371. T_Loss: 2.6958. Mask: 0.9563. :  70%|███████   | 35/50 [00:18<00:06,  2.31it/s]total : 1000  current step :  283
total : 1000  current step :  284
total : 1000  current step :  285
Train Iter: 286/1000. LR: 0.0000. Data: 0.31s. Batch: 0.55s. S_Loss: 2.2369. T_Loss: 2.6997. Mask: 0.9568. :  70%|███████   | 35/50 [00:19<00:06,  2.31it/s]Train Iter: 286/1000. LR: 0.0000. Data: 0.31s. Batch: 0.55s. S_Loss: 2.2369. T_Loss: 2.6997. Mask: 0.9568. :  72%|███████▏  | 36/50 [00:19<00:08,  1.68it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2376. T_Loss: 2.6949. Mask: 0.9571. :  72%|███████▏  | 36/50 [00:19<00:08,  1.68it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2376. T_Loss: 2.6949. Mask: 0.9571. :  74%|███████▍  | 37/50 [00:19<00:06,  2.10it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2376. T_Loss: 2.6963. Mask: 0.9568. :  74%|███████▍  | 37/50 [00:20<00:06,  2.10it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2376. T_Loss: 2.6963. Mask: 0.9568. :  76%|███████▌  | 38/50 [00:20<00:05,  2.36it/s]total : 1000  current step :  286
total : 1000  current step :  287
total : 1000  current step :  288
Train Iter: 289/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2371. T_Loss: 2.7051. Mask: 0.9574. :  76%|███████▌  | 38/50 [00:21<00:05,  2.36it/s]Train Iter: 289/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2371. T_Loss: 2.7051. Mask: 0.9574. :  78%|███████▊  | 39/50 [00:21<00:06,  1.65it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2374. T_Loss: 2.7151. Mask: 0.9580. :  78%|███████▊  | 39/50 [00:21<00:06,  1.65it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2374. T_Loss: 2.7151. Mask: 0.9580. :  80%|████████  | 40/50 [00:21<00:05,  1.96it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2365. T_Loss: 2.7242. Mask: 0.9583. :  80%|████████  | 40/50 [00:21<00:05,  1.96it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2365. T_Loss: 2.7242. Mask: 0.9583. :  82%|████████▏ | 41/50 [00:21<00:03,  2.30it/s]total : 1000  current step :  289
total : 1000  current step :  290
total : 1000  current step :  291
Train Iter: 292/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2358. T_Loss: 2.7269. Mask: 0.9581. :  82%|████████▏ | 41/50 [00:22<00:03,  2.30it/s]Train Iter: 292/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2358. T_Loss: 2.7269. Mask: 0.9581. :  84%|████████▍ | 42/50 [00:22<00:04,  1.69it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2357. T_Loss: 2.7248. Mask: 0.9578. :  84%|████████▍ | 42/50 [00:23<00:04,  1.69it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2357. T_Loss: 2.7248. Mask: 0.9578. :  86%|████████▌ | 43/50 [00:23<00:03,  2.05it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2357. T_Loss: 2.7294. Mask: 0.9577. :  86%|████████▌ | 43/50 [00:23<00:03,  2.05it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.30s. Batch: 0.53s. S_Loss: 2.2357. T_Loss: 2.7294. Mask: 0.9577. :  88%|████████▊ | 44/50 [00:23<00:02,  2.46it/s]total : 1000  current step :  292
total : 1000  current step :  293
total : 1000  current step :  294
Train Iter: 295/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2358. T_Loss: 2.7330. Mask: 0.9575. :  88%|████████▊ | 44/50 [00:24<00:02,  2.46it/s]Train Iter: 295/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2358. T_Loss: 2.7330. Mask: 0.9575. :  90%|█████████ | 45/50 [00:24<00:03,  1.59it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2361. T_Loss: 2.7440. Mask: 0.9574. :  90%|█████████ | 45/50 [00:24<00:03,  1.59it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.31s. Batch: 0.54s. S_Loss: 2.2361. T_Loss: 2.7440. Mask: 0.9574. :  92%|█████████▏| 46/50 [00:24<00:02,  1.78it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2373. T_Loss: 2.7442. Mask: 0.9570. :  92%|█████████▏| 46/50 [00:25<00:02,  1.78it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.2373. T_Loss: 2.7442. Mask: 0.9570. :  94%|█████████▍| 47/50 [00:25<00:01,  1.96it/s]total : 1000  current step :  295
total : 1000  current step :  296
total : 1000  current step :  297
Train Iter: 298/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2375. T_Loss: 2.7467. Mask: 0.9571. :  94%|█████████▍| 47/50 [00:26<00:01,  1.96it/s]Train Iter: 298/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.2375. T_Loss: 2.7467. Mask: 0.9571. :  96%|█████████▌| 48/50 [00:26<00:01,  1.38it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2386. T_Loss: 2.7492. Mask: 0.9571. :  96%|█████████▌| 48/50 [00:26<00:01,  1.38it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2386. T_Loss: 2.7492. Mask: 0.9571. :  98%|█████████▊| 49/50 [00:26<00:00,  1.65it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2385. T_Loss: 2.7467. Mask: 0.9576. :  98%|█████████▊| 49/50 [00:27<00:00,  1.65it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2385. T_Loss: 2.7467. Mask: 0.9576. : 100%|██████████| 50/50 [00:27<00:00,  1.90it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.32s. Batch: 0.54s. S_Loss: 2.2385. T_Loss: 2.7467. Mask: 0.9576. : 100%|██████████| 50/50 [00:27<00:00,  1.85it/s]
total : 1000  current step :  298
total : 1000  current step :  299
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 2.0949. top1: 2.34. top5: 93.36. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 2.0949. top1: 2.34. top5: 93.36. :  12%|█▎        | 1/8 [00:00<00:03,  1.81it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 2.0965. top1: 2.73. top5: 94.14. :  12%|█▎        | 1/8 [00:00<00:03,  1.81it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 2.0965. top1: 2.73. top5: 94.14. :  25%|██▌       | 2/8 [00:00<00:02,  2.71it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.0949. top1: 2.73. top5: 94.40. :  25%|██▌       | 2/8 [00:01<00:02,  2.71it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.0949. top1: 2.73. top5: 94.40. :  38%|███▊      | 3/8 [00:01<00:01,  3.13it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.1055. top1: 2.64. top5: 92.97. :  38%|███▊      | 3/8 [00:01<00:01,  3.13it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.1055. top1: 2.64. top5: 92.97. :  50%|█████     | 4/8 [00:01<00:01,  3.50it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.1576. top1: 2.19. top5: 85.00. :  50%|█████     | 4/8 [00:01<00:01,  3.50it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.1576. top1: 2.19. top5: 85.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.1941. top1: 1.89. top5: 79.43. :  62%|██████▎   | 5/8 [00:01<00:00,  3.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.1941. top1: 1.89. top5: 79.43. :  75%|███████▌  | 6/8 [00:01<00:00,  3.87it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2175. top1: 1.79. top5: 76.17. :  75%|███████▌  | 6/8 [00:01<00:00,  3.87it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2175. top1: 1.79. top5: 76.17. :  88%|████████▊ | 7/8 [00:01<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.2379. top1: 1.60. top5: 73.05. :  88%|████████▊ | 7/8 [00:02<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.2379. top1: 1.60. top5: 73.05. : 100%|██████████| 8/8 [00:02<00:00,  4.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.2379. top1: 1.60. top5: 73.05. : 100%|██████████| 8/8 [00:02<00:00,  3.35it/s]
total : 1000  current step :  300
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.86s. Batch: 1.05s. S_Loss: 2.2305. T_Loss: 2.4865. Mask: 0.9688. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.86s. Batch: 1.05s. S_Loss: 2.2305. T_Loss: 2.4865. Mask: 0.9688. :   2%|▏         | 1/50 [00:01<00:51,  1.05s/it]Train Iter: 302/1000. LR: 0.0189. Data: 0.47s. Batch: 0.67s. S_Loss: 1.8572. T_Loss: 2.5777. Mask: 0.9688. :   2%|▏         | 1/50 [00:01<00:51,  1.05s/it]Train Iter: 302/1000. LR: 0.0189. Data: 0.47s. Batch: 0.67s. S_Loss: 1.8572. T_Loss: 2.5777. Mask: 0.9688. :   4%|▍         | 2/50 [00:01<00:28,  1.67it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.34s. Batch: 0.53s. S_Loss: 1.6547. T_Loss: 2.5271. Mask: 0.9622. :   4%|▍         | 2/50 [00:01<00:28,  1.67it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.34s. Batch: 0.53s. S_Loss: 1.6547. T_Loss: 2.5271. Mask: 0.9622. :   6%|▌         | 3/50 [00:01<00:20,  2.28it/s]total : 1000  current step :  301
total : 1000  current step :  302
total : 1000  current step :  303
Train Iter: 304/1000. LR: 0.0190. Data: 0.45s. Batch: 0.64s. S_Loss: 1.5608. T_Loss: 2.5744. Mask: 0.9658. :   6%|▌         | 3/50 [00:02<00:20,  2.28it/s]Train Iter: 304/1000. LR: 0.0190. Data: 0.45s. Batch: 0.64s. S_Loss: 1.5608. T_Loss: 2.5744. Mask: 0.9658. :   8%|▊         | 4/50 [00:02<00:30,  1.53it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.38s. Batch: 0.58s. S_Loss: 1.5103. T_Loss: 2.5603. Mask: 0.9680. :   8%|▊         | 4/50 [00:02<00:30,  1.53it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.38s. Batch: 0.58s. S_Loss: 1.5103. T_Loss: 2.5603. Mask: 0.9680. :  10%|█         | 5/50 [00:02<00:24,  1.87it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.33s. Batch: 0.54s. S_Loss: 1.4869. T_Loss: 2.5519. Mask: 0.9648. :  10%|█         | 5/50 [00:03<00:24,  1.87it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.33s. Batch: 0.54s. S_Loss: 1.4869. T_Loss: 2.5519. Mask: 0.9648. :  12%|█▏        | 6/50 [00:03<00:20,  2.12it/s]total : 1000  current step :  304
total : 1000  current step :  305
total : 1000  current step :  306
Train Iter: 307/1000. LR: 0.0192. Data: 0.40s. Batch: 0.62s. S_Loss: 1.4746. T_Loss: 2.5807. Mask: 0.9654. :  12%|█▏        | 6/50 [00:04<00:20,  2.12it/s]Train Iter: 307/1000. LR: 0.0192. Data: 0.40s. Batch: 0.62s. S_Loss: 1.4746. T_Loss: 2.5807. Mask: 0.9654. :  14%|█▍        | 7/50 [00:04<00:28,  1.49it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.35s. Batch: 0.58s. S_Loss: 1.4738. T_Loss: 2.5812. Mask: 0.9648. :  14%|█▍        | 7/50 [00:04<00:28,  1.49it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.35s. Batch: 0.58s. S_Loss: 1.4738. T_Loss: 2.5812. Mask: 0.9648. :  16%|█▌        | 8/50 [00:04<00:23,  1.81it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.32s. Batch: 0.54s. S_Loss: 1.4777. T_Loss: 2.6119. Mask: 0.9648. :  16%|█▌        | 8/50 [00:04<00:23,  1.81it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.32s. Batch: 0.54s. S_Loss: 1.4777. T_Loss: 2.6119. Mask: 0.9648. :  18%|█▊        | 9/50 [00:04<00:18,  2.22it/s]total : 1000  current step :  307
total : 1000  current step :  308
total : 1000  current step :  309
Train Iter: 310/1000. LR: 0.0194. Data: 0.37s. Batch: 0.60s. S_Loss: 1.4820. T_Loss: 2.6326. Mask: 0.9660. :  18%|█▊        | 9/50 [00:05<00:18,  2.22it/s]Train Iter: 310/1000. LR: 0.0194. Data: 0.37s. Batch: 0.60s. S_Loss: 1.4820. T_Loss: 2.6326. Mask: 0.9660. :  20%|██        | 10/50 [00:05<00:26,  1.51it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.34s. Batch: 0.57s. S_Loss: 1.4894. T_Loss: 2.6648. Mask: 0.9666. :  20%|██        | 10/50 [00:06<00:26,  1.51it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.34s. Batch: 0.57s. S_Loss: 1.4894. T_Loss: 2.6648. Mask: 0.9666. :  22%|██▏       | 11/50 [00:06<00:20,  1.86it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.33s. Batch: 0.56s. S_Loss: 1.4950. T_Loss: 2.6686. Mask: 0.9665. :  22%|██▏       | 11/50 [00:06<00:20,  1.86it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.33s. Batch: 0.56s. S_Loss: 1.4950. T_Loss: 2.6686. Mask: 0.9665. :  24%|██▍       | 12/50 [00:06<00:20,  1.88it/s]total : 1000  current step :  310
total : 1000  current step :  311
total : 1000  current step :  312
Train Iter: 313/1000. LR: 0.0196. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5012. T_Loss: 2.6901. Mask: 0.9675. :  24%|██▍       | 12/50 [00:07<00:20,  1.88it/s]Train Iter: 313/1000. LR: 0.0196. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5012. T_Loss: 2.6901. Mask: 0.9675. :  26%|██▌       | 13/50 [00:07<00:25,  1.46it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.34s. Batch: 0.58s. S_Loss: 1.5070. T_Loss: 2.7171. Mask: 0.9679. :  26%|██▌       | 13/50 [00:08<00:25,  1.46it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.34s. Batch: 0.58s. S_Loss: 1.5070. T_Loss: 2.7171. Mask: 0.9679. :  28%|██▊       | 14/50 [00:08<00:20,  1.78it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.32s. Batch: 0.56s. S_Loss: 1.5119. T_Loss: 2.7495. Mask: 0.9669. :  28%|██▊       | 14/50 [00:08<00:20,  1.78it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.32s. Batch: 0.56s. S_Loss: 1.5119. T_Loss: 2.7495. Mask: 0.9669. :  30%|███       | 15/50 [00:08<00:17,  2.06it/s]total : 1000  current step :  313
total : 1000  current step :  314
total : 1000  current step :  315
Train Iter: 316/1000. LR: 0.0198. Data: 0.35s. Batch: 0.59s. S_Loss: 1.5149. T_Loss: 2.7704. Mask: 0.9663. :  30%|███       | 15/50 [00:09<00:17,  2.06it/s]Train Iter: 316/1000. LR: 0.0198. Data: 0.35s. Batch: 0.59s. S_Loss: 1.5149. T_Loss: 2.7704. Mask: 0.9663. :  32%|███▏      | 16/50 [00:09<00:21,  1.56it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.33s. Batch: 0.57s. S_Loss: 1.5160. T_Loss: 2.7738. Mask: 0.9655. :  32%|███▏      | 16/50 [00:09<00:21,  1.56it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.33s. Batch: 0.57s. S_Loss: 1.5160. T_Loss: 2.7738. Mask: 0.9655. :  34%|███▍      | 17/50 [00:09<00:18,  1.81it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.32s. Batch: 0.56s. S_Loss: 1.5176. T_Loss: 2.7761. Mask: 0.9657. :  34%|███▍      | 17/50 [00:10<00:18,  1.81it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.32s. Batch: 0.56s. S_Loss: 1.5176. T_Loss: 2.7761. Mask: 0.9657. :  36%|███▌      | 18/50 [00:10<00:15,  2.08it/s]total : 1000  current step :  316
total : 1000  current step :  317
total : 1000  current step :  318
Train Iter: 319/1000. LR: 0.0199. Data: 0.36s. Batch: 0.59s. S_Loss: 1.5176. T_Loss: 2.7892. Mask: 0.9646. :  36%|███▌      | 18/50 [00:11<00:15,  2.08it/s]Train Iter: 319/1000. LR: 0.0199. Data: 0.36s. Batch: 0.59s. S_Loss: 1.5176. T_Loss: 2.7892. Mask: 0.9646. :  38%|███▊      | 19/50 [00:11<00:20,  1.48it/s]total : 1000  current step :  319
Train Iter: 320/1000. LR: 0.0200. Data: 0.36s. Batch: 0.59s. S_Loss: 1.5168. T_Loss: 2.7999. Mask: 0.9645. :  38%|███▊      | 19/50 [00:11<00:20,  1.48it/s]Train Iter: 320/1000. LR: 0.0200. Data: 0.36s. Batch: 0.59s. S_Loss: 1.5168. T_Loss: 2.7999. Mask: 0.9645. :  40%|████      | 20/50 [00:11<00:20,  1.46it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.38s. Batch: 0.60s. S_Loss: 1.5149. T_Loss: 2.8074. Mask: 0.9647. :  40%|████      | 20/50 [00:12<00:20,  1.46it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.38s. Batch: 0.60s. S_Loss: 1.5149. T_Loss: 2.8074. Mask: 0.9647. :  42%|████▏     | 21/50 [00:12<00:20,  1.40it/s]total : 1000  current step :  320
total : 1000  current step :  321
Train Iter: 322/1000. LR: 0.0201. Data: 0.40s. Batch: 0.62s. S_Loss: 1.5127. T_Loss: 2.8177. Mask: 0.9652. :  42%|████▏     | 21/50 [00:13<00:20,  1.40it/s]Train Iter: 322/1000. LR: 0.0201. Data: 0.40s. Batch: 0.62s. S_Loss: 1.5127. T_Loss: 2.8177. Mask: 0.9652. :  44%|████▍     | 22/50 [00:13<00:22,  1.24it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.39s. Batch: 0.61s. S_Loss: 1.5102. T_Loss: 2.8163. Mask: 0.9643. :  44%|████▍     | 22/50 [00:14<00:22,  1.24it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.39s. Batch: 0.61s. S_Loss: 1.5102. T_Loss: 2.8163. Mask: 0.9643. :  46%|████▌     | 23/50 [00:14<00:17,  1.51it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5058. T_Loss: 2.8214. Mask: 0.9645. :  46%|████▌     | 23/50 [00:14<00:17,  1.51it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5058. T_Loss: 2.8214. Mask: 0.9645. :  48%|████▊     | 24/50 [00:14<00:14,  1.76it/s]total : 1000  current step :  322
total : 1000  current step :  323
total : 1000  current step :  324
Train Iter: 325/1000. LR: 0.0203. Data: 0.40s. Batch: 0.62s. S_Loss: 1.5013. T_Loss: 2.8232. Mask: 0.9641. :  48%|████▊     | 24/50 [00:15<00:14,  1.76it/s]Train Iter: 325/1000. LR: 0.0203. Data: 0.40s. Batch: 0.62s. S_Loss: 1.5013. T_Loss: 2.8232. Mask: 0.9641. :  50%|█████     | 25/50 [00:15<00:18,  1.35it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4964. T_Loss: 2.8236. Mask: 0.9639. :  50%|█████     | 25/50 [00:15<00:18,  1.35it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4964. T_Loss: 2.8236. Mask: 0.9639. :  52%|█████▏    | 26/50 [00:15<00:14,  1.67it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4905. T_Loss: 2.8185. Mask: 0.9643. :  52%|█████▏    | 26/50 [00:16<00:14,  1.67it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4905. T_Loss: 2.8185. Mask: 0.9643. :  54%|█████▍    | 27/50 [00:16<00:11,  2.00it/s]total : 1000  current step :  325
total : 1000  current step :  326
total : 1000  current step :  327
Train Iter: 328/1000. LR: 0.0205. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4838. T_Loss: 2.8145. Mask: 0.9639. :  54%|█████▍    | 27/50 [00:17<00:11,  2.00it/s]Train Iter: 328/1000. LR: 0.0205. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4838. T_Loss: 2.8145. Mask: 0.9639. :  56%|█████▌    | 28/50 [00:17<00:15,  1.41it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.39s. Batch: 0.60s. S_Loss: 1.4771. T_Loss: 2.8222. Mask: 0.9640. :  56%|█████▌    | 28/50 [00:17<00:15,  1.41it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.39s. Batch: 0.60s. S_Loss: 1.4771. T_Loss: 2.8222. Mask: 0.9640. :  58%|█████▊    | 29/50 [00:17<00:12,  1.68it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.38s. Batch: 0.59s. S_Loss: 1.4704. T_Loss: 2.8255. Mask: 0.9646. :  58%|█████▊    | 29/50 [00:17<00:12,  1.68it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.38s. Batch: 0.59s. S_Loss: 1.4704. T_Loss: 2.8255. Mask: 0.9646. :  60%|██████    | 30/50 [00:17<00:10,  1.98it/s]total : 1000  current step :  328
total : 1000  current step :  329
total : 1000  current step :  330
Train Iter: 331/1000. LR: 0.0207. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4622. T_Loss: 2.8307. Mask: 0.9646. :  60%|██████    | 30/50 [00:19<00:10,  1.98it/s]Train Iter: 331/1000. LR: 0.0207. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4622. T_Loss: 2.8307. Mask: 0.9646. :  62%|██████▏   | 31/50 [00:19<00:13,  1.42it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.39s. Batch: 0.60s. S_Loss: 1.4560. T_Loss: 2.8432. Mask: 0.9645. :  62%|██████▏   | 31/50 [00:19<00:13,  1.42it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.39s. Batch: 0.60s. S_Loss: 1.4560. T_Loss: 2.8432. Mask: 0.9645. :  64%|██████▍   | 32/50 [00:19<00:10,  1.65it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.38s. Batch: 0.59s. S_Loss: 1.4489. T_Loss: 2.8570. Mask: 0.9648. :  64%|██████▍   | 32/50 [00:19<00:10,  1.65it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.38s. Batch: 0.59s. S_Loss: 1.4489. T_Loss: 2.8570. Mask: 0.9648. :  66%|██████▌   | 33/50 [00:19<00:08,  2.00it/s]total : 1000  current step :  331
total : 1000  current step :  332
total : 1000  current step :  333
Train Iter: 334/1000. LR: 0.0209. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4410. T_Loss: 2.8694. Mask: 0.9650. :  66%|██████▌   | 33/50 [00:20<00:08,  2.00it/s]Train Iter: 334/1000. LR: 0.0209. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4410. T_Loss: 2.8694. Mask: 0.9650. :  68%|██████▊   | 34/50 [00:20<00:10,  1.50it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4332. T_Loss: 2.8732. Mask: 0.9651. :  68%|██████▊   | 34/50 [00:21<00:10,  1.50it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4332. T_Loss: 2.8732. Mask: 0.9651. :  70%|███████   | 35/50 [00:21<00:08,  1.77it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4256. T_Loss: 2.8776. Mask: 0.9650. :  70%|███████   | 35/50 [00:21<00:08,  1.77it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4256. T_Loss: 2.8776. Mask: 0.9650. :  72%|███████▏  | 36/50 [00:21<00:06,  2.08it/s]total : 1000  current step :  334
total : 1000  current step :  335
total : 1000  current step :  336
Train Iter: 337/1000. LR: 0.0211. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4178. T_Loss: 2.8964. Mask: 0.9646. :  72%|███████▏  | 36/50 [00:22<00:06,  2.08it/s]Train Iter: 337/1000. LR: 0.0211. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4178. T_Loss: 2.8964. Mask: 0.9646. :  74%|███████▍  | 37/50 [00:22<00:08,  1.45it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4104. T_Loss: 2.9024. Mask: 0.9644. :  74%|███████▍  | 37/50 [00:22<00:08,  1.45it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4104. T_Loss: 2.9024. Mask: 0.9644. :  76%|███████▌  | 38/50 [00:22<00:06,  1.81it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4022. T_Loss: 2.9052. Mask: 0.9644. :  76%|███████▌  | 38/50 [00:23<00:06,  1.81it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4022. T_Loss: 2.9052. Mask: 0.9644. :  78%|███████▊  | 39/50 [00:23<00:05,  1.94it/s]total : 1000  current step :  337
total : 1000  current step :  338
total : 1000  current step :  339
Train Iter: 340/1000. LR: 0.0213. Data: 0.39s. Batch: 0.61s. S_Loss: 1.3943. T_Loss: 2.9078. Mask: 0.9637. :  78%|███████▊  | 39/50 [00:24<00:05,  1.94it/s]Train Iter: 340/1000. LR: 0.0213. Data: 0.39s. Batch: 0.61s. S_Loss: 1.3943. T_Loss: 2.9078. Mask: 0.9637. :  80%|████████  | 40/50 [00:24<00:07,  1.39it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3877. T_Loss: 2.9200. Mask: 0.9639. :  80%|████████  | 40/50 [00:24<00:07,  1.39it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3877. T_Loss: 2.9200. Mask: 0.9639. :  82%|████████▏ | 41/50 [00:24<00:05,  1.65it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.37s. Batch: 0.59s. S_Loss: 1.3811. T_Loss: 2.9333. Mask: 0.9634. :  82%|████████▏ | 41/50 [00:24<00:05,  1.65it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.37s. Batch: 0.59s. S_Loss: 1.3811. T_Loss: 2.9333. Mask: 0.9634. :  84%|████████▍ | 42/50 [00:24<00:04,  2.00it/s]total : 1000  current step :  340
total : 1000  current step :  341
total : 1000  current step :  342
Train Iter: 343/1000. LR: 0.0214. Data: 0.39s. Batch: 0.61s. S_Loss: 1.3737. T_Loss: 2.9363. Mask: 0.9631. :  84%|████████▍ | 42/50 [00:26<00:04,  2.00it/s]Train Iter: 343/1000. LR: 0.0214. Data: 0.39s. Batch: 0.61s. S_Loss: 1.3737. T_Loss: 2.9363. Mask: 0.9631. :  86%|████████▌ | 43/50 [00:26<00:04,  1.42it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3682. T_Loss: 2.9495. Mask: 0.9631. :  86%|████████▌ | 43/50 [00:26<00:04,  1.42it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3682. T_Loss: 2.9495. Mask: 0.9631. :  88%|████████▊ | 44/50 [00:26<00:03,  1.72it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3633. T_Loss: 2.9668. Mask: 0.9633. :  88%|████████▊ | 44/50 [00:26<00:03,  1.72it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3633. T_Loss: 2.9668. Mask: 0.9633. :  90%|█████████ | 45/50 [00:26<00:02,  1.79it/s]total : 1000  current step :  343
total : 1000  current step :  344
total : 1000  current step :  345
Train Iter: 346/1000. LR: 0.0216. Data: 0.39s. Batch: 0.61s. S_Loss: 1.3574. T_Loss: 2.9761. Mask: 0.9631. :  90%|█████████ | 45/50 [00:28<00:02,  1.79it/s]Train Iter: 346/1000. LR: 0.0216. Data: 0.39s. Batch: 0.61s. S_Loss: 1.3574. T_Loss: 2.9761. Mask: 0.9631. :  92%|█████████▏| 46/50 [00:28<00:02,  1.35it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3510. T_Loss: 2.9813. Mask: 0.9633. :  92%|█████████▏| 46/50 [00:28<00:02,  1.35it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3510. T_Loss: 2.9813. Mask: 0.9633. :  94%|█████████▍| 47/50 [00:28<00:01,  1.63it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.37s. Batch: 0.59s. S_Loss: 1.3454. T_Loss: 2.9871. Mask: 0.9628. :  94%|█████████▍| 47/50 [00:28<00:01,  1.63it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.37s. Batch: 0.59s. S_Loss: 1.3454. T_Loss: 2.9871. Mask: 0.9628. :  96%|█████████▌| 48/50 [00:28<00:00,  2.07it/s]total : 1000  current step :  346
total : 1000  current step :  347
total : 1000  current step :  348
Train Iter: 349/1000. LR: 0.0218. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3390. T_Loss: 2.9891. Mask: 0.9626. :  96%|█████████▌| 48/50 [00:29<00:00,  2.07it/s]Train Iter: 349/1000. LR: 0.0218. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3390. T_Loss: 2.9891. Mask: 0.9626. :  98%|█████████▊| 49/50 [00:29<00:00,  1.47it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3336. T_Loss: 2.9895. Mask: 0.9627. :  98%|█████████▊| 49/50 [00:30<00:00,  1.47it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3336. T_Loss: 2.9895. Mask: 0.9627. : 100%|██████████| 50/50 [00:30<00:00,  1.80it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.38s. Batch: 0.60s. S_Loss: 1.3336. T_Loss: 2.9895. Mask: 0.9627. : 100%|██████████| 50/50 [00:30<00:00,  1.67it/s]
total : 1000  current step :  349
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 1.2745. top1: 98.05. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 1.2745. top1: 98.05. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.2730. top1: 98.24. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.2730. top1: 98.24. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.64it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.2697. top1: 98.05. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.64it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.2697. top1: 98.05. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.04it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.2814. top1: 97.07. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.04it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.2814. top1: 97.07. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.3607. top1: 87.34. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.3607. top1: 87.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.4111. top1: 81.32. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.4111. top1: 81.32. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.54it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.4485. top1: 76.67. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.54it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.4485. top1: 76.67. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.41it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.4730. top1: 73.35. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.41it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.4730. top1: 73.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.4730. top1: 73.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]
total : 1000  current step :  350
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.21s. S_Loss: 1.0365. T_Loss: 3.1244. Mask: 0.9453. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.21s. S_Loss: 1.0365. T_Loss: 3.1244. Mask: 0.9453. :   2%|▏         | 1/50 [00:00<00:10,  4.67it/s]total : 1000  current step :  351
Train Iter: 352/1000. LR: 0.0220. Data: 0.45s. Batch: 0.66s. S_Loss: 1.0557. T_Loss: 3.0456. Mask: 0.9531. :   2%|▏         | 1/50 [00:01<00:10,  4.67it/s]Train Iter: 352/1000. LR: 0.0220. Data: 0.45s. Batch: 0.66s. S_Loss: 1.0557. T_Loss: 3.0456. Mask: 0.9531. :   4%|▍         | 2/50 [00:01<00:35,  1.35it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.32s. Batch: 0.51s. S_Loss: 1.0528. T_Loss: 2.9665. Mask: 0.9583. :   4%|▍         | 2/50 [00:01<00:35,  1.35it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.32s. Batch: 0.51s. S_Loss: 1.0528. T_Loss: 2.9665. Mask: 0.9583. :   6%|▌         | 3/50 [00:01<00:23,  1.98it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.29s. Batch: 0.49s. S_Loss: 1.0597. T_Loss: 2.9674. Mask: 0.9600. :   6%|▌         | 3/50 [00:01<00:23,  1.98it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.29s. Batch: 0.49s. S_Loss: 1.0597. T_Loss: 2.9674. Mask: 0.9600. :   8%|▊         | 4/50 [00:01<00:21,  2.12it/s]total : 1000  current step :  352
total : 1000  current step :  353
total : 1000  current step :  354
Train Iter: 355/1000. LR: 0.0222. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0598. T_Loss: 2.9783. Mask: 0.9586. :   8%|▊         | 4/50 [00:03<00:21,  2.12it/s]Train Iter: 355/1000. LR: 0.0222. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0598. T_Loss: 2.9783. Mask: 0.9586. :  10%|█         | 5/50 [00:03<00:32,  1.37it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0614. T_Loss: 2.9584. Mask: 0.9551. :  10%|█         | 5/50 [00:03<00:32,  1.37it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0614. T_Loss: 2.9584. Mask: 0.9551. :  12%|█▏        | 6/50 [00:03<00:26,  1.68it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0527. T_Loss: 2.9963. Mask: 0.9570. :  12%|█▏        | 6/50 [00:03<00:26,  1.68it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0527. T_Loss: 2.9963. Mask: 0.9570. :  14%|█▍        | 7/50 [00:03<00:21,  2.03it/s]total : 1000  current step :  355
total : 1000  current step :  356
total : 1000  current step :  357
Train Iter: 358/1000. LR: 0.0224. Data: 0.41s. Batch: 0.61s. S_Loss: 1.0558. T_Loss: 3.0218. Mask: 0.9600. :  14%|█▍        | 7/50 [00:04<00:21,  2.03it/s]Train Iter: 358/1000. LR: 0.0224. Data: 0.41s. Batch: 0.61s. S_Loss: 1.0558. T_Loss: 3.0218. Mask: 0.9600. :  16%|█▌        | 8/50 [00:04<00:29,  1.43it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.40s. Batch: 0.60s. S_Loss: 1.0548. T_Loss: 3.0567. Mask: 0.9601. :  16%|█▌        | 8/50 [00:05<00:29,  1.43it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.40s. Batch: 0.60s. S_Loss: 1.0548. T_Loss: 3.0567. Mask: 0.9601. :  18%|█▊        | 9/50 [00:05<00:25,  1.60it/s]total : 1000  current step :  358
total : 1000  current step :  359
Train Iter: 360/1000. LR: 0.0225. Data: 0.40s. Batch: 0.60s. S_Loss: 1.0553. T_Loss: 3.0715. Mask: 0.9578. :  18%|█▊        | 9/50 [00:05<00:25,  1.60it/s]Train Iter: 360/1000. LR: 0.0225. Data: 0.40s. Batch: 0.60s. S_Loss: 1.0553. T_Loss: 3.0715. Mask: 0.9578. :  20%|██        | 10/50 [00:05<00:24,  1.60it/s]total : 1000  current step :  360
Train Iter: 361/1000. LR: 0.0226. Data: 0.44s. Batch: 0.64s. S_Loss: 1.0540. T_Loss: 3.0797. Mask: 0.9567. :  20%|██        | 10/50 [00:07<00:24,  1.60it/s]Train Iter: 361/1000. LR: 0.0226. Data: 0.44s. Batch: 0.64s. S_Loss: 1.0540. T_Loss: 3.0797. Mask: 0.9567. :  22%|██▏       | 11/50 [00:07<00:29,  1.30it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.41s. Batch: 0.61s. S_Loss: 1.0495. T_Loss: 3.1081. Mask: 0.9590. :  22%|██▏       | 11/50 [00:07<00:29,  1.30it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.41s. Batch: 0.61s. S_Loss: 1.0495. T_Loss: 3.1081. Mask: 0.9590. :  24%|██▍       | 12/50 [00:07<00:23,  1.61it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.39s. Batch: 0.59s. S_Loss: 1.0459. T_Loss: 3.1440. Mask: 0.9600. :  24%|██▍       | 12/50 [00:07<00:23,  1.61it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.39s. Batch: 0.59s. S_Loss: 1.0459. T_Loss: 3.1440. Mask: 0.9600. :  26%|██▌       | 13/50 [00:07<00:19,  1.93it/s]total : 1000  current step :  361
total : 1000  current step :  362
total : 1000  current step :  363
Train Iter: 364/1000. LR: 0.0228. Data: 0.41s. Batch: 0.61s. S_Loss: 1.0425. T_Loss: 3.1546. Mask: 0.9601. :  26%|██▌       | 13/50 [00:08<00:19,  1.93it/s]Train Iter: 364/1000. LR: 0.0228. Data: 0.41s. Batch: 0.61s. S_Loss: 1.0425. T_Loss: 3.1546. Mask: 0.9601. :  28%|██▊       | 14/50 [00:08<00:23,  1.56it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.39s. Batch: 0.59s. S_Loss: 1.0398. T_Loss: 3.1428. Mask: 0.9583. :  28%|██▊       | 14/50 [00:08<00:23,  1.56it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.39s. Batch: 0.59s. S_Loss: 1.0398. T_Loss: 3.1428. Mask: 0.9583. :  30%|███       | 15/50 [00:08<00:18,  1.88it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0397. T_Loss: 3.1820. Mask: 0.9580. :  30%|███       | 15/50 [00:09<00:18,  1.88it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0397. T_Loss: 3.1820. Mask: 0.9580. :  32%|███▏      | 16/50 [00:09<00:16,  2.06it/s]total : 1000  current step :  364
total : 1000  current step :  365
total : 1000  current step :  366
Train Iter: 367/1000. LR: 0.0229. Data: 0.39s. Batch: 0.60s. S_Loss: 1.0374. T_Loss: 3.2022. Mask: 0.9591. :  32%|███▏      | 16/50 [00:10<00:16,  2.06it/s]Train Iter: 367/1000. LR: 0.0229. Data: 0.39s. Batch: 0.60s. S_Loss: 1.0374. T_Loss: 3.2022. Mask: 0.9591. :  34%|███▍      | 17/50 [00:10<00:21,  1.55it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.38s. Batch: 0.59s. S_Loss: 1.0345. T_Loss: 3.2259. Mask: 0.9594. :  34%|███▍      | 17/50 [00:10<00:21,  1.55it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.38s. Batch: 0.59s. S_Loss: 1.0345. T_Loss: 3.2259. Mask: 0.9594. :  36%|███▌      | 18/50 [00:10<00:18,  1.77it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0322. T_Loss: 3.2321. Mask: 0.9593. :  36%|███▌      | 18/50 [00:10<00:18,  1.77it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0322. T_Loss: 3.2321. Mask: 0.9593. :  38%|███▊      | 19/50 [00:10<00:15,  2.02it/s]total : 1000  current step :  367
total : 1000  current step :  368
total : 1000  current step :  369
Train Iter: 370/1000. LR: 0.0231. Data: 0.39s. Batch: 0.60s. S_Loss: 1.0335. T_Loss: 3.2426. Mask: 0.9588. :  38%|███▊      | 19/50 [00:12<00:15,  2.02it/s]Train Iter: 370/1000. LR: 0.0231. Data: 0.39s. Batch: 0.60s. S_Loss: 1.0335. T_Loss: 3.2426. Mask: 0.9588. :  40%|████      | 20/50 [00:12<00:20,  1.48it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0327. T_Loss: 3.2466. Mask: 0.9591. :  40%|████      | 20/50 [00:12<00:20,  1.48it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0327. T_Loss: 3.2466. Mask: 0.9591. :  42%|████▏     | 21/50 [00:12<00:15,  1.84it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0307. T_Loss: 3.2462. Mask: 0.9590. :  42%|████▏     | 21/50 [00:12<00:15,  1.84it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0307. T_Loss: 3.2462. Mask: 0.9590. :  44%|████▍     | 22/50 [00:12<00:12,  2.19it/s]total : 1000  current step :  370
total : 1000  current step :  371
total : 1000  current step :  372
Train Iter: 373/1000. LR: 0.0233. Data: 0.38s. Batch: 0.59s. S_Loss: 1.0315. T_Loss: 3.2560. Mask: 0.9596. :  44%|████▍     | 22/50 [00:13<00:12,  2.19it/s]Train Iter: 373/1000. LR: 0.0233. Data: 0.38s. Batch: 0.59s. S_Loss: 1.0315. T_Loss: 3.2560. Mask: 0.9596. :  46%|████▌     | 23/50 [00:13<00:17,  1.53it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0296. T_Loss: 3.2547. Mask: 0.9593. :  46%|████▌     | 23/50 [00:14<00:17,  1.53it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0296. T_Loss: 3.2547. Mask: 0.9593. :  48%|████▊     | 24/50 [00:14<00:15,  1.69it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0283. T_Loss: 3.2647. Mask: 0.9600. :  48%|████▊     | 24/50 [00:14<00:15,  1.69it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0283. T_Loss: 3.2647. Mask: 0.9600. :  50%|█████     | 25/50 [00:14<00:12,  1.99it/s]total : 1000  current step :  373
total : 1000  current step :  374
total : 1000  current step :  375
Train Iter: 376/1000. LR: 0.0235. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0279. T_Loss: 3.2751. Mask: 0.9597. :  50%|█████     | 25/50 [00:15<00:12,  1.99it/s]Train Iter: 376/1000. LR: 0.0235. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0279. T_Loss: 3.2751. Mask: 0.9597. :  52%|█████▏    | 26/50 [00:15<00:15,  1.55it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0264. T_Loss: 3.2650. Mask: 0.9599. :  52%|█████▏    | 26/50 [00:15<00:15,  1.55it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0264. T_Loss: 3.2650. Mask: 0.9599. :  54%|█████▍    | 27/50 [00:15<00:12,  1.84it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0245. T_Loss: 3.2653. Mask: 0.9605. :  54%|█████▍    | 27/50 [00:15<00:12,  1.84it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0245. T_Loss: 3.2653. Mask: 0.9605. :  56%|█████▌    | 28/50 [00:15<00:10,  2.12it/s]total : 1000  current step :  376
total : 1000  current step :  377
total : 1000  current step :  378
Train Iter: 379/1000. LR: 0.0237. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0253. T_Loss: 3.2698. Mask: 0.9599. :  56%|█████▌    | 28/50 [00:17<00:10,  2.12it/s]Train Iter: 379/1000. LR: 0.0237. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0253. T_Loss: 3.2698. Mask: 0.9599. :  58%|█████▊    | 29/50 [00:17<00:14,  1.49it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0248. T_Loss: 3.2717. Mask: 0.9608. :  58%|█████▊    | 29/50 [00:17<00:14,  1.49it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0248. T_Loss: 3.2717. Mask: 0.9608. :  60%|██████    | 30/50 [00:17<00:11,  1.70it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0239. T_Loss: 3.2629. Mask: 0.9601. :  60%|██████    | 30/50 [00:17<00:11,  1.70it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0239. T_Loss: 3.2629. Mask: 0.9601. :  62%|██████▏   | 31/50 [00:17<00:10,  1.88it/s]total : 1000  current step :  379
total : 1000  current step :  380
total : 1000  current step :  381
Train Iter: 382/1000. LR: 0.0239. Data: 0.38s. Batch: 0.60s. S_Loss: 1.0225. T_Loss: 3.2637. Mask: 0.9607. :  62%|██████▏   | 31/50 [00:19<00:10,  1.88it/s]Train Iter: 382/1000. LR: 0.0239. Data: 0.38s. Batch: 0.60s. S_Loss: 1.0225. T_Loss: 3.2637. Mask: 0.9607. :  64%|██████▍   | 32/50 [00:19<00:13,  1.37it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0222. T_Loss: 3.2712. Mask: 0.9606. :  64%|██████▍   | 32/50 [00:19<00:13,  1.37it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0222. T_Loss: 3.2712. Mask: 0.9606. :  66%|██████▌   | 33/50 [00:19<00:10,  1.69it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0203. T_Loss: 3.2741. Mask: 0.9611. :  66%|██████▌   | 33/50 [00:19<00:10,  1.69it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0203. T_Loss: 3.2741. Mask: 0.9611. :  68%|██████▊   | 34/50 [00:19<00:07,  2.14it/s]total : 1000  current step :  382
total : 1000  current step :  383
total : 1000  current step :  384
Train Iter: 385/1000. LR: 0.0241. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0208. T_Loss: 3.2825. Mask: 0.9613. :  68%|██████▊   | 34/50 [00:20<00:07,  2.14it/s]Train Iter: 385/1000. LR: 0.0241. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0208. T_Loss: 3.2825. Mask: 0.9613. :  70%|███████   | 35/50 [00:20<00:09,  1.57it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0197. T_Loss: 3.2833. Mask: 0.9614. :  70%|███████   | 35/50 [00:20<00:09,  1.57it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0197. T_Loss: 3.2833. Mask: 0.9614. :  72%|███████▏  | 36/50 [00:20<00:07,  1.80it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0185. T_Loss: 3.2908. Mask: 0.9615. :  72%|███████▏  | 36/50 [00:21<00:07,  1.80it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0185. T_Loss: 3.2908. Mask: 0.9615. :  74%|███████▍  | 37/50 [00:21<00:06,  2.06it/s]total : 1000  current step :  385
total : 1000  current step :  386
total : 1000  current step :  387
Train Iter: 388/1000. LR: 0.0243. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0179. T_Loss: 3.2897. Mask: 0.9618. :  74%|███████▍  | 37/50 [00:22<00:06,  2.06it/s]Train Iter: 388/1000. LR: 0.0243. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0179. T_Loss: 3.2897. Mask: 0.9618. :  76%|███████▌  | 38/50 [00:22<00:07,  1.52it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0176. T_Loss: 3.2956. Mask: 0.9616. :  76%|███████▌  | 38/50 [00:22<00:07,  1.52it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0176. T_Loss: 3.2956. Mask: 0.9616. :  78%|███████▊  | 39/50 [00:22<00:06,  1.72it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0176. T_Loss: 3.3000. Mask: 0.9619. :  78%|███████▊  | 39/50 [00:23<00:06,  1.72it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0176. T_Loss: 3.3000. Mask: 0.9619. :  80%|████████  | 40/50 [00:23<00:05,  1.99it/s]total : 1000  current step :  388
total : 1000  current step :  389
total : 1000  current step :  390
Train Iter: 391/1000. LR: 0.0244. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0180. T_Loss: 3.3017. Mask: 0.9621. :  80%|████████  | 40/50 [00:24<00:05,  1.99it/s]Train Iter: 391/1000. LR: 0.0244. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0180. T_Loss: 3.3017. Mask: 0.9621. :  82%|████████▏ | 41/50 [00:24<00:05,  1.53it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0170. T_Loss: 3.3062. Mask: 0.9623. :  82%|████████▏ | 41/50 [00:24<00:05,  1.53it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0170. T_Loss: 3.3062. Mask: 0.9623. :  84%|████████▍ | 42/50 [00:24<00:04,  1.75it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0161. T_Loss: 3.3051. Mask: 0.9626. :  84%|████████▍ | 42/50 [00:24<00:04,  1.75it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0161. T_Loss: 3.3051. Mask: 0.9626. :  86%|████████▌ | 43/50 [00:24<00:03,  2.02it/s]total : 1000  current step :  391
total : 1000  current step :  392
total : 1000  current step :  393
Train Iter: 394/1000. LR: 0.0246. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0160. T_Loss: 3.3072. Mask: 0.9622. :  86%|████████▌ | 43/50 [00:25<00:03,  2.02it/s]Train Iter: 394/1000. LR: 0.0246. Data: 0.37s. Batch: 0.59s. S_Loss: 1.0160. T_Loss: 3.3072. Mask: 0.9622. :  88%|████████▊ | 44/50 [00:25<00:04,  1.46it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0147. T_Loss: 3.3190. Mask: 0.9622. :  88%|████████▊ | 44/50 [00:26<00:04,  1.46it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0147. T_Loss: 3.3190. Mask: 0.9622. :  90%|█████████ | 45/50 [00:26<00:02,  1.81it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0138. T_Loss: 3.3193. Mask: 0.9621. :  90%|█████████ | 45/50 [00:26<00:02,  1.81it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0138. T_Loss: 3.3193. Mask: 0.9621. :  92%|█████████▏| 46/50 [00:26<00:01,  2.18it/s]total : 1000  current step :  394
total : 1000  current step :  395
total : 1000  current step :  396
Train Iter: 397/1000. LR: 0.0248. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0134. T_Loss: 3.3240. Mask: 0.9622. :  92%|█████████▏| 46/50 [00:27<00:01,  2.18it/s]Train Iter: 397/1000. LR: 0.0248. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0134. T_Loss: 3.3240. Mask: 0.9622. :  94%|█████████▍| 47/50 [00:27<00:01,  1.63it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0136. T_Loss: 3.3337. Mask: 0.9622. :  94%|█████████▍| 47/50 [00:27<00:01,  1.63it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0136. T_Loss: 3.3337. Mask: 0.9622. :  96%|█████████▌| 48/50 [00:27<00:01,  1.87it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0135. T_Loss: 3.3420. Mask: 0.9622. :  96%|█████████▌| 48/50 [00:28<00:01,  1.87it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0135. T_Loss: 3.3420. Mask: 0.9622. :  98%|█████████▊| 49/50 [00:28<00:00,  2.08it/s]total : 1000  current step :  397
total : 1000  current step :  398
total : 1000  current step :  399
Train Iter: 400/1000. LR: 0.0250. Data: 0.38s. Batch: 0.59s. S_Loss: 1.0129. T_Loss: 3.3486. Mask: 0.9623. :  98%|█████████▊| 49/50 [00:29<00:00,  2.08it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.38s. Batch: 0.59s. S_Loss: 1.0129. T_Loss: 3.3486. Mask: 0.9623. : 100%|██████████| 50/50 [00:29<00:00,  1.22it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.38s. Batch: 0.59s. S_Loss: 1.0129. T_Loss: 3.3486. Mask: 0.9623. : 100%|██████████| 50/50 [00:29<00:00,  1.68it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.2324. top1: 94.92. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.2324. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.2291. top1: 95.70. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.2291. top1: 95.70. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.2267. top1: 95.83. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.2267. top1: 95.83. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.05it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.2337. top1: 95.12. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.05it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.2337. top1: 95.12. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.53it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2856. top1: 89.45. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.53it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2856. top1: 89.45. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.3163. top1: 85.61. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.3163. top1: 85.61. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.3394. top1: 82.87. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.3394. top1: 82.87. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.3553. top1: 80.60. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.3553. top1: 80.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.3553. top1: 80.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.21it/s]
total : 1000  current step :  400
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.16s. S_Loss: 0.9943. T_Loss: 3.5227. Mask: 0.9570. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.16s. S_Loss: 0.9943. T_Loss: 3.5227. Mask: 0.9570. :   2%|▏         | 1/50 [00:00<00:07,  6.24it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.16s. S_Loss: 0.9717. T_Loss: 3.4833. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:07,  6.24it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.16s. S_Loss: 0.9717. T_Loss: 3.4833. Mask: 0.9688. :   4%|▍         | 2/50 [00:00<00:07,  6.04it/s]total : 1000  current step :  401
total : 1000  current step :  402
Train Iter: 403/1000. LR: 0.0252. Data: 0.26s. Batch: 0.44s. S_Loss: 0.9779. T_Loss: 3.5001. Mask: 0.9674. :   4%|▍         | 2/50 [00:01<00:07,  6.04it/s]Train Iter: 403/1000. LR: 0.0252. Data: 0.26s. Batch: 0.44s. S_Loss: 0.9779. T_Loss: 3.5001. Mask: 0.9674. :   6%|▌         | 3/50 [00:01<00:25,  1.85it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.21s. Batch: 0.39s. S_Loss: 0.9868. T_Loss: 3.5285. Mask: 0.9727. :   6%|▌         | 3/50 [00:01<00:25,  1.85it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.21s. Batch: 0.39s. S_Loss: 0.9868. T_Loss: 3.5285. Mask: 0.9727. :   8%|▊         | 4/50 [00:01<00:19,  2.33it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.21s. Batch: 0.39s. S_Loss: 0.9835. T_Loss: 3.5185. Mask: 0.9719. :   8%|▊         | 4/50 [00:01<00:19,  2.33it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.21s. Batch: 0.39s. S_Loss: 0.9835. T_Loss: 3.5185. Mask: 0.9719. :  10%|█         | 5/50 [00:01<00:18,  2.38it/s]total : 1000  current step :  403
total : 1000  current step :  404
total : 1000  current step :  405
Train Iter: 406/1000. LR: 0.0254. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9797. T_Loss: 3.5222. Mask: 0.9740. :  10%|█         | 5/50 [00:02<00:18,  2.38it/s]Train Iter: 406/1000. LR: 0.0254. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9797. T_Loss: 3.5222. Mask: 0.9740. :  12%|█▏        | 6/50 [00:02<00:27,  1.60it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.30s. Batch: 0.48s. S_Loss: 0.9769. T_Loss: 3.4508. Mask: 0.9721. :  12%|█▏        | 6/50 [00:03<00:27,  1.60it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.30s. Batch: 0.48s. S_Loss: 0.9769. T_Loss: 3.4508. Mask: 0.9721. :  14%|█▍        | 7/50 [00:03<00:22,  1.87it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.29s. Batch: 0.47s. S_Loss: 0.9761. T_Loss: 3.4590. Mask: 0.9712. :  14%|█▍        | 7/50 [00:03<00:22,  1.87it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.29s. Batch: 0.47s. S_Loss: 0.9761. T_Loss: 3.4590. Mask: 0.9712. :  16%|█▌        | 8/50 [00:03<00:20,  2.02it/s]total : 1000  current step :  406
total : 1000  current step :  407
total : 1000  current step :  408
Train Iter: 409/1000. LR: 0.0256. Data: 0.35s. Batch: 0.52s. S_Loss: 0.9762. T_Loss: 3.4297. Mask: 0.9705. :  16%|█▌        | 8/50 [00:04<00:20,  2.02it/s]Train Iter: 409/1000. LR: 0.0256. Data: 0.35s. Batch: 0.52s. S_Loss: 0.9762. T_Loss: 3.4297. Mask: 0.9705. :  18%|█▊        | 9/50 [00:04<00:26,  1.56it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.33s. Batch: 0.51s. S_Loss: 0.9737. T_Loss: 3.4002. Mask: 0.9688. :  18%|█▊        | 9/50 [00:05<00:26,  1.56it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.33s. Batch: 0.51s. S_Loss: 0.9737. T_Loss: 3.4002. Mask: 0.9688. :  20%|██        | 10/50 [00:05<00:22,  1.80it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.30s. Batch: 0.48s. S_Loss: 0.9717. T_Loss: 3.3793. Mask: 0.9688. :  20%|██        | 10/50 [00:05<00:22,  1.80it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.30s. Batch: 0.48s. S_Loss: 0.9717. T_Loss: 3.3793. Mask: 0.9688. :  22%|██▏       | 11/50 [00:05<00:17,  2.17it/s]total : 1000  current step :  409
total : 1000  current step :  410
total : 1000  current step :  411
Train Iter: 412/1000. LR: 0.0258. Data: 0.35s. Batch: 0.53s. S_Loss: 0.9739. T_Loss: 3.3852. Mask: 0.9674. :  22%|██▏       | 11/50 [00:06<00:17,  2.17it/s]Train Iter: 412/1000. LR: 0.0258. Data: 0.35s. Batch: 0.53s. S_Loss: 0.9739. T_Loss: 3.3852. Mask: 0.9674. :  24%|██▍       | 12/50 [00:06<00:24,  1.55it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.33s. Batch: 0.51s. S_Loss: 0.9749. T_Loss: 3.4064. Mask: 0.9648. :  24%|██▍       | 12/50 [00:06<00:24,  1.55it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.33s. Batch: 0.51s. S_Loss: 0.9749. T_Loss: 3.4064. Mask: 0.9648. :  26%|██▌       | 13/50 [00:06<00:20,  1.85it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9741. T_Loss: 3.3922. Mask: 0.9651. :  26%|██▌       | 13/50 [00:07<00:20,  1.85it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9741. T_Loss: 3.3922. Mask: 0.9651. :  28%|██▊       | 14/50 [00:07<00:16,  2.12it/s]total : 1000  current step :  412
total : 1000  current step :  413
total : 1000  current step :  414
Train Iter: 415/1000. LR: 0.0259. Data: 0.35s. Batch: 0.54s. S_Loss: 0.9745. T_Loss: 3.3840. Mask: 0.9643. :  28%|██▊       | 14/50 [00:08<00:16,  2.12it/s]Train Iter: 415/1000. LR: 0.0259. Data: 0.35s. Batch: 0.54s. S_Loss: 0.9745. T_Loss: 3.3840. Mask: 0.9643. :  30%|███       | 15/50 [00:08<00:23,  1.52it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9764. T_Loss: 3.3739. Mask: 0.9634. :  30%|███       | 15/50 [00:08<00:23,  1.52it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9764. T_Loss: 3.3739. Mask: 0.9634. :  32%|███▏      | 16/50 [00:08<00:17,  1.96it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9754. T_Loss: 3.3685. Mask: 0.9621. :  32%|███▏      | 16/50 [00:08<00:17,  1.96it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9754. T_Loss: 3.3685. Mask: 0.9621. :  34%|███▍      | 17/50 [00:08<00:14,  2.31it/s]total : 1000  current step :  415
total : 1000  current step :  416
total : 1000  current step :  417
Train Iter: 418/1000. LR: 0.0261. Data: 0.35s. Batch: 0.53s. S_Loss: 0.9756. T_Loss: 3.3803. Mask: 0.9622. :  34%|███▍      | 17/50 [00:09<00:14,  2.31it/s]Train Iter: 418/1000. LR: 0.0261. Data: 0.35s. Batch: 0.53s. S_Loss: 0.9756. T_Loss: 3.3803. Mask: 0.9622. :  36%|███▌      | 18/50 [00:09<00:20,  1.58it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9760. T_Loss: 3.3909. Mask: 0.9620. :  36%|███▌      | 18/50 [00:09<00:20,  1.58it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9760. T_Loss: 3.3909. Mask: 0.9620. :  38%|███▊      | 19/50 [00:09<00:16,  1.93it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.32s. Batch: 0.51s. S_Loss: 0.9759. T_Loss: 3.4031. Mask: 0.9625. :  38%|███▊      | 19/50 [00:10<00:16,  1.93it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.32s. Batch: 0.51s. S_Loss: 0.9759. T_Loss: 3.4031. Mask: 0.9625. :  40%|████      | 20/50 [00:10<00:13,  2.23it/s]total : 1000  current step :  418
total : 1000  current step :  419
total : 1000  current step :  420
Train Iter: 421/1000. LR: 0.0263. Data: 0.34s. Batch: 0.53s. S_Loss: 0.9755. T_Loss: 3.4112. Mask: 0.9628. :  40%|████      | 20/50 [00:11<00:13,  2.23it/s]Train Iter: 421/1000. LR: 0.0263. Data: 0.34s. Batch: 0.53s. S_Loss: 0.9755. T_Loss: 3.4112. Mask: 0.9628. :  42%|████▏     | 21/50 [00:11<00:17,  1.61it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9760. T_Loss: 3.4174. Mask: 0.9629. :  42%|████▏     | 21/50 [00:11<00:17,  1.61it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9760. T_Loss: 3.4174. Mask: 0.9629. :  44%|████▍     | 22/50 [00:11<00:14,  1.89it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.32s. Batch: 0.51s. S_Loss: 0.9774. T_Loss: 3.4180. Mask: 0.9620. :  44%|████▍     | 22/50 [00:11<00:14,  1.89it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.32s. Batch: 0.51s. S_Loss: 0.9774. T_Loss: 3.4180. Mask: 0.9620. :  46%|████▌     | 23/50 [00:11<00:12,  2.19it/s]total : 1000  current step :  421
total : 1000  current step :  422
total : 1000  current step :  423
Train Iter: 424/1000. LR: 0.0265. Data: 0.34s. Batch: 0.53s. S_Loss: 0.9768. T_Loss: 3.4139. Mask: 0.9619. :  46%|████▌     | 23/50 [00:12<00:12,  2.19it/s]Train Iter: 424/1000. LR: 0.0265. Data: 0.34s. Batch: 0.53s. S_Loss: 0.9768. T_Loss: 3.4139. Mask: 0.9619. :  48%|████▊     | 24/50 [00:12<00:15,  1.66it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9758. T_Loss: 3.4205. Mask: 0.9616. :  48%|████▊     | 24/50 [00:13<00:15,  1.66it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9758. T_Loss: 3.4205. Mask: 0.9616. :  50%|█████     | 25/50 [00:13<00:13,  1.80it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.32s. Batch: 0.51s. S_Loss: 0.9752. T_Loss: 3.4087. Mask: 0.9611. :  50%|█████     | 25/50 [00:13<00:13,  1.80it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.32s. Batch: 0.51s. S_Loss: 0.9752. T_Loss: 3.4087. Mask: 0.9611. :  52%|█████▏    | 26/50 [00:13<00:11,  2.17it/s]total : 1000  current step :  424
total : 1000  current step :  425
total : 1000  current step :  426
Train Iter: 427/1000. LR: 0.0267. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9772. T_Loss: 3.4015. Mask: 0.9605. :  52%|█████▏    | 26/50 [00:14<00:11,  2.17it/s]Train Iter: 427/1000. LR: 0.0267. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9772. T_Loss: 3.4015. Mask: 0.9605. :  54%|█████▍    | 27/50 [00:14<00:15,  1.51it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9756. T_Loss: 3.4029. Mask: 0.9615. :  54%|█████▍    | 27/50 [00:14<00:15,  1.51it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9756. T_Loss: 3.4029. Mask: 0.9615. :  56%|█████▌    | 28/50 [00:14<00:12,  1.77it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9761. T_Loss: 3.4092. Mask: 0.9616. :  56%|█████▌    | 28/50 [00:15<00:12,  1.77it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9761. T_Loss: 3.4092. Mask: 0.9616. :  58%|█████▊    | 29/50 [00:15<00:10,  2.09it/s]total : 1000  current step :  427
total : 1000  current step :  428
total : 1000  current step :  429
Train Iter: 430/1000. LR: 0.0269. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9761. T_Loss: 3.4088. Mask: 0.9617. :  58%|█████▊    | 29/50 [00:16<00:10,  2.09it/s]Train Iter: 430/1000. LR: 0.0269. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9761. T_Loss: 3.4088. Mask: 0.9617. :  60%|██████    | 30/50 [00:16<00:13,  1.49it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9751. T_Loss: 3.4020. Mask: 0.9616. :  60%|██████    | 30/50 [00:16<00:13,  1.49it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9751. T_Loss: 3.4020. Mask: 0.9616. :  62%|██████▏   | 31/50 [00:16<00:11,  1.69it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9752. T_Loss: 3.4098. Mask: 0.9619. :  62%|██████▏   | 31/50 [00:16<00:11,  1.69it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9752. T_Loss: 3.4098. Mask: 0.9619. :  64%|██████▍   | 32/50 [00:16<00:09,  1.98it/s]total : 1000  current step :  430
total : 1000  current step :  431
total : 1000  current step :  432
Train Iter: 433/1000. LR: 0.0271. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9749. T_Loss: 3.4197. Mask: 0.9624. :  64%|██████▍   | 32/50 [00:17<00:09,  1.98it/s]Train Iter: 433/1000. LR: 0.0271. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9749. T_Loss: 3.4197. Mask: 0.9624. :  66%|██████▌   | 33/50 [00:17<00:11,  1.54it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.34s. Batch: 0.53s. S_Loss: 0.9756. T_Loss: 3.4333. Mask: 0.9624. :  66%|██████▌   | 33/50 [00:18<00:11,  1.54it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.34s. Batch: 0.53s. S_Loss: 0.9756. T_Loss: 3.4333. Mask: 0.9624. :  68%|██████▊   | 34/50 [00:18<00:08,  1.86it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9755. T_Loss: 3.4408. Mask: 0.9618. :  68%|██████▊   | 34/50 [00:18<00:08,  1.86it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9755. T_Loss: 3.4408. Mask: 0.9618. :  70%|███████   | 35/50 [00:18<00:07,  1.94it/s]total : 1000  current step :  433
total : 1000  current step :  434
total : 1000  current step :  435
Train Iter: 436/1000. LR: 0.0273. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9751. T_Loss: 3.4441. Mask: 0.9615. :  70%|███████   | 35/50 [00:19<00:07,  1.94it/s]Train Iter: 436/1000. LR: 0.0273. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9751. T_Loss: 3.4441. Mask: 0.9615. :  72%|███████▏  | 36/50 [00:19<00:09,  1.52it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9750. T_Loss: 3.4436. Mask: 0.9614. :  72%|███████▏  | 36/50 [00:20<00:09,  1.52it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9750. T_Loss: 3.4436. Mask: 0.9614. :  74%|███████▍  | 37/50 [00:20<00:07,  1.79it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9747. T_Loss: 3.4547. Mask: 0.9621. :  74%|███████▍  | 37/50 [00:20<00:07,  1.79it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9747. T_Loss: 3.4547. Mask: 0.9621. :  76%|███████▌  | 38/50 [00:20<00:06,  1.98it/s]total : 1000  current step :  436
total : 1000  current step :  437
total : 1000  current step :  438
Train Iter: 439/1000. LR: 0.0274. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9761. T_Loss: 3.4757. Mask: 0.9626. :  76%|███████▌  | 38/50 [00:21<00:06,  1.98it/s]Train Iter: 439/1000. LR: 0.0274. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9761. T_Loss: 3.4757. Mask: 0.9626. :  78%|███████▊  | 39/50 [00:21<00:07,  1.55it/s]total : 1000  current step :  439
Train Iter: 440/1000. LR: 0.0275. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9767. T_Loss: 3.4808. Mask: 0.9626. :  78%|███████▊  | 39/50 [00:22<00:07,  1.55it/s]Train Iter: 440/1000. LR: 0.0275. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9767. T_Loss: 3.4808. Mask: 0.9626. :  80%|████████  | 40/50 [00:22<00:06,  1.57it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9759. T_Loss: 3.4791. Mask: 0.9625. :  80%|████████  | 40/50 [00:22<00:06,  1.57it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9759. T_Loss: 3.4791. Mask: 0.9625. :  82%|████████▏ | 41/50 [00:22<00:06,  1.43it/s]total : 1000  current step :  440
total : 1000  current step :  441
Train Iter: 442/1000. LR: 0.0276. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9772. T_Loss: 3.4799. Mask: 0.9618. :  82%|████████▏ | 41/50 [00:23<00:06,  1.43it/s]Train Iter: 442/1000. LR: 0.0276. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9772. T_Loss: 3.4799. Mask: 0.9618. :  84%|████████▍ | 42/50 [00:23<00:06,  1.26it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9774. T_Loss: 3.4875. Mask: 0.9622. :  84%|████████▍ | 42/50 [00:24<00:06,  1.26it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9774. T_Loss: 3.4875. Mask: 0.9622. :  86%|████████▌ | 43/50 [00:24<00:04,  1.50it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9771. T_Loss: 3.4874. Mask: 0.9622. :  86%|████████▌ | 43/50 [00:24<00:04,  1.50it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9771. T_Loss: 3.4874. Mask: 0.9622. :  88%|████████▊ | 44/50 [00:24<00:03,  1.61it/s]total : 1000  current step :  442
total : 1000  current step :  443
total : 1000  current step :  444
Train Iter: 445/1000. LR: 0.0278. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9765. T_Loss: 3.4874. Mask: 0.9628. :  88%|████████▊ | 44/50 [00:25<00:03,  1.61it/s]Train Iter: 445/1000. LR: 0.0278. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9765. T_Loss: 3.4874. Mask: 0.9628. :  90%|█████████ | 45/50 [00:25<00:03,  1.28it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9763. T_Loss: 3.4900. Mask: 0.9627. :  90%|█████████ | 45/50 [00:26<00:03,  1.28it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9763. T_Loss: 3.4900. Mask: 0.9627. :  92%|█████████▏| 46/50 [00:26<00:02,  1.59it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9760. T_Loss: 3.4880. Mask: 0.9631. :  92%|█████████▏| 46/50 [00:26<00:02,  1.59it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9760. T_Loss: 3.4880. Mask: 0.9631. :  94%|█████████▍| 47/50 [00:26<00:01,  2.02it/s]total : 1000  current step :  445
total : 1000  current step :  446
total : 1000  current step :  447
Train Iter: 448/1000. LR: 0.0280. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9763. T_Loss: 3.4850. Mask: 0.9633. :  94%|█████████▍| 47/50 [00:27<00:01,  2.02it/s]Train Iter: 448/1000. LR: 0.0280. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9763. T_Loss: 3.4850. Mask: 0.9633. :  96%|█████████▌| 48/50 [00:27<00:01,  1.47it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9765. T_Loss: 3.4927. Mask: 0.9633. :  96%|█████████▌| 48/50 [00:27<00:01,  1.47it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9765. T_Loss: 3.4927. Mask: 0.9633. :  98%|█████████▊| 49/50 [00:27<00:00,  1.78it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9764. T_Loss: 3.5013. Mask: 0.9636. :  98%|█████████▊| 49/50 [00:27<00:00,  1.78it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9764. T_Loss: 3.5013. Mask: 0.9636. : 100%|██████████| 50/50 [00:27<00:00,  2.21it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9764. T_Loss: 3.5013. Mask: 0.9636. : 100%|██████████| 50/50 [00:27<00:00,  1.79it/s]
total : 1000  current step :  448
total : 1000  current step :  449
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.1427. top1: 93.75. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.1427. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.86it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.1339. top1: 94.34. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.86it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.1339. top1: 94.34. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.87it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1352. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.87it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1352. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.26it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1398. top1: 93.85. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.26it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1398. top1: 93.85. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.57it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1736. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.57it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.1736. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1915. top1: 88.67. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.1915. top1: 88.67. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.2058. top1: 86.77. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.2058. top1: 86.77. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.2151. top1: 86.05. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.2151. top1: 86.05. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.2151. top1: 86.05. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.50it/s]
total : 1000  current step :  450
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.74s. Batch: 0.99s. S_Loss: 0.9414. T_Loss: 3.1889. Mask: 0.9648. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.74s. Batch: 0.99s. S_Loss: 0.9414. T_Loss: 3.1889. Mask: 0.9648. :   2%|▏         | 1/50 [00:00<00:48,  1.01it/s]Train Iter: 452/1000. LR: 0.0282. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9634. T_Loss: 3.4858. Mask: 0.9668. :   2%|▏         | 1/50 [00:01<00:48,  1.01it/s]Train Iter: 452/1000. LR: 0.0282. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9634. T_Loss: 3.4858. Mask: 0.9668. :   4%|▍         | 2/50 [00:01<00:29,  1.62it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9612. T_Loss: 3.5312. Mask: 0.9648. :   4%|▍         | 2/50 [00:01<00:29,  1.62it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9612. T_Loss: 3.5312. Mask: 0.9648. :   6%|▌         | 3/50 [00:01<00:20,  2.30it/s]total : 1000  current step :  451
total : 1000  current step :  452
total : 1000  current step :  453
Train Iter: 454/1000. LR: 0.0284. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9518. T_Loss: 3.4770. Mask: 0.9658. :   6%|▌         | 3/50 [00:02<00:20,  2.30it/s]Train Iter: 454/1000. LR: 0.0284. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9518. T_Loss: 3.4770. Mask: 0.9658. :   8%|▊         | 4/50 [00:02<00:30,  1.53it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.33s. Batch: 0.57s. S_Loss: 0.9526. T_Loss: 3.5335. Mask: 0.9648. :   8%|▊         | 4/50 [00:02<00:30,  1.53it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.33s. Batch: 0.57s. S_Loss: 0.9526. T_Loss: 3.5335. Mask: 0.9648. :  10%|█         | 5/50 [00:02<00:23,  1.88it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9580. T_Loss: 3.5951. Mask: 0.9655. :  10%|█         | 5/50 [00:03<00:23,  1.88it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9580. T_Loss: 3.5951. Mask: 0.9655. :  12%|█▏        | 6/50 [00:03<00:19,  2.25it/s]total : 1000  current step :  454
total : 1000  current step :  455
total : 1000  current step :  456
Train Iter: 457/1000. LR: 0.0286. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9588. T_Loss: 3.5975. Mask: 0.9648. :  12%|█▏        | 6/50 [00:04<00:19,  2.25it/s]Train Iter: 457/1000. LR: 0.0286. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9588. T_Loss: 3.5975. Mask: 0.9648. :  14%|█▍        | 7/50 [00:04<00:27,  1.57it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9605. T_Loss: 3.5744. Mask: 0.9639. :  14%|█▍        | 7/50 [00:04<00:27,  1.57it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9605. T_Loss: 3.5744. Mask: 0.9639. :  16%|█▌        | 8/50 [00:04<00:21,  1.94it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9586. T_Loss: 3.6428. Mask: 0.9648. :  16%|█▌        | 8/50 [00:04<00:21,  1.94it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9586. T_Loss: 3.6428. Mask: 0.9648. :  18%|█▊        | 9/50 [00:04<00:17,  2.30it/s]total : 1000  current step :  457
total : 1000  current step :  458
total : 1000  current step :  459
Train Iter: 460/1000. LR: 0.0287. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9628. T_Loss: 3.6497. Mask: 0.9660. :  18%|█▊        | 9/50 [00:05<00:17,  2.30it/s]Train Iter: 460/1000. LR: 0.0287. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9628. T_Loss: 3.6497. Mask: 0.9660. :  20%|██        | 10/50 [00:05<00:22,  1.76it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9656. T_Loss: 3.7491. Mask: 0.9663. :  20%|██        | 10/50 [00:05<00:22,  1.76it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9656. T_Loss: 3.7491. Mask: 0.9663. :  22%|██▏       | 11/50 [00:05<00:18,  2.07it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9643. T_Loss: 3.7362. Mask: 0.9661. :  22%|██▏       | 11/50 [00:06<00:18,  2.07it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9643. T_Loss: 3.7362. Mask: 0.9661. :  24%|██▍       | 12/50 [00:06<00:16,  2.37it/s]total : 1000  current step :  460
total : 1000  current step :  461
total : 1000  current step :  462
Train Iter: 463/1000. LR: 0.0289. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9612. T_Loss: 3.7256. Mask: 0.9672. :  24%|██▍       | 12/50 [00:06<00:16,  2.37it/s]Train Iter: 463/1000. LR: 0.0289. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9612. T_Loss: 3.7256. Mask: 0.9672. :  26%|██▌       | 13/50 [00:06<00:19,  1.93it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9638. T_Loss: 3.7381. Mask: 0.9665. :  26%|██▌       | 13/50 [00:07<00:19,  1.93it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9638. T_Loss: 3.7381. Mask: 0.9665. :  28%|██▊       | 14/50 [00:07<00:16,  2.12it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9631. T_Loss: 3.7447. Mask: 0.9680. :  28%|██▊       | 14/50 [00:07<00:16,  2.12it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9631. T_Loss: 3.7447. Mask: 0.9680. :  30%|███       | 15/50 [00:07<00:15,  2.27it/s]total : 1000  current step :  463
total : 1000  current step :  464
total : 1000  current step :  465
Train Iter: 466/1000. LR: 0.0291. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9646. T_Loss: 3.7633. Mask: 0.9668. :  30%|███       | 15/50 [00:08<00:15,  2.27it/s]Train Iter: 466/1000. LR: 0.0291. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9646. T_Loss: 3.7633. Mask: 0.9668. :  32%|███▏      | 16/50 [00:08<00:18,  1.81it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9633. T_Loss: 3.7608. Mask: 0.9671. :  32%|███▏      | 16/50 [00:08<00:18,  1.81it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9633. T_Loss: 3.7608. Mask: 0.9671. :  34%|███▍      | 17/50 [00:08<00:15,  2.11it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9637. T_Loss: 3.7700. Mask: 0.9670. :  34%|███▍      | 17/50 [00:08<00:15,  2.11it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9637. T_Loss: 3.7700. Mask: 0.9670. :  36%|███▌      | 18/50 [00:08<00:13,  2.46it/s]total : 1000  current step :  466
total : 1000  current step :  467
total : 1000  current step :  468
Train Iter: 469/1000. LR: 0.0293. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9657. T_Loss: 3.7704. Mask: 0.9671. :  36%|███▌      | 18/50 [00:09<00:13,  2.46it/s]Train Iter: 469/1000. LR: 0.0293. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9657. T_Loss: 3.7704. Mask: 0.9671. :  38%|███▊      | 19/50 [00:09<00:18,  1.71it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9638. T_Loss: 3.7567. Mask: 0.9676. :  38%|███▊      | 19/50 [00:10<00:18,  1.71it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9638. T_Loss: 3.7567. Mask: 0.9676. :  40%|████      | 20/50 [00:10<00:13,  2.17it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9644. T_Loss: 3.7451. Mask: 0.9669. :  40%|████      | 20/50 [00:10<00:13,  2.17it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9644. T_Loss: 3.7451. Mask: 0.9669. :  42%|████▏     | 21/50 [00:10<00:11,  2.51it/s]total : 1000  current step :  469
total : 1000  current step :  470
total : 1000  current step :  471
Train Iter: 472/1000. LR: 0.0295. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9643. T_Loss: 3.7433. Mask: 0.9675. :  42%|████▏     | 21/50 [00:11<00:11,  2.51it/s]Train Iter: 472/1000. LR: 0.0295. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9643. T_Loss: 3.7433. Mask: 0.9675. :  44%|████▍     | 22/50 [00:11<00:16,  1.72it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9648. T_Loss: 3.7326. Mask: 0.9672. :  44%|████▍     | 22/50 [00:11<00:16,  1.72it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9648. T_Loss: 3.7326. Mask: 0.9672. :  46%|████▌     | 23/50 [00:11<00:13,  2.07it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9636. T_Loss: 3.7192. Mask: 0.9674. :  46%|████▌     | 23/50 [00:11<00:13,  2.07it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9636. T_Loss: 3.7192. Mask: 0.9674. :  48%|████▊     | 24/50 [00:11<00:11,  2.34it/s]total : 1000  current step :  472
total : 1000  current step :  473
total : 1000  current step :  474
Train Iter: 475/1000. LR: 0.0297. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9624. T_Loss: 3.7094. Mask: 0.9670. :  48%|████▊     | 24/50 [00:12<00:11,  2.34it/s]Train Iter: 475/1000. LR: 0.0297. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9624. T_Loss: 3.7094. Mask: 0.9670. :  50%|█████     | 25/50 [00:12<00:13,  1.80it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9613. T_Loss: 3.7027. Mask: 0.9677. :  50%|█████     | 25/50 [00:13<00:13,  1.80it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9613. T_Loss: 3.7027. Mask: 0.9677. :  52%|█████▏    | 26/50 [00:13<00:11,  2.10it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9615. T_Loss: 3.7043. Mask: 0.9683. :  52%|█████▏    | 26/50 [00:13<00:11,  2.10it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9615. T_Loss: 3.7043. Mask: 0.9683. :  54%|█████▍    | 27/50 [00:13<00:09,  2.45it/s]total : 1000  current step :  475
total : 1000  current step :  476
total : 1000  current step :  477
Train Iter: 478/1000. LR: 0.0299. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9607. T_Loss: 3.7103. Mask: 0.9686. :  54%|█████▍    | 27/50 [00:14<00:09,  2.45it/s]Train Iter: 478/1000. LR: 0.0299. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9607. T_Loss: 3.7103. Mask: 0.9686. :  56%|█████▌    | 28/50 [00:14<00:12,  1.73it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9594. T_Loss: 3.6936. Mask: 0.9682. :  56%|█████▌    | 28/50 [00:14<00:12,  1.73it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9594. T_Loss: 3.6936. Mask: 0.9682. :  58%|█████▊    | 29/50 [00:14<00:10,  2.01it/s]total : 1000  current step :  478
total : 1000  current step :  479
Train Iter: 480/1000. LR: 0.0300. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9583. T_Loss: 3.6838. Mask: 0.9684. :  58%|█████▊    | 29/50 [00:14<00:10,  2.01it/s]Train Iter: 480/1000. LR: 0.0300. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9583. T_Loss: 3.6838. Mask: 0.9684. :  60%|██████    | 30/50 [00:14<00:09,  2.19it/s]total : 1000  current step :  480
Train Iter: 481/1000. LR: 0.0301. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9566. T_Loss: 3.6706. Mask: 0.9684. :  60%|██████    | 30/50 [00:15<00:09,  2.19it/s]Train Iter: 481/1000. LR: 0.0301. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9566. T_Loss: 3.6706. Mask: 0.9684. :  62%|██████▏   | 31/50 [00:15<00:11,  1.72it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9548. T_Loss: 3.6580. Mask: 0.9684. :  62%|██████▏   | 31/50 [00:16<00:11,  1.72it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9548. T_Loss: 3.6580. Mask: 0.9684. :  64%|██████▍   | 32/50 [00:16<00:08,  2.07it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9534. T_Loss: 3.6552. Mask: 0.9689. :  64%|██████▍   | 32/50 [00:16<00:08,  2.07it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9534. T_Loss: 3.6552. Mask: 0.9689. :  66%|██████▌   | 33/50 [00:16<00:06,  2.49it/s]total : 1000  current step :  481
total : 1000  current step :  482
total : 1000  current step :  483
Train Iter: 484/1000. LR: 0.0302. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9533. T_Loss: 3.6605. Mask: 0.9684. :  66%|██████▌   | 33/50 [00:17<00:06,  2.49it/s]Train Iter: 484/1000. LR: 0.0302. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9533. T_Loss: 3.6605. Mask: 0.9684. :  68%|██████▊   | 34/50 [00:17<00:08,  1.83it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9535. T_Loss: 3.6665. Mask: 0.9683. :  68%|██████▊   | 34/50 [00:17<00:08,  1.83it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9535. T_Loss: 3.6665. Mask: 0.9683. :  70%|███████   | 35/50 [00:17<00:06,  2.23it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9531. T_Loss: 3.6879. Mask: 0.9684. :  70%|███████   | 35/50 [00:17<00:06,  2.23it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9531. T_Loss: 3.6879. Mask: 0.9684. :  72%|███████▏  | 36/50 [00:17<00:05,  2.56it/s]total : 1000  current step :  484
total : 1000  current step :  485
total : 1000  current step :  486
Train Iter: 487/1000. LR: 0.0304. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9520. T_Loss: 3.7067. Mask: 0.9686. :  72%|███████▏  | 36/50 [00:18<00:05,  2.56it/s]Train Iter: 487/1000. LR: 0.0304. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9520. T_Loss: 3.7067. Mask: 0.9686. :  74%|███████▍  | 37/50 [00:18<00:06,  2.01it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9520. T_Loss: 3.7177. Mask: 0.9688. :  74%|███████▍  | 37/50 [00:18<00:06,  2.01it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9520. T_Loss: 3.7177. Mask: 0.9688. :  76%|███████▌  | 38/50 [00:18<00:05,  2.28it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9523. T_Loss: 3.7408. Mask: 0.9683. :  76%|███████▌  | 38/50 [00:19<00:05,  2.28it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9523. T_Loss: 3.7408. Mask: 0.9683. :  78%|███████▊  | 39/50 [00:19<00:04,  2.41it/s]total : 1000  current step :  487
total : 1000  current step :  488
total : 1000  current step :  489
Train Iter: 490/1000. LR: 0.0306. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9533. T_Loss: 3.7564. Mask: 0.9679. :  78%|███████▊  | 39/50 [00:19<00:04,  2.41it/s]Train Iter: 490/1000. LR: 0.0306. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9533. T_Loss: 3.7564. Mask: 0.9679. :  80%|████████  | 40/50 [00:19<00:05,  1.89it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9529. T_Loss: 3.7530. Mask: 0.9679. :  80%|████████  | 40/50 [00:20<00:05,  1.89it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9529. T_Loss: 3.7530. Mask: 0.9679. :  82%|████████▏ | 41/50 [00:20<00:04,  2.16it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9533. T_Loss: 3.7498. Mask: 0.9680. :  82%|████████▏ | 41/50 [00:20<00:04,  2.16it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9533. T_Loss: 3.7498. Mask: 0.9680. :  84%|████████▍ | 42/50 [00:20<00:03,  2.43it/s]total : 1000  current step :  490
total : 1000  current step :  491
total : 1000  current step :  492
Train Iter: 493/1000. LR: 0.0308. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9545. T_Loss: 3.7699. Mask: 0.9682. :  84%|████████▍ | 42/50 [00:21<00:03,  2.43it/s]Train Iter: 493/1000. LR: 0.0308. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9545. T_Loss: 3.7699. Mask: 0.9682. :  86%|████████▌ | 43/50 [00:21<00:03,  1.84it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9555. T_Loss: 3.7734. Mask: 0.9679. :  86%|████████▌ | 43/50 [00:21<00:03,  1.84it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9555. T_Loss: 3.7734. Mask: 0.9679. :  88%|████████▊ | 44/50 [00:21<00:02,  2.06it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.26s. Batch: 0.48s. S_Loss: 0.9559. T_Loss: 3.7836. Mask: 0.9677. :  88%|████████▊ | 44/50 [00:21<00:02,  2.06it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.26s. Batch: 0.48s. S_Loss: 0.9559. T_Loss: 3.7836. Mask: 0.9677. :  90%|█████████ | 45/50 [00:21<00:01,  2.50it/s]total : 1000  current step :  493
total : 1000  current step :  494
total : 1000  current step :  495
Train Iter: 496/1000. LR: 0.0310. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9561. T_Loss: 3.7817. Mask: 0.9671. :  90%|█████████ | 45/50 [00:22<00:01,  2.50it/s]Train Iter: 496/1000. LR: 0.0310. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9561. T_Loss: 3.7817. Mask: 0.9671. :  92%|█████████▏| 46/50 [00:22<00:02,  1.64it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9564. T_Loss: 3.7827. Mask: 0.9670. :  92%|█████████▏| 46/50 [00:23<00:02,  1.64it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9564. T_Loss: 3.7827. Mask: 0.9670. :  94%|█████████▍| 47/50 [00:23<00:01,  1.94it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9565. T_Loss: 3.7788. Mask: 0.9666. :  94%|█████████▍| 47/50 [00:23<00:01,  1.94it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9565. T_Loss: 3.7788. Mask: 0.9666. :  96%|█████████▌| 48/50 [00:23<00:00,  2.37it/s]total : 1000  current step :  496
total : 1000  current step :  497
total : 1000  current step :  498
Train Iter: 499/1000. LR: 0.0312. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9563. T_Loss: 3.7732. Mask: 0.9665. :  96%|█████████▌| 48/50 [00:24<00:00,  2.37it/s]Train Iter: 499/1000. LR: 0.0312. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9563. T_Loss: 3.7732. Mask: 0.9665. :  98%|█████████▊| 49/50 [00:24<00:00,  1.61it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9562. T_Loss: 3.7723. Mask: 0.9665. :  98%|█████████▊| 49/50 [00:24<00:00,  1.61it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9562. T_Loss: 3.7723. Mask: 0.9665. : 100%|██████████| 50/50 [00:24<00:00,  2.07it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9562. T_Loss: 3.7723. Mask: 0.9665. : 100%|██████████| 50/50 [00:24<00:00,  2.02it/s]
total : 1000  current step :  499
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.0559. top1: 93.36. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.0559. top1: 93.36. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0442. top1: 93.95. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0442. top1: 93.95. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.84it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0484. top1: 94.01. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.84it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0484. top1: 94.01. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0526. top1: 93.46. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0526. top1: 93.46. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0804. top1: 91.25. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0804. top1: 91.25. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  4.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0935. top1: 90.10. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  4.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0935. top1: 90.10. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1060. top1: 88.67. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.1060. top1: 88.67. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1124. top1: 88.10. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1124. top1: 88.10. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.1124. top1: 88.10. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.58it/s]
total : 1000  current step :  500
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.18s. S_Loss: 0.9017. T_Loss: 3.5525. Mask: 0.9688. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.18s. S_Loss: 0.9017. T_Loss: 3.5525. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:08,  5.51it/s]total : 1000  current step :  501
Train Iter: 502/1000. LR: 0.0314. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9271. T_Loss: 3.6357. Mask: 0.9668. :   2%|▏         | 1/50 [00:01<00:08,  5.51it/s]Train Iter: 502/1000. LR: 0.0314. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9271. T_Loss: 3.6357. Mask: 0.9668. :   4%|▍         | 2/50 [00:01<00:32,  1.46it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9409. T_Loss: 3.6424. Mask: 0.9635. :   4%|▍         | 2/50 [00:01<00:32,  1.46it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.27s. Batch: 0.49s. S_Loss: 0.9409. T_Loss: 3.6424. Mask: 0.9635. :   6%|▌         | 3/50 [00:01<00:23,  2.03it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.21s. Batch: 0.42s. S_Loss: 0.9392. T_Loss: 3.6513. Mask: 0.9639. :   6%|▌         | 3/50 [00:01<00:23,  2.03it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.21s. Batch: 0.42s. S_Loss: 0.9392. T_Loss: 3.6513. Mask: 0.9639. :   8%|▊         | 4/50 [00:01<00:17,  2.68it/s]total : 1000  current step :  502
total : 1000  current step :  503
total : 1000  current step :  504
Train Iter: 505/1000. LR: 0.0316. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9485. T_Loss: 3.7470. Mask: 0.9656. :   8%|▊         | 4/50 [00:02<00:17,  2.68it/s]Train Iter: 505/1000. LR: 0.0316. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9485. T_Loss: 3.7470. Mask: 0.9656. :  10%|█         | 5/50 [00:02<00:27,  1.66it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9477. T_Loss: 3.7051. Mask: 0.9668. :  10%|█         | 5/50 [00:02<00:27,  1.66it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9477. T_Loss: 3.7051. Mask: 0.9668. :  12%|█▏        | 6/50 [00:02<00:22,  1.99it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.23s. Batch: 0.45s. S_Loss: 0.9520. T_Loss: 3.7654. Mask: 0.9660. :  12%|█▏        | 6/50 [00:03<00:22,  1.99it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.23s. Batch: 0.45s. S_Loss: 0.9520. T_Loss: 3.7654. Mask: 0.9660. :  14%|█▍        | 7/50 [00:03<00:16,  2.60it/s]total : 1000  current step :  505
total : 1000  current step :  506
total : 1000  current step :  507
Train Iter: 508/1000. LR: 0.0318. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9483. T_Loss: 3.7642. Mask: 0.9668. :  14%|█▍        | 7/50 [00:04<00:16,  2.60it/s]Train Iter: 508/1000. LR: 0.0318. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9483. T_Loss: 3.7642. Mask: 0.9668. :  16%|█▌        | 8/50 [00:04<00:23,  1.81it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.25s. Batch: 0.48s. S_Loss: 0.9514. T_Loss: 3.7753. Mask: 0.9657. :  16%|█▌        | 8/50 [00:04<00:23,  1.81it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.25s. Batch: 0.48s. S_Loss: 0.9514. T_Loss: 3.7753. Mask: 0.9657. :  18%|█▊        | 9/50 [00:04<00:19,  2.13it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.23s. Batch: 0.46s. S_Loss: 0.9475. T_Loss: 3.7791. Mask: 0.9660. :  18%|█▊        | 9/50 [00:04<00:19,  2.13it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.23s. Batch: 0.46s. S_Loss: 0.9475. T_Loss: 3.7791. Mask: 0.9660. :  20%|██        | 10/50 [00:04<00:15,  2.51it/s]total : 1000  current step :  508
total : 1000  current step :  509
total : 1000  current step :  510
Train Iter: 511/1000. LR: 0.0319. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9489. T_Loss: 3.8269. Mask: 0.9677. :  20%|██        | 10/50 [00:05<00:15,  2.51it/s]Train Iter: 511/1000. LR: 0.0319. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9489. T_Loss: 3.8269. Mask: 0.9677. :  22%|██▏       | 11/50 [00:05<00:22,  1.71it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9497. T_Loss: 3.8408. Mask: 0.9661. :  22%|██▏       | 11/50 [00:05<00:22,  1.71it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9497. T_Loss: 3.8408. Mask: 0.9661. :  24%|██▍       | 12/50 [00:05<00:19,  1.99it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.24s. Batch: 0.48s. S_Loss: 0.9505. T_Loss: 3.8817. Mask: 0.9663. :  24%|██▍       | 12/50 [00:06<00:19,  1.99it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.24s. Batch: 0.48s. S_Loss: 0.9505. T_Loss: 3.8817. Mask: 0.9663. :  26%|██▌       | 13/50 [00:06<00:16,  2.25it/s]total : 1000  current step :  511
total : 1000  current step :  512
total : 1000  current step :  513
Train Iter: 514/1000. LR: 0.0321. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9553. T_Loss: 3.9132. Mask: 0.9651. :  26%|██▌       | 13/50 [00:07<00:16,  2.25it/s]Train Iter: 514/1000. LR: 0.0321. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9553. T_Loss: 3.9132. Mask: 0.9651. :  28%|██▊       | 14/50 [00:07<00:21,  1.69it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9547. T_Loss: 3.8966. Mask: 0.9648. :  28%|██▊       | 14/50 [00:07<00:21,  1.69it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9547. T_Loss: 3.8966. Mask: 0.9648. :  30%|███       | 15/50 [00:07<00:17,  2.01it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.24s. Batch: 0.48s. S_Loss: 0.9554. T_Loss: 3.8943. Mask: 0.9646. :  30%|███       | 15/50 [00:07<00:17,  2.01it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.24s. Batch: 0.48s. S_Loss: 0.9554. T_Loss: 3.8943. Mask: 0.9646. :  32%|███▏      | 16/50 [00:07<00:14,  2.28it/s]total : 1000  current step :  514
total : 1000  current step :  515
total : 1000  current step :  516
Train Iter: 517/1000. LR: 0.0323. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9537. T_Loss: 3.8881. Mask: 0.9642. :  32%|███▏      | 16/50 [00:08<00:14,  2.28it/s]Train Iter: 517/1000. LR: 0.0323. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9537. T_Loss: 3.8881. Mask: 0.9642. :  34%|███▍      | 17/50 [00:08<00:19,  1.68it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9542. T_Loss: 3.8729. Mask: 0.9640. :  34%|███▍      | 17/50 [00:08<00:19,  1.68it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9542. T_Loss: 3.8729. Mask: 0.9640. :  36%|███▌      | 18/50 [00:08<00:16,  1.97it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9534. T_Loss: 3.8783. Mask: 0.9648. :  36%|███▌      | 18/50 [00:09<00:16,  1.97it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9534. T_Loss: 3.8783. Mask: 0.9648. :  38%|███▊      | 19/50 [00:09<00:14,  2.17it/s]total : 1000  current step :  517
total : 1000  current step :  518
total : 1000  current step :  519
Train Iter: 520/1000. LR: 0.0325. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9516. T_Loss: 3.8579. Mask: 0.9658. :  38%|███▊      | 19/50 [00:10<00:14,  2.17it/s]Train Iter: 520/1000. LR: 0.0325. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9516. T_Loss: 3.8579. Mask: 0.9658. :  40%|████      | 20/50 [00:10<00:21,  1.41it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9514. T_Loss: 3.8442. Mask: 0.9667. :  40%|████      | 20/50 [00:11<00:21,  1.41it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9514. T_Loss: 3.8442. Mask: 0.9667. :  42%|████▏     | 21/50 [00:11<00:18,  1.54it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9521. T_Loss: 3.8416. Mask: 0.9675. :  42%|████▏     | 21/50 [00:11<00:18,  1.54it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9521. T_Loss: 3.8416. Mask: 0.9675. :  44%|████▍     | 22/50 [00:11<00:15,  1.76it/s]total : 1000  current step :  520
total : 1000  current step :  521
total : 1000  current step :  522
Train Iter: 523/1000. LR: 0.0327. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9518. T_Loss: 3.8541. Mask: 0.9679. :  44%|████▍     | 22/50 [00:12<00:15,  1.76it/s]Train Iter: 523/1000. LR: 0.0327. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9518. T_Loss: 3.8541. Mask: 0.9679. :  46%|████▌     | 23/50 [00:12<00:17,  1.51it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9518. T_Loss: 3.8550. Mask: 0.9684. :  46%|████▌     | 23/50 [00:12<00:17,  1.51it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9518. T_Loss: 3.8550. Mask: 0.9684. :  48%|████▊     | 24/50 [00:12<00:13,  1.88it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9503. T_Loss: 3.8358. Mask: 0.9689. :  48%|████▊     | 24/50 [00:12<00:13,  1.88it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9503. T_Loss: 3.8358. Mask: 0.9689. :  50%|█████     | 25/50 [00:12<00:11,  2.25it/s]total : 1000  current step :  523
total : 1000  current step :  524
total : 1000  current step :  525
Train Iter: 526/1000. LR: 0.0329. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9496. T_Loss: 3.8252. Mask: 0.9684. :  50%|█████     | 25/50 [00:13<00:11,  2.25it/s]Train Iter: 526/1000. LR: 0.0329. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9496. T_Loss: 3.8252. Mask: 0.9684. :  52%|█████▏    | 26/50 [00:13<00:13,  1.80it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9489. T_Loss: 3.8069. Mask: 0.9682. :  52%|█████▏    | 26/50 [00:13<00:13,  1.80it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9489. T_Loss: 3.8069. Mask: 0.9682. :  54%|█████▍    | 27/50 [00:13<00:10,  2.21it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9477. T_Loss: 3.7972. Mask: 0.9679. :  54%|█████▍    | 27/50 [00:14<00:10,  2.21it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9477. T_Loss: 3.7972. Mask: 0.9679. :  56%|█████▌    | 28/50 [00:14<00:08,  2.59it/s]total : 1000  current step :  526
total : 1000  current step :  527
total : 1000  current step :  528
Train Iter: 529/1000. LR: 0.0331. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9475. T_Loss: 3.8019. Mask: 0.9671. :  56%|█████▌    | 28/50 [00:14<00:08,  2.59it/s]Train Iter: 529/1000. LR: 0.0331. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9475. T_Loss: 3.8019. Mask: 0.9671. :  58%|█████▊    | 29/50 [00:14<00:11,  1.88it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9483. T_Loss: 3.8037. Mask: 0.9674. :  58%|█████▊    | 29/50 [00:15<00:11,  1.88it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9483. T_Loss: 3.8037. Mask: 0.9674. :  60%|██████    | 30/50 [00:15<00:08,  2.23it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9486. T_Loss: 3.8022. Mask: 0.9677. :  60%|██████    | 30/50 [00:15<00:08,  2.23it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9486. T_Loss: 3.8022. Mask: 0.9677. :  62%|██████▏   | 31/50 [00:15<00:07,  2.47it/s]total : 1000  current step :  529
total : 1000  current step :  530
total : 1000  current step :  531
Train Iter: 532/1000. LR: 0.0333. Data: 0.30s. Batch: 0.51s. S_Loss: 0.9495. T_Loss: 3.8065. Mask: 0.9675. :  62%|██████▏   | 31/50 [00:16<00:07,  2.47it/s]Train Iter: 532/1000. LR: 0.0333. Data: 0.30s. Batch: 0.51s. S_Loss: 0.9495. T_Loss: 3.8065. Mask: 0.9675. :  64%|██████▍   | 32/50 [00:16<00:10,  1.79it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9489. T_Loss: 3.8009. Mask: 0.9677. :  64%|██████▍   | 32/50 [00:16<00:10,  1.79it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9489. T_Loss: 3.8009. Mask: 0.9677. :  66%|██████▌   | 33/50 [00:16<00:07,  2.21it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9482. T_Loss: 3.7890. Mask: 0.9677. :  66%|██████▌   | 33/50 [00:16<00:07,  2.21it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9482. T_Loss: 3.7890. Mask: 0.9677. :  68%|██████▊   | 34/50 [00:16<00:06,  2.42it/s]total : 1000  current step :  532
total : 1000  current step :  533
total : 1000  current step :  534
Train Iter: 535/1000. LR: 0.0334. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9485. T_Loss: 3.7916. Mask: 0.9679. :  68%|██████▊   | 34/50 [00:17<00:06,  2.42it/s]Train Iter: 535/1000. LR: 0.0334. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9485. T_Loss: 3.7916. Mask: 0.9679. :  70%|███████   | 35/50 [00:17<00:08,  1.82it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9477. T_Loss: 3.7974. Mask: 0.9686. :  70%|███████   | 35/50 [00:18<00:08,  1.82it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9477. T_Loss: 3.7974. Mask: 0.9686. :  72%|███████▏  | 36/50 [00:18<00:06,  2.27it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9472. T_Loss: 3.7903. Mask: 0.9691. :  72%|███████▏  | 36/50 [00:18<00:06,  2.27it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9472. T_Loss: 3.7903. Mask: 0.9691. :  74%|███████▍  | 37/50 [00:18<00:04,  2.61it/s]total : 1000  current step :  535
total : 1000  current step :  536
total : 1000  current step :  537
Train Iter: 538/1000. LR: 0.0336. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9474. T_Loss: 3.7939. Mask: 0.9691. :  74%|███████▍  | 37/50 [00:19<00:04,  2.61it/s]Train Iter: 538/1000. LR: 0.0336. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9474. T_Loss: 3.7939. Mask: 0.9691. :  76%|███████▌  | 38/50 [00:19<00:06,  1.79it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9470. T_Loss: 3.7898. Mask: 0.9688. :  76%|███████▌  | 38/50 [00:19<00:06,  1.79it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9470. T_Loss: 3.7898. Mask: 0.9688. :  78%|███████▊  | 39/50 [00:19<00:04,  2.21it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9468. T_Loss: 3.7841. Mask: 0.9686. :  78%|███████▊  | 39/50 [00:19<00:04,  2.21it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9468. T_Loss: 3.7841. Mask: 0.9686. :  80%|████████  | 40/50 [00:19<00:04,  2.46it/s]total : 1000  current step :  538
total : 1000  current step :  539
total : 1000  current step :  540
Train Iter: 541/1000. LR: 0.0338. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9466. T_Loss: 3.7853. Mask: 0.9686. :  80%|████████  | 40/50 [00:20<00:04,  2.46it/s]Train Iter: 541/1000. LR: 0.0338. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9466. T_Loss: 3.7853. Mask: 0.9686. :  82%|████████▏ | 41/50 [00:20<00:05,  1.80it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9467. T_Loss: 3.7873. Mask: 0.9685. :  82%|████████▏ | 41/50 [00:20<00:05,  1.80it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9467. T_Loss: 3.7873. Mask: 0.9685. :  84%|████████▍ | 42/50 [00:20<00:03,  2.14it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9466. T_Loss: 3.7889. Mask: 0.9687. :  84%|████████▍ | 42/50 [00:21<00:03,  2.14it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9466. T_Loss: 3.7889. Mask: 0.9687. :  86%|████████▌ | 43/50 [00:21<00:02,  2.40it/s]total : 1000  current step :  541
total : 1000  current step :  542
total : 1000  current step :  543
Train Iter: 544/1000. LR: 0.0340. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9470. T_Loss: 3.7845. Mask: 0.9685. :  86%|████████▌ | 43/50 [00:22<00:02,  2.40it/s]Train Iter: 544/1000. LR: 0.0340. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9470. T_Loss: 3.7845. Mask: 0.9685. :  88%|████████▊ | 44/50 [00:22<00:03,  1.78it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9475. T_Loss: 3.7828. Mask: 0.9685. :  88%|████████▊ | 44/50 [00:22<00:03,  1.78it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9475. T_Loss: 3.7828. Mask: 0.9685. :  90%|█████████ | 45/50 [00:22<00:02,  2.04it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9472. T_Loss: 3.7744. Mask: 0.9684. :  90%|█████████ | 45/50 [00:22<00:02,  2.04it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9472. T_Loss: 3.7744. Mask: 0.9684. :  92%|█████████▏| 46/50 [00:22<00:01,  2.21it/s]total : 1000  current step :  544
total : 1000  current step :  545
total : 1000  current step :  546
Train Iter: 547/1000. LR: 0.0342. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9473. T_Loss: 3.7786. Mask: 0.9683. :  92%|█████████▏| 46/50 [00:23<00:01,  2.21it/s]Train Iter: 547/1000. LR: 0.0342. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9473. T_Loss: 3.7786. Mask: 0.9683. :  94%|█████████▍| 47/50 [00:23<00:01,  1.69it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9465. T_Loss: 3.7769. Mask: 0.9684. :  94%|█████████▍| 47/50 [00:24<00:01,  1.69it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.28s. Batch: 0.50s. S_Loss: 0.9465. T_Loss: 3.7769. Mask: 0.9684. :  96%|█████████▌| 48/50 [00:24<00:01,  1.98it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9464. T_Loss: 3.7794. Mask: 0.9685. :  96%|█████████▌| 48/50 [00:24<00:01,  1.98it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9464. T_Loss: 3.7794. Mask: 0.9685. :  98%|█████████▊| 49/50 [00:24<00:00,  2.29it/s]total : 1000  current step :  547
total : 1000  current step :  548
total : 1000  current step :  549
Train Iter: 550/1000. LR: 0.0344. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9459. T_Loss: 3.7697. Mask: 0.9684. :  98%|█████████▊| 49/50 [00:25<00:00,  2.29it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9459. T_Loss: 3.7697. Mask: 0.9684. : 100%|██████████| 50/50 [00:25<00:00,  1.75it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.29s. Batch: 0.50s. S_Loss: 0.9459. T_Loss: 3.7697. Mask: 0.9684. : 100%|██████████| 50/50 [00:25<00:00,  1.98it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.0008. top1: 93.75. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.0008. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.91it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9893. top1: 94.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.91it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9893. top1: 94.73. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9951. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9951. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9997. top1: 93.65. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9997. top1: 93.65. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0246. top1: 91.33. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0246. top1: 91.33. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0362. top1: 90.36. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 1.0362. top1: 90.36. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0485. top1: 89.23. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 1.0485. top1: 89.23. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0540. top1: 88.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0540. top1: 88.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 1.0540. top1: 88.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]
total : 1000  current step :  550
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.17s. S_Loss: 0.9308. T_Loss: 3.2696. Mask: 0.9609. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.17s. S_Loss: 0.9308. T_Loss: 3.2696. Mask: 0.9609. :   2%|▏         | 1/50 [00:00<00:08,  5.87it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.16s. S_Loss: 0.9720. T_Loss: 3.8311. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:08,  5.87it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.16s. S_Loss: 0.9720. T_Loss: 3.8311. Mask: 0.9688. :   4%|▍         | 2/50 [00:00<00:07,  6.28it/s]total : 1000  current step :  551
total : 1000  current step :  552
Train Iter: 553/1000. LR: 0.0346. Data: 0.21s. Batch: 0.36s. S_Loss: 0.9635. T_Loss: 3.8363. Mask: 0.9570. :   4%|▍         | 2/50 [00:01<00:07,  6.28it/s]Train Iter: 553/1000. LR: 0.0346. Data: 0.21s. Batch: 0.36s. S_Loss: 0.9635. T_Loss: 3.8363. Mask: 0.9570. :   6%|▌         | 3/50 [00:01<00:20,  2.27it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.18s. Batch: 0.33s. S_Loss: 0.9525. T_Loss: 3.8422. Mask: 0.9619. :   6%|▌         | 3/50 [00:01<00:20,  2.27it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.18s. Batch: 0.33s. S_Loss: 0.9525. T_Loss: 3.8422. Mask: 0.9619. :   8%|▊         | 4/50 [00:01<00:16,  2.75it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.17s. Batch: 0.33s. S_Loss: 0.9530. T_Loss: 3.9632. Mask: 0.9656. :   8%|▊         | 4/50 [00:01<00:16,  2.75it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.17s. Batch: 0.33s. S_Loss: 0.9530. T_Loss: 3.9632. Mask: 0.9656. :  10%|█         | 5/50 [00:01<00:15,  2.91it/s]total : 1000  current step :  553
total : 1000  current step :  554
total : 1000  current step :  555
Train Iter: 556/1000. LR: 0.0347. Data: 0.25s. Batch: 0.41s. S_Loss: 0.9498. T_Loss: 3.9221. Mask: 0.9694. :  10%|█         | 5/50 [00:02<00:15,  2.91it/s]Train Iter: 556/1000. LR: 0.0347. Data: 0.25s. Batch: 0.41s. S_Loss: 0.9498. T_Loss: 3.9221. Mask: 0.9694. :  12%|█▏        | 6/50 [00:02<00:21,  2.01it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.23s. Batch: 0.40s. S_Loss: 0.9505. T_Loss: 3.8915. Mask: 0.9671. :  12%|█▏        | 6/50 [00:02<00:21,  2.01it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.23s. Batch: 0.40s. S_Loss: 0.9505. T_Loss: 3.8915. Mask: 0.9671. :  14%|█▍        | 7/50 [00:02<00:19,  2.24it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.22s. Batch: 0.40s. S_Loss: 0.9521. T_Loss: 3.8611. Mask: 0.9658. :  14%|█▍        | 7/50 [00:03<00:19,  2.24it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.22s. Batch: 0.40s. S_Loss: 0.9521. T_Loss: 3.8611. Mask: 0.9658. :  16%|█▌        | 8/50 [00:03<00:18,  2.32it/s]total : 1000  current step :  556
total : 1000  current step :  557
total : 1000  current step :  558
Train Iter: 559/1000. LR: 0.0349. Data: 0.27s. Batch: 0.45s. S_Loss: 0.9516. T_Loss: 3.8429. Mask: 0.9657. :  16%|█▌        | 8/50 [00:04<00:18,  2.32it/s]Train Iter: 559/1000. LR: 0.0349. Data: 0.27s. Batch: 0.45s. S_Loss: 0.9516. T_Loss: 3.8429. Mask: 0.9657. :  18%|█▊        | 9/50 [00:04<00:23,  1.75it/s]total : 1000  current step :  559
Train Iter: 560/1000. LR: 0.0350. Data: 0.29s. Batch: 0.48s. S_Loss: 0.9485. T_Loss: 3.7922. Mask: 0.9648. :  18%|█▊        | 9/50 [00:04<00:23,  1.75it/s]Train Iter: 560/1000. LR: 0.0350. Data: 0.29s. Batch: 0.48s. S_Loss: 0.9485. T_Loss: 3.7922. Mask: 0.9648. :  20%|██        | 10/50 [00:04<00:25,  1.59it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.29s. Batch: 0.48s. S_Loss: 0.9499. T_Loss: 3.7596. Mask: 0.9648. :  20%|██        | 10/50 [00:05<00:25,  1.59it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.29s. Batch: 0.48s. S_Loss: 0.9499. T_Loss: 3.7596. Mask: 0.9648. :  22%|██▏       | 11/50 [00:05<00:23,  1.69it/s]total : 1000  current step :  560
total : 1000  current step :  561
Train Iter: 562/1000. LR: 0.0351. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9495. T_Loss: 3.7401. Mask: 0.9652. :  22%|██▏       | 11/50 [00:06<00:23,  1.69it/s]Train Iter: 562/1000. LR: 0.0351. Data: 0.33s. Batch: 0.52s. S_Loss: 0.9495. T_Loss: 3.7401. Mask: 0.9652. :  24%|██▍       | 12/50 [00:06<00:26,  1.46it/s]Train Iter: 563/1000. LR: 0.0352. Data: 0.31s. Batch: 0.50s. S_Loss: 0.9480. T_Loss: 3.7524. Mask: 0.9669. :  24%|██▍       | 12/50 [00:06<00:26,  1.46it/s]Train Iter: 563/1000. LR: 0.0352. Data: 0.31s. Batch: 0.50s. S_Loss: 0.9480. T_Loss: 3.7524. Mask: 0.9669. :  26%|██▌       | 13/50 [00:06<00:20,  1.77it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.29s. Batch: 0.49s. S_Loss: 0.9545. T_Loss: 3.7996. Mask: 0.9665. :  26%|██▌       | 13/50 [00:06<00:20,  1.77it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.29s. Batch: 0.49s. S_Loss: 0.9545. T_Loss: 3.7996. Mask: 0.9665. :  28%|██▊       | 14/50 [00:06<00:17,  2.04it/s]total : 1000  current step :  562
total : 1000  current step :  563
total : 1000  current step :  564
Train Iter: 565/1000. LR: 0.0353. Data: 0.32s. Batch: 0.51s. S_Loss: 0.9566. T_Loss: 3.8234. Mask: 0.9659. :  28%|██▊       | 14/50 [00:07<00:17,  2.04it/s]Train Iter: 565/1000. LR: 0.0353. Data: 0.32s. Batch: 0.51s. S_Loss: 0.9566. T_Loss: 3.8234. Mask: 0.9659. :  30%|███       | 15/50 [00:07<00:21,  1.64it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.31s. Batch: 0.50s. S_Loss: 0.9560. T_Loss: 3.8416. Mask: 0.9658. :  30%|███       | 15/50 [00:08<00:21,  1.64it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.31s. Batch: 0.50s. S_Loss: 0.9560. T_Loss: 3.8416. Mask: 0.9658. :  32%|███▏      | 16/50 [00:08<00:17,  1.91it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.30s. Batch: 0.50s. S_Loss: 0.9573. T_Loss: 3.8392. Mask: 0.9642. :  32%|███▏      | 16/50 [00:08<00:17,  1.91it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.30s. Batch: 0.50s. S_Loss: 0.9573. T_Loss: 3.8392. Mask: 0.9642. :  34%|███▍      | 17/50 [00:08<00:17,  1.93it/s]total : 1000  current step :  565
total : 1000  current step :  566
total : 1000  current step :  567
Train Iter: 568/1000. LR: 0.0355. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9561. T_Loss: 3.8374. Mask: 0.9644. :  34%|███▍      | 17/50 [00:09<00:17,  1.93it/s]Train Iter: 568/1000. LR: 0.0355. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9561. T_Loss: 3.8374. Mask: 0.9644. :  36%|███▌      | 18/50 [00:09<00:23,  1.35it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9528. T_Loss: 3.8067. Mask: 0.9632. :  36%|███▌      | 18/50 [00:10<00:23,  1.35it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9528. T_Loss: 3.8067. Mask: 0.9632. :  38%|███▊      | 19/50 [00:10<00:19,  1.62it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9532. T_Loss: 3.7915. Mask: 0.9629. :  38%|███▊      | 19/50 [00:10<00:19,  1.62it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9532. T_Loss: 3.7915. Mask: 0.9629. :  40%|████      | 20/50 [00:10<00:15,  1.95it/s]total : 1000  current step :  568
total : 1000  current step :  569
total : 1000  current step :  570
Train Iter: 571/1000. LR: 0.0357. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9527. T_Loss: 3.7767. Mask: 0.9634. :  40%|████      | 20/50 [00:11<00:15,  1.95it/s]Train Iter: 571/1000. LR: 0.0357. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9527. T_Loss: 3.7767. Mask: 0.9634. :  42%|████▏     | 21/50 [00:11<00:18,  1.53it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9519. T_Loss: 3.7687. Mask: 0.9625. :  42%|████▏     | 21/50 [00:11<00:18,  1.53it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9519. T_Loss: 3.7687. Mask: 0.9625. :  44%|████▍     | 22/50 [00:11<00:15,  1.81it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9515. T_Loss: 3.7698. Mask: 0.9630. :  44%|████▍     | 22/50 [00:11<00:15,  1.81it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9515. T_Loss: 3.7698. Mask: 0.9630. :  46%|████▌     | 23/50 [00:11<00:12,  2.16it/s]total : 1000  current step :  571
total : 1000  current step :  572
total : 1000  current step :  573
Train Iter: 574/1000. LR: 0.0359. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9498. T_Loss: 3.7483. Mask: 0.9640. :  46%|████▌     | 23/50 [00:12<00:12,  2.16it/s]Train Iter: 574/1000. LR: 0.0359. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9498. T_Loss: 3.7483. Mask: 0.9640. :  48%|████▊     | 24/50 [00:12<00:15,  1.65it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9489. T_Loss: 3.7427. Mask: 0.9650. :  48%|████▊     | 24/50 [00:13<00:15,  1.65it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9489. T_Loss: 3.7427. Mask: 0.9650. :  50%|█████     | 25/50 [00:13<00:13,  1.89it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9487. T_Loss: 3.7278. Mask: 0.9651. :  50%|█████     | 25/50 [00:13<00:13,  1.89it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9487. T_Loss: 3.7278. Mask: 0.9651. :  52%|█████▏    | 26/50 [00:13<00:11,  2.17it/s]total : 1000  current step :  574
total : 1000  current step :  575
total : 1000  current step :  576
Train Iter: 577/1000. LR: 0.0361. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9472. T_Loss: 3.7156. Mask: 0.9661. :  52%|█████▏    | 26/50 [00:14<00:11,  2.17it/s]Train Iter: 577/1000. LR: 0.0361. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9472. T_Loss: 3.7156. Mask: 0.9661. :  54%|█████▍    | 27/50 [00:14<00:15,  1.51it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9460. T_Loss: 3.6926. Mask: 0.9658. :  54%|█████▍    | 27/50 [00:15<00:15,  1.51it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9460. T_Loss: 3.6926. Mask: 0.9658. :  56%|█████▌    | 28/50 [00:15<00:12,  1.77it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9468. T_Loss: 3.6981. Mask: 0.9657. :  56%|█████▌    | 28/50 [00:15<00:12,  1.77it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9468. T_Loss: 3.6981. Mask: 0.9657. :  58%|█████▊    | 29/50 [00:15<00:09,  2.17it/s]total : 1000  current step :  577
total : 1000  current step :  578
total : 1000  current step :  579
Train Iter: 580/1000. LR: 0.0362. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9467. T_Loss: 3.6970. Mask: 0.9658. :  58%|█████▊    | 29/50 [00:16<00:09,  2.17it/s]Train Iter: 580/1000. LR: 0.0362. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9467. T_Loss: 3.6970. Mask: 0.9658. :  60%|██████    | 30/50 [00:16<00:13,  1.54it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9471. T_Loss: 3.6993. Mask: 0.9659. :  60%|██████    | 30/50 [00:16<00:13,  1.54it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9471. T_Loss: 3.6993. Mask: 0.9659. :  62%|██████▏   | 31/50 [00:16<00:10,  1.82it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9465. T_Loss: 3.7016. Mask: 0.9663. :  62%|██████▏   | 31/50 [00:16<00:10,  1.82it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9465. T_Loss: 3.7016. Mask: 0.9663. :  64%|██████▍   | 32/50 [00:16<00:08,  2.07it/s]total : 1000  current step :  580
total : 1000  current step :  581
total : 1000  current step :  582
Train Iter: 583/1000. LR: 0.0364. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9473. T_Loss: 3.7019. Mask: 0.9666. :  64%|██████▍   | 32/50 [00:17<00:08,  2.07it/s]Train Iter: 583/1000. LR: 0.0364. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9473. T_Loss: 3.7019. Mask: 0.9666. :  66%|██████▌   | 33/50 [00:17<00:10,  1.63it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9479. T_Loss: 3.7063. Mask: 0.9668. :  66%|██████▌   | 33/50 [00:18<00:10,  1.63it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9479. T_Loss: 3.7063. Mask: 0.9668. :  68%|██████▊   | 34/50 [00:18<00:07,  2.02it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9485. T_Loss: 3.7080. Mask: 0.9672. :  68%|██████▊   | 34/50 [00:18<00:07,  2.02it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9485. T_Loss: 3.7080. Mask: 0.9672. :  70%|███████   | 35/50 [00:18<00:06,  2.32it/s]total : 1000  current step :  583
total : 1000  current step :  584
total : 1000  current step :  585
Train Iter: 586/1000. LR: 0.0366. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9488. T_Loss: 3.7085. Mask: 0.9668. :  70%|███████   | 35/50 [00:19<00:06,  2.32it/s]Train Iter: 586/1000. LR: 0.0366. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9488. T_Loss: 3.7085. Mask: 0.9668. :  72%|███████▏  | 36/50 [00:19<00:08,  1.74it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9490. T_Loss: 3.7098. Mask: 0.9668. :  72%|███████▏  | 36/50 [00:19<00:08,  1.74it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9490. T_Loss: 3.7098. Mask: 0.9668. :  74%|███████▍  | 37/50 [00:19<00:06,  2.09it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9490. T_Loss: 3.7121. Mask: 0.9673. :  74%|███████▍  | 37/50 [00:19<00:06,  2.09it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9490. T_Loss: 3.7121. Mask: 0.9673. :  76%|███████▌  | 38/50 [00:19<00:05,  2.35it/s]total : 1000  current step :  586
total : 1000  current step :  587
total : 1000  current step :  588
Train Iter: 589/1000. LR: 0.0368. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9481. T_Loss: 3.7124. Mask: 0.9674. :  76%|███████▌  | 38/50 [00:20<00:05,  2.35it/s]Train Iter: 589/1000. LR: 0.0368. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9481. T_Loss: 3.7124. Mask: 0.9674. :  78%|███████▊  | 39/50 [00:20<00:06,  1.61it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9466. T_Loss: 3.6999. Mask: 0.9673. :  78%|███████▊  | 39/50 [00:21<00:06,  1.61it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9466. T_Loss: 3.6999. Mask: 0.9673. :  80%|████████  | 40/50 [00:21<00:05,  1.88it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9465. T_Loss: 3.7064. Mask: 0.9674. :  80%|████████  | 40/50 [00:21<00:05,  1.88it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9465. T_Loss: 3.7064. Mask: 0.9674. :  82%|████████▏ | 41/50 [00:21<00:03,  2.26it/s]total : 1000  current step :  589
total : 1000  current step :  590
total : 1000  current step :  591
Train Iter: 592/1000. LR: 0.0370. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9461. T_Loss: 3.7104. Mask: 0.9675. :  82%|████████▏ | 41/50 [00:22<00:03,  2.26it/s]Train Iter: 592/1000. LR: 0.0370. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9461. T_Loss: 3.7104. Mask: 0.9675. :  84%|████████▍ | 42/50 [00:22<00:04,  1.69it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9469. T_Loss: 3.7061. Mask: 0.9674. :  84%|████████▍ | 42/50 [00:22<00:04,  1.69it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9469. T_Loss: 3.7061. Mask: 0.9674. :  86%|████████▌ | 43/50 [00:22<00:03,  1.93it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9460. T_Loss: 3.7058. Mask: 0.9676. :  86%|████████▌ | 43/50 [00:23<00:03,  1.93it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9460. T_Loss: 3.7058. Mask: 0.9676. :  88%|████████▊ | 44/50 [00:23<00:02,  2.15it/s]total : 1000  current step :  592
total : 1000  current step :  593
total : 1000  current step :  594
Train Iter: 595/1000. LR: 0.0372. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9463. T_Loss: 3.7054. Mask: 0.9678. :  88%|████████▊ | 44/50 [00:24<00:02,  2.15it/s]Train Iter: 595/1000. LR: 0.0372. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9463. T_Loss: 3.7054. Mask: 0.9678. :  90%|█████████ | 45/50 [00:24<00:03,  1.60it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9464. T_Loss: 3.7038. Mask: 0.9681. :  90%|█████████ | 45/50 [00:24<00:03,  1.60it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9464. T_Loss: 3.7038. Mask: 0.9681. :  92%|█████████▏| 46/50 [00:24<00:02,  1.90it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9456. T_Loss: 3.6996. Mask: 0.9682. :  92%|█████████▏| 46/50 [00:24<00:02,  1.90it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9456. T_Loss: 3.6996. Mask: 0.9682. :  94%|█████████▍| 47/50 [00:24<00:01,  2.01it/s]total : 1000  current step :  595
total : 1000  current step :  596
total : 1000  current step :  597
Train Iter: 598/1000. LR: 0.0374. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9461. T_Loss: 3.7087. Mask: 0.9683. :  94%|█████████▍| 47/50 [00:25<00:01,  2.01it/s]Train Iter: 598/1000. LR: 0.0374. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9461. T_Loss: 3.7087. Mask: 0.9683. :  96%|█████████▌| 48/50 [00:25<00:01,  1.62it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9455. T_Loss: 3.7095. Mask: 0.9682. :  96%|█████████▌| 48/50 [00:26<00:01,  1.62it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9455. T_Loss: 3.7095. Mask: 0.9682. :  98%|█████████▊| 49/50 [00:26<00:00,  1.94it/s]total : 1000  current step :  598
total : 1000  current step :  599
Train Iter: 600/1000. LR: 0.0375. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9448. T_Loss: 3.7027. Mask: 0.9678. :  98%|█████████▊| 49/50 [00:26<00:00,  1.94it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9448. T_Loss: 3.7027. Mask: 0.9678. : 100%|██████████| 50/50 [00:26<00:00,  1.79it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9448. T_Loss: 3.7027. Mask: 0.9678. : 100%|██████████| 50/50 [00:26<00:00,  1.87it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.9667. top1: 93.75. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.9667. top1: 93.75. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9568. top1: 95.12. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9568. top1: 95.12. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.57it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9631. top1: 94.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.57it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9631. top1: 94.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9682. top1: 93.75. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9682. top1: 93.75. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.34it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9930. top1: 91.64. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.34it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9930. top1: 91.64. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.31it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0055. top1: 90.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.31it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0055. top1: 90.69. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0188. top1: 89.51. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0188. top1: 89.51. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0248. top1: 88.95. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0248. top1: 88.95. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.93it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0248. top1: 88.95. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.17it/s]
total : 1000  current step :  600
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.90s. Batch: 1.15s. S_Loss: 0.8938. T_Loss: 3.6934. Mask: 0.9805. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.90s. Batch: 1.15s. S_Loss: 0.8938. T_Loss: 3.6934. Mask: 0.9805. :   2%|▏         | 1/50 [00:01<00:56,  1.15s/it]Train Iter: 602/1000. LR: 0.0376. Data: 0.49s. Batch: 0.72s. S_Loss: 0.8992. T_Loss: 3.7167. Mask: 0.9766. :   2%|▏         | 1/50 [00:01<00:56,  1.15s/it]Train Iter: 602/1000. LR: 0.0376. Data: 0.49s. Batch: 0.72s. S_Loss: 0.8992. T_Loss: 3.7167. Mask: 0.9766. :   4%|▍         | 2/50 [00:01<00:30,  1.56it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.37s. Batch: 0.58s. S_Loss: 0.8923. T_Loss: 3.7307. Mask: 0.9753. :   4%|▍         | 2/50 [00:01<00:30,  1.56it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.37s. Batch: 0.58s. S_Loss: 0.8923. T_Loss: 3.7307. Mask: 0.9753. :   6%|▌         | 3/50 [00:01<00:22,  2.04it/s]total : 1000  current step :  601
total : 1000  current step :  602
total : 1000  current step :  603
Train Iter: 604/1000. LR: 0.0378. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8998. T_Loss: 3.6815. Mask: 0.9746. :   6%|▌         | 3/50 [00:02<00:22,  2.04it/s]Train Iter: 604/1000. LR: 0.0378. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8998. T_Loss: 3.6815. Mask: 0.9746. :   8%|▊         | 4/50 [00:02<00:33,  1.38it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9152. T_Loss: 3.7672. Mask: 0.9727. :   8%|▊         | 4/50 [00:03<00:33,  1.38it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9152. T_Loss: 3.7672. Mask: 0.9727. :  10%|█         | 5/50 [00:03<00:24,  1.84it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9155. T_Loss: 3.7858. Mask: 0.9733. :  10%|█         | 5/50 [00:03<00:24,  1.84it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9155. T_Loss: 3.7858. Mask: 0.9733. :  12%|█▏        | 6/50 [00:03<00:18,  2.39it/s]total : 1000  current step :  604
total : 1000  current step :  605
total : 1000  current step :  606
Train Iter: 607/1000. LR: 0.0379. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9204. T_Loss: 3.8155. Mask: 0.9715. :  12%|█▏        | 6/50 [00:04<00:18,  2.39it/s]Train Iter: 607/1000. LR: 0.0379. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9204. T_Loss: 3.8155. Mask: 0.9715. :  14%|█▍        | 7/50 [00:04<00:27,  1.59it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.34s. Batch: 0.56s. S_Loss: 0.9208. T_Loss: 3.8047. Mask: 0.9697. :  14%|█▍        | 7/50 [00:04<00:27,  1.59it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.34s. Batch: 0.56s. S_Loss: 0.9208. T_Loss: 3.8047. Mask: 0.9697. :  16%|█▌        | 8/50 [00:04<00:20,  2.05it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9278. T_Loss: 3.8456. Mask: 0.9696. :  16%|█▌        | 8/50 [00:04<00:20,  2.05it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9278. T_Loss: 3.8456. Mask: 0.9696. :  18%|█▊        | 9/50 [00:04<00:17,  2.37it/s]total : 1000  current step :  607
total : 1000  current step :  608
total : 1000  current step :  609
Train Iter: 610/1000. LR: 0.0381. Data: 0.36s. Batch: 0.57s. S_Loss: 0.9302. T_Loss: 3.8313. Mask: 0.9695. :  18%|█▊        | 9/50 [00:05<00:17,  2.37it/s]Train Iter: 610/1000. LR: 0.0381. Data: 0.36s. Batch: 0.57s. S_Loss: 0.9302. T_Loss: 3.8313. Mask: 0.9695. :  20%|██        | 10/50 [00:05<00:23,  1.69it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9332. T_Loss: 3.8487. Mask: 0.9688. :  20%|██        | 10/50 [00:05<00:23,  1.69it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9332. T_Loss: 3.8487. Mask: 0.9688. :  22%|██▏       | 11/50 [00:05<00:18,  2.11it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.31s. Batch: 0.51s. S_Loss: 0.9323. T_Loss: 3.8236. Mask: 0.9684. :  22%|██▏       | 11/50 [00:06<00:18,  2.11it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.31s. Batch: 0.51s. S_Loss: 0.9323. T_Loss: 3.8236. Mask: 0.9684. :  24%|██▍       | 12/50 [00:06<00:15,  2.43it/s]total : 1000  current step :  610
total : 1000  current step :  611
total : 1000  current step :  612
Train Iter: 613/1000. LR: 0.0383. Data: 0.35s. Batch: 0.56s. S_Loss: 0.9308. T_Loss: 3.8199. Mask: 0.9666. :  24%|██▍       | 12/50 [00:07<00:15,  2.43it/s]Train Iter: 613/1000. LR: 0.0383. Data: 0.35s. Batch: 0.56s. S_Loss: 0.9308. T_Loss: 3.8199. Mask: 0.9666. :  26%|██▌       | 13/50 [00:07<00:22,  1.62it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9307. T_Loss: 3.8319. Mask: 0.9671. :  26%|██▌       | 13/50 [00:07<00:22,  1.62it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9307. T_Loss: 3.8319. Mask: 0.9671. :  28%|██▊       | 14/50 [00:07<00:18,  1.91it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9318. T_Loss: 3.8188. Mask: 0.9677. :  28%|██▊       | 14/50 [00:07<00:18,  1.91it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9318. T_Loss: 3.8188. Mask: 0.9677. :  30%|███       | 15/50 [00:07<00:15,  2.31it/s]total : 1000  current step :  613
total : 1000  current step :  614
total : 1000  current step :  615
Train Iter: 616/1000. LR: 0.0385. Data: 0.35s. Batch: 0.56s. S_Loss: 0.9313. T_Loss: 3.8144. Mask: 0.9685. :  30%|███       | 15/50 [00:08<00:15,  2.31it/s]Train Iter: 616/1000. LR: 0.0385. Data: 0.35s. Batch: 0.56s. S_Loss: 0.9313. T_Loss: 3.8144. Mask: 0.9685. :  32%|███▏      | 16/50 [00:08<00:21,  1.56it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9299. T_Loss: 3.7997. Mask: 0.9690. :  32%|███▏      | 16/50 [00:09<00:21,  1.56it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9299. T_Loss: 3.7997. Mask: 0.9690. :  34%|███▍      | 17/50 [00:09<00:17,  1.89it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9304. T_Loss: 3.7811. Mask: 0.9688. :  34%|███▍      | 17/50 [00:09<00:17,  1.89it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9304. T_Loss: 3.7811. Mask: 0.9688. :  36%|███▌      | 18/50 [00:09<00:14,  2.28it/s]total : 1000  current step :  616
total : 1000  current step :  617
total : 1000  current step :  618
Train Iter: 619/1000. LR: 0.0387. Data: 0.34s. Batch: 0.55s. S_Loss: 0.9305. T_Loss: 3.7571. Mask: 0.9688. :  36%|███▌      | 18/50 [00:10<00:14,  2.28it/s]Train Iter: 619/1000. LR: 0.0387. Data: 0.34s. Batch: 0.55s. S_Loss: 0.9305. T_Loss: 3.7571. Mask: 0.9688. :  38%|███▊      | 19/50 [00:10<00:19,  1.60it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9306. T_Loss: 3.7478. Mask: 0.9689. :  38%|███▊      | 19/50 [00:10<00:19,  1.60it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9306. T_Loss: 3.7478. Mask: 0.9689. :  40%|████      | 20/50 [00:10<00:15,  1.95it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9295. T_Loss: 3.7234. Mask: 0.9695. :  40%|████      | 20/50 [00:11<00:15,  1.95it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9295. T_Loss: 3.7234. Mask: 0.9695. :  42%|████▏     | 21/50 [00:11<00:12,  2.30it/s]total : 1000  current step :  619
total : 1000  current step :  620
total : 1000  current step :  621
Train Iter: 622/1000. LR: 0.0389. Data: 0.34s. Batch: 0.55s. S_Loss: 0.9286. T_Loss: 3.7115. Mask: 0.9702. :  42%|████▏     | 21/50 [00:12<00:12,  2.30it/s]Train Iter: 622/1000. LR: 0.0389. Data: 0.34s. Batch: 0.55s. S_Loss: 0.9286. T_Loss: 3.7115. Mask: 0.9702. :  44%|████▍     | 22/50 [00:12<00:18,  1.55it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9291. T_Loss: 3.7006. Mask: 0.9701. :  44%|████▍     | 22/50 [00:12<00:18,  1.55it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9291. T_Loss: 3.7006. Mask: 0.9701. :  46%|████▌     | 23/50 [00:12<00:14,  1.85it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9285. T_Loss: 3.6822. Mask: 0.9702. :  46%|████▌     | 23/50 [00:12<00:14,  1.85it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9285. T_Loss: 3.6822. Mask: 0.9702. :  48%|████▊     | 24/50 [00:12<00:11,  2.22it/s]total : 1000  current step :  622
total : 1000  current step :  623
total : 1000  current step :  624
Train Iter: 625/1000. LR: 0.0391. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9289. T_Loss: 3.6859. Mask: 0.9703. :  48%|████▊     | 24/50 [00:13<00:11,  2.22it/s]Train Iter: 625/1000. LR: 0.0391. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9289. T_Loss: 3.6859. Mask: 0.9703. :  50%|█████     | 25/50 [00:13<00:14,  1.74it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9296. T_Loss: 3.6736. Mask: 0.9694. :  50%|█████     | 25/50 [00:13<00:14,  1.74it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9296. T_Loss: 3.6736. Mask: 0.9694. :  52%|█████▏    | 26/50 [00:13<00:11,  2.07it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9299. T_Loss: 3.6719. Mask: 0.9689. :  52%|█████▏    | 26/50 [00:14<00:11,  2.07it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9299. T_Loss: 3.6719. Mask: 0.9689. :  54%|█████▍    | 27/50 [00:14<00:10,  2.23it/s]total : 1000  current step :  625
total : 1000  current step :  626
total : 1000  current step :  627
Train Iter: 628/1000. LR: 0.0393. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9307. T_Loss: 3.6795. Mask: 0.9694. :  54%|█████▍    | 27/50 [00:15<00:10,  2.23it/s]Train Iter: 628/1000. LR: 0.0393. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9307. T_Loss: 3.6795. Mask: 0.9694. :  56%|█████▌    | 28/50 [00:15<00:13,  1.65it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9333. T_Loss: 3.7090. Mask: 0.9700. :  56%|█████▌    | 28/50 [00:15<00:13,  1.65it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9333. T_Loss: 3.7090. Mask: 0.9700. :  58%|█████▊    | 29/50 [00:15<00:10,  1.92it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9322. T_Loss: 3.7031. Mask: 0.9704. :  58%|█████▊    | 29/50 [00:15<00:10,  1.92it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9322. T_Loss: 3.7031. Mask: 0.9704. :  60%|██████    | 30/50 [00:15<00:08,  2.28it/s]total : 1000  current step :  628
total : 1000  current step :  629
total : 1000  current step :  630
Train Iter: 631/1000. LR: 0.0394. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9326. T_Loss: 3.7073. Mask: 0.9704. :  60%|██████    | 30/50 [00:16<00:08,  2.28it/s]Train Iter: 631/1000. LR: 0.0394. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9326. T_Loss: 3.7073. Mask: 0.9704. :  62%|██████▏   | 31/50 [00:16<00:12,  1.52it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9341. T_Loss: 3.7168. Mask: 0.9697. :  62%|██████▏   | 31/50 [00:17<00:12,  1.52it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9341. T_Loss: 3.7168. Mask: 0.9697. :  64%|██████▍   | 32/50 [00:17<00:09,  1.94it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9337. T_Loss: 3.7124. Mask: 0.9701. :  64%|██████▍   | 32/50 [00:17<00:09,  1.94it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9337. T_Loss: 3.7124. Mask: 0.9701. :  66%|██████▌   | 33/50 [00:17<00:07,  2.24it/s]total : 1000  current step :  631
total : 1000  current step :  632
total : 1000  current step :  633
Train Iter: 634/1000. LR: 0.0396. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9327. T_Loss: 3.7119. Mask: 0.9705. :  66%|██████▌   | 33/50 [00:18<00:07,  2.24it/s]Train Iter: 634/1000. LR: 0.0396. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9327. T_Loss: 3.7119. Mask: 0.9705. :  68%|██████▊   | 34/50 [00:18<00:09,  1.63it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9330. T_Loss: 3.7144. Mask: 0.9703. :  68%|██████▊   | 34/50 [00:18<00:09,  1.63it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9330. T_Loss: 3.7144. Mask: 0.9703. :  70%|███████   | 35/50 [00:18<00:07,  1.97it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9328. T_Loss: 3.7071. Mask: 0.9704. :  70%|███████   | 35/50 [00:18<00:07,  1.97it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9328. T_Loss: 3.7071. Mask: 0.9704. :  72%|███████▏  | 36/50 [00:18<00:05,  2.36it/s]total : 1000  current step :  634
total : 1000  current step :  635
total : 1000  current step :  636
Train Iter: 637/1000. LR: 0.0398. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9334. T_Loss: 3.7052. Mask: 0.9710. :  72%|███████▏  | 36/50 [00:19<00:05,  2.36it/s]Train Iter: 637/1000. LR: 0.0398. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9334. T_Loss: 3.7052. Mask: 0.9710. :  74%|███████▍  | 37/50 [00:19<00:07,  1.69it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9333. T_Loss: 3.7025. Mask: 0.9710. :  74%|███████▍  | 37/50 [00:20<00:07,  1.69it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9333. T_Loss: 3.7025. Mask: 0.9710. :  76%|███████▌  | 38/50 [00:20<00:06,  1.99it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9327. T_Loss: 3.6874. Mask: 0.9709. :  76%|███████▌  | 38/50 [00:20<00:06,  1.99it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9327. T_Loss: 3.6874. Mask: 0.9709. :  78%|███████▊  | 39/50 [00:20<00:04,  2.35it/s]total : 1000  current step :  637
total : 1000  current step :  638
total : 1000  current step :  639
Train Iter: 640/1000. LR: 0.0400. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9313. T_Loss: 3.6672. Mask: 0.9711. :  78%|███████▊  | 39/50 [00:21<00:04,  2.35it/s]Train Iter: 640/1000. LR: 0.0400. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9313. T_Loss: 3.6672. Mask: 0.9711. :  80%|████████  | 40/50 [00:21<00:06,  1.44it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9306. T_Loss: 3.6600. Mask: 0.9717. :  80%|████████  | 40/50 [00:22<00:06,  1.44it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9306. T_Loss: 3.6600. Mask: 0.9717. :  82%|████████▏ | 41/50 [00:22<00:05,  1.58it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9308. T_Loss: 3.6541. Mask: 0.9719. :  82%|████████▏ | 41/50 [00:22<00:05,  1.58it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9308. T_Loss: 3.6541. Mask: 0.9719. :  84%|████████▍ | 42/50 [00:22<00:04,  1.70it/s]total : 1000  current step :  640
total : 1000  current step :  641
total : 1000  current step :  642
Train Iter: 643/1000. LR: 0.0402. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9314. T_Loss: 3.6601. Mask: 0.9720. :  84%|████████▍ | 42/50 [00:23<00:04,  1.70it/s]Train Iter: 643/1000. LR: 0.0402. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9314. T_Loss: 3.6601. Mask: 0.9720. :  86%|████████▌ | 43/50 [00:23<00:04,  1.49it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9314. T_Loss: 3.6660. Mask: 0.9720. :  86%|████████▌ | 43/50 [00:23<00:04,  1.49it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9314. T_Loss: 3.6660. Mask: 0.9720. :  88%|████████▊ | 44/50 [00:23<00:03,  1.76it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9316. T_Loss: 3.6695. Mask: 0.9721. :  88%|████████▊ | 44/50 [00:24<00:03,  1.76it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9316. T_Loss: 3.6695. Mask: 0.9721. :  90%|█████████ | 45/50 [00:24<00:02,  2.14it/s]total : 1000  current step :  643
total : 1000  current step :  644
total : 1000  current step :  645
Train Iter: 646/1000. LR: 0.0404. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9318. T_Loss: 3.6790. Mask: 0.9721. :  90%|█████████ | 45/50 [00:24<00:02,  2.14it/s]Train Iter: 646/1000. LR: 0.0404. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9318. T_Loss: 3.6790. Mask: 0.9721. :  92%|█████████▏| 46/50 [00:24<00:02,  1.71it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9324. T_Loss: 3.6878. Mask: 0.9720. :  92%|█████████▏| 46/50 [00:25<00:02,  1.71it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9324. T_Loss: 3.6878. Mask: 0.9720. :  94%|█████████▍| 47/50 [00:25<00:01,  1.95it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9322. T_Loss: 3.6877. Mask: 0.9720. :  94%|█████████▍| 47/50 [00:25<00:01,  1.95it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9322. T_Loss: 3.6877. Mask: 0.9720. :  96%|█████████▌| 48/50 [00:25<00:00,  2.10it/s]total : 1000  current step :  646
total : 1000  current step :  647
total : 1000  current step :  648
Train Iter: 649/1000. LR: 0.0406. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9319. T_Loss: 3.6921. Mask: 0.9722. :  96%|█████████▌| 48/50 [00:26<00:00,  2.10it/s]Train Iter: 649/1000. LR: 0.0406. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9319. T_Loss: 3.6921. Mask: 0.9722. :  98%|█████████▊| 49/50 [00:26<00:00,  1.62it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9319. T_Loss: 3.6883. Mask: 0.9720. :  98%|█████████▊| 49/50 [00:26<00:00,  1.62it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9319. T_Loss: 3.6883. Mask: 0.9720. : 100%|██████████| 50/50 [00:26<00:00,  1.87it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9319. T_Loss: 3.6883. Mask: 0.9720. : 100%|██████████| 50/50 [00:26<00:00,  1.85it/s]
total : 1000  current step :  649
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.9529. top1: 94.92. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.9529. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9442. top1: 95.31. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9442. top1: 95.31. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.61it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9510. top1: 94.66. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.61it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9510. top1: 94.66. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9561. top1: 93.95. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9561. top1: 93.95. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.19it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9761. top1: 92.19. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.19it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9761. top1: 92.19. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.49it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9865. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.49it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9865. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.69it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9985. top1: 90.35. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.69it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9985. top1: 90.35. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.91it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0035. top1: 89.85. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.91it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0035. top1: 89.85. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0035. top1: 89.85. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.27it/s]
total : 1000  current step :  650
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.01s. Batch: 0.18s. S_Loss: 0.8914. T_Loss: 3.2437. Mask: 0.9648. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.01s. Batch: 0.18s. S_Loss: 0.8914. T_Loss: 3.2437. Mask: 0.9648. :   2%|▏         | 1/50 [00:00<00:09,  5.36it/s]total : 1000  current step :  651
Train Iter: 652/1000. LR: 0.0408. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9206. T_Loss: 3.3839. Mask: 0.9707. :   2%|▏         | 1/50 [00:01<00:09,  5.36it/s]Train Iter: 652/1000. LR: 0.0408. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9206. T_Loss: 3.3839. Mask: 0.9707. :   4%|▍         | 2/50 [00:01<00:31,  1.52it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9308. T_Loss: 3.4402. Mask: 0.9701. :   4%|▍         | 2/50 [00:01<00:31,  1.52it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9308. T_Loss: 3.4402. Mask: 0.9701. :   6%|▌         | 3/50 [00:01<00:23,  1.96it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.19s. Batch: 0.42s. S_Loss: 0.9304. T_Loss: 3.4235. Mask: 0.9746. :   6%|▌         | 3/50 [00:01<00:23,  1.96it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.19s. Batch: 0.42s. S_Loss: 0.9304. T_Loss: 3.4235. Mask: 0.9746. :   8%|▊         | 4/50 [00:01<00:17,  2.63it/s]total : 1000  current step :  652
total : 1000  current step :  653
total : 1000  current step :  654
Train Iter: 655/1000. LR: 0.0409. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9296. T_Loss: 3.4174. Mask: 0.9766. :   8%|▊         | 4/50 [00:02<00:17,  2.63it/s]Train Iter: 655/1000. LR: 0.0409. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9296. T_Loss: 3.4174. Mask: 0.9766. :  10%|█         | 5/50 [00:02<00:26,  1.67it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9332. T_Loss: 3.4332. Mask: 0.9766. :  10%|█         | 5/50 [00:02<00:26,  1.67it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.25s. Batch: 0.49s. S_Loss: 0.9332. T_Loss: 3.4332. Mask: 0.9766. :  12%|█▏        | 6/50 [00:02<00:21,  2.05it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.22s. Batch: 0.45s. S_Loss: 0.9374. T_Loss: 3.4915. Mask: 0.9749. :  12%|█▏        | 6/50 [00:03<00:21,  2.05it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.22s. Batch: 0.45s. S_Loss: 0.9374. T_Loss: 3.4915. Mask: 0.9749. :  14%|█▍        | 7/50 [00:03<00:17,  2.46it/s]total : 1000  current step :  655
total : 1000  current step :  656
total : 1000  current step :  657
Train Iter: 658/1000. LR: 0.0411. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9351. T_Loss: 3.5071. Mask: 0.9756. :  14%|█▍        | 7/50 [00:04<00:17,  2.46it/s]Train Iter: 658/1000. LR: 0.0411. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9351. T_Loss: 3.5071. Mask: 0.9756. :  16%|█▌        | 8/50 [00:04<00:27,  1.54it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9343. T_Loss: 3.5560. Mask: 0.9757. :  16%|█▌        | 8/50 [00:04<00:27,  1.54it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9343. T_Loss: 3.5560. Mask: 0.9757. :  18%|█▊        | 9/50 [00:04<00:21,  1.92it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9327. T_Loss: 3.5704. Mask: 0.9762. :  18%|█▊        | 9/50 [00:04<00:21,  1.92it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.26s. Batch: 0.49s. S_Loss: 0.9327. T_Loss: 3.5704. Mask: 0.9762. :  20%|██        | 10/50 [00:04<00:18,  2.18it/s]total : 1000  current step :  658
total : 1000  current step :  659
total : 1000  current step :  660
Train Iter: 661/1000. LR: 0.0413. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9356. T_Loss: 3.6185. Mask: 0.9762. :  20%|██        | 10/50 [00:05<00:18,  2.18it/s]Train Iter: 661/1000. LR: 0.0413. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9356. T_Loss: 3.6185. Mask: 0.9762. :  22%|██▏       | 11/50 [00:05<00:24,  1.56it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9341. T_Loss: 3.6442. Mask: 0.9759. :  22%|██▏       | 11/50 [00:06<00:24,  1.56it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9341. T_Loss: 3.6442. Mask: 0.9759. :  24%|██▍       | 12/50 [00:06<00:21,  1.76it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9337. T_Loss: 3.6618. Mask: 0.9763. :  24%|██▍       | 12/50 [00:06<00:21,  1.76it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9337. T_Loss: 3.6618. Mask: 0.9763. :  26%|██▌       | 13/50 [00:06<00:18,  2.01it/s]total : 1000  current step :  661
total : 1000  current step :  662
total : 1000  current step :  663
Train Iter: 664/1000. LR: 0.0415. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9302. T_Loss: 3.6556. Mask: 0.9763. :  26%|██▌       | 13/50 [00:07<00:18,  2.01it/s]Train Iter: 664/1000. LR: 0.0415. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9302. T_Loss: 3.6556. Mask: 0.9763. :  28%|██▊       | 14/50 [00:07<00:22,  1.61it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9288. T_Loss: 3.6527. Mask: 0.9753. :  28%|██▊       | 14/50 [00:07<00:22,  1.61it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9288. T_Loss: 3.6527. Mask: 0.9753. :  30%|███       | 15/50 [00:07<00:18,  1.89it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9300. T_Loss: 3.6409. Mask: 0.9753. :  30%|███       | 15/50 [00:08<00:18,  1.89it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9300. T_Loss: 3.6409. Mask: 0.9753. :  32%|███▏      | 16/50 [00:08<00:15,  2.23it/s]total : 1000  current step :  664
total : 1000  current step :  665
total : 1000  current step :  666
Train Iter: 667/1000. LR: 0.0417. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9301. T_Loss: 3.6331. Mask: 0.9754. :  32%|███▏      | 16/50 [00:09<00:15,  2.23it/s]Train Iter: 667/1000. LR: 0.0417. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9301. T_Loss: 3.6331. Mask: 0.9754. :  34%|███▍      | 17/50 [00:09<00:19,  1.70it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9291. T_Loss: 3.6176. Mask: 0.9753. :  34%|███▍      | 17/50 [00:09<00:19,  1.70it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9291. T_Loss: 3.6176. Mask: 0.9753. :  36%|███▌      | 18/50 [00:09<00:16,  1.94it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9271. T_Loss: 3.5917. Mask: 0.9751. :  36%|███▌      | 18/50 [00:09<00:16,  1.94it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9271. T_Loss: 3.5917. Mask: 0.9751. :  38%|███▊      | 19/50 [00:09<00:12,  2.40it/s]total : 1000  current step :  667
total : 1000  current step :  668
total : 1000  current step :  669
Train Iter: 670/1000. LR: 0.0419. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9277. T_Loss: 3.6063. Mask: 0.9754. :  38%|███▊      | 19/50 [00:10<00:12,  2.40it/s]Train Iter: 670/1000. LR: 0.0419. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9277. T_Loss: 3.6063. Mask: 0.9754. :  40%|████      | 20/50 [00:10<00:16,  1.79it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9306. T_Loss: 3.6166. Mask: 0.9756. :  40%|████      | 20/50 [00:10<00:16,  1.79it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9306. T_Loss: 3.6166. Mask: 0.9756. :  42%|████▏     | 21/50 [00:10<00:13,  2.12it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9299. T_Loss: 3.6134. Mask: 0.9759. :  42%|████▏     | 21/50 [00:11<00:13,  2.12it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.27s. Batch: 0.50s. S_Loss: 0.9299. T_Loss: 3.6134. Mask: 0.9759. :  44%|████▍     | 22/50 [00:11<00:11,  2.45it/s]total : 1000  current step :  670
total : 1000  current step :  671
total : 1000  current step :  672
Train Iter: 673/1000. LR: 0.0421. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9303. T_Loss: 3.6005. Mask: 0.9762. :  44%|████▍     | 22/50 [00:11<00:11,  2.45it/s]Train Iter: 673/1000. LR: 0.0421. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9303. T_Loss: 3.6005. Mask: 0.9762. :  46%|████▌     | 23/50 [00:11<00:15,  1.79it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9297. T_Loss: 3.6064. Mask: 0.9762. :  46%|████▌     | 23/50 [00:12<00:15,  1.79it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9297. T_Loss: 3.6064. Mask: 0.9762. :  48%|████▊     | 24/50 [00:12<00:14,  1.82it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9302. T_Loss: 3.5895. Mask: 0.9762. :  48%|████▊     | 24/50 [00:12<00:14,  1.82it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9302. T_Loss: 3.5895. Mask: 0.9762. :  50%|█████     | 25/50 [00:12<00:11,  2.16it/s]total : 1000  current step :  673
total : 1000  current step :  674
total : 1000  current step :  675
Train Iter: 676/1000. LR: 0.0423. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9295. T_Loss: 3.5884. Mask: 0.9769. :  50%|█████     | 25/50 [00:13<00:11,  2.16it/s]Train Iter: 676/1000. LR: 0.0423. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9295. T_Loss: 3.5884. Mask: 0.9769. :  52%|█████▏    | 26/50 [00:13<00:14,  1.67it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9282. T_Loss: 3.5797. Mask: 0.9774. :  52%|█████▏    | 26/50 [00:13<00:14,  1.67it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9282. T_Loss: 3.5797. Mask: 0.9774. :  54%|█████▍    | 27/50 [00:13<00:11,  1.95it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9273. T_Loss: 3.5750. Mask: 0.9771. :  54%|█████▍    | 27/50 [00:14<00:11,  1.95it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9273. T_Loss: 3.5750. Mask: 0.9771. :  56%|█████▌    | 28/50 [00:14<00:09,  2.30it/s]total : 1000  current step :  676
total : 1000  current step :  677
total : 1000  current step :  678
Train Iter: 679/1000. LR: 0.0424. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9278. T_Loss: 3.5756. Mask: 0.9774. :  56%|█████▌    | 28/50 [00:15<00:09,  2.30it/s]Train Iter: 679/1000. LR: 0.0424. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9278. T_Loss: 3.5756. Mask: 0.9774. :  58%|█████▊    | 29/50 [00:15<00:12,  1.66it/s]total : 1000  current step :  679
Train Iter: 680/1000. LR: 0.0425. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9278. T_Loss: 3.5657. Mask: 0.9776. :  58%|█████▊    | 29/50 [00:15<00:12,  1.66it/s]Train Iter: 680/1000. LR: 0.0425. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9278. T_Loss: 3.5657. Mask: 0.9776. :  60%|██████    | 30/50 [00:15<00:12,  1.66it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9273. T_Loss: 3.5622. Mask: 0.9779. :  60%|██████    | 30/50 [00:16<00:12,  1.66it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9273. T_Loss: 3.5622. Mask: 0.9779. :  62%|██████▏   | 31/50 [00:16<00:12,  1.57it/s]total : 1000  current step :  680
total : 1000  current step :  681
Train Iter: 682/1000. LR: 0.0426. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9277. T_Loss: 3.5707. Mask: 0.9778. :  62%|██████▏   | 31/50 [00:17<00:12,  1.57it/s]Train Iter: 682/1000. LR: 0.0426. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9277. T_Loss: 3.5707. Mask: 0.9778. :  64%|██████▍   | 32/50 [00:17<00:13,  1.30it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9277. T_Loss: 3.5607. Mask: 0.9776. :  64%|██████▍   | 32/50 [00:17<00:13,  1.30it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9277. T_Loss: 3.5607. Mask: 0.9776. :  66%|██████▌   | 33/50 [00:17<00:11,  1.54it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9271. T_Loss: 3.5605. Mask: 0.9774. :  66%|██████▌   | 33/50 [00:18<00:11,  1.54it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9271. T_Loss: 3.5605. Mask: 0.9774. :  68%|██████▊   | 34/50 [00:18<00:08,  1.87it/s]total : 1000  current step :  682
total : 1000  current step :  683
total : 1000  current step :  684
Train Iter: 685/1000. LR: 0.0428. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9279. T_Loss: 3.5710. Mask: 0.9775. :  68%|██████▊   | 34/50 [00:19<00:08,  1.87it/s]Train Iter: 685/1000. LR: 0.0428. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9279. T_Loss: 3.5710. Mask: 0.9775. :  70%|███████   | 35/50 [00:19<00:10,  1.46it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9270. T_Loss: 3.5664. Mask: 0.9774. :  70%|███████   | 35/50 [00:19<00:10,  1.46it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9270. T_Loss: 3.5664. Mask: 0.9774. :  72%|███████▏  | 36/50 [00:19<00:07,  1.77it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9277. T_Loss: 3.5802. Mask: 0.9773. :  72%|███████▏  | 36/50 [00:19<00:07,  1.77it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9277. T_Loss: 3.5802. Mask: 0.9773. :  74%|███████▍  | 37/50 [00:19<00:06,  2.03it/s]total : 1000  current step :  685
total : 1000  current step :  686
total : 1000  current step :  687
Train Iter: 688/1000. LR: 0.0430. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9281. T_Loss: 3.5852. Mask: 0.9773. :  74%|███████▍  | 37/50 [00:20<00:06,  2.03it/s]Train Iter: 688/1000. LR: 0.0430. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9281. T_Loss: 3.5852. Mask: 0.9773. :  76%|███████▌  | 38/50 [00:20<00:07,  1.55it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9274. T_Loss: 3.5855. Mask: 0.9773. :  76%|███████▌  | 38/50 [00:21<00:07,  1.55it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9274. T_Loss: 3.5855. Mask: 0.9773. :  78%|███████▊  | 39/50 [00:21<00:06,  1.76it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9281. T_Loss: 3.5929. Mask: 0.9768. :  78%|███████▊  | 39/50 [00:21<00:06,  1.76it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9281. T_Loss: 3.5929. Mask: 0.9768. :  80%|████████  | 40/50 [00:21<00:04,  2.14it/s]total : 1000  current step :  688
total : 1000  current step :  689
total : 1000  current step :  690
Train Iter: 691/1000. LR: 0.0432. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9269. T_Loss: 3.5848. Mask: 0.9768. :  80%|████████  | 40/50 [00:22<00:04,  2.14it/s]Train Iter: 691/1000. LR: 0.0432. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9269. T_Loss: 3.5848. Mask: 0.9768. :  82%|████████▏ | 41/50 [00:22<00:05,  1.65it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9276. T_Loss: 3.5865. Mask: 0.9766. :  82%|████████▏ | 41/50 [00:22<00:05,  1.65it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9276. T_Loss: 3.5865. Mask: 0.9766. :  84%|████████▍ | 42/50 [00:22<00:04,  1.95it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9273. T_Loss: 3.5846. Mask: 0.9766. :  84%|████████▍ | 42/50 [00:23<00:04,  1.95it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9273. T_Loss: 3.5846. Mask: 0.9766. :  86%|████████▌ | 43/50 [00:23<00:03,  2.26it/s]total : 1000  current step :  691
total : 1000  current step :  692
total : 1000  current step :  693
Train Iter: 694/1000. LR: 0.0434. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9279. T_Loss: 3.5909. Mask: 0.9767. :  86%|████████▌ | 43/50 [00:23<00:03,  2.26it/s]Train Iter: 694/1000. LR: 0.0434. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9279. T_Loss: 3.5909. Mask: 0.9767. :  88%|████████▊ | 44/50 [00:23<00:03,  1.74it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9290. T_Loss: 3.5970. Mask: 0.9765. :  88%|████████▊ | 44/50 [00:24<00:03,  1.74it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9290. T_Loss: 3.5970. Mask: 0.9765. :  90%|█████████ | 45/50 [00:24<00:02,  1.97it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9304. T_Loss: 3.6030. Mask: 0.9765. :  90%|█████████ | 45/50 [00:24<00:02,  1.97it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9304. T_Loss: 3.6030. Mask: 0.9765. :  92%|█████████▏| 46/50 [00:24<00:01,  2.31it/s]total : 1000  current step :  694
total : 1000  current step :  695
total : 1000  current step :  696
Train Iter: 697/1000. LR: 0.0436. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9307. T_Loss: 3.6042. Mask: 0.9764. :  92%|█████████▏| 46/50 [00:25<00:01,  2.31it/s]Train Iter: 697/1000. LR: 0.0436. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9307. T_Loss: 3.6042. Mask: 0.9764. :  94%|█████████▍| 47/50 [00:25<00:01,  1.65it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9314. T_Loss: 3.6132. Mask: 0.9766. :  94%|█████████▍| 47/50 [00:25<00:01,  1.65it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9314. T_Loss: 3.6132. Mask: 0.9766. :  96%|█████████▌| 48/50 [00:25<00:01,  1.99it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9323. T_Loss: 3.6146. Mask: 0.9763. :  96%|█████████▌| 48/50 [00:26<00:01,  1.99it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9323. T_Loss: 3.6146. Mask: 0.9763. :  98%|█████████▊| 49/50 [00:26<00:00,  2.22it/s]total : 1000  current step :  697
total : 1000  current step :  698
total : 1000  current step :  699
Train Iter: 700/1000. LR: 0.0438. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9324. T_Loss: 3.6188. Mask: 0.9762. :  98%|█████████▊| 49/50 [00:27<00:00,  2.22it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9324. T_Loss: 3.6188. Mask: 0.9762. : 100%|██████████| 50/50 [00:27<00:00,  1.62it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9324. T_Loss: 3.6188. Mask: 0.9762. : 100%|██████████| 50/50 [00:27<00:00,  1.84it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.9408. top1: 94.53. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.9408. top1: 94.53. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9340. top1: 94.53. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9340. top1: 94.53. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.58it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9411. top1: 94.27. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.58it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9411. top1: 94.27. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9464. top1: 93.75. top5: 99.90. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9464. top1: 93.75. top5: 99.90. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9613. top1: 92.34. top5: 99.92. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9613. top1: 92.34. top5: 99.92. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9690. top1: 91.80. top5: 99.93. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9690. top1: 91.80. top5: 99.93. :  75%|███████▌  | 6/8 [00:01<00:00,  3.55it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9793. top1: 90.85. top5: 99.89. :  75%|███████▌  | 6/8 [00:02<00:00,  3.55it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9793. top1: 90.85. top5: 99.89. :  88%|████████▊ | 7/8 [00:02<00:00,  3.85it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9832. top1: 90.60. top5: 99.90. :  88%|████████▊ | 7/8 [00:02<00:00,  3.85it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9832. top1: 90.60. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  4.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9832. top1: 90.60. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  3.32it/s]
total : 1000  current step :  700
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.21s. S_Loss: 1.0090. T_Loss: 3.6864. Mask: 0.9570. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.21s. S_Loss: 1.0090. T_Loss: 3.6864. Mask: 0.9570. :   2%|▏         | 1/50 [00:00<00:10,  4.83it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.23s. S_Loss: 0.9771. T_Loss: 3.5788. Mask: 0.9707. :   2%|▏         | 1/50 [00:00<00:10,  4.83it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.23s. S_Loss: 0.9771. T_Loss: 3.5788. Mask: 0.9707. :   4%|▍         | 2/50 [00:00<00:11,  4.33it/s]total : 1000  current step :  701
total : 1000  current step :  702
Train Iter: 703/1000. LR: 0.0439. Data: 0.26s. Batch: 0.47s. S_Loss: 0.9532. T_Loss: 3.5848. Mask: 0.9753. :   4%|▍         | 2/50 [00:01<00:11,  4.33it/s]Train Iter: 703/1000. LR: 0.0439. Data: 0.26s. Batch: 0.47s. S_Loss: 0.9532. T_Loss: 3.5848. Mask: 0.9753. :   6%|▌         | 3/50 [00:01<00:26,  1.77it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.22s. Batch: 0.43s. S_Loss: 0.9452. T_Loss: 3.5274. Mask: 0.9785. :   6%|▌         | 3/50 [00:01<00:26,  1.77it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.22s. Batch: 0.43s. S_Loss: 0.9452. T_Loss: 3.5274. Mask: 0.9785. :   8%|▊         | 4/50 [00:01<00:21,  2.17it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.18s. Batch: 0.39s. S_Loss: 0.9410. T_Loss: 3.6081. Mask: 0.9781. :   8%|▊         | 4/50 [00:01<00:21,  2.17it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.18s. Batch: 0.39s. S_Loss: 0.9410. T_Loss: 3.6081. Mask: 0.9781. :  10%|█         | 5/50 [00:01<00:17,  2.59it/s]total : 1000  current step :  703
total : 1000  current step :  704
total : 1000  current step :  705
Train Iter: 706/1000. LR: 0.0441. Data: 0.27s. Batch: 0.48s. S_Loss: 0.9432. T_Loss: 3.6087. Mask: 0.9785. :  10%|█         | 5/50 [00:02<00:17,  2.59it/s]Train Iter: 706/1000. LR: 0.0441. Data: 0.27s. Batch: 0.48s. S_Loss: 0.9432. T_Loss: 3.6087. Mask: 0.9785. :  12%|█▏        | 6/50 [00:02<00:25,  1.74it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.25s. Batch: 0.47s. S_Loss: 0.9373. T_Loss: 3.5867. Mask: 0.9794. :  12%|█▏        | 6/50 [00:03<00:25,  1.74it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.25s. Batch: 0.47s. S_Loss: 0.9373. T_Loss: 3.5867. Mask: 0.9794. :  14%|█▍        | 7/50 [00:03<00:22,  1.91it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.22s. Batch: 0.45s. S_Loss: 0.9356. T_Loss: 3.5926. Mask: 0.9780. :  14%|█▍        | 7/50 [00:03<00:22,  1.91it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.22s. Batch: 0.45s. S_Loss: 0.9356. T_Loss: 3.5926. Mask: 0.9780. :  16%|█▌        | 8/50 [00:03<00:18,  2.23it/s]total : 1000  current step :  706
total : 1000  current step :  707
total : 1000  current step :  708
Train Iter: 709/1000. LR: 0.0443. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9354. T_Loss: 3.5685. Mask: 0.9783. :  16%|█▌        | 8/50 [00:04<00:18,  2.23it/s]Train Iter: 709/1000. LR: 0.0443. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9354. T_Loss: 3.5685. Mask: 0.9783. :  18%|█▊        | 9/50 [00:04<00:27,  1.50it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9341. T_Loss: 3.5374. Mask: 0.9777. :  18%|█▊        | 9/50 [00:05<00:27,  1.50it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9341. T_Loss: 3.5374. Mask: 0.9777. :  20%|██        | 10/50 [00:05<00:22,  1.79it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.25s. Batch: 0.48s. S_Loss: 0.9329. T_Loss: 3.5375. Mask: 0.9776. :  20%|██        | 10/50 [00:05<00:22,  1.79it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.25s. Batch: 0.48s. S_Loss: 0.9329. T_Loss: 3.5375. Mask: 0.9776. :  22%|██▏       | 11/50 [00:05<00:18,  2.13it/s]total : 1000  current step :  709
total : 1000  current step :  710
total : 1000  current step :  711
Train Iter: 712/1000. LR: 0.0445. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9326. T_Loss: 3.5368. Mask: 0.9766. :  22%|██▏       | 11/50 [00:06<00:18,  2.13it/s]Train Iter: 712/1000. LR: 0.0445. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9326. T_Loss: 3.5368. Mask: 0.9766. :  24%|██▍       | 12/50 [00:06<00:25,  1.51it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9325. T_Loss: 3.5396. Mask: 0.9760. :  24%|██▍       | 12/50 [00:06<00:25,  1.51it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9325. T_Loss: 3.5396. Mask: 0.9760. :  26%|██▌       | 13/50 [00:06<00:21,  1.75it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9311. T_Loss: 3.5507. Mask: 0.9766. :  26%|██▌       | 13/50 [00:07<00:21,  1.75it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9311. T_Loss: 3.5507. Mask: 0.9766. :  28%|██▊       | 14/50 [00:07<00:16,  2.17it/s]total : 1000  current step :  712
total : 1000  current step :  713
total : 1000  current step :  714
Train Iter: 715/1000. LR: 0.0447. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9285. T_Loss: 3.5325. Mask: 0.9771. :  28%|██▊       | 14/50 [00:07<00:16,  2.17it/s]Train Iter: 715/1000. LR: 0.0447. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9285. T_Loss: 3.5325. Mask: 0.9771. :  30%|███       | 15/50 [00:07<00:21,  1.64it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9271. T_Loss: 3.5396. Mask: 0.9768. :  30%|███       | 15/50 [00:08<00:21,  1.64it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9271. T_Loss: 3.5396. Mask: 0.9768. :  32%|███▏      | 16/50 [00:08<00:17,  1.96it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9275. T_Loss: 3.5497. Mask: 0.9763. :  32%|███▏      | 16/50 [00:08<00:17,  1.96it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9275. T_Loss: 3.5497. Mask: 0.9763. :  34%|███▍      | 17/50 [00:08<00:14,  2.30it/s]total : 1000  current step :  715
total : 1000  current step :  716
total : 1000  current step :  717
Train Iter: 718/1000. LR: 0.0449. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9267. T_Loss: 3.5443. Mask: 0.9766. :  34%|███▍      | 17/50 [00:09<00:14,  2.30it/s]Train Iter: 718/1000. LR: 0.0449. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9267. T_Loss: 3.5443. Mask: 0.9766. :  36%|███▌      | 18/50 [00:09<00:18,  1.71it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9235. T_Loss: 3.5345. Mask: 0.9766. :  36%|███▌      | 18/50 [00:09<00:18,  1.71it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.28s. Batch: 0.51s. S_Loss: 0.9235. T_Loss: 3.5345. Mask: 0.9766. :  38%|███▊      | 19/50 [00:09<00:15,  1.99it/s]total : 1000  current step :  718
total : 1000  current step :  719
Train Iter: 720/1000. LR: 0.0450. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9209. T_Loss: 3.5179. Mask: 0.9762. :  38%|███▊      | 19/50 [00:10<00:15,  1.99it/s]Train Iter: 720/1000. LR: 0.0450. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9209. T_Loss: 3.5179. Mask: 0.9762. :  40%|████      | 20/50 [00:10<00:16,  1.86it/s]total : 1000  current step :  720
Train Iter: 721/1000. LR: 0.0451. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9202. T_Loss: 3.5216. Mask: 0.9771. :  40%|████      | 20/50 [00:11<00:16,  1.86it/s]Train Iter: 721/1000. LR: 0.0451. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9202. T_Loss: 3.5216. Mask: 0.9771. :  42%|████▏     | 21/50 [00:11<00:20,  1.45it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9201. T_Loss: 3.5205. Mask: 0.9773. :  42%|████▏     | 21/50 [00:11<00:20,  1.45it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9201. T_Loss: 3.5205. Mask: 0.9773. :  44%|████▍     | 22/50 [00:11<00:16,  1.73it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9185. T_Loss: 3.5047. Mask: 0.9772. :  44%|████▍     | 22/50 [00:12<00:16,  1.73it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.29s. Batch: 0.52s. S_Loss: 0.9185. T_Loss: 3.5047. Mask: 0.9772. :  46%|████▌     | 23/50 [00:12<00:13,  1.95it/s]total : 1000  current step :  721
total : 1000  current step :  722
total : 1000  current step :  723
Train Iter: 724/1000. LR: 0.0453. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9186. T_Loss: 3.4942. Mask: 0.9777. :  46%|████▌     | 23/50 [00:13<00:13,  1.95it/s]Train Iter: 724/1000. LR: 0.0453. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9186. T_Loss: 3.4942. Mask: 0.9777. :  48%|████▊     | 24/50 [00:13<00:17,  1.46it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9182. T_Loss: 3.5006. Mask: 0.9769. :  48%|████▊     | 24/50 [00:13<00:17,  1.46it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9182. T_Loss: 3.5006. Mask: 0.9769. :  50%|█████     | 25/50 [00:13<00:14,  1.77it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9175. T_Loss: 3.4855. Mask: 0.9769. :  50%|█████     | 25/50 [00:13<00:14,  1.77it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9175. T_Loss: 3.4855. Mask: 0.9769. :  52%|█████▏    | 26/50 [00:13<00:11,  2.12it/s]total : 1000  current step :  724
total : 1000  current step :  725
total : 1000  current step :  726
Train Iter: 727/1000. LR: 0.0454. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9190. T_Loss: 3.5010. Mask: 0.9770. :  52%|█████▏    | 26/50 [00:14<00:11,  2.12it/s]Train Iter: 727/1000. LR: 0.0454. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9190. T_Loss: 3.5010. Mask: 0.9770. :  54%|█████▍    | 27/50 [00:14<00:14,  1.57it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9193. T_Loss: 3.5092. Mask: 0.9770. :  54%|█████▍    | 27/50 [00:15<00:14,  1.57it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9193. T_Loss: 3.5092. Mask: 0.9770. :  56%|█████▌    | 28/50 [00:15<00:11,  1.89it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9189. T_Loss: 3.5070. Mask: 0.9768. :  56%|█████▌    | 28/50 [00:15<00:11,  1.89it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9189. T_Loss: 3.5070. Mask: 0.9768. :  58%|█████▊    | 29/50 [00:15<00:09,  2.24it/s]total : 1000  current step :  727
total : 1000  current step :  728
total : 1000  current step :  729
Train Iter: 730/1000. LR: 0.0456. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9188. T_Loss: 3.5079. Mask: 0.9768. :  58%|█████▊    | 29/50 [00:16<00:09,  2.24it/s]Train Iter: 730/1000. LR: 0.0456. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9188. T_Loss: 3.5079. Mask: 0.9768. :  60%|██████    | 30/50 [00:16<00:11,  1.72it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9189. T_Loss: 3.5126. Mask: 0.9772. :  60%|██████    | 30/50 [00:16<00:11,  1.72it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9189. T_Loss: 3.5126. Mask: 0.9772. :  62%|██████▏   | 31/50 [00:16<00:09,  1.95it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9190. T_Loss: 3.5145. Mask: 0.9771. :  62%|██████▏   | 31/50 [00:16<00:09,  1.95it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9190. T_Loss: 3.5145. Mask: 0.9771. :  64%|██████▍   | 32/50 [00:16<00:08,  2.17it/s]total : 1000  current step :  730
total : 1000  current step :  731
total : 1000  current step :  732
Train Iter: 733/1000. LR: 0.0458. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9190. T_Loss: 3.5198. Mask: 0.9772. :  64%|██████▍   | 32/50 [00:17<00:08,  2.17it/s]Train Iter: 733/1000. LR: 0.0458. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9190. T_Loss: 3.5198. Mask: 0.9772. :  66%|██████▌   | 33/50 [00:17<00:10,  1.68it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9191. T_Loss: 3.5247. Mask: 0.9773. :  66%|██████▌   | 33/50 [00:18<00:10,  1.68it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9191. T_Loss: 3.5247. Mask: 0.9773. :  68%|██████▊   | 34/50 [00:18<00:08,  1.88it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9196. T_Loss: 3.5349. Mask: 0.9772. :  68%|██████▊   | 34/50 [00:18<00:08,  1.88it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9196. T_Loss: 3.5349. Mask: 0.9772. :  70%|███████   | 35/50 [00:18<00:07,  2.06it/s]total : 1000  current step :  733
total : 1000  current step :  734
total : 1000  current step :  735
Train Iter: 736/1000. LR: 0.0460. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9196. T_Loss: 3.5380. Mask: 0.9774. :  70%|███████   | 35/50 [00:19<00:07,  2.06it/s]Train Iter: 736/1000. LR: 0.0460. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9196. T_Loss: 3.5380. Mask: 0.9774. :  72%|███████▏  | 36/50 [00:19<00:09,  1.53it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9195. T_Loss: 3.5430. Mask: 0.9779. :  72%|███████▏  | 36/50 [00:19<00:09,  1.53it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9195. T_Loss: 3.5430. Mask: 0.9779. :  74%|███████▍  | 37/50 [00:19<00:07,  1.76it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9195. T_Loss: 3.5276. Mask: 0.9777. :  74%|███████▍  | 37/50 [00:20<00:07,  1.76it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9195. T_Loss: 3.5276. Mask: 0.9777. :  76%|███████▌  | 38/50 [00:20<00:05,  2.05it/s]total : 1000  current step :  736
total : 1000  current step :  737
total : 1000  current step :  738
Train Iter: 739/1000. LR: 0.0462. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9181. T_Loss: 3.5170. Mask: 0.9777. :  76%|███████▌  | 38/50 [00:21<00:05,  2.05it/s]Train Iter: 739/1000. LR: 0.0462. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9181. T_Loss: 3.5170. Mask: 0.9777. :  78%|███████▊  | 39/50 [00:21<00:06,  1.58it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9182. T_Loss: 3.5059. Mask: 0.9774. :  78%|███████▊  | 39/50 [00:21<00:06,  1.58it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9182. T_Loss: 3.5059. Mask: 0.9774. :  80%|████████  | 40/50 [00:21<00:05,  1.70it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9188. T_Loss: 3.4949. Mask: 0.9775. :  80%|████████  | 40/50 [00:22<00:05,  1.70it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9188. T_Loss: 3.4949. Mask: 0.9775. :  82%|████████▏ | 41/50 [00:22<00:04,  1.98it/s]total : 1000  current step :  739
total : 1000  current step :  740
total : 1000  current step :  741
Train Iter: 742/1000. LR: 0.0464. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9197. T_Loss: 3.4878. Mask: 0.9775. :  82%|████████▏ | 41/50 [00:23<00:04,  1.98it/s]Train Iter: 742/1000. LR: 0.0464. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9197. T_Loss: 3.4878. Mask: 0.9775. :  84%|████████▍ | 42/50 [00:23<00:05,  1.50it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9190. T_Loss: 3.4797. Mask: 0.9775. :  84%|████████▍ | 42/50 [00:23<00:05,  1.50it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9190. T_Loss: 3.4797. Mask: 0.9775. :  86%|████████▌ | 43/50 [00:23<00:03,  1.77it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9191. T_Loss: 3.4793. Mask: 0.9776. :  86%|████████▌ | 43/50 [00:23<00:03,  1.77it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9191. T_Loss: 3.4793. Mask: 0.9776. :  88%|████████▊ | 44/50 [00:23<00:02,  2.08it/s]total : 1000  current step :  742
total : 1000  current step :  743
total : 1000  current step :  744
Train Iter: 745/1000. LR: 0.0466. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9190. T_Loss: 3.4776. Mask: 0.9778. :  88%|████████▊ | 44/50 [00:24<00:02,  2.08it/s]Train Iter: 745/1000. LR: 0.0466. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9190. T_Loss: 3.4776. Mask: 0.9778. :  90%|█████████ | 45/50 [00:24<00:03,  1.65it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9189. T_Loss: 3.4691. Mask: 0.9778. :  90%|█████████ | 45/50 [00:25<00:03,  1.65it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9189. T_Loss: 3.4691. Mask: 0.9778. :  92%|█████████▏| 46/50 [00:25<00:02,  1.76it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9191. T_Loss: 3.4672. Mask: 0.9778. :  92%|█████████▏| 46/50 [00:25<00:02,  1.76it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9191. T_Loss: 3.4672. Mask: 0.9778. :  94%|█████████▍| 47/50 [00:25<00:01,  2.08it/s]total : 1000  current step :  745
total : 1000  current step :  746
total : 1000  current step :  747
Train Iter: 748/1000. LR: 0.0468. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9185. T_Loss: 3.4616. Mask: 0.9777. :  94%|█████████▍| 47/50 [00:26<00:01,  2.08it/s]Train Iter: 748/1000. LR: 0.0468. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9185. T_Loss: 3.4616. Mask: 0.9777. :  96%|█████████▌| 48/50 [00:26<00:01,  1.55it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9176. T_Loss: 3.4566. Mask: 0.9777. :  96%|█████████▌| 48/50 [00:26<00:01,  1.55it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9176. T_Loss: 3.4566. Mask: 0.9777. :  98%|█████████▊| 49/50 [00:26<00:00,  1.83it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9173. T_Loss: 3.4467. Mask: 0.9773. :  98%|█████████▊| 49/50 [00:26<00:00,  1.83it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9173. T_Loss: 3.4467. Mask: 0.9773. : 100%|██████████| 50/50 [00:26<00:00,  2.14it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9173. T_Loss: 3.4467. Mask: 0.9773. : 100%|██████████| 50/50 [00:26<00:00,  1.85it/s]
total : 1000  current step :  748
total : 1000  current step :  749
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.9222. top1: 94.14. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.9222. top1: 94.14. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9183. top1: 94.34. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9183. top1: 94.34. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.56it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9254. top1: 94.14. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.56it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9254. top1: 94.14. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  3.12it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9313. top1: 93.75. top5: 99.90. :  38%|███▊      | 3/8 [00:01<00:01,  3.12it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9313. top1: 93.75. top5: 99.90. :  50%|█████     | 4/8 [00:01<00:01,  3.29it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9443. top1: 92.73. top5: 99.92. :  50%|█████     | 4/8 [00:01<00:01,  3.29it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9443. top1: 92.73. top5: 99.92. :  62%|██████▎   | 5/8 [00:01<00:00,  3.68it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9507. top1: 92.19. top5: 99.93. :  62%|██████▎   | 5/8 [00:01<00:00,  3.68it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9507. top1: 92.19. top5: 99.93. :  75%|███████▌  | 6/8 [00:01<00:00,  3.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9604. top1: 91.24. top5: 99.89. :  75%|███████▌  | 6/8 [00:02<00:00,  3.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9604. top1: 91.24. top5: 99.89. :  88%|████████▊ | 7/8 [00:02<00:00,  3.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9639. top1: 91.00. top5: 99.90. :  88%|████████▊ | 7/8 [00:02<00:00,  3.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9639. top1: 91.00. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  4.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9639. top1: 91.00. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  3.28it/s]
total : 1000  current step :  750
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 0.76s. Batch: 0.98s. S_Loss: 0.9623. T_Loss: 3.6524. Mask: 0.9609. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 0.76s. Batch: 0.98s. S_Loss: 0.9623. T_Loss: 3.6524. Mask: 0.9609. :   2%|▏         | 1/50 [00:00<00:48,  1.02it/s]Train Iter: 752/1000. LR: 0.0470. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9338. T_Loss: 3.6044. Mask: 0.9629. :   2%|▏         | 1/50 [00:01<00:48,  1.02it/s]Train Iter: 752/1000. LR: 0.0470. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9338. T_Loss: 3.6044. Mask: 0.9629. :   4%|▍         | 2/50 [00:01<00:28,  1.67it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9275. T_Loss: 3.5792. Mask: 0.9701. :   4%|▍         | 2/50 [00:01<00:28,  1.67it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9275. T_Loss: 3.5792. Mask: 0.9701. :   6%|▌         | 3/50 [00:01<00:20,  2.32it/s]total : 1000  current step :  751
total : 1000  current step :  752
total : 1000  current step :  753
Train Iter: 754/1000. LR: 0.0471. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9286. T_Loss: 3.6362. Mask: 0.9766. :   6%|▌         | 3/50 [00:02<00:20,  2.32it/s]Train Iter: 754/1000. LR: 0.0471. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9286. T_Loss: 3.6362. Mask: 0.9766. :   8%|▊         | 4/50 [00:02<00:31,  1.47it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9325. T_Loss: 3.6951. Mask: 0.9750. :   8%|▊         | 4/50 [00:02<00:31,  1.47it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9325. T_Loss: 3.6951. Mask: 0.9750. :  10%|█         | 5/50 [00:02<00:24,  1.87it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9368. T_Loss: 3.6730. Mask: 0.9746. :  10%|█         | 5/50 [00:03<00:24,  1.87it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9368. T_Loss: 3.6730. Mask: 0.9746. :  12%|█▏        | 6/50 [00:03<00:19,  2.30it/s]total : 1000  current step :  754
total : 1000  current step :  755
total : 1000  current step :  756
Train Iter: 757/1000. LR: 0.0473. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9391. T_Loss: 3.6703. Mask: 0.9749. :  12%|█▏        | 6/50 [00:04<00:19,  2.30it/s]Train Iter: 757/1000. LR: 0.0473. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9391. T_Loss: 3.6703. Mask: 0.9749. :  14%|█▍        | 7/50 [00:04<00:26,  1.59it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.35s. Batch: 0.56s. S_Loss: 0.9375. T_Loss: 3.6553. Mask: 0.9746. :  14%|█▍        | 7/50 [00:04<00:26,  1.59it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.35s. Batch: 0.56s. S_Loss: 0.9375. T_Loss: 3.6553. Mask: 0.9746. :  16%|█▌        | 8/50 [00:04<00:22,  1.88it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9343. T_Loss: 3.6374. Mask: 0.9757. :  16%|█▌        | 8/50 [00:04<00:22,  1.88it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9343. T_Loss: 3.6374. Mask: 0.9757. :  18%|█▊        | 9/50 [00:04<00:17,  2.37it/s]total : 1000  current step :  757
total : 1000  current step :  758
total : 1000  current step :  759
Train Iter: 760/1000. LR: 0.0475. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9295. T_Loss: 3.6051. Mask: 0.9773. :  18%|█▊        | 9/50 [00:05<00:17,  2.37it/s]Train Iter: 760/1000. LR: 0.0475. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9295. T_Loss: 3.6051. Mask: 0.9773. :  20%|██        | 10/50 [00:05<00:26,  1.50it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9279. T_Loss: 3.6181. Mask: 0.9776. :  20%|██        | 10/50 [00:06<00:26,  1.50it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9279. T_Loss: 3.6181. Mask: 0.9776. :  22%|██▏       | 11/50 [00:06<00:25,  1.51it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9300. T_Loss: 3.6135. Mask: 0.9766. :  22%|██▏       | 11/50 [00:06<00:25,  1.51it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9300. T_Loss: 3.6135. Mask: 0.9766. :  24%|██▍       | 12/50 [00:06<00:22,  1.71it/s]total : 1000  current step :  760
total : 1000  current step :  761
total : 1000  current step :  762
Train Iter: 763/1000. LR: 0.0477. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9287. T_Loss: 3.6029. Mask: 0.9760. :  24%|██▍       | 12/50 [00:07<00:22,  1.71it/s]Train Iter: 763/1000. LR: 0.0477. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9287. T_Loss: 3.6029. Mask: 0.9760. :  26%|██▌       | 13/50 [00:07<00:24,  1.50it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9300. T_Loss: 3.6081. Mask: 0.9752. :  26%|██▌       | 13/50 [00:08<00:24,  1.50it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9300. T_Loss: 3.6081. Mask: 0.9752. :  28%|██▊       | 14/50 [00:08<00:20,  1.74it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9296. T_Loss: 3.5934. Mask: 0.9760. :  28%|██▊       | 14/50 [00:08<00:20,  1.74it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9296. T_Loss: 3.5934. Mask: 0.9760. :  30%|███       | 15/50 [00:08<00:17,  2.04it/s]total : 1000  current step :  763
total : 1000  current step :  764
total : 1000  current step :  765
Train Iter: 766/1000. LR: 0.0479. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9288. T_Loss: 3.5859. Mask: 0.9771. :  30%|███       | 15/50 [00:09<00:17,  2.04it/s]Train Iter: 766/1000. LR: 0.0479. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9288. T_Loss: 3.5859. Mask: 0.9771. :  32%|███▏      | 16/50 [00:09<00:20,  1.65it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.37s. Batch: 0.56s. S_Loss: 0.9296. T_Loss: 3.5760. Mask: 0.9775. :  32%|███▏      | 16/50 [00:09<00:20,  1.65it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.37s. Batch: 0.56s. S_Loss: 0.9296. T_Loss: 3.5760. Mask: 0.9775. :  34%|███▍      | 17/50 [00:09<00:16,  1.95it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9306. T_Loss: 3.5704. Mask: 0.9770. :  34%|███▍      | 17/50 [00:09<00:16,  1.95it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9306. T_Loss: 3.5704. Mask: 0.9770. :  36%|███▌      | 18/50 [00:09<00:14,  2.20it/s]total : 1000  current step :  766
total : 1000  current step :  767
total : 1000  current step :  768
Train Iter: 769/1000. LR: 0.0481. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9299. T_Loss: 3.5545. Mask: 0.9776. :  36%|███▌      | 18/50 [00:10<00:14,  2.20it/s]Train Iter: 769/1000. LR: 0.0481. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9299. T_Loss: 3.5545. Mask: 0.9776. :  38%|███▊      | 19/50 [00:10<00:17,  1.75it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9288. T_Loss: 3.5536. Mask: 0.9775. :  38%|███▊      | 19/50 [00:11<00:17,  1.75it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9288. T_Loss: 3.5536. Mask: 0.9775. :  40%|████      | 20/50 [00:11<00:14,  2.02it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9279. T_Loss: 3.5473. Mask: 0.9779. :  40%|████      | 20/50 [00:11<00:14,  2.02it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9279. T_Loss: 3.5473. Mask: 0.9779. :  42%|████▏     | 21/50 [00:11<00:11,  2.44it/s]total : 1000  current step :  769
total : 1000  current step :  770
total : 1000  current step :  771
Train Iter: 772/1000. LR: 0.0483. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9285. T_Loss: 3.5539. Mask: 0.9778. :  42%|████▏     | 21/50 [00:12<00:11,  2.44it/s]Train Iter: 772/1000. LR: 0.0483. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9285. T_Loss: 3.5539. Mask: 0.9778. :  44%|████▍     | 22/50 [00:12<00:15,  1.78it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9284. T_Loss: 3.5390. Mask: 0.9781. :  44%|████▍     | 22/50 [00:12<00:15,  1.78it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9284. T_Loss: 3.5390. Mask: 0.9781. :  46%|████▌     | 23/50 [00:12<00:13,  2.05it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9275. T_Loss: 3.5358. Mask: 0.9782. :  46%|████▌     | 23/50 [00:12<00:13,  2.05it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9275. T_Loss: 3.5358. Mask: 0.9782. :  48%|████▊     | 24/50 [00:12<00:11,  2.23it/s]total : 1000  current step :  772
total : 1000  current step :  773
total : 1000  current step :  774
Train Iter: 775/1000. LR: 0.0484. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9270. T_Loss: 3.5285. Mask: 0.9788. :  48%|████▊     | 24/50 [00:13<00:11,  2.23it/s]Train Iter: 775/1000. LR: 0.0484. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9270. T_Loss: 3.5285. Mask: 0.9788. :  50%|█████     | 25/50 [00:13<00:15,  1.65it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9259. T_Loss: 3.5267. Mask: 0.9790. :  50%|█████     | 25/50 [00:14<00:15,  1.65it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9259. T_Loss: 3.5267. Mask: 0.9790. :  52%|█████▏    | 26/50 [00:14<00:12,  1.93it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9252. T_Loss: 3.5370. Mask: 0.9789. :  52%|█████▏    | 26/50 [00:14<00:12,  1.93it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9252. T_Loss: 3.5370. Mask: 0.9789. :  54%|█████▍    | 27/50 [00:14<00:09,  2.34it/s]total : 1000  current step :  775
total : 1000  current step :  776
total : 1000  current step :  777
Train Iter: 778/1000. LR: 0.0486. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9232. T_Loss: 3.5274. Mask: 0.9787. :  54%|█████▍    | 27/50 [00:15<00:09,  2.34it/s]Train Iter: 778/1000. LR: 0.0486. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9232. T_Loss: 3.5274. Mask: 0.9787. :  56%|█████▌    | 28/50 [00:15<00:12,  1.79it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9237. T_Loss: 3.5333. Mask: 0.9784. :  56%|█████▌    | 28/50 [00:15<00:12,  1.79it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9237. T_Loss: 3.5333. Mask: 0.9784. :  58%|█████▊    | 29/50 [00:15<00:10,  2.07it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9245. T_Loss: 3.5370. Mask: 0.9788. :  58%|█████▊    | 29/50 [00:15<00:10,  2.07it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9245. T_Loss: 3.5370. Mask: 0.9788. :  60%|██████    | 30/50 [00:15<00:08,  2.22it/s]total : 1000  current step :  778
total : 1000  current step :  779
total : 1000  current step :  780
Train Iter: 781/1000. LR: 0.0488. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9238. T_Loss: 3.5358. Mask: 0.9783. :  60%|██████    | 30/50 [00:16<00:08,  2.22it/s]Train Iter: 781/1000. LR: 0.0488. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9238. T_Loss: 3.5358. Mask: 0.9783. :  62%|██████▏   | 31/50 [00:16<00:11,  1.73it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9232. T_Loss: 3.5353. Mask: 0.9789. :  62%|██████▏   | 31/50 [00:17<00:11,  1.73it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9232. T_Loss: 3.5353. Mask: 0.9789. :  64%|██████▍   | 32/50 [00:17<00:09,  1.96it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9226. T_Loss: 3.5338. Mask: 0.9785. :  64%|██████▍   | 32/50 [00:17<00:09,  1.96it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9226. T_Loss: 3.5338. Mask: 0.9785. :  66%|██████▌   | 33/50 [00:17<00:07,  2.23it/s]total : 1000  current step :  781
total : 1000  current step :  782
total : 1000  current step :  783
Train Iter: 784/1000. LR: 0.0490. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9212. T_Loss: 3.5216. Mask: 0.9786. :  66%|██████▌   | 33/50 [00:18<00:07,  2.23it/s]Train Iter: 784/1000. LR: 0.0490. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9212. T_Loss: 3.5216. Mask: 0.9786. :  68%|██████▊   | 34/50 [00:18<00:09,  1.63it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9201. T_Loss: 3.5089. Mask: 0.9790. :  68%|██████▊   | 34/50 [00:18<00:09,  1.63it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9201. T_Loss: 3.5089. Mask: 0.9790. :  70%|███████   | 35/50 [00:18<00:08,  1.85it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9202. T_Loss: 3.4986. Mask: 0.9788. :  70%|███████   | 35/50 [00:19<00:08,  1.85it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9202. T_Loss: 3.4986. Mask: 0.9788. :  72%|███████▏  | 36/50 [00:19<00:06,  2.21it/s]total : 1000  current step :  784
total : 1000  current step :  785
total : 1000  current step :  786
Train Iter: 787/1000. LR: 0.0492. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9196. T_Loss: 3.4921. Mask: 0.9792. :  72%|███████▏  | 36/50 [00:19<00:06,  2.21it/s]Train Iter: 787/1000. LR: 0.0492. Data: 0.33s. Batch: 0.54s. S_Loss: 0.9196. T_Loss: 3.4921. Mask: 0.9792. :  74%|███████▍  | 37/50 [00:19<00:07,  1.69it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9201. T_Loss: 3.4909. Mask: 0.9794. :  74%|███████▍  | 37/50 [00:20<00:07,  1.69it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9201. T_Loss: 3.4909. Mask: 0.9794. :  76%|███████▌  | 38/50 [00:20<00:06,  1.99it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9202. T_Loss: 3.4852. Mask: 0.9795. :  76%|███████▌  | 38/50 [00:20<00:06,  1.99it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9202. T_Loss: 3.4852. Mask: 0.9795. :  78%|███████▊  | 39/50 [00:20<00:04,  2.35it/s]total : 1000  current step :  787
total : 1000  current step :  788
total : 1000  current step :  789
Train Iter: 790/1000. LR: 0.0494. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9190. T_Loss: 3.4782. Mask: 0.9799. :  78%|███████▊  | 39/50 [00:21<00:04,  2.35it/s]Train Iter: 790/1000. LR: 0.0494. Data: 0.33s. Batch: 0.53s. S_Loss: 0.9190. T_Loss: 3.4782. Mask: 0.9799. :  80%|████████  | 40/50 [00:21<00:05,  1.83it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9192. T_Loss: 3.4758. Mask: 0.9796. :  80%|████████  | 40/50 [00:21<00:05,  1.83it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9192. T_Loss: 3.4758. Mask: 0.9796. :  82%|████████▏ | 41/50 [00:21<00:04,  2.00it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9193. T_Loss: 3.4699. Mask: 0.9794. :  82%|████████▏ | 41/50 [00:21<00:04,  2.00it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9193. T_Loss: 3.4699. Mask: 0.9794. :  84%|████████▍ | 42/50 [00:21<00:03,  2.42it/s]total : 1000  current step :  790
total : 1000  current step :  791
total : 1000  current step :  792
Train Iter: 793/1000. LR: 0.0496. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9187. T_Loss: 3.4700. Mask: 0.9792. :  84%|████████▍ | 42/50 [00:22<00:03,  2.42it/s]Train Iter: 793/1000. LR: 0.0496. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9187. T_Loss: 3.4700. Mask: 0.9792. :  86%|████████▌ | 43/50 [00:22<00:04,  1.72it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9177. T_Loss: 3.4608. Mask: 0.9795. :  86%|████████▌ | 43/50 [00:23<00:04,  1.72it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9177. T_Loss: 3.4608. Mask: 0.9795. :  88%|████████▊ | 44/50 [00:23<00:02,  2.04it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9177. T_Loss: 3.4549. Mask: 0.9796. :  88%|████████▊ | 44/50 [00:23<00:02,  2.04it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9177. T_Loss: 3.4549. Mask: 0.9796. :  90%|█████████ | 45/50 [00:23<00:02,  2.42it/s]total : 1000  current step :  793
total : 1000  current step :  794
total : 1000  current step :  795
Train Iter: 796/1000. LR: 0.0498. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9168. T_Loss: 3.4466. Mask: 0.9794. :  90%|█████████ | 45/50 [00:24<00:02,  2.42it/s]Train Iter: 796/1000. LR: 0.0498. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9168. T_Loss: 3.4466. Mask: 0.9794. :  92%|█████████▏| 46/50 [00:24<00:02,  1.65it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9167. T_Loss: 3.4455. Mask: 0.9796. :  92%|█████████▏| 46/50 [00:24<00:02,  1.65it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9167. T_Loss: 3.4455. Mask: 0.9796. :  94%|█████████▍| 47/50 [00:24<00:01,  1.99it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9157. T_Loss: 3.4445. Mask: 0.9797. :  94%|█████████▍| 47/50 [00:25<00:01,  1.99it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9157. T_Loss: 3.4445. Mask: 0.9797. :  96%|█████████▌| 48/50 [00:25<00:00,  2.21it/s]total : 1000  current step :  796
total : 1000  current step :  797
total : 1000  current step :  798
Train Iter: 799/1000. LR: 0.0499. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9150. T_Loss: 3.4403. Mask: 0.9796. :  96%|█████████▌| 48/50 [00:26<00:00,  2.21it/s]Train Iter: 799/1000. LR: 0.0499. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9150. T_Loss: 3.4403. Mask: 0.9796. :  98%|█████████▊| 49/50 [00:26<00:00,  1.67it/s]total : 1000  current step :  799
Train Iter: 800/1000. LR: 0.0500. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9140. T_Loss: 3.4324. Mask: 0.9795. :  98%|█████████▊| 49/50 [00:26<00:00,  1.67it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9140. T_Loss: 3.4324. Mask: 0.9795. : 100%|██████████| 50/50 [00:26<00:00,  1.69it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9140. T_Loss: 3.4324. Mask: 0.9795. : 100%|██████████| 50/50 [00:26<00:00,  1.88it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.9079. top1: 94.92. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.9079. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.17it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9066. top1: 94.53. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.17it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9066. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9140. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9140. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9206. top1: 93.85. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9206. top1: 93.85. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9302. top1: 93.12. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.95it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9302. top1: 93.12. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  4.12it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9346. top1: 92.64. top5: 99.87. :  62%|██████▎   | 5/8 [00:01<00:00,  4.12it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.9346. top1: 92.64. top5: 99.87. :  75%|███████▌  | 6/8 [00:01<00:00,  4.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9426. top1: 91.74. top5: 99.83. :  75%|███████▌  | 6/8 [00:01<00:00,  4.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.25s. Loss: 0.9426. top1: 91.74. top5: 99.83. :  88%|████████▊ | 7/8 [00:01<00:00,  4.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9451. top1: 91.55. top5: 99.85. :  88%|████████▊ | 7/8 [00:01<00:00,  4.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9451. top1: 91.55. top5: 99.85. : 100%|██████████| 8/8 [00:01<00:00,  4.77it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.24s. Loss: 0.9451. top1: 91.55. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.84it/s]
total : 1000  current step :  800
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.00s. Batch: 0.26s. S_Loss: 0.9205. T_Loss: 3.0253. Mask: 0.9688. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.00s. Batch: 0.26s. S_Loss: 0.9205. T_Loss: 3.0253. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:12,  3.87it/s]total : 1000  current step :  801
Train Iter: 802/1000. LR: 0.0500. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9113. T_Loss: 3.2327. Mask: 0.9746. :   2%|▏         | 1/50 [00:01<00:12,  3.87it/s]Train Iter: 802/1000. LR: 0.0500. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9113. T_Loss: 3.2327. Mask: 0.9746. :   4%|▍         | 2/50 [00:01<00:30,  1.56it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.27s. Batch: 0.49s. S_Loss: 0.8982. T_Loss: 3.1554. Mask: 0.9740. :   4%|▍         | 2/50 [00:01<00:30,  1.56it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.27s. Batch: 0.49s. S_Loss: 0.8982. T_Loss: 3.1554. Mask: 0.9740. :   6%|▌         | 3/50 [00:01<00:22,  2.06it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.23s. Batch: 0.47s. S_Loss: 0.9004. T_Loss: 3.1611. Mask: 0.9775. :   6%|▌         | 3/50 [00:01<00:22,  2.06it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.23s. Batch: 0.47s. S_Loss: 0.9004. T_Loss: 3.1611. Mask: 0.9775. :   8%|▊         | 4/50 [00:01<00:21,  2.19it/s]total : 1000  current step :  802
total : 1000  current step :  803
total : 1000  current step :  804
Train Iter: 805/1000. LR: 0.0499. Data: 0.34s. Batch: 0.57s. S_Loss: 0.9002. T_Loss: 3.1627. Mask: 0.9789. :   8%|▊         | 4/50 [00:02<00:21,  2.19it/s]Train Iter: 805/1000. LR: 0.0499. Data: 0.34s. Batch: 0.57s. S_Loss: 0.9002. T_Loss: 3.1627. Mask: 0.9789. :  10%|█         | 5/50 [00:02<00:29,  1.53it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9069. T_Loss: 3.1886. Mask: 0.9811. :  10%|█         | 5/50 [00:03<00:29,  1.53it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9069. T_Loss: 3.1886. Mask: 0.9811. :  12%|█▏        | 6/50 [00:03<00:24,  1.80it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9118. T_Loss: 3.2710. Mask: 0.9816. :  12%|█▏        | 6/50 [00:03<00:24,  1.80it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.26s. Batch: 0.50s. S_Loss: 0.9118. T_Loss: 3.2710. Mask: 0.9816. :  14%|█▍        | 7/50 [00:03<00:19,  2.19it/s]total : 1000  current step :  805
total : 1000  current step :  806
total : 1000  current step :  807
Train Iter: 808/1000. LR: 0.0498. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9117. T_Loss: 3.2873. Mask: 0.9814. :  14%|█▍        | 7/50 [00:04<00:19,  2.19it/s]Train Iter: 808/1000. LR: 0.0498. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9117. T_Loss: 3.2873. Mask: 0.9814. :  16%|█▌        | 8/50 [00:04<00:24,  1.71it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9107. T_Loss: 3.2631. Mask: 0.9805. :  16%|█▌        | 8/50 [00:04<00:24,  1.71it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9107. T_Loss: 3.2631. Mask: 0.9805. :  18%|█▊        | 9/50 [00:04<00:21,  1.93it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9141. T_Loss: 3.2962. Mask: 0.9801. :  18%|█▊        | 9/50 [00:05<00:21,  1.93it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9141. T_Loss: 3.2962. Mask: 0.9801. :  20%|██        | 10/50 [00:05<00:19,  2.02it/s]total : 1000  current step :  808
total : 1000  current step :  809
total : 1000  current step :  810
Train Iter: 811/1000. LR: 0.0496. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9134. T_Loss: 3.3017. Mask: 0.9801. :  20%|██        | 10/50 [00:06<00:19,  2.02it/s]Train Iter: 811/1000. LR: 0.0496. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9134. T_Loss: 3.3017. Mask: 0.9801. :  22%|██▏       | 11/50 [00:06<00:27,  1.42it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9133. T_Loss: 3.2941. Mask: 0.9805. :  22%|██▏       | 11/50 [00:06<00:27,  1.42it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9133. T_Loss: 3.2941. Mask: 0.9805. :  24%|██▍       | 12/50 [00:06<00:21,  1.77it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9128. T_Loss: 3.3297. Mask: 0.9811. :  24%|██▍       | 12/50 [00:06<00:21,  1.77it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9128. T_Loss: 3.3297. Mask: 0.9811. :  26%|██▌       | 13/50 [00:06<00:17,  2.11it/s]total : 1000  current step :  811
total : 1000  current step :  812
total : 1000  current step :  813
Train Iter: 814/1000. LR: 0.0494. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9123. T_Loss: 3.3423. Mask: 0.9824. :  26%|██▌       | 13/50 [00:07<00:17,  2.11it/s]Train Iter: 814/1000. LR: 0.0494. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9123. T_Loss: 3.3423. Mask: 0.9824. :  28%|██▊       | 14/50 [00:07<00:21,  1.69it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9112. T_Loss: 3.3372. Mask: 0.9828. :  28%|██▊       | 14/50 [00:08<00:21,  1.69it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9112. T_Loss: 3.3372. Mask: 0.9828. :  30%|███       | 15/50 [00:08<00:18,  1.88it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9113. T_Loss: 3.3404. Mask: 0.9822. :  30%|███       | 15/50 [00:08<00:18,  1.88it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9113. T_Loss: 3.3404. Mask: 0.9822. :  32%|███▏      | 16/50 [00:08<00:14,  2.31it/s]total : 1000  current step :  814
total : 1000  current step :  815
total : 1000  current step :  816
Train Iter: 817/1000. LR: 0.0491. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9122. T_Loss: 3.3457. Mask: 0.9825. :  32%|███▏      | 16/50 [00:09<00:14,  2.31it/s]Train Iter: 817/1000. LR: 0.0491. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9122. T_Loss: 3.3457. Mask: 0.9825. :  34%|███▍      | 17/50 [00:09<00:18,  1.80it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9100. T_Loss: 3.3614. Mask: 0.9833. :  34%|███▍      | 17/50 [00:09<00:18,  1.80it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9100. T_Loss: 3.3614. Mask: 0.9833. :  36%|███▌      | 18/50 [00:09<00:15,  2.06it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9117. T_Loss: 3.3569. Mask: 0.9823. :  36%|███▌      | 18/50 [00:09<00:15,  2.06it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9117. T_Loss: 3.3569. Mask: 0.9823. :  38%|███▊      | 19/50 [00:09<00:13,  2.38it/s]total : 1000  current step :  817
total : 1000  current step :  818
total : 1000  current step :  819
Train Iter: 820/1000. LR: 0.0488. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9130. T_Loss: 3.3671. Mask: 0.9820. :  38%|███▊      | 19/50 [00:10<00:13,  2.38it/s]Train Iter: 820/1000. LR: 0.0488. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9130. T_Loss: 3.3671. Mask: 0.9820. :  40%|████      | 20/50 [00:10<00:17,  1.74it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9136. T_Loss: 3.3723. Mask: 0.9814. :  40%|████      | 20/50 [00:11<00:17,  1.74it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9136. T_Loss: 3.3723. Mask: 0.9814. :  42%|████▏     | 21/50 [00:11<00:14,  1.98it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9121. T_Loss: 3.3589. Mask: 0.9819. :  42%|████▏     | 21/50 [00:11<00:14,  1.98it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9121. T_Loss: 3.3589. Mask: 0.9819. :  44%|████▍     | 22/50 [00:11<00:11,  2.35it/s]total : 1000  current step :  820
total : 1000  current step :  821
total : 1000  current step :  822
Train Iter: 823/1000. LR: 0.0484. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9095. T_Loss: 3.3421. Mask: 0.9820. :  44%|████▍     | 22/50 [00:12<00:11,  2.35it/s]Train Iter: 823/1000. LR: 0.0484. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9095. T_Loss: 3.3421. Mask: 0.9820. :  46%|████▌     | 23/50 [00:12<00:15,  1.72it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9101. T_Loss: 3.3449. Mask: 0.9821. :  46%|████▌     | 23/50 [00:12<00:15,  1.72it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9101. T_Loss: 3.3449. Mask: 0.9821. :  48%|████▊     | 24/50 [00:12<00:12,  2.07it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9119. T_Loss: 3.3556. Mask: 0.9819. :  48%|████▊     | 24/50 [00:12<00:12,  2.07it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9119. T_Loss: 3.3556. Mask: 0.9819. :  50%|█████     | 25/50 [00:12<00:11,  2.21it/s]total : 1000  current step :  823
total : 1000  current step :  824
total : 1000  current step :  825
Train Iter: 826/1000. LR: 0.0479. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9123. T_Loss: 3.3487. Mask: 0.9818. :  50%|█████     | 25/50 [00:13<00:11,  2.21it/s]Train Iter: 826/1000. LR: 0.0479. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9123. T_Loss: 3.3487. Mask: 0.9818. :  52%|█████▏    | 26/50 [00:13<00:15,  1.57it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9125. T_Loss: 3.3467. Mask: 0.9818. :  52%|█████▏    | 26/50 [00:14<00:15,  1.57it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9125. T_Loss: 3.3467. Mask: 0.9818. :  54%|█████▍    | 27/50 [00:14<00:11,  2.00it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9114. T_Loss: 3.3452. Mask: 0.9817. :  54%|█████▍    | 27/50 [00:14<00:11,  2.00it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9114. T_Loss: 3.3452. Mask: 0.9817. :  56%|█████▌    | 28/50 [00:14<00:09,  2.37it/s]total : 1000  current step :  826
total : 1000  current step :  827
total : 1000  current step :  828
Train Iter: 829/1000. LR: 0.0475. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9117. T_Loss: 3.3439. Mask: 0.9814. :  56%|█████▌    | 28/50 [00:15<00:09,  2.37it/s]Train Iter: 829/1000. LR: 0.0475. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9117. T_Loss: 3.3439. Mask: 0.9814. :  58%|█████▊    | 29/50 [00:15<00:13,  1.59it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9115. T_Loss: 3.3396. Mask: 0.9815. :  58%|█████▊    | 29/50 [00:15<00:13,  1.59it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9115. T_Loss: 3.3396. Mask: 0.9815. :  60%|██████    | 30/50 [00:15<00:10,  1.99it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.30s. Batch: 0.51s. S_Loss: 0.9119. T_Loss: 3.3413. Mask: 0.9817. :  60%|██████    | 30/50 [00:15<00:10,  1.99it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.30s. Batch: 0.51s. S_Loss: 0.9119. T_Loss: 3.3413. Mask: 0.9817. :  62%|██████▏   | 31/50 [00:15<00:08,  2.33it/s]total : 1000  current step :  829
total : 1000  current step :  830
total : 1000  current step :  831
Train Iter: 832/1000. LR: 0.0469. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9116. T_Loss: 3.3285. Mask: 0.9817. :  62%|██████▏   | 31/50 [00:16<00:08,  2.33it/s]Train Iter: 832/1000. LR: 0.0469. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9116. T_Loss: 3.3285. Mask: 0.9817. :  64%|██████▍   | 32/50 [00:16<00:10,  1.65it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9116. T_Loss: 3.3193. Mask: 0.9818. :  64%|██████▍   | 32/50 [00:17<00:10,  1.65it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9116. T_Loss: 3.3193. Mask: 0.9818. :  66%|██████▌   | 33/50 [00:17<00:08,  2.01it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9111. T_Loss: 3.3086. Mask: 0.9817. :  66%|██████▌   | 33/50 [00:17<00:08,  2.01it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9111. T_Loss: 3.3086. Mask: 0.9817. :  68%|██████▊   | 34/50 [00:17<00:06,  2.49it/s]total : 1000  current step :  832
total : 1000  current step :  833
total : 1000  current step :  834
Train Iter: 835/1000. LR: 0.0463. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9102. T_Loss: 3.2991. Mask: 0.9817. :  68%|██████▊   | 34/50 [00:18<00:06,  2.49it/s]Train Iter: 835/1000. LR: 0.0463. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9102. T_Loss: 3.2991. Mask: 0.9817. :  70%|███████   | 35/50 [00:18<00:09,  1.65it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9093. T_Loss: 3.2896. Mask: 0.9818. :  70%|███████   | 35/50 [00:18<00:09,  1.65it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9093. T_Loss: 3.2896. Mask: 0.9818. :  72%|███████▏  | 36/50 [00:18<00:07,  1.96it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9084. T_Loss: 3.2780. Mask: 0.9822. :  72%|███████▏  | 36/50 [00:18<00:07,  1.96it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9084. T_Loss: 3.2780. Mask: 0.9822. :  74%|███████▍  | 37/50 [00:18<00:05,  2.40it/s]total : 1000  current step :  835
total : 1000  current step :  836
total : 1000  current step :  837
Train Iter: 838/1000. LR: 0.0457. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9083. T_Loss: 3.2791. Mask: 0.9821. :  74%|███████▍  | 37/50 [00:20<00:05,  2.40it/s]Train Iter: 838/1000. LR: 0.0457. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9083. T_Loss: 3.2791. Mask: 0.9821. :  76%|███████▌  | 38/50 [00:20<00:07,  1.59it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9078. T_Loss: 3.2672. Mask: 0.9821. :  76%|███████▌  | 38/50 [00:20<00:07,  1.59it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9078. T_Loss: 3.2672. Mask: 0.9821. :  78%|███████▊  | 39/50 [00:20<00:05,  1.97it/s]total : 1000  current step :  838
total : 1000  current step :  839
Train Iter: 840/1000. LR: 0.0452. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9071. T_Loss: 3.2524. Mask: 0.9819. :  78%|███████▊  | 39/50 [00:20<00:05,  1.97it/s]Train Iter: 840/1000. LR: 0.0452. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9071. T_Loss: 3.2524. Mask: 0.9819. :  80%|████████  | 40/50 [00:20<00:05,  1.89it/s]total : 1000  current step :  840
Train Iter: 841/1000. LR: 0.0450. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9067. T_Loss: 3.2547. Mask: 0.9821. :  80%|████████  | 40/50 [00:21<00:05,  1.89it/s]Train Iter: 841/1000. LR: 0.0450. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9067. T_Loss: 3.2547. Mask: 0.9821. :  82%|████████▏ | 41/50 [00:21<00:06,  1.44it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9068. T_Loss: 3.2538. Mask: 0.9823. :  82%|████████▏ | 41/50 [00:22<00:06,  1.44it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9068. T_Loss: 3.2538. Mask: 0.9823. :  84%|████████▍ | 42/50 [00:22<00:04,  1.77it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9066. T_Loss: 3.2598. Mask: 0.9824. :  84%|████████▍ | 42/50 [00:22<00:04,  1.77it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.30s. Batch: 0.52s. S_Loss: 0.9066. T_Loss: 3.2598. Mask: 0.9824. :  86%|████████▌ | 43/50 [00:22<00:03,  2.08it/s]total : 1000  current step :  841
total : 1000  current step :  842
total : 1000  current step :  843
Train Iter: 844/1000. LR: 0.0443. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9073. T_Loss: 3.2691. Mask: 0.9825. :  86%|████████▌ | 43/50 [00:23<00:03,  2.08it/s]Train Iter: 844/1000. LR: 0.0443. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9073. T_Loss: 3.2691. Mask: 0.9825. :  88%|████████▊ | 44/50 [00:23<00:03,  1.59it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9087. T_Loss: 3.2729. Mask: 0.9828. :  88%|████████▊ | 44/50 [00:23<00:03,  1.59it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.31s. Batch: 0.53s. S_Loss: 0.9087. T_Loss: 3.2729. Mask: 0.9828. :  90%|█████████ | 45/50 [00:23<00:02,  1.87it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9090. T_Loss: 3.2778. Mask: 0.9826. :  90%|█████████ | 45/50 [00:24<00:02,  1.87it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9090. T_Loss: 3.2778. Mask: 0.9826. :  92%|█████████▏| 46/50 [00:24<00:01,  2.11it/s]total : 1000  current step :  844
total : 1000  current step :  845
total : 1000  current step :  846
Train Iter: 847/1000. LR: 0.0435. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9090. T_Loss: 3.2835. Mask: 0.9827. :  92%|█████████▏| 46/50 [00:24<00:01,  2.11it/s]Train Iter: 847/1000. LR: 0.0435. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9090. T_Loss: 3.2835. Mask: 0.9827. :  94%|█████████▍| 47/50 [00:24<00:01,  1.73it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9086. T_Loss: 3.2896. Mask: 0.9831. :  94%|█████████▍| 47/50 [00:25<00:01,  1.73it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9086. T_Loss: 3.2896. Mask: 0.9831. :  96%|█████████▌| 48/50 [00:25<00:00,  2.04it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9083. T_Loss: 3.2880. Mask: 0.9831. :  96%|█████████▌| 48/50 [00:25<00:00,  2.04it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.31s. Batch: 0.52s. S_Loss: 0.9083. T_Loss: 3.2880. Mask: 0.9831. :  98%|█████████▊| 49/50 [00:25<00:00,  2.20it/s]total : 1000  current step :  847
total : 1000  current step :  848
total : 1000  current step :  849
Train Iter: 850/1000. LR: 0.0427. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9075. T_Loss: 3.2786. Mask: 0.9831. :  98%|█████████▊| 49/50 [00:26<00:00,  2.20it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9075. T_Loss: 3.2786. Mask: 0.9831. : 100%|██████████| 50/50 [00:26<00:00,  1.67it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.32s. Batch: 0.53s. S_Loss: 0.9075. T_Loss: 3.2786. Mask: 0.9831. : 100%|██████████| 50/50 [00:26<00:00,  1.89it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8970. top1: 95.70. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8970. top1: 95.70. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.81it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8978. top1: 95.12. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.81it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8978. top1: 95.12. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.75it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9050. top1: 94.92. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.75it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9050. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.22it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9127. top1: 94.24. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.22it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9127. top1: 94.24. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.42it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9169. top1: 93.75. top5: 99.92. :  50%|█████     | 4/8 [00:01<00:01,  3.42it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9169. top1: 93.75. top5: 99.92. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9180. top1: 93.29. top5: 99.93. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9180. top1: 93.29. top5: 99.93. :  75%|███████▌  | 6/8 [00:01<00:00,  3.74it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9234. top1: 92.69. top5: 99.89. :  75%|███████▌  | 6/8 [00:02<00:00,  3.74it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9234. top1: 92.69. top5: 99.89. :  88%|████████▊ | 7/8 [00:02<00:00,  3.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9242. top1: 92.70. top5: 99.90. :  88%|████████▊ | 7/8 [00:02<00:00,  3.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9242. top1: 92.70. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  4.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9242. top1: 92.70. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  3.33it/s]
total : 1000  current step :  850
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.22s. S_Loss: 0.9266. T_Loss: 3.5904. Mask: 0.9688. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.22s. S_Loss: 0.9266. T_Loss: 3.5904. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:11,  4.45it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.25s. S_Loss: 0.9196. T_Loss: 3.4694. Mask: 0.9805. :   2%|▏         | 1/50 [00:00<00:11,  4.45it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.25s. S_Loss: 0.9196. T_Loss: 3.4694. Mask: 0.9805. :   4%|▍         | 2/50 [00:00<00:12,  3.93it/s]total : 1000  current step :  851
total : 1000  current step :  852
Train Iter: 853/1000. LR: 0.0418. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9102. T_Loss: 3.4886. Mask: 0.9831. :   4%|▍         | 2/50 [00:01<00:12,  3.93it/s]Train Iter: 853/1000. LR: 0.0418. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9102. T_Loss: 3.4886. Mask: 0.9831. :   6%|▌         | 3/50 [00:01<00:29,  1.59it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.22s. Batch: 0.47s. S_Loss: 0.9122. T_Loss: 3.5396. Mask: 0.9805. :   6%|▌         | 3/50 [00:01<00:29,  1.59it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.22s. Batch: 0.47s. S_Loss: 0.9122. T_Loss: 3.5396. Mask: 0.9805. :   8%|▊         | 4/50 [00:01<00:23,  1.97it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.18s. Batch: 0.44s. S_Loss: 0.9191. T_Loss: 3.6436. Mask: 0.9805. :   8%|▊         | 4/50 [00:02<00:23,  1.97it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.18s. Batch: 0.44s. S_Loss: 0.9191. T_Loss: 3.6436. Mask: 0.9805. :  10%|█         | 5/50 [00:02<00:19,  2.32it/s]total : 1000  current step :  853
total : 1000  current step :  854
total : 1000  current step :  855
Train Iter: 856/1000. LR: 0.0409. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9165. T_Loss: 3.6333. Mask: 0.9818. :  10%|█         | 5/50 [00:03<00:19,  2.32it/s]Train Iter: 856/1000. LR: 0.0409. Data: 0.28s. Batch: 0.52s. S_Loss: 0.9165. T_Loss: 3.6333. Mask: 0.9818. :  12%|█▏        | 6/50 [00:03<00:26,  1.64it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9068. T_Loss: 3.5674. Mask: 0.9821. :  12%|█▏        | 6/50 [00:03<00:26,  1.64it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9068. T_Loss: 3.5674. Mask: 0.9821. :  14%|█▍        | 7/50 [00:03<00:23,  1.86it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.23s. Batch: 0.48s. S_Loss: 0.9097. T_Loss: 3.5174. Mask: 0.9800. :  14%|█▍        | 7/50 [00:03<00:23,  1.86it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.23s. Batch: 0.48s. S_Loss: 0.9097. T_Loss: 3.5174. Mask: 0.9800. :  16%|█▌        | 8/50 [00:03<00:20,  2.09it/s]total : 1000  current step :  856
total : 1000  current step :  857
total : 1000  current step :  858
Train Iter: 859/1000. LR: 0.0400. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9105. T_Loss: 3.5122. Mask: 0.9813. :  16%|█▌        | 8/50 [00:05<00:20,  2.09it/s]Train Iter: 859/1000. LR: 0.0400. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9105. T_Loss: 3.5122. Mask: 0.9813. :  18%|█▊        | 9/50 [00:05<00:28,  1.42it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.28s. Batch: 0.54s. S_Loss: 0.9085. T_Loss: 3.5346. Mask: 0.9816. :  18%|█▊        | 9/50 [00:05<00:28,  1.42it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.28s. Batch: 0.54s. S_Loss: 0.9085. T_Loss: 3.5346. Mask: 0.9816. :  20%|██        | 10/50 [00:05<00:23,  1.71it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.25s. Batch: 0.51s. S_Loss: 0.9049. T_Loss: 3.5258. Mask: 0.9830. :  20%|██        | 10/50 [00:05<00:23,  1.71it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.25s. Batch: 0.51s. S_Loss: 0.9049. T_Loss: 3.5258. Mask: 0.9830. :  22%|██▏       | 11/50 [00:05<00:18,  2.11it/s]total : 1000  current step :  859
total : 1000  current step :  860
total : 1000  current step :  861
Train Iter: 862/1000. LR: 0.0391. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9091. T_Loss: 3.5508. Mask: 0.9821. :  22%|██▏       | 11/50 [00:06<00:18,  2.11it/s]Train Iter: 862/1000. LR: 0.0391. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9091. T_Loss: 3.5508. Mask: 0.9821. :  24%|██▍       | 12/50 [00:06<00:24,  1.57it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.28s. Batch: 0.54s. S_Loss: 0.9076. T_Loss: 3.5439. Mask: 0.9826. :  24%|██▍       | 12/50 [00:07<00:24,  1.57it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.28s. Batch: 0.54s. S_Loss: 0.9076. T_Loss: 3.5439. Mask: 0.9826. :  26%|██▌       | 13/50 [00:07<00:20,  1.81it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9082. T_Loss: 3.5162. Mask: 0.9816. :  26%|██▌       | 13/50 [00:07<00:20,  1.81it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9082. T_Loss: 3.5162. Mask: 0.9816. :  28%|██▊       | 14/50 [00:07<00:16,  2.18it/s]total : 1000  current step :  862
total : 1000  current step :  863
total : 1000  current step :  864
Train Iter: 865/1000. LR: 0.0381. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9066. T_Loss: 3.4829. Mask: 0.9810. :  28%|██▊       | 14/50 [00:08<00:16,  2.18it/s]Train Iter: 865/1000. LR: 0.0381. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9066. T_Loss: 3.4829. Mask: 0.9810. :  30%|███       | 15/50 [00:08<00:21,  1.63it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9059. T_Loss: 3.4825. Mask: 0.9805. :  30%|███       | 15/50 [00:08<00:21,  1.63it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9059. T_Loss: 3.4825. Mask: 0.9805. :  32%|███▏      | 16/50 [00:08<00:17,  1.89it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9040. T_Loss: 3.4648. Mask: 0.9812. :  32%|███▏      | 16/50 [00:08<00:17,  1.89it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.27s. Batch: 0.52s. S_Loss: 0.9040. T_Loss: 3.4648. Mask: 0.9812. :  34%|███▍      | 17/50 [00:08<00:15,  2.08it/s]total : 1000  current step :  865
total : 1000  current step :  866
total : 1000  current step :  867
Train Iter: 868/1000. LR: 0.0370. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9053. T_Loss: 3.4579. Mask: 0.9813. :  34%|███▍      | 17/50 [00:09<00:15,  2.08it/s]Train Iter: 868/1000. LR: 0.0370. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9053. T_Loss: 3.4579. Mask: 0.9813. :  36%|███▌      | 18/50 [00:09<00:20,  1.55it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9037. T_Loss: 3.4370. Mask: 0.9813. :  36%|███▌      | 18/50 [00:10<00:20,  1.55it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9037. T_Loss: 3.4370. Mask: 0.9813. :  38%|███▊      | 19/50 [00:10<00:17,  1.82it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9039. T_Loss: 3.4298. Mask: 0.9809. :  38%|███▊      | 19/50 [00:10<00:17,  1.82it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.29s. Batch: 0.53s. S_Loss: 0.9039. T_Loss: 3.4298. Mask: 0.9809. :  40%|████      | 20/50 [00:10<00:15,  1.99it/s]total : 1000  current step :  868
total : 1000  current step :  869
total : 1000  current step :  870
Train Iter: 871/1000. LR: 0.0360. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9027. T_Loss: 3.4081. Mask: 0.9812. :  40%|████      | 20/50 [00:11<00:15,  1.99it/s]Train Iter: 871/1000. LR: 0.0360. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9027. T_Loss: 3.4081. Mask: 0.9812. :  42%|████▏     | 21/50 [00:11<00:19,  1.52it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9024. T_Loss: 3.3964. Mask: 0.9817. :  42%|████▏     | 21/50 [00:12<00:19,  1.52it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9024. T_Loss: 3.3964. Mask: 0.9817. :  44%|████▍     | 22/50 [00:12<00:15,  1.76it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9015. T_Loss: 3.3765. Mask: 0.9818. :  44%|████▍     | 22/50 [00:12<00:15,  1.76it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.30s. Batch: 0.54s. S_Loss: 0.9015. T_Loss: 3.3765. Mask: 0.9818. :  46%|████▌     | 23/50 [00:12<00:13,  2.04it/s]total : 1000  current step :  871
total : 1000  current step :  872
total : 1000  current step :  873
Train Iter: 874/1000. LR: 0.0349. Data: 0.32s. Batch: 0.56s. S_Loss: 0.9029. T_Loss: 3.3898. Mask: 0.9821. :  46%|████▌     | 23/50 [00:13<00:13,  2.04it/s]Train Iter: 874/1000. LR: 0.0349. Data: 0.32s. Batch: 0.56s. S_Loss: 0.9029. T_Loss: 3.3898. Mask: 0.9821. :  48%|████▊     | 24/50 [00:13<00:16,  1.53it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9031. T_Loss: 3.3793. Mask: 0.9816. :  48%|████▊     | 24/50 [00:13<00:16,  1.53it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9031. T_Loss: 3.3793. Mask: 0.9816. :  50%|█████     | 25/50 [00:13<00:13,  1.81it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9024. T_Loss: 3.3619. Mask: 0.9820. :  50%|█████     | 25/50 [00:13<00:13,  1.81it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.30s. Batch: 0.53s. S_Loss: 0.9024. T_Loss: 3.3619. Mask: 0.9820. :  52%|█████▏    | 26/50 [00:13<00:10,  2.19it/s]total : 1000  current step :  874
total : 1000  current step :  875
total : 1000  current step :  876
Train Iter: 877/1000. LR: 0.0338. Data: 0.33s. Batch: 0.56s. S_Loss: 0.9034. T_Loss: 3.3555. Mask: 0.9821. :  52%|█████▏    | 26/50 [00:15<00:10,  2.19it/s]Train Iter: 877/1000. LR: 0.0338. Data: 0.33s. Batch: 0.56s. S_Loss: 0.9034. T_Loss: 3.3555. Mask: 0.9821. :  54%|█████▍    | 27/50 [00:15<00:15,  1.49it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9016. T_Loss: 3.3369. Mask: 0.9821. :  54%|█████▍    | 27/50 [00:15<00:15,  1.49it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.32s. Batch: 0.55s. S_Loss: 0.9016. T_Loss: 3.3369. Mask: 0.9821. :  56%|█████▌    | 28/50 [00:15<00:12,  1.70it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9023. T_Loss: 3.3392. Mask: 0.9825. :  56%|█████▌    | 28/50 [00:15<00:12,  1.70it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9023. T_Loss: 3.3392. Mask: 0.9825. :  58%|█████▊    | 29/50 [00:15<00:10,  2.03it/s]total : 1000  current step :  877
total : 1000  current step :  878
total : 1000  current step :  879
Train Iter: 880/1000. LR: 0.0327. Data: 0.34s. Batch: 0.57s. S_Loss: 0.9026. T_Loss: 3.3368. Mask: 0.9826. :  58%|█████▊    | 29/50 [00:17<00:10,  2.03it/s]Train Iter: 880/1000. LR: 0.0327. Data: 0.34s. Batch: 0.57s. S_Loss: 0.9026. T_Loss: 3.3368. Mask: 0.9826. :  60%|██████    | 30/50 [00:17<00:15,  1.25it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9016. T_Loss: 3.3257. Mask: 0.9827. :  60%|██████    | 30/50 [00:18<00:15,  1.25it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9016. T_Loss: 3.3257. Mask: 0.9827. :  62%|██████▏   | 31/50 [00:18<00:14,  1.29it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9007. T_Loss: 3.3109. Mask: 0.9827. :  62%|██████▏   | 31/50 [00:18<00:14,  1.29it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9007. T_Loss: 3.3109. Mask: 0.9827. :  64%|██████▍   | 32/50 [00:18<00:12,  1.46it/s]total : 1000  current step :  880
total : 1000  current step :  881
total : 1000  current step :  882
Train Iter: 883/1000. LR: 0.0316. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9007. T_Loss: 3.3077. Mask: 0.9826. :  64%|██████▍   | 32/50 [00:19<00:12,  1.46it/s]Train Iter: 883/1000. LR: 0.0316. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9007. T_Loss: 3.3077. Mask: 0.9826. :  66%|██████▌   | 33/50 [00:19<00:13,  1.25it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9017. T_Loss: 3.3070. Mask: 0.9822. :  66%|██████▌   | 33/50 [00:19<00:13,  1.25it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9017. T_Loss: 3.3070. Mask: 0.9822. :  68%|██████▊   | 34/50 [00:19<00:10,  1.50it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9036. T_Loss: 3.3074. Mask: 0.9819. :  68%|██████▊   | 34/50 [00:20<00:10,  1.50it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9036. T_Loss: 3.3074. Mask: 0.9819. :  70%|███████   | 35/50 [00:20<00:08,  1.80it/s]total : 1000  current step :  883
total : 1000  current step :  884
total : 1000  current step :  885
Train Iter: 886/1000. LR: 0.0305. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9025. T_Loss: 3.2998. Mask: 0.9822. :  70%|███████   | 35/50 [00:21<00:08,  1.80it/s]Train Iter: 886/1000. LR: 0.0305. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9025. T_Loss: 3.2998. Mask: 0.9822. :  72%|███████▏  | 36/50 [00:21<00:10,  1.39it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9023. T_Loss: 3.2910. Mask: 0.9823. :  72%|███████▏  | 36/50 [00:21<00:10,  1.39it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9023. T_Loss: 3.2910. Mask: 0.9823. :  74%|███████▍  | 37/50 [00:21<00:07,  1.63it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9027. T_Loss: 3.2867. Mask: 0.9823. :  74%|███████▍  | 37/50 [00:22<00:07,  1.63it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9027. T_Loss: 3.2867. Mask: 0.9823. :  76%|███████▌  | 38/50 [00:22<00:06,  1.76it/s]total : 1000  current step :  886
total : 1000  current step :  887
total : 1000  current step :  888
Train Iter: 889/1000. LR: 0.0293. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9028. T_Loss: 3.2799. Mask: 0.9823. :  76%|███████▌  | 38/50 [00:23<00:06,  1.76it/s]Train Iter: 889/1000. LR: 0.0293. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9028. T_Loss: 3.2799. Mask: 0.9823. :  78%|███████▊  | 39/50 [00:23<00:07,  1.47it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9023. T_Loss: 3.2687. Mask: 0.9824. :  78%|███████▊  | 39/50 [00:23<00:07,  1.47it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9023. T_Loss: 3.2687. Mask: 0.9824. :  80%|████████  | 40/50 [00:23<00:06,  1.64it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9022. T_Loss: 3.2640. Mask: 0.9827. :  80%|████████  | 40/50 [00:23<00:06,  1.64it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9022. T_Loss: 3.2640. Mask: 0.9827. :  82%|████████▏ | 41/50 [00:23<00:04,  1.95it/s]total : 1000  current step :  889
total : 1000  current step :  890
total : 1000  current step :  891
Train Iter: 892/1000. LR: 0.0281. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9012. T_Loss: 3.2585. Mask: 0.9831. :  82%|████████▏ | 41/50 [00:25<00:04,  1.95it/s]Train Iter: 892/1000. LR: 0.0281. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9012. T_Loss: 3.2585. Mask: 0.9831. :  84%|████████▍ | 42/50 [00:25<00:05,  1.37it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9003. T_Loss: 3.2484. Mask: 0.9831. :  84%|████████▍ | 42/50 [00:25<00:05,  1.37it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9003. T_Loss: 3.2484. Mask: 0.9831. :  86%|████████▌ | 43/50 [00:25<00:04,  1.62it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9010. T_Loss: 3.2544. Mask: 0.9832. :  86%|████████▌ | 43/50 [00:25<00:04,  1.62it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9010. T_Loss: 3.2544. Mask: 0.9832. :  88%|████████▊ | 44/50 [00:25<00:03,  1.85it/s]total : 1000  current step :  892
total : 1000  current step :  893
total : 1000  current step :  894
Train Iter: 895/1000. LR: 0.0270. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9002. T_Loss: 3.2426. Mask: 0.9833. :  88%|████████▊ | 44/50 [00:26<00:03,  1.85it/s]Train Iter: 895/1000. LR: 0.0270. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9002. T_Loss: 3.2426. Mask: 0.9833. :  90%|█████████ | 45/50 [00:26<00:03,  1.34it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8999. T_Loss: 3.2405. Mask: 0.9837. :  90%|█████████ | 45/50 [00:27<00:03,  1.34it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8999. T_Loss: 3.2405. Mask: 0.9837. :  92%|█████████▏| 46/50 [00:27<00:02,  1.51it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9002. T_Loss: 3.2334. Mask: 0.9835. :  92%|█████████▏| 46/50 [00:27<00:02,  1.51it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9002. T_Loss: 3.2334. Mask: 0.9835. :  94%|█████████▍| 47/50 [00:27<00:01,  1.88it/s]total : 1000  current step :  895
total : 1000  current step :  896
total : 1000  current step :  897
Train Iter: 898/1000. LR: 0.0258. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8993. T_Loss: 3.2297. Mask: 0.9834. :  94%|█████████▍| 47/50 [00:28<00:01,  1.88it/s]Train Iter: 898/1000. LR: 0.0258. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8993. T_Loss: 3.2297. Mask: 0.9834. :  96%|█████████▌| 48/50 [00:28<00:01,  1.40it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.36s. Batch: 0.60s. S_Loss: 0.8996. T_Loss: 3.2278. Mask: 0.9833. :  96%|█████████▌| 48/50 [00:29<00:01,  1.40it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.36s. Batch: 0.60s. S_Loss: 0.8996. T_Loss: 3.2278. Mask: 0.9833. :  98%|█████████▊| 49/50 [00:29<00:00,  1.55it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8994. T_Loss: 3.2236. Mask: 0.9834. :  98%|█████████▊| 49/50 [00:29<00:00,  1.55it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8994. T_Loss: 3.2236. Mask: 0.9834. : 100%|██████████| 50/50 [00:29<00:00,  1.91it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8994. T_Loss: 3.2236. Mask: 0.9834. : 100%|██████████| 50/50 [00:29<00:00,  1.69it/s]
total : 1000  current step :  898
total : 1000  current step :  899
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8842. top1: 95.70. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8842. top1: 95.70. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8877. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8877. top1: 94.92. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.54it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8947. top1: 94.92. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.54it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8947. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9030. top1: 94.24. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9030. top1: 94.24. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.15it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9030. top1: 93.91. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.15it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9030. top1: 93.91. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9014. top1: 93.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9014. top1: 93.62. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9046. top1: 93.19. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9046. top1: 93.19. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.61it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9040. top1: 93.20. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.61it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9040. top1: 93.20. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9040. top1: 93.20. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.91it/s]
total : 1000  current step :  900
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 0.91s. Batch: 1.07s. S_Loss: 0.8917. T_Loss: 3.2125. Mask: 0.9844. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 0.91s. Batch: 1.07s. S_Loss: 0.8917. T_Loss: 3.2125. Mask: 0.9844. :   2%|▏         | 1/50 [00:01<00:52,  1.07s/it]Train Iter: 902/1000. LR: 0.0242. Data: 0.51s. Batch: 0.69s. S_Loss: 0.8966. T_Loss: 3.2546. Mask: 0.9863. :   2%|▏         | 1/50 [00:01<00:52,  1.07s/it]Train Iter: 902/1000. LR: 0.0242. Data: 0.51s. Batch: 0.69s. S_Loss: 0.8966. T_Loss: 3.2546. Mask: 0.9863. :   4%|▍         | 2/50 [00:01<00:30,  1.59it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9203. T_Loss: 3.3187. Mask: 0.9870. :   4%|▍         | 2/50 [00:01<00:30,  1.59it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9203. T_Loss: 3.3187. Mask: 0.9870. :   6%|▌         | 3/50 [00:01<00:24,  1.95it/s]total : 1000  current step :  901
total : 1000  current step :  902
total : 1000  current step :  903
Train Iter: 904/1000. LR: 0.0234. Data: 0.54s. Batch: 0.75s. S_Loss: 0.9140. T_Loss: 3.3205. Mask: 0.9854. :   6%|▌         | 3/50 [00:02<00:24,  1.95it/s]Train Iter: 904/1000. LR: 0.0234. Data: 0.54s. Batch: 0.75s. S_Loss: 0.9140. T_Loss: 3.3205. Mask: 0.9854. :   8%|▊         | 4/50 [00:02<00:36,  1.26it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9184. T_Loss: 3.3812. Mask: 0.9828. :   8%|▊         | 4/50 [00:03<00:36,  1.26it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9184. T_Loss: 3.3812. Mask: 0.9828. :  10%|█         | 5/50 [00:03<00:28,  1.58it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9106. T_Loss: 3.3105. Mask: 0.9844. :  10%|█         | 5/50 [00:03<00:28,  1.58it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9106. T_Loss: 3.3105. Mask: 0.9844. :  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]total : 1000  current step :  904
total : 1000  current step :  905
total : 1000  current step :  906
Train Iter: 907/1000. LR: 0.0223. Data: 0.48s. Batch: 0.71s. S_Loss: 0.9101. T_Loss: 3.2892. Mask: 0.9838. :  12%|█▏        | 6/50 [00:04<00:24,  1.78it/s]Train Iter: 907/1000. LR: 0.0223. Data: 0.48s. Batch: 0.71s. S_Loss: 0.9101. T_Loss: 3.2892. Mask: 0.9838. :  14%|█▍        | 7/50 [00:04<00:33,  1.29it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9094. T_Loss: 3.2823. Mask: 0.9839. :  14%|█▍        | 7/50 [00:05<00:33,  1.29it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9094. T_Loss: 3.2823. Mask: 0.9839. :  16%|█▌        | 8/50 [00:05<00:26,  1.61it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9112. T_Loss: 3.2588. Mask: 0.9818. :  16%|█▌        | 8/50 [00:05<00:26,  1.61it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9112. T_Loss: 3.2588. Mask: 0.9818. :  18%|█▊        | 9/50 [00:05<00:20,  1.98it/s]total : 1000  current step :  907
total : 1000  current step :  908
total : 1000  current step :  909
Train Iter: 910/1000. LR: 0.0211. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9090. T_Loss: 3.2712. Mask: 0.9824. :  18%|█▊        | 9/50 [00:06<00:20,  1.98it/s]Train Iter: 910/1000. LR: 0.0211. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9090. T_Loss: 3.2712. Mask: 0.9824. :  20%|██        | 10/50 [00:06<00:28,  1.42it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9045. T_Loss: 3.2620. Mask: 0.9830. :  20%|██        | 10/50 [00:07<00:28,  1.42it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9045. T_Loss: 3.2620. Mask: 0.9830. :  22%|██▏       | 11/50 [00:07<00:24,  1.62it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9070. T_Loss: 3.2974. Mask: 0.9824. :  22%|██▏       | 11/50 [00:07<00:24,  1.62it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9070. T_Loss: 3.2974. Mask: 0.9824. :  24%|██▍       | 12/50 [00:07<00:19,  1.92it/s]total : 1000  current step :  910
total : 1000  current step :  911
total : 1000  current step :  912
Train Iter: 913/1000. LR: 0.0199. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9047. T_Loss: 3.2959. Mask: 0.9826. :  24%|██▍       | 12/50 [00:08<00:19,  1.92it/s]Train Iter: 913/1000. LR: 0.0199. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9047. T_Loss: 3.2959. Mask: 0.9826. :  26%|██▌       | 13/50 [00:08<00:28,  1.28it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9023. T_Loss: 3.2856. Mask: 0.9830. :  26%|██▌       | 13/50 [00:09<00:28,  1.28it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9023. T_Loss: 3.2856. Mask: 0.9830. :  28%|██▊       | 14/50 [00:09<00:23,  1.54it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9009. T_Loss: 3.2852. Mask: 0.9839. :  28%|██▊       | 14/50 [00:09<00:23,  1.54it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9009. T_Loss: 3.2852. Mask: 0.9839. :  30%|███       | 15/50 [00:09<00:19,  1.77it/s]total : 1000  current step :  913
total : 1000  current step :  914
total : 1000  current step :  915
Train Iter: 916/1000. LR: 0.0188. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9004. T_Loss: 3.2778. Mask: 0.9841. :  30%|███       | 15/50 [00:10<00:19,  1.77it/s]Train Iter: 916/1000. LR: 0.0188. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9004. T_Loss: 3.2778. Mask: 0.9841. :  32%|███▏      | 16/50 [00:10<00:24,  1.39it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8996. T_Loss: 3.2643. Mask: 0.9839. :  32%|███▏      | 16/50 [00:10<00:24,  1.39it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8996. T_Loss: 3.2643. Mask: 0.9839. :  34%|███▍      | 17/50 [00:10<00:19,  1.66it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8972. T_Loss: 3.2592. Mask: 0.9846. :  34%|███▍      | 17/50 [00:11<00:19,  1.66it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8972. T_Loss: 3.2592. Mask: 0.9846. :  36%|███▌      | 18/50 [00:11<00:16,  1.90it/s]total : 1000  current step :  916
total : 1000  current step :  917
total : 1000  current step :  918
Train Iter: 919/1000. LR: 0.0176. Data: 0.42s. Batch: 0.65s. S_Loss: 0.8949. T_Loss: 3.2485. Mask: 0.9848. :  36%|███▌      | 18/50 [00:12<00:16,  1.90it/s]Train Iter: 919/1000. LR: 0.0176. Data: 0.42s. Batch: 0.65s. S_Loss: 0.8949. T_Loss: 3.2485. Mask: 0.9848. :  38%|███▊      | 19/50 [00:12<00:21,  1.43it/s]total : 1000  current step :  919
Train Iter: 920/1000. LR: 0.0173. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8924. T_Loss: 3.2334. Mask: 0.9846. :  38%|███▊      | 19/50 [00:13<00:21,  1.43it/s]Train Iter: 920/1000. LR: 0.0173. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8924. T_Loss: 3.2334. Mask: 0.9846. :  40%|████      | 20/50 [00:13<00:23,  1.26it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8940. T_Loss: 3.2363. Mask: 0.9847. :  40%|████      | 20/50 [00:14<00:23,  1.26it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8940. T_Loss: 3.2363. Mask: 0.9847. :  42%|████▏     | 21/50 [00:14<00:21,  1.32it/s]total : 1000  current step :  920
total : 1000  current step :  921
Train Iter: 922/1000. LR: 0.0165. Data: 0.47s. Batch: 0.69s. S_Loss: 0.8955. T_Loss: 3.2466. Mask: 0.9849. :  42%|████▏     | 21/50 [00:15<00:21,  1.32it/s]Train Iter: 922/1000. LR: 0.0165. Data: 0.47s. Batch: 0.69s. S_Loss: 0.8955. T_Loss: 3.2466. Mask: 0.9849. :  44%|████▍     | 22/50 [00:15<00:24,  1.13it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8938. T_Loss: 3.2392. Mask: 0.9849. :  44%|████▍     | 22/50 [00:15<00:24,  1.13it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8938. T_Loss: 3.2392. Mask: 0.9849. :  46%|████▌     | 23/50 [00:15<00:18,  1.44it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8921. T_Loss: 3.2299. Mask: 0.9849. :  46%|████▌     | 23/50 [00:15<00:18,  1.44it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8921. T_Loss: 3.2299. Mask: 0.9849. :  48%|████▊     | 24/50 [00:15<00:14,  1.78it/s]total : 1000  current step :  922
total : 1000  current step :  923
total : 1000  current step :  924
Train Iter: 925/1000. LR: 0.0154. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8922. T_Loss: 3.2378. Mask: 0.9847. :  48%|████▊     | 24/50 [00:16<00:14,  1.78it/s]Train Iter: 925/1000. LR: 0.0154. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8922. T_Loss: 3.2378. Mask: 0.9847. :  50%|█████     | 25/50 [00:16<00:19,  1.29it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.44s. Batch: 0.66s. S_Loss: 0.8924. T_Loss: 3.2377. Mask: 0.9845. :  50%|█████     | 25/50 [00:17<00:19,  1.29it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.44s. Batch: 0.66s. S_Loss: 0.8924. T_Loss: 3.2377. Mask: 0.9845. :  52%|█████▏    | 26/50 [00:17<00:15,  1.59it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.42s. Batch: 0.65s. S_Loss: 0.8926. T_Loss: 3.2461. Mask: 0.9841. :  52%|█████▏    | 26/50 [00:17<00:15,  1.59it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.42s. Batch: 0.65s. S_Loss: 0.8926. T_Loss: 3.2461. Mask: 0.9841. :  54%|█████▍    | 27/50 [00:17<00:11,  1.92it/s]total : 1000  current step :  925
total : 1000  current step :  926
total : 1000  current step :  927
Train Iter: 928/1000. LR: 0.0144. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8928. T_Loss: 3.2472. Mask: 0.9842. :  54%|█████▍    | 27/50 [00:18<00:11,  1.92it/s]Train Iter: 928/1000. LR: 0.0144. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8928. T_Loss: 3.2472. Mask: 0.9842. :  56%|█████▌    | 28/50 [00:18<00:15,  1.41it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8930. T_Loss: 3.2447. Mask: 0.9842. :  56%|█████▌    | 28/50 [00:19<00:15,  1.41it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8930. T_Loss: 3.2447. Mask: 0.9842. :  58%|█████▊    | 29/50 [00:19<00:12,  1.66it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8936. T_Loss: 3.2470. Mask: 0.9842. :  58%|█████▊    | 29/50 [00:19<00:12,  1.66it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8936. T_Loss: 3.2470. Mask: 0.9842. :  60%|██████    | 30/50 [00:19<00:09,  2.03it/s]total : 1000  current step :  928
total : 1000  current step :  929
total : 1000  current step :  930
Train Iter: 931/1000. LR: 0.0133. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8946. T_Loss: 3.2460. Mask: 0.9845. :  60%|██████    | 30/50 [00:20<00:09,  2.03it/s]Train Iter: 931/1000. LR: 0.0133. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8946. T_Loss: 3.2460. Mask: 0.9845. :  62%|██████▏   | 31/50 [00:20<00:11,  1.59it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8942. T_Loss: 3.2482. Mask: 0.9846. :  62%|██████▏   | 31/50 [00:20<00:11,  1.59it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8942. T_Loss: 3.2482. Mask: 0.9846. :  64%|██████▍   | 32/50 [00:20<00:09,  1.91it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8935. T_Loss: 3.2453. Mask: 0.9845. :  64%|██████▍   | 32/50 [00:20<00:09,  1.91it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8935. T_Loss: 3.2453. Mask: 0.9845. :  66%|██████▌   | 33/50 [00:20<00:07,  2.19it/s]total : 1000  current step :  931
total : 1000  current step :  932
total : 1000  current step :  933
Train Iter: 934/1000. LR: 0.0123. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8933. T_Loss: 3.2494. Mask: 0.9844. :  66%|██████▌   | 33/50 [00:21<00:07,  2.19it/s]Train Iter: 934/1000. LR: 0.0123. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8933. T_Loss: 3.2494. Mask: 0.9844. :  68%|██████▊   | 34/50 [00:21<00:10,  1.51it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8935. T_Loss: 3.2473. Mask: 0.9844. :  68%|██████▊   | 34/50 [00:22<00:10,  1.51it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8935. T_Loss: 3.2473. Mask: 0.9844. :  70%|███████   | 35/50 [00:22<00:08,  1.76it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8936. T_Loss: 3.2446. Mask: 0.9845. :  70%|███████   | 35/50 [00:22<00:08,  1.76it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8936. T_Loss: 3.2446. Mask: 0.9845. :  72%|███████▏  | 36/50 [00:22<00:06,  2.18it/s]total : 1000  current step :  934
total : 1000  current step :  935
total : 1000  current step :  936
Train Iter: 937/1000. LR: 0.0113. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8936. T_Loss: 3.2444. Mask: 0.9849. :  72%|███████▏  | 36/50 [00:23<00:06,  2.18it/s]Train Iter: 937/1000. LR: 0.0113. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8936. T_Loss: 3.2444. Mask: 0.9849. :  74%|███████▍  | 37/50 [00:23<00:08,  1.54it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8928. T_Loss: 3.2351. Mask: 0.9848. :  74%|███████▍  | 37/50 [00:24<00:08,  1.54it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8928. T_Loss: 3.2351. Mask: 0.9848. :  76%|███████▌  | 38/50 [00:24<00:06,  1.74it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8926. T_Loss: 3.2321. Mask: 0.9847. :  76%|███████▌  | 38/50 [00:24<00:06,  1.74it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8926. T_Loss: 3.2321. Mask: 0.9847. :  78%|███████▊  | 39/50 [00:24<00:05,  2.04it/s]total : 1000  current step :  937
total : 1000  current step :  938
total : 1000  current step :  939
Train Iter: 940/1000. LR: 0.0103. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8923. T_Loss: 3.2277. Mask: 0.9845. :  78%|███████▊  | 39/50 [00:25<00:05,  2.04it/s]Train Iter: 940/1000. LR: 0.0103. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8923. T_Loss: 3.2277. Mask: 0.9845. :  80%|████████  | 40/50 [00:25<00:06,  1.54it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8921. T_Loss: 3.2212. Mask: 0.9845. :  80%|████████  | 40/50 [00:25<00:06,  1.54it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8921. T_Loss: 3.2212. Mask: 0.9845. :  82%|████████▏ | 41/50 [00:25<00:05,  1.78it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8920. T_Loss: 3.2234. Mask: 0.9845. :  82%|████████▏ | 41/50 [00:25<00:05,  1.78it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8920. T_Loss: 3.2234. Mask: 0.9845. :  84%|████████▍ | 42/50 [00:25<00:03,  2.08it/s]total : 1000  current step :  940
total : 1000  current step :  941
total : 1000  current step :  942
Train Iter: 943/1000. LR: 0.0094. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8923. T_Loss: 3.2259. Mask: 0.9845. :  84%|████████▍ | 42/50 [00:26<00:03,  2.08it/s]Train Iter: 943/1000. LR: 0.0094. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8923. T_Loss: 3.2259. Mask: 0.9845. :  86%|████████▌ | 43/50 [00:26<00:04,  1.59it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8921. T_Loss: 3.2277. Mask: 0.9847. :  86%|████████▌ | 43/50 [00:27<00:04,  1.59it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8921. T_Loss: 3.2277. Mask: 0.9847. :  88%|████████▊ | 44/50 [00:27<00:03,  1.84it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.38s. Batch: 0.61s. S_Loss: 0.8917. T_Loss: 3.2228. Mask: 0.9845. :  88%|████████▊ | 44/50 [00:27<00:03,  1.84it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.38s. Batch: 0.61s. S_Loss: 0.8917. T_Loss: 3.2228. Mask: 0.9845. :  90%|█████████ | 45/50 [00:27<00:02,  2.15it/s]total : 1000  current step :  943
total : 1000  current step :  944
total : 1000  current step :  945
Train Iter: 946/1000. LR: 0.0085. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8922. T_Loss: 3.2240. Mask: 0.9843. :  90%|█████████ | 45/50 [00:28<00:02,  2.15it/s]Train Iter: 946/1000. LR: 0.0085. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8922. T_Loss: 3.2240. Mask: 0.9843. :  92%|█████████▏| 46/50 [00:28<00:02,  1.62it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8924. T_Loss: 3.2177. Mask: 0.9843. :  92%|█████████▏| 46/50 [00:28<00:02,  1.62it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8924. T_Loss: 3.2177. Mask: 0.9843. :  94%|█████████▍| 47/50 [00:28<00:01,  1.92it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8921. T_Loss: 3.2161. Mask: 0.9842. :  94%|█████████▍| 47/50 [00:29<00:01,  1.92it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8921. T_Loss: 3.2161. Mask: 0.9842. :  96%|█████████▌| 48/50 [00:29<00:00,  2.22it/s]total : 1000  current step :  946
total : 1000  current step :  947
total : 1000  current step :  948
Train Iter: 949/1000. LR: 0.0076. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8914. T_Loss: 3.2105. Mask: 0.9843. :  96%|█████████▌| 48/50 [00:30<00:00,  2.22it/s]Train Iter: 949/1000. LR: 0.0076. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8914. T_Loss: 3.2105. Mask: 0.9843. :  98%|█████████▊| 49/50 [00:30<00:00,  1.66it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.38s. Batch: 0.61s. S_Loss: 0.8909. T_Loss: 3.2033. Mask: 0.9843. :  98%|█████████▊| 49/50 [00:30<00:00,  1.66it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.38s. Batch: 0.61s. S_Loss: 0.8909. T_Loss: 3.2033. Mask: 0.9843. : 100%|██████████| 50/50 [00:30<00:00,  1.97it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.38s. Batch: 0.61s. S_Loss: 0.8909. T_Loss: 3.2033. Mask: 0.9843. : 100%|██████████| 50/50 [00:30<00:00,  1.65it/s]
total : 1000  current step :  949
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8674. top1: 95.70. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8674. top1: 95.70. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.84it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8729. top1: 95.31. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.84it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8729. top1: 95.31. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.63it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8791. top1: 95.31. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.63it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8791. top1: 95.31. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.97it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8884. top1: 94.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.97it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8884. top1: 94.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.31it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8865. top1: 94.22. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.31it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8865. top1: 94.22. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8836. top1: 94.08. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8836. top1: 94.08. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.58it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8855. top1: 93.58. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.58it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8855. top1: 93.58. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8841. top1: 93.75. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8841. top1: 93.75. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.97it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8841. top1: 93.75. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]
total : 1000  current step :  950
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.24s. S_Loss: 0.8613. T_Loss: 2.8567. Mask: 0.9805. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.24s. S_Loss: 0.8613. T_Loss: 2.8567. Mask: 0.9805. :   2%|▏         | 1/50 [00:00<00:11,  4.15it/s]total : 1000  current step :  951
Train Iter: 952/1000. LR: 0.0068. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8708. T_Loss: 2.8738. Mask: 0.9863. :   2%|▏         | 1/50 [00:01<00:11,  4.15it/s]Train Iter: 952/1000. LR: 0.0068. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8708. T_Loss: 2.8738. Mask: 0.9863. :   4%|▍         | 2/50 [00:01<00:31,  1.54it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.32s. Batch: 0.54s. S_Loss: 0.8707. T_Loss: 2.9296. Mask: 0.9883. :   4%|▍         | 2/50 [00:01<00:31,  1.54it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.32s. Batch: 0.54s. S_Loss: 0.8707. T_Loss: 2.9296. Mask: 0.9883. :   6%|▌         | 3/50 [00:01<00:26,  1.79it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.26s. Batch: 0.48s. S_Loss: 0.8674. T_Loss: 2.9413. Mask: 0.9873. :   6%|▌         | 3/50 [00:01<00:26,  1.79it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.26s. Batch: 0.48s. S_Loss: 0.8674. T_Loss: 2.9413. Mask: 0.9873. :   8%|▊         | 4/50 [00:01<00:21,  2.19it/s]total : 1000  current step :  952
total : 1000  current step :  953
total : 1000  current step :  954
Train Iter: 955/1000. LR: 0.0060. Data: 0.35s. Batch: 0.56s. S_Loss: 0.8773. T_Loss: 3.0157. Mask: 0.9875. :   8%|▊         | 4/50 [00:02<00:21,  2.19it/s]Train Iter: 955/1000. LR: 0.0060. Data: 0.35s. Batch: 0.56s. S_Loss: 0.8773. T_Loss: 3.0157. Mask: 0.9875. :  10%|█         | 5/50 [00:02<00:27,  1.62it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8802. T_Loss: 2.9905. Mask: 0.9876. :  10%|█         | 5/50 [00:03<00:27,  1.62it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8802. T_Loss: 2.9905. Mask: 0.9876. :  12%|█▏        | 6/50 [00:03<00:25,  1.70it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.28s. Batch: 0.52s. S_Loss: 0.8831. T_Loss: 3.0384. Mask: 0.9877. :  12%|█▏        | 6/50 [00:03<00:25,  1.70it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.28s. Batch: 0.52s. S_Loss: 0.8831. T_Loss: 3.0384. Mask: 0.9877. :  14%|█▍        | 7/50 [00:03<00:21,  2.02it/s]total : 1000  current step :  955
total : 1000  current step :  956
total : 1000  current step :  957
Train Iter: 958/1000. LR: 0.0052. Data: 0.34s. Batch: 0.56s. S_Loss: 0.8814. T_Loss: 3.0211. Mask: 0.9888. :  14%|█▍        | 7/50 [00:04<00:21,  2.02it/s]Train Iter: 958/1000. LR: 0.0052. Data: 0.34s. Batch: 0.56s. S_Loss: 0.8814. T_Loss: 3.0211. Mask: 0.9888. :  16%|█▌        | 8/50 [00:04<00:25,  1.64it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.31s. Batch: 0.53s. S_Loss: 0.8781. T_Loss: 3.0087. Mask: 0.9887. :  16%|█▌        | 8/50 [00:04<00:25,  1.64it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.31s. Batch: 0.53s. S_Loss: 0.8781. T_Loss: 3.0087. Mask: 0.9887. :  18%|█▊        | 9/50 [00:04<00:21,  1.95it/s]total : 1000  current step :  958
total : 1000  current step :  959
Train Iter: 960/1000. LR: 0.0048. Data: 0.32s. Batch: 0.53s. S_Loss: 0.8740. T_Loss: 3.0037. Mask: 0.9883. :  18%|█▊        | 9/50 [00:05<00:21,  1.95it/s]Train Iter: 960/1000. LR: 0.0048. Data: 0.32s. Batch: 0.53s. S_Loss: 0.8740. T_Loss: 3.0037. Mask: 0.9883. :  20%|██        | 10/50 [00:05<00:20,  1.93it/s]total : 1000  current step :  960
Train Iter: 961/1000. LR: 0.0045. Data: 0.37s. Batch: 0.59s. S_Loss: 0.8731. T_Loss: 2.9796. Mask: 0.9886. :  20%|██        | 10/50 [00:06<00:20,  1.93it/s]Train Iter: 961/1000. LR: 0.0045. Data: 0.37s. Batch: 0.59s. S_Loss: 0.8731. T_Loss: 2.9796. Mask: 0.9886. :  22%|██▏       | 11/50 [00:06<00:27,  1.40it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8738. T_Loss: 2.9958. Mask: 0.9880. :  22%|██▏       | 11/50 [00:06<00:27,  1.40it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8738. T_Loss: 2.9958. Mask: 0.9880. :  24%|██▍       | 12/50 [00:06<00:23,  1.64it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.32s. Batch: 0.54s. S_Loss: 0.8721. T_Loss: 2.9900. Mask: 0.9877. :  24%|██▍       | 12/50 [00:07<00:23,  1.64it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.32s. Batch: 0.54s. S_Loss: 0.8721. T_Loss: 2.9900. Mask: 0.9877. :  26%|██▌       | 13/50 [00:07<00:17,  2.09it/s]total : 1000  current step :  961
total : 1000  current step :  962
total : 1000  current step :  963
Train Iter: 964/1000. LR: 0.0039. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8705. T_Loss: 2.9914. Mask: 0.9874. :  26%|██▌       | 13/50 [00:08<00:17,  2.09it/s]Train Iter: 964/1000. LR: 0.0039. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8705. T_Loss: 2.9914. Mask: 0.9874. :  28%|██▊       | 14/50 [00:08<00:23,  1.52it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.34s. Batch: 0.56s. S_Loss: 0.8702. T_Loss: 2.9970. Mask: 0.9870. :  28%|██▊       | 14/50 [00:08<00:23,  1.52it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.34s. Batch: 0.56s. S_Loss: 0.8702. T_Loss: 2.9970. Mask: 0.9870. :  30%|███       | 15/50 [00:08<00:19,  1.82it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.32s. Batch: 0.54s. S_Loss: 0.8706. T_Loss: 2.9919. Mask: 0.9866. :  30%|███       | 15/50 [00:08<00:19,  1.82it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.32s. Batch: 0.54s. S_Loss: 0.8706. T_Loss: 2.9919. Mask: 0.9866. :  32%|███▏      | 16/50 [00:08<00:15,  2.13it/s]total : 1000  current step :  964
total : 1000  current step :  965
total : 1000  current step :  966
Train Iter: 967/1000. LR: 0.0033. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8720. T_Loss: 3.0069. Mask: 0.9871. :  32%|███▏      | 16/50 [00:09<00:15,  2.13it/s]Train Iter: 967/1000. LR: 0.0033. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8720. T_Loss: 3.0069. Mask: 0.9871. :  34%|███▍      | 17/50 [00:09<00:21,  1.50it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8732. T_Loss: 3.0321. Mask: 0.9870. :  34%|███▍      | 17/50 [00:10<00:21,  1.50it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8732. T_Loss: 3.0321. Mask: 0.9870. :  36%|███▌      | 18/50 [00:10<00:17,  1.82it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.31s. Batch: 0.54s. S_Loss: 0.8721. T_Loss: 3.0294. Mask: 0.9873. :  36%|███▌      | 18/50 [00:10<00:17,  1.82it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.31s. Batch: 0.54s. S_Loss: 0.8721. T_Loss: 3.0294. Mask: 0.9873. :  38%|███▊      | 19/50 [00:10<00:14,  2.21it/s]total : 1000  current step :  967
total : 1000  current step :  968
total : 1000  current step :  969
Train Iter: 970/1000. LR: 0.0027. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8723. T_Loss: 3.0509. Mask: 0.9871. :  38%|███▊      | 19/50 [00:11<00:14,  2.21it/s]Train Iter: 970/1000. LR: 0.0027. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8723. T_Loss: 3.0509. Mask: 0.9871. :  40%|████      | 20/50 [00:11<00:20,  1.46it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8717. T_Loss: 3.0472. Mask: 0.9870. :  40%|████      | 20/50 [00:11<00:20,  1.46it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8717. T_Loss: 3.0472. Mask: 0.9870. :  42%|████▏     | 21/50 [00:11<00:16,  1.76it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.32s. Batch: 0.55s. S_Loss: 0.8721. T_Loss: 3.0635. Mask: 0.9865. :  42%|████▏     | 21/50 [00:12<00:16,  1.76it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.32s. Batch: 0.55s. S_Loss: 0.8721. T_Loss: 3.0635. Mask: 0.9865. :  44%|████▍     | 22/50 [00:12<00:13,  2.06it/s]total : 1000  current step :  970
total : 1000  current step :  971
total : 1000  current step :  972
Train Iter: 973/1000. LR: 0.0022. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8731. T_Loss: 3.0623. Mask: 0.9862. :  44%|████▍     | 22/50 [00:13<00:13,  2.06it/s]Train Iter: 973/1000. LR: 0.0022. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8731. T_Loss: 3.0623. Mask: 0.9862. :  46%|████▌     | 23/50 [00:13<00:17,  1.59it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8735. T_Loss: 3.0579. Mask: 0.9863. :  46%|████▌     | 23/50 [00:13<00:17,  1.59it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8735. T_Loss: 3.0579. Mask: 0.9863. :  48%|████▊     | 24/50 [00:13<00:15,  1.69it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.33s. Batch: 0.55s. S_Loss: 0.8749. T_Loss: 3.0715. Mask: 0.9862. :  48%|████▊     | 24/50 [00:13<00:15,  1.69it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.33s. Batch: 0.55s. S_Loss: 0.8749. T_Loss: 3.0715. Mask: 0.9862. :  50%|█████     | 25/50 [00:13<00:12,  2.01it/s]total : 1000  current step :  973
total : 1000  current step :  974
total : 1000  current step :  975
Train Iter: 976/1000. LR: 0.0018. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8750. T_Loss: 3.0692. Mask: 0.9863. :  50%|█████     | 25/50 [00:14<00:12,  2.01it/s]Train Iter: 976/1000. LR: 0.0018. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8750. T_Loss: 3.0692. Mask: 0.9863. :  52%|█████▏    | 26/50 [00:14<00:16,  1.49it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8737. T_Loss: 3.0636. Mask: 0.9861. :  52%|█████▏    | 26/50 [00:15<00:16,  1.49it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8737. T_Loss: 3.0636. Mask: 0.9861. :  54%|█████▍    | 27/50 [00:15<00:13,  1.66it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8754. T_Loss: 3.0675. Mask: 0.9859. :  54%|█████▍    | 27/50 [00:15<00:13,  1.66it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8754. T_Loss: 3.0675. Mask: 0.9859. :  56%|█████▌    | 28/50 [00:15<00:11,  1.93it/s]total : 1000  current step :  976
total : 1000  current step :  977
total : 1000  current step :  978
Train Iter: 979/1000. LR: 0.0013. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8733. T_Loss: 3.0506. Mask: 0.9861. :  56%|█████▌    | 28/50 [00:16<00:11,  1.93it/s]Train Iter: 979/1000. LR: 0.0013. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8733. T_Loss: 3.0506. Mask: 0.9861. :  58%|█████▊    | 29/50 [00:16<00:14,  1.49it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8753. T_Loss: 3.0710. Mask: 0.9863. :  58%|█████▊    | 29/50 [00:17<00:14,  1.49it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8753. T_Loss: 3.0710. Mask: 0.9863. :  60%|██████    | 30/50 [00:17<00:11,  1.71it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8747. T_Loss: 3.0680. Mask: 0.9864. :  60%|██████    | 30/50 [00:17<00:11,  1.71it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8747. T_Loss: 3.0680. Mask: 0.9864. :  62%|██████▏   | 31/50 [00:17<00:10,  1.85it/s]total : 1000  current step :  979
total : 1000  current step :  980
total : 1000  current step :  981
Train Iter: 982/1000. LR: 0.0010. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8746. T_Loss: 3.0627. Mask: 0.9866. :  62%|██████▏   | 31/50 [00:18<00:10,  1.85it/s]Train Iter: 982/1000. LR: 0.0010. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8746. T_Loss: 3.0627. Mask: 0.9866. :  64%|██████▍   | 32/50 [00:18<00:13,  1.37it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8735. T_Loss: 3.0613. Mask: 0.9866. :  64%|██████▍   | 32/50 [00:19<00:13,  1.37it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8735. T_Loss: 3.0613. Mask: 0.9866. :  66%|██████▌   | 33/50 [00:19<00:09,  1.70it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8736. T_Loss: 3.0643. Mask: 0.9869. :  66%|██████▌   | 33/50 [00:19<00:09,  1.70it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8736. T_Loss: 3.0643. Mask: 0.9869. :  68%|██████▊   | 34/50 [00:19<00:08,  1.97it/s]total : 1000  current step :  982
total : 1000  current step :  983
total : 1000  current step :  984
Train Iter: 985/1000. LR: 0.0007. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8746. T_Loss: 3.0651. Mask: 0.9867. :  68%|██████▊   | 34/50 [00:20<00:08,  1.97it/s]Train Iter: 985/1000. LR: 0.0007. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8746. T_Loss: 3.0651. Mask: 0.9867. :  70%|███████   | 35/50 [00:20<00:10,  1.37it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8741. T_Loss: 3.0668. Mask: 0.9868. :  70%|███████   | 35/50 [00:20<00:10,  1.37it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8741. T_Loss: 3.0668. Mask: 0.9868. :  72%|███████▏  | 36/50 [00:20<00:08,  1.73it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8748. T_Loss: 3.0680. Mask: 0.9869. :  72%|███████▏  | 36/50 [00:21<00:08,  1.73it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8748. T_Loss: 3.0680. Mask: 0.9869. :  74%|███████▍  | 37/50 [00:21<00:06,  2.06it/s]total : 1000  current step :  985
total : 1000  current step :  986
total : 1000  current step :  987
Train Iter: 988/1000. LR: 0.0004. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8747. T_Loss: 3.0661. Mask: 0.9873. :  74%|███████▍  | 37/50 [00:22<00:06,  2.06it/s]Train Iter: 988/1000. LR: 0.0004. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8747. T_Loss: 3.0661. Mask: 0.9873. :  76%|███████▌  | 38/50 [00:22<00:08,  1.43it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8748. T_Loss: 3.0663. Mask: 0.9874. :  76%|███████▌  | 38/50 [00:22<00:08,  1.43it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8748. T_Loss: 3.0663. Mask: 0.9874. :  78%|███████▊  | 39/50 [00:22<00:06,  1.77it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8760. T_Loss: 3.0717. Mask: 0.9870. :  78%|███████▊  | 39/50 [00:22<00:06,  1.77it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8760. T_Loss: 3.0717. Mask: 0.9870. :  80%|████████  | 40/50 [00:22<00:04,  2.15it/s]total : 1000  current step :  988
total : 1000  current step :  989
total : 1000  current step :  990
Train Iter: 991/1000. LR: 0.0002. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8761. T_Loss: 3.0690. Mask: 0.9869. :  80%|████████  | 40/50 [00:23<00:04,  2.15it/s]Train Iter: 991/1000. LR: 0.0002. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8761. T_Loss: 3.0690. Mask: 0.9869. :  82%|████████▏ | 41/50 [00:23<00:06,  1.46it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8766. T_Loss: 3.0742. Mask: 0.9867. :  82%|████████▏ | 41/50 [00:24<00:06,  1.46it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8766. T_Loss: 3.0742. Mask: 0.9867. :  84%|████████▍ | 42/50 [00:24<00:04,  1.80it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8769. T_Loss: 3.0827. Mask: 0.9868. :  84%|████████▍ | 42/50 [00:24<00:04,  1.80it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8769. T_Loss: 3.0827. Mask: 0.9868. :  86%|████████▌ | 43/50 [00:24<00:03,  2.10it/s]total : 1000  current step :  991
total : 1000  current step :  992
total : 1000  current step :  993
Train Iter: 994/1000. LR: 0.0001. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8770. T_Loss: 3.0844. Mask: 0.9869. :  86%|████████▌ | 43/50 [00:25<00:03,  2.10it/s]Train Iter: 994/1000. LR: 0.0001. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8770. T_Loss: 3.0844. Mask: 0.9869. :  88%|████████▊ | 44/50 [00:25<00:03,  1.51it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8771. T_Loss: 3.0856. Mask: 0.9869. :  88%|████████▊ | 44/50 [00:25<00:03,  1.51it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8771. T_Loss: 3.0856. Mask: 0.9869. :  90%|█████████ | 45/50 [00:25<00:02,  1.85it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8773. T_Loss: 3.0828. Mask: 0.9868. :  90%|█████████ | 45/50 [00:26<00:02,  1.85it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8773. T_Loss: 3.0828. Mask: 0.9868. :  92%|█████████▏| 46/50 [00:26<00:01,  2.16it/s]total : 1000  current step :  994
total : 1000  current step :  995
total : 1000  current step :  996
Train Iter: 997/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 0.8771. T_Loss: 3.0807. Mask: 0.9869. :  92%|█████████▏| 46/50 [00:27<00:01,  2.16it/s]Train Iter: 997/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 0.8771. T_Loss: 3.0807. Mask: 0.9869. :  94%|█████████▍| 47/50 [00:27<00:02,  1.34it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8770. T_Loss: 3.0800. Mask: 0.9870. :  94%|█████████▍| 47/50 [00:27<00:02,  1.34it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8770. T_Loss: 3.0800. Mask: 0.9870. :  96%|█████████▌| 48/50 [00:27<00:01,  1.68it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8779. T_Loss: 3.0835. Mask: 0.9869. :  96%|█████████▌| 48/50 [00:28<00:01,  1.68it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8779. T_Loss: 3.0835. Mask: 0.9869. :  98%|█████████▊| 49/50 [00:28<00:00,  2.03it/s]total : 1000  current step :  997
total : 1000  current step :  998
total : 1000  current step :  999
Train Iter: 1000/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 0.8789. T_Loss: 3.0872. Mask: 0.9870. :  98%|█████████▊| 49/50 [00:29<00:00,  2.03it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 0.8789. T_Loss: 3.0872. Mask: 0.9870. : 100%|██████████| 50/50 [00:29<00:00,  1.22it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 0.8789. T_Loss: 3.0872. Mask: 0.9870. : 100%|██████████| 50/50 [00:29<00:00,  1.69it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8523. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8523. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8587. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8587. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.29it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8638. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.29it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8638. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8739. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8739. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8712. top1: 94.61. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8712. top1: 94.61. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.28it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8674. top1: 94.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.28it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8674. top1: 94.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.58it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8686. top1: 94.14. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.58it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8686. top1: 94.14. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8666. top1: 94.35. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8666. top1: 94.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8666. top1: 94.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.74s. Batch: 0.81s. Loss: 0.8671. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.74s. Batch: 0.81s. Loss: 0.8671. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch:  1/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8313. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch:  1/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8313. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch:  1/70. Data: 1.05s. Batch: 1.11s. Loss: 0.8332. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch:  1/70. Data: 1.05s. Batch: 1.11s. Loss: 0.8332. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]Finetune Epoch:  1/70. Data: 1.05s. Batch: 1.11s. Loss: 0.8332. : 100%|██████████| 3/3 [00:01<00:00,  1.94it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8520. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8520. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8584. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8584. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8635. top1: 95.83. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8635. top1: 95.83. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8736. top1: 94.82. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8736. top1: 94.82. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8708. top1: 94.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8708. top1: 94.53. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.12it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8670. top1: 94.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.12it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8670. top1: 94.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.32it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8681. top1: 94.14. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.32it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8681. top1: 94.14. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8662. top1: 94.35. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8662. top1: 94.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8662. top1: 94.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.58s. Batch: 0.64s. Loss: 0.8325. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.58s. Batch: 0.64s. Loss: 0.8325. :  33%|███▎      | 1/3 [00:00<00:01,  1.55it/s]Finetune Epoch:  2/70. Data: 0.70s. Batch: 0.78s. Loss: 0.8310. :  33%|███▎      | 1/3 [00:00<00:01,  1.55it/s]Finetune Epoch:  2/70. Data: 0.70s. Batch: 0.78s. Loss: 0.8310. :  67%|██████▋   | 2/3 [00:00<00:00,  2.38it/s]Finetune Epoch:  2/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8310. :  67%|██████▋   | 2/3 [00:01<00:00,  2.38it/s]Finetune Epoch:  2/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8310. : 100%|██████████| 3/3 [00:01<00:00,  2.63it/s]Finetune Epoch:  2/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8310. : 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8516. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8516. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8580. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8580. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8631. top1: 95.83. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8631. top1: 95.83. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8732. top1: 94.82. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8732. top1: 94.82. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8703. top1: 94.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8703. top1: 94.53. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.07it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8666. top1: 94.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.07it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8666. top1: 94.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.32it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8676. top1: 94.14. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.32it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8676. top1: 94.14. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8657. top1: 94.35. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8657. top1: 94.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8657. top1: 94.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.70s. Batch: 0.76s. Loss: 0.8490. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.70s. Batch: 0.76s. Loss: 0.8490. :  33%|███▎      | 1/3 [00:00<00:01,  1.32it/s]Finetune Epoch:  3/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8340. :  33%|███▎      | 1/3 [00:01<00:01,  1.32it/s]Finetune Epoch:  3/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8340. :  67%|██████▋   | 2/3 [00:01<00:00,  2.13it/s]Finetune Epoch:  3/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8273. :  67%|██████▋   | 2/3 [00:01<00:00,  2.13it/s]Finetune Epoch:  3/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8273. : 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]Finetune Epoch:  3/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8273. : 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8512. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8512. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8576. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8576. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.44it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8627. top1: 95.83. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.44it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8627. top1: 95.83. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8728. top1: 94.82. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8728. top1: 94.82. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8699. top1: 94.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8699. top1: 94.53. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8661. top1: 94.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8661. top1: 94.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.34it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8672. top1: 94.14. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.34it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8672. top1: 94.14. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8652. top1: 94.35. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8652. top1: 94.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8652. top1: 94.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.67s. Batch: 0.74s. Loss: 0.8295. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.67s. Batch: 0.74s. Loss: 0.8295. :  33%|███▎      | 1/3 [00:00<00:01,  1.36it/s]Finetune Epoch:  4/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8318. :  33%|███▎      | 1/3 [00:01<00:01,  1.36it/s]Finetune Epoch:  4/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8318. :  67%|██████▋   | 2/3 [00:01<00:00,  2.11it/s]Finetune Epoch:  4/70. Data: 0.99s. Batch: 1.05s. Loss: 0.8310. :  67%|██████▋   | 2/3 [00:01<00:00,  2.11it/s]Finetune Epoch:  4/70. Data: 0.99s. Batch: 1.05s. Loss: 0.8310. : 100%|██████████| 3/3 [00:01<00:00,  2.39it/s]Finetune Epoch:  4/70. Data: 0.99s. Batch: 1.05s. Loss: 0.8310. : 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 0.8506. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 0.8506. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.33it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8571. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.33it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8571. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.11it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8621. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.11it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8621. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8723. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8723. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.10it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8694. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.10it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8694. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.35it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8656. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.35it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8656. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.53it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8667. top1: 94.25. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.53it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8667. top1: 94.25. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8647. top1: 94.45. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8647. top1: 94.45. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8647. top1: 94.45. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.60s. Batch: 0.68s. Loss: 0.8774. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.60s. Batch: 0.68s. Loss: 0.8774. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch:  5/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8420. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch:  5/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8420. :  67%|██████▋   | 2/3 [00:00<00:00,  2.29it/s]Finetune Epoch:  5/70. Data: 0.91s. Batch: 0.97s. Loss: 0.8361. :  67%|██████▋   | 2/3 [00:01<00:00,  2.29it/s]Finetune Epoch:  5/70. Data: 0.91s. Batch: 0.97s. Loss: 0.8361. : 100%|██████████| 3/3 [00:01<00:00,  2.57it/s]Finetune Epoch:  5/70. Data: 0.91s. Batch: 0.97s. Loss: 0.8361. : 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8501. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8501. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.56it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8566. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.56it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8566. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.55it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8616. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.55it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8616. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.20it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8718. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.20it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8718. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8689. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8689. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8651. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8651. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.10it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8662. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.10it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8662. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8642. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8642. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8642. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.63s. Batch: 0.68s. Loss: 0.8489. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.63s. Batch: 0.68s. Loss: 0.8489. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch:  6/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8195. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch:  6/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8195. :  67%|██████▋   | 2/3 [00:00<00:00,  2.17it/s]Finetune Epoch:  6/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8316. :  67%|██████▋   | 2/3 [00:01<00:00,  2.17it/s]Finetune Epoch:  6/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8316. : 100%|██████████| 3/3 [00:01<00:00,  2.32it/s]Finetune Epoch:  6/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8316. : 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8497. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8497. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.55it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8561. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.55it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8561. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8612. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8612. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8714. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8714. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8685. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8685. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.23it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8647. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.23it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8647. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.41it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8657. top1: 94.25. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.41it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8657. top1: 94.25. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8637. top1: 94.45. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8637. top1: 94.45. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8637. top1: 94.45. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.94it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.61s. Batch: 0.67s. Loss: 0.8447. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.61s. Batch: 0.67s. Loss: 0.8447. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch:  7/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8415. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch:  7/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8415. :  67%|██████▋   | 2/3 [00:01<00:00,  2.12it/s]Finetune Epoch:  7/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8343. :  67%|██████▋   | 2/3 [00:01<00:00,  2.12it/s]Finetune Epoch:  7/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8343. : 100%|██████████| 3/3 [00:01<00:00,  2.43it/s]Finetune Epoch:  7/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8343. : 100%|██████████| 3/3 [00:01<00:00,  1.98it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8491. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8491. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8556. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8556. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8606. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8606. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8708. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8708. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8679. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8679. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8641. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8641. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8652. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.24it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8652. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8632. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8632. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8632. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8360. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8360. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch:  8/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8318. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch:  8/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8318. :  67%|██████▋   | 2/3 [00:00<00:00,  2.17it/s]Finetune Epoch:  8/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8285. :  67%|██████▋   | 2/3 [00:01<00:00,  2.17it/s]Finetune Epoch:  8/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8285. : 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]Finetune Epoch:  8/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8285. : 100%|██████████| 3/3 [00:01<00:00,  2.02it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8486. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8486. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8551. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8551. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.84it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8601. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.84it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8601. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8703. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8703. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.57it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8674. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.57it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8674. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8637. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8637. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.74it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8647. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.74it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8647. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8627. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8627. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8627. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.22it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.69s. Batch: 0.73s. Loss: 0.8751. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.69s. Batch: 0.73s. Loss: 0.8751. :  33%|███▎      | 1/3 [00:00<00:01,  1.37it/s]Finetune Epoch:  9/70. Data: 0.82s. Batch: 0.86s. Loss: 0.8514. :  33%|███▎      | 1/3 [00:00<00:01,  1.37it/s]Finetune Epoch:  9/70. Data: 0.82s. Batch: 0.86s. Loss: 0.8514. :  67%|██████▋   | 2/3 [00:00<00:00,  2.22it/s]Finetune Epoch:  9/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8410. :  67%|██████▋   | 2/3 [00:01<00:00,  2.22it/s]Finetune Epoch:  9/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8410. : 100%|██████████| 3/3 [00:01<00:00,  2.56it/s]Finetune Epoch:  9/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8410. : 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8480. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8480. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8545. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8545. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.45it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8595. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.45it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8595. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8697. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8697. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8668. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8668. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8631. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8631. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8642. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8642. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8622. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8622. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8622. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.95it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.65s. Batch: 0.71s. Loss: 0.8141. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.65s. Batch: 0.71s. Loss: 0.8141. :  33%|███▎      | 1/3 [00:00<00:01,  1.41it/s]Finetune Epoch: 10/70. Data: 0.81s. Batch: 0.87s. Loss: 0.8300. :  33%|███▎      | 1/3 [00:01<00:01,  1.41it/s]Finetune Epoch: 10/70. Data: 0.81s. Batch: 0.87s. Loss: 0.8300. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 10/70. Data: 0.98s. Batch: 1.03s. Loss: 0.8215. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 10/70. Data: 0.98s. Batch: 1.03s. Loss: 0.8215. : 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]Finetune Epoch: 10/70. Data: 0.98s. Batch: 1.03s. Loss: 0.8215. : 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8476. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8476. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8541. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8541. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8590. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8590. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8693. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8693. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.92it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8664. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.92it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8664. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8627. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8627. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.50it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8638. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.50it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8638. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8617. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8617. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8617. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.97it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.58s. Batch: 0.64s. Loss: 0.8747. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.58s. Batch: 0.64s. Loss: 0.8747. :  33%|███▎      | 1/3 [00:00<00:01,  1.57it/s]Finetune Epoch: 11/70. Data: 0.73s. Batch: 0.79s. Loss: 0.8514. :  33%|███▎      | 1/3 [00:00<00:01,  1.57it/s]Finetune Epoch: 11/70. Data: 0.73s. Batch: 0.79s. Loss: 0.8514. :  67%|██████▋   | 2/3 [00:00<00:00,  2.28it/s]Finetune Epoch: 11/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8370. :  67%|██████▋   | 2/3 [00:01<00:00,  2.28it/s]Finetune Epoch: 11/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8370. : 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]Finetune Epoch: 11/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8370. : 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8470. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8470. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8535. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8535. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8584. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8584. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8687. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8687. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8658. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8658. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8621. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8621. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.36it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8633. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.36it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8633. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8613. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8613. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.77it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8613. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.91it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.63s. Batch: 0.70s. Loss: 0.8241. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.63s. Batch: 0.70s. Loss: 0.8241. :  33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]Finetune Epoch: 12/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8287. :  33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]Finetune Epoch: 12/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8287. :  67%|██████▋   | 2/3 [00:00<00:00,  2.20it/s]Finetune Epoch: 12/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8259. :  67%|██████▋   | 2/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 12/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8259. : 100%|██████████| 3/3 [00:01<00:00,  2.58it/s]Finetune Epoch: 12/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8259. : 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8465. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8465. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8531. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8531. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.45it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8580. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.45it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8580. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.92it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8683. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.92it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8683. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8654. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8654. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8617. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8617. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.53it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8628. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.53it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8628. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8608. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8608. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8608. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.68s. Batch: 0.75s. Loss: 0.8718. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.68s. Batch: 0.75s. Loss: 0.8718. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch: 13/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8432. :  33%|███▎      | 1/3 [00:01<00:01,  1.33it/s]Finetune Epoch: 13/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8432. :  67%|██████▋   | 2/3 [00:01<00:00,  2.12it/s]Finetune Epoch: 13/70. Data: 0.97s. Batch: 1.04s. Loss: 0.8310. :  67%|██████▋   | 2/3 [00:01<00:00,  2.12it/s]Finetune Epoch: 13/70. Data: 0.97s. Batch: 1.04s. Loss: 0.8310. : 100%|██████████| 3/3 [00:01<00:00,  2.52it/s]Finetune Epoch: 13/70. Data: 0.97s. Batch: 1.04s. Loss: 0.8310. : 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8460. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8460. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8525. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8525. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8575. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8575. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8677. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8677. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8649. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8649. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8612. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8612. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8623. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8623. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8603. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8603. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8603. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.57s. Batch: 0.62s. Loss: 0.8123. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.57s. Batch: 0.62s. Loss: 0.8123. :  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s]Finetune Epoch: 14/70. Data: 0.71s. Batch: 0.77s. Loss: 0.8261. :  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s]Finetune Epoch: 14/70. Data: 0.71s. Batch: 0.77s. Loss: 0.8261. :  67%|██████▋   | 2/3 [00:00<00:00,  2.34it/s]Finetune Epoch: 14/70. Data: 0.86s. Batch: 0.91s. Loss: 0.8235. :  67%|██████▋   | 2/3 [00:01<00:00,  2.34it/s]Finetune Epoch: 14/70. Data: 0.86s. Batch: 0.91s. Loss: 0.8235. : 100%|██████████| 3/3 [00:01<00:00,  2.70it/s]Finetune Epoch: 14/70. Data: 0.86s. Batch: 0.91s. Loss: 0.8235. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8455. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8455. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.70it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8521. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.70it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8521. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.46it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8570. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.46it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8570. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8673. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8673. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.96it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8644. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.96it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8644. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.29it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8607. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.29it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8607. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.39it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8618. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.39it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8618. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8598. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8598. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.59it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8598. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.92it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.68s. Batch: 0.72s. Loss: 0.8509. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.68s. Batch: 0.72s. Loss: 0.8509. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 15/70. Data: 0.82s. Batch: 0.86s. Loss: 0.8398. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 15/70. Data: 0.82s. Batch: 0.86s. Loss: 0.8398. :  67%|██████▋   | 2/3 [00:01<00:00,  2.17it/s]Finetune Epoch: 15/70. Data: 0.96s. Batch: 1.00s. Loss: 0.8326. :  67%|██████▋   | 2/3 [00:01<00:00,  2.17it/s]Finetune Epoch: 15/70. Data: 0.96s. Batch: 1.00s. Loss: 0.8326. : 100%|██████████| 3/3 [00:01<00:00,  2.64it/s]Finetune Epoch: 15/70. Data: 0.96s. Batch: 1.00s. Loss: 0.8326. : 100%|██████████| 3/3 [00:01<00:00,  2.02it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8451. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8451. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8516. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8516. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8565. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8565. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8668. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8668. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8639. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8639. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8602. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8602. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8614. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.45it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8614. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8593. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8593. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8593. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.90it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.60s. Batch: 0.67s. Loss: 0.8143. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.60s. Batch: 0.67s. Loss: 0.8143. :  33%|███▎      | 1/3 [00:00<00:01,  1.49it/s]Finetune Epoch: 16/70. Data: 0.78s. Batch: 0.85s. Loss: 0.8310. :  33%|███▎      | 1/3 [00:01<00:01,  1.49it/s]Finetune Epoch: 16/70. Data: 0.78s. Batch: 0.85s. Loss: 0.8310. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 16/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8301. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 16/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8301. : 100%|██████████| 3/3 [00:01<00:00,  2.52it/s]Finetune Epoch: 16/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8301. : 100%|██████████| 3/3 [00:01<00:00,  1.94it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8446. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8446. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.63it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8512. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.63it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8512. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8560. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8560. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8664. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8664. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.85it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8634. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.85it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8634. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8598. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8598. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8609. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8609. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.51it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8589. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.51it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8589. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8589. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.97it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.60s. Batch: 0.65s. Loss: 0.8346. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.60s. Batch: 0.65s. Loss: 0.8346. :  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]Finetune Epoch: 17/70. Data: 0.75s. Batch: 0.80s. Loss: 0.8317. :  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]Finetune Epoch: 17/70. Data: 0.75s. Batch: 0.80s. Loss: 0.8317. :  67%|██████▋   | 2/3 [00:00<00:00,  2.26it/s]Finetune Epoch: 17/70. Data: 0.89s. Batch: 0.93s. Loss: 0.8247. :  67%|██████▋   | 2/3 [00:01<00:00,  2.26it/s]Finetune Epoch: 17/70. Data: 0.89s. Batch: 0.93s. Loss: 0.8247. : 100%|██████████| 3/3 [00:01<00:00,  2.80it/s]Finetune Epoch: 17/70. Data: 0.89s. Batch: 0.93s. Loss: 0.8247. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8442. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8442. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8508. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8508. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.68it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8556. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.68it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8556. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.80it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8659. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.80it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8659. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8630. top1: 94.69. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8630. top1: 94.69. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8593. top1: 94.66. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8593. top1: 94.66. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8605. top1: 94.20. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.45it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8605. top1: 94.20. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.62it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8584. top1: 94.40. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.62it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8584. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8584. top1: 94.40. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.99it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.58s. Batch: 0.65s. Loss: 0.8481. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.58s. Batch: 0.65s. Loss: 0.8481. :  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]Finetune Epoch: 18/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8348. :  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]Finetune Epoch: 18/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8348. :  67%|██████▋   | 2/3 [00:00<00:00,  2.39it/s]Finetune Epoch: 18/70. Data: 0.88s. Batch: 0.93s. Loss: 0.8260. :  67%|██████▋   | 2/3 [00:01<00:00,  2.39it/s]Finetune Epoch: 18/70. Data: 0.88s. Batch: 0.93s. Loss: 0.8260. : 100%|██████████| 3/3 [00:01<00:00,  2.61it/s]Finetune Epoch: 18/70. Data: 0.88s. Batch: 0.93s. Loss: 0.8260. : 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8436. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8436. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.11it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8502. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.11it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8502. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8550. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8550. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.40it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8653. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.40it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8653. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.74it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8625. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.74it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8625. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.66it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8588. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.66it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8588. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.63it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8600. top1: 94.25. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.63it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8600. top1: 94.25. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8579. top1: 94.45. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8579. top1: 94.45. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.97it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8579. top1: 94.45. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.36it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.61s. Batch: 0.70s. Loss: 0.8166. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.61s. Batch: 0.70s. Loss: 0.8166. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch: 19/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8121. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch: 19/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8121. :  67%|██████▋   | 2/3 [00:00<00:00,  2.36it/s]Finetune Epoch: 19/70. Data: 0.88s. Batch: 0.94s. Loss: 0.8256. :  67%|██████▋   | 2/3 [00:01<00:00,  2.36it/s]Finetune Epoch: 19/70. Data: 0.88s. Batch: 0.94s. Loss: 0.8256. : 100%|██████████| 3/3 [00:01<00:00,  2.81it/s]Finetune Epoch: 19/70. Data: 0.88s. Batch: 0.94s. Loss: 0.8256. : 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8431. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8431. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8497. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8497. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.59it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8545. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.59it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8545. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8648. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8648. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.36it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8620. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.36it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8620. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8584. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8584. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8595. top1: 94.36. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.43it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8595. top1: 94.36. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8575. top1: 94.55. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8575. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8575. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.53s. Batch: 0.58s. Loss: 0.8238. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.53s. Batch: 0.58s. Loss: 0.8238. :  33%|███▎      | 1/3 [00:00<00:01,  1.73it/s]Finetune Epoch: 20/70. Data: 0.68s. Batch: 0.74s. Loss: 0.8176. :  33%|███▎      | 1/3 [00:00<00:01,  1.73it/s]Finetune Epoch: 20/70. Data: 0.68s. Batch: 0.74s. Loss: 0.8176. :  67%|██████▋   | 2/3 [00:00<00:00,  2.35it/s]Finetune Epoch: 20/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8215. :  67%|██████▋   | 2/3 [00:01<00:00,  2.35it/s]Finetune Epoch: 20/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8215. : 100%|██████████| 3/3 [00:01<00:00,  2.86it/s]Finetune Epoch: 20/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8215. : 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8427. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8427. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8493. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8493. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8540. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8540. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8644. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8644. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.33it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8615. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.33it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8615. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8579. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8579. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.68it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8591. top1: 94.36. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.68it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8591. top1: 94.36. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8570. top1: 94.55. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8570. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8570. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.27it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.64s. Batch: 0.69s. Loss: 0.8331. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.64s. Batch: 0.69s. Loss: 0.8331. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 21/70. Data: 0.77s. Batch: 0.82s. Loss: 0.8388. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 21/70. Data: 0.77s. Batch: 0.82s. Loss: 0.8388. :  67%|██████▋   | 2/3 [00:00<00:00,  2.28it/s]Finetune Epoch: 21/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8332. :  67%|██████▋   | 2/3 [00:01<00:00,  2.28it/s]Finetune Epoch: 21/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8332. : 100%|██████████| 3/3 [00:01<00:00,  2.74it/s]Finetune Epoch: 21/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8332. : 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8420. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8420. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.97it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8486. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.97it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8486. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8534. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8534. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.28it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8638. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.28it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8638. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8609. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8609. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8574. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8574. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.82it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8586. top1: 94.36. top5: 99.94. :  75%|███████▌  | 6/8 [00:01<00:00,  3.82it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8586. top1: 94.36. top5: 99.94. :  88%|████████▊ | 7/8 [00:01<00:00,  4.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8566. top1: 94.55. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8566. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8566. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.47it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.60s. Batch: 0.68s. Loss: 0.8376. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.60s. Batch: 0.68s. Loss: 0.8376. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 22/70. Data: 0.74s. Batch: 0.81s. Loss: 0.8280. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 22/70. Data: 0.74s. Batch: 0.81s. Loss: 0.8280. :  67%|██████▋   | 2/3 [00:00<00:00,  2.30it/s]Finetune Epoch: 22/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8404. :  67%|██████▋   | 2/3 [00:01<00:00,  2.30it/s]Finetune Epoch: 22/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8404. : 100%|██████████| 3/3 [00:01<00:00,  2.65it/s]Finetune Epoch: 22/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8404. : 100%|██████████| 3/3 [00:01<00:00,  2.11it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8416. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8416. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.88it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8482. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.88it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8482. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.90it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8529. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.90it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8529. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.14it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8633. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.14it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8633. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.44it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8605. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.44it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8605. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8569. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8569. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.98it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8582. top1: 94.36. top5: 99.94. :  75%|███████▌  | 6/8 [00:01<00:00,  3.98it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8582. top1: 94.36. top5: 99.94. :  88%|████████▊ | 7/8 [00:01<00:00,  4.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8561. top1: 94.55. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8561. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.28it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8561. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.35it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.55s. Batch: 0.60s. Loss: 0.8116. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.55s. Batch: 0.60s. Loss: 0.8116. :  33%|███▎      | 1/3 [00:00<00:01,  1.66it/s]Finetune Epoch: 23/70. Data: 0.68s. Batch: 0.73s. Loss: 0.8313. :  33%|███▎      | 1/3 [00:00<00:01,  1.66it/s]Finetune Epoch: 23/70. Data: 0.68s. Batch: 0.73s. Loss: 0.8313. :  67%|██████▋   | 2/3 [00:00<00:00,  2.51it/s]Finetune Epoch: 23/70. Data: 0.83s. Batch: 0.88s. Loss: 0.8323. :  67%|██████▋   | 2/3 [00:01<00:00,  2.51it/s]Finetune Epoch: 23/70. Data: 0.83s. Batch: 0.88s. Loss: 0.8323. : 100%|██████████| 3/3 [00:01<00:00,  2.76it/s]Finetune Epoch: 23/70. Data: 0.83s. Batch: 0.88s. Loss: 0.8323. : 100%|██████████| 3/3 [00:01<00:00,  2.26it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8411. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8411. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.04it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8477. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.04it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8477. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8524. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8524. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8628. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8628. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.47it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8600. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.47it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8600. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.66it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8565. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.66it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8565. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.87it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8577. top1: 94.36. top5: 99.94. :  75%|███████▌  | 6/8 [00:01<00:00,  3.87it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8577. top1: 94.36. top5: 99.94. :  88%|████████▊ | 7/8 [00:01<00:00,  4.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8557. top1: 94.55. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8557. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8557. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.44it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.62s. Batch: 0.67s. Loss: 0.8711. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.62s. Batch: 0.67s. Loss: 0.8711. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch: 24/70. Data: 0.75s. Batch: 0.80s. Loss: 0.8432. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch: 24/70. Data: 0.75s. Batch: 0.80s. Loss: 0.8432. :  67%|██████▋   | 2/3 [00:00<00:00,  2.32it/s]Finetune Epoch: 24/70. Data: 0.91s. Batch: 0.97s. Loss: 0.8248. :  67%|██████▋   | 2/3 [00:01<00:00,  2.32it/s]Finetune Epoch: 24/70. Data: 0.91s. Batch: 0.97s. Loss: 0.8248. : 100%|██████████| 3/3 [00:01<00:00,  2.48it/s]Finetune Epoch: 24/70. Data: 0.91s. Batch: 0.97s. Loss: 0.8248. : 100%|██████████| 3/3 [00:01<00:00,  2.01it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8407. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8407. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8473. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8473. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8520. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8520. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.15it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8624. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.15it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8624. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.62it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8596. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.62it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8596. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8561. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8561. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8573. top1: 94.36. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.67it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8573. top1: 94.36. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8552. top1: 94.55. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8552. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8552. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.39it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.51s. Batch: 0.56s. Loss: 0.8432. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.51s. Batch: 0.56s. Loss: 0.8432. :  33%|███▎      | 1/3 [00:00<00:01,  1.79it/s]Finetune Epoch: 25/70. Data: 0.63s. Batch: 0.68s. Loss: 0.8421. :  33%|███▎      | 1/3 [00:00<00:01,  1.79it/s]Finetune Epoch: 25/70. Data: 0.63s. Batch: 0.68s. Loss: 0.8421. :  67%|██████▋   | 2/3 [00:00<00:00,  2.69it/s]Finetune Epoch: 25/70. Data: 0.76s. Batch: 0.81s. Loss: 0.8231. :  67%|██████▋   | 2/3 [00:01<00:00,  2.69it/s]Finetune Epoch: 25/70. Data: 0.76s. Batch: 0.81s. Loss: 0.8231. : 100%|██████████| 3/3 [00:01<00:00,  3.05it/s]Finetune Epoch: 25/70. Data: 0.76s. Batch: 0.81s. Loss: 0.8231. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8403. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8403. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8469. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8469. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.70it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8516. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.70it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8516. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.13it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8621. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.13it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8621. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.56it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8592. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.56it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8592. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.60it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8556. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.60it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8556. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.44it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8569. top1: 94.36. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.44it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8569. top1: 94.36. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8548. top1: 94.55. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8548. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8548. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.32it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.46s. Batch: 0.51s. Loss: 0.8054. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.46s. Batch: 0.51s. Loss: 0.8054. :  33%|███▎      | 1/3 [00:00<00:01,  1.95it/s]Finetune Epoch: 26/70. Data: 0.59s. Batch: 0.64s. Loss: 0.8111. :  33%|███▎      | 1/3 [00:00<00:01,  1.95it/s]Finetune Epoch: 26/70. Data: 0.59s. Batch: 0.64s. Loss: 0.8111. :  67%|██████▋   | 2/3 [00:00<00:00,  2.77it/s]Finetune Epoch: 26/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8241. :  67%|██████▋   | 2/3 [00:01<00:00,  2.77it/s]Finetune Epoch: 26/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8241. : 100%|██████████| 3/3 [00:01<00:00,  3.01it/s]Finetune Epoch: 26/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8241. : 100%|██████████| 3/3 [00:01<00:00,  2.34it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8399. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8399. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.17it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8465. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.17it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8465. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8511. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8511. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8616. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8616. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8587. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8587. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.89it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8552. top1: 94.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.89it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8552. top1: 94.73. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.95it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8564. top1: 94.36. top5: 99.94. :  75%|███████▌  | 6/8 [00:01<00:00,  3.95it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8564. top1: 94.36. top5: 99.94. :  88%|████████▊ | 7/8 [00:01<00:00,  3.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8543. top1: 94.55. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8543. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8543. top1: 94.55. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.39it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.55s. Batch: 0.62s. Loss: 0.8162. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.55s. Batch: 0.62s. Loss: 0.8162. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch: 27/70. Data: 0.67s. Batch: 0.74s. Loss: 0.8150. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch: 27/70. Data: 0.67s. Batch: 0.74s. Loss: 0.8150. :  67%|██████▋   | 2/3 [00:00<00:00,  2.49it/s]Finetune Epoch: 27/70. Data: 0.82s. Batch: 0.89s. Loss: 0.8243. :  67%|██████▋   | 2/3 [00:01<00:00,  2.49it/s]Finetune Epoch: 27/70. Data: 0.82s. Batch: 0.89s. Loss: 0.8243. : 100%|██████████| 3/3 [00:01<00:00,  2.78it/s]Finetune Epoch: 27/70. Data: 0.82s. Batch: 0.89s. Loss: 0.8243. : 100%|██████████| 3/3 [00:01<00:00,  2.22it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8393. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8393. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.09it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8459. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.09it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8459. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.76it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8506. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.76it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8506. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.15it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8610. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.15it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8610. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.29it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8582. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.29it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8582. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.39it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8547. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.39it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8547. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.70it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8560. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.70it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8560. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8539. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8539. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8539. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.86s. Batch: 0.92s. Loss: 0.8327. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.86s. Batch: 0.92s. Loss: 0.8327. :  33%|███▎      | 1/3 [00:00<00:01,  1.09it/s]Finetune Epoch: 28/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8208. :  33%|███▎      | 1/3 [00:01<00:01,  1.09it/s]Finetune Epoch: 28/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8208. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 28/70. Data: 1.19s. Batch: 1.24s. Loss: 0.8343. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 28/70. Data: 1.19s. Batch: 1.24s. Loss: 0.8343. : 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]Finetune Epoch: 28/70. Data: 1.19s. Batch: 1.24s. Loss: 0.8343. : 100%|██████████| 3/3 [00:01<00:00,  1.67it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8390. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8390. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8456. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8456. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8502. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8502. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8607. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8607. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.28it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8578. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.28it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8578. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.41it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8543. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.41it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8543. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8556. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.67it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8556. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8535. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8535. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8535. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.20it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8172. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8172. :  33%|███▎      | 1/3 [00:00<00:01,  1.22it/s]Finetune Epoch: 29/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8148. :  33%|███▎      | 1/3 [00:01<00:01,  1.22it/s]Finetune Epoch: 29/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8148. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 29/70. Data: 1.06s. Batch: 1.11s. Loss: 0.8220. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 29/70. Data: 1.06s. Batch: 1.11s. Loss: 0.8220. : 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]Finetune Epoch: 29/70. Data: 1.06s. Batch: 1.11s. Loss: 0.8220. : 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8385. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8385. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8452. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8452. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.59it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8498. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.59it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8498. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8603. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8603. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8574. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8574. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8539. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8539. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8551. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8551. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8530. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8530. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8530. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.65s. Batch: 0.70s. Loss: 0.8377. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.65s. Batch: 0.70s. Loss: 0.8377. :  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]Finetune Epoch: 30/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8220. :  33%|███▎      | 1/3 [00:01<00:01,  1.43it/s]Finetune Epoch: 30/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8220. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 30/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8255. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 30/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8255. : 100%|██████████| 3/3 [00:01<00:00,  2.59it/s]Finetune Epoch: 30/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8255. : 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8382. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8382. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8448. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8448. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8494. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8494. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8599. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8599. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.16it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8570. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.16it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8570. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8535. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8535. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.41it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8547. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.41it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8547. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8526. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8526. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.90it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8526. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8288. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8288. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch: 31/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8195. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch: 31/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8195. :  67%|██████▋   | 2/3 [00:00<00:00,  2.28it/s]Finetune Epoch: 31/70. Data: 0.88s. Batch: 0.95s. Loss: 0.8217. :  67%|██████▋   | 2/3 [00:01<00:00,  2.28it/s]Finetune Epoch: 31/70. Data: 0.88s. Batch: 0.95s. Loss: 0.8217. : 100%|██████████| 3/3 [00:01<00:00,  2.82it/s]Finetune Epoch: 31/70. Data: 0.88s. Batch: 0.95s. Loss: 0.8217. : 100%|██████████| 3/3 [00:01<00:00,  2.13it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8378. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8378. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8444. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8444. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.65it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8490. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.65it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8490. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.23it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8594. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.23it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8594. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8566. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8566. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8531. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8531. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8543. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:01<00:00,  3.97it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8543. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:01<00:00,  4.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8522. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8522. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8522. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.41it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.60s. Batch: 0.67s. Loss: 0.8052. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.60s. Batch: 0.67s. Loss: 0.8052. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch: 32/70. Data: 0.74s. Batch: 0.80s. Loss: 0.8238. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch: 32/70. Data: 0.74s. Batch: 0.80s. Loss: 0.8238. :  67%|██████▋   | 2/3 [00:00<00:00,  2.29it/s]Finetune Epoch: 32/70. Data: 0.89s. Batch: 0.94s. Loss: 0.8268. :  67%|██████▋   | 2/3 [00:01<00:00,  2.29it/s]Finetune Epoch: 32/70. Data: 0.89s. Batch: 0.94s. Loss: 0.8268. : 100%|██████████| 3/3 [00:01<00:00,  2.72it/s]Finetune Epoch: 32/70. Data: 0.89s. Batch: 0.94s. Loss: 0.8268. : 100%|██████████| 3/3 [00:01<00:00,  2.12it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8373. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8373. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.15it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8439. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.15it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8439. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8485. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8485. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.43it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8590. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.43it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8590. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8561. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8561. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.67it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8526. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.67it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8526. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8539. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.64it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8539. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8518. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8518. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8518. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.32it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.68s. Batch: 0.73s. Loss: 0.8345. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.68s. Batch: 0.73s. Loss: 0.8345. :  33%|███▎      | 1/3 [00:00<00:01,  1.37it/s]Finetune Epoch: 33/70. Data: 0.85s. Batch: 0.90s. Loss: 0.8249. :  33%|███▎      | 1/3 [00:01<00:01,  1.37it/s]Finetune Epoch: 33/70. Data: 0.85s. Batch: 0.90s. Loss: 0.8249. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch: 33/70. Data: 1.02s. Batch: 1.06s. Loss: 0.8250. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch: 33/70. Data: 1.02s. Batch: 1.06s. Loss: 0.8250. : 100%|██████████| 3/3 [00:01<00:00,  2.36it/s]Finetune Epoch: 33/70. Data: 1.02s. Batch: 1.06s. Loss: 0.8250. : 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8368. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8368. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8435. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8435. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8480. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8480. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8586. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8586. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.20it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8557. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.20it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8557. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.93it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8522. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.93it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8522. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8535. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8535. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8513. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8513. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8513. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.85it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.60s. Batch: 0.65s. Loss: 0.8100. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.60s. Batch: 0.65s. Loss: 0.8100. :  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]Finetune Epoch: 34/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8268. :  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]Finetune Epoch: 34/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8268. :  67%|██████▋   | 2/3 [00:00<00:00,  2.36it/s]Finetune Epoch: 34/70. Data: 0.87s. Batch: 0.92s. Loss: 0.8224. :  67%|██████▋   | 2/3 [00:01<00:00,  2.36it/s]Finetune Epoch: 34/70. Data: 0.87s. Batch: 0.92s. Loss: 0.8224. : 100%|██████████| 3/3 [00:01<00:00,  2.78it/s]Finetune Epoch: 34/70. Data: 0.87s. Batch: 0.92s. Loss: 0.8224. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8365. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8365. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8431. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8431. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.58it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8477. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.58it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8477. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.91it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8582. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.91it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8582. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8553. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8553. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.26it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8518. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.26it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8518. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8531. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.45it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8531. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8509. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8509. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8509. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.53s. Batch: 0.58s. Loss: 0.8678. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.53s. Batch: 0.58s. Loss: 0.8678. :  33%|███▎      | 1/3 [00:00<00:01,  1.73it/s]Finetune Epoch: 35/70. Data: 0.66s. Batch: 0.71s. Loss: 0.8512. :  33%|███▎      | 1/3 [00:00<00:01,  1.73it/s]Finetune Epoch: 35/70. Data: 0.66s. Batch: 0.71s. Loss: 0.8512. :  67%|██████▋   | 2/3 [00:00<00:00,  2.52it/s]Finetune Epoch: 35/70. Data: 0.84s. Batch: 0.89s. Loss: 0.8303. :  67%|██████▋   | 2/3 [00:01<00:00,  2.52it/s]Finetune Epoch: 35/70. Data: 0.84s. Batch: 0.89s. Loss: 0.8303. : 100%|██████████| 3/3 [00:01<00:00,  2.49it/s]Finetune Epoch: 35/70. Data: 0.84s. Batch: 0.89s. Loss: 0.8303. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8360. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8360. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8426. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8426. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8472. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8472. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8577. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8577. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8548. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8548. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8514. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8514. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.53it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8527. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.53it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8527. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8505. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8505. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8505. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.52s. Batch: 0.56s. Loss: 0.8461. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.52s. Batch: 0.56s. Loss: 0.8461. :  33%|███▎      | 1/3 [00:00<00:01,  1.78it/s]Finetune Epoch: 36/70. Data: 0.68s. Batch: 0.73s. Loss: 0.8212. :  33%|███▎      | 1/3 [00:00<00:01,  1.78it/s]Finetune Epoch: 36/70. Data: 0.68s. Batch: 0.73s. Loss: 0.8212. :  67%|██████▋   | 2/3 [00:00<00:00,  2.33it/s]Finetune Epoch: 36/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8198. :  67%|██████▋   | 2/3 [00:01<00:00,  2.33it/s]Finetune Epoch: 36/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8198. : 100%|██████████| 3/3 [00:01<00:00,  2.78it/s]Finetune Epoch: 36/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8198. : 100%|██████████| 3/3 [00:01<00:00,  2.23it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8355. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8355. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.87it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8422. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.87it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8422. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.78it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8467. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.78it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8467. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8572. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8572. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.60it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8544. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.60it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8544. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.81it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8510. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.81it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8510. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8523. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:01<00:00,  3.85it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8523. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:01<00:00,  4.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8501. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8501. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8501. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.41it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.59s. Batch: 0.65s. Loss: 0.8002. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.59s. Batch: 0.65s. Loss: 0.8002. :  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]Finetune Epoch: 37/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8269. :  33%|███▎      | 1/3 [00:01<00:01,  1.54it/s]Finetune Epoch: 37/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8269. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 37/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8361. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 37/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8361. : 100%|██████████| 3/3 [00:01<00:00,  2.53it/s]Finetune Epoch: 37/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8361. : 100%|██████████| 3/3 [00:01<00:00,  1.98it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8351. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8351. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.84it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8418. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.84it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8418. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.78it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8463. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.78it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8463. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8568. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8568. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8540. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8540. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.75it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8506. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.75it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8506. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.87it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8519. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:01<00:00,  3.87it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8519. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:01<00:00,  4.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8497. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  4.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8497. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8497. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.33it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.57s. Batch: 0.62s. Loss: 0.8520. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.57s. Batch: 0.62s. Loss: 0.8520. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch: 38/70. Data: 0.70s. Batch: 0.75s. Loss: 0.8454. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch: 38/70. Data: 0.70s. Batch: 0.75s. Loss: 0.8454. :  67%|██████▋   | 2/3 [00:00<00:00,  2.44it/s]Finetune Epoch: 38/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8299. :  67%|██████▋   | 2/3 [00:01<00:00,  2.44it/s]Finetune Epoch: 38/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8299. : 100%|██████████| 3/3 [00:01<00:00,  3.04it/s]Finetune Epoch: 38/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8299. : 100%|██████████| 3/3 [00:01<00:00,  2.22it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8348. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8348. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8414. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8414. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8459. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8459. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.28it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8565. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.28it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8565. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.64it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8536. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.64it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8536. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.79it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8502. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.79it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8502. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8515. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.64it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8515. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8493. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8493. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.94it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8493. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.26it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.63s. Batch: 0.71s. Loss: 0.8317. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.63s. Batch: 0.71s. Loss: 0.8317. :  33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]Finetune Epoch: 39/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8528. :  33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]Finetune Epoch: 39/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8528. :  67%|██████▋   | 2/3 [00:00<00:00,  2.21it/s]Finetune Epoch: 39/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8377. :  67%|██████▋   | 2/3 [00:01<00:00,  2.21it/s]Finetune Epoch: 39/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8377. : 100%|██████████| 3/3 [00:01<00:00,  2.76it/s]Finetune Epoch: 39/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8377. : 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8344. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8344. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8410. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8410. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8455. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8455. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8561. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8561. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.48it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8532. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.48it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8532. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8498. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8498. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.83it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8511. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.83it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8511. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8489. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8489. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8489. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.33it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 40/70. Data: 0.57s. Batch: 0.62s. Loss: 0.8172. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 40/70. Data: 0.57s. Batch: 0.62s. Loss: 0.8172. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch: 40/70. Data: 0.72s. Batch: 0.78s. Loss: 0.8178. :  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s]Finetune Epoch: 40/70. Data: 0.72s. Batch: 0.78s. Loss: 0.8178. :  67%|██████▋   | 2/3 [00:00<00:00,  2.26it/s]Finetune Epoch: 40/70. Data: 0.87s. Batch: 0.92s. Loss: 0.8208. :  67%|██████▋   | 2/3 [00:01<00:00,  2.26it/s]Finetune Epoch: 40/70. Data: 0.87s. Batch: 0.92s. Loss: 0.8208. : 100%|██████████| 3/3 [00:01<00:00,  2.72it/s]Finetune Epoch: 40/70. Data: 0.87s. Batch: 0.92s. Loss: 0.8208. : 100%|██████████| 3/3 [00:01<00:00,  2.11it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8340. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8340. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8407. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8407. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.43it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8451. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.43it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8451. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8557. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8557. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8528. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8528. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8494. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8494. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.68it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8507. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.68it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8507. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8486. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8486. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8486. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.97it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.69s. Batch: 0.75s. Loss: 0.8271. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.69s. Batch: 0.75s. Loss: 0.8271. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch: 41/70. Data: 0.81s. Batch: 0.86s. Loss: 0.8246. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch: 41/70. Data: 0.81s. Batch: 0.86s. Loss: 0.8246. :  67%|██████▋   | 2/3 [00:00<00:00,  2.27it/s]Finetune Epoch: 41/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8289. :  67%|██████▋   | 2/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 41/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8289. : 100%|██████████| 3/3 [00:01<00:00,  2.78it/s]Finetune Epoch: 41/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8289. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8337. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8337. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8404. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8404. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.44it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8448. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.44it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8448. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8554. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8554. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.38it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8525. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.38it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8525. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8491. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8491. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.18it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8503. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.18it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8503. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8482. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8482. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8482. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.61s. Batch: 0.67s. Loss: 0.8279. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.61s. Batch: 0.67s. Loss: 0.8279. :  33%|███▎      | 1/3 [00:00<00:01,  1.49it/s]Finetune Epoch: 42/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8114. :  33%|███▎      | 1/3 [00:01<00:01,  1.49it/s]Finetune Epoch: 42/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8114. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 42/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8262. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 42/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8262. : 100%|██████████| 3/3 [00:01<00:00,  2.44it/s]Finetune Epoch: 42/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8262. : 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8333. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8333. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.85it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8400. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.85it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8400. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.61it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8444. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.61it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8444. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8550. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8550. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.08it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8521. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.08it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8521. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8487. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8487. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.58it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8500. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  2.58it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8500. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  2.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8478. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  2.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8478. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8478. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.71it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.58s. Batch: 0.63s. Loss: 0.8308. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.58s. Batch: 0.63s. Loss: 0.8308. :  33%|███▎      | 1/3 [00:00<00:01,  1.59it/s]Finetune Epoch: 43/70. Data: 0.72s. Batch: 0.78s. Loss: 0.8128. :  33%|███▎      | 1/3 [00:00<00:01,  1.59it/s]Finetune Epoch: 43/70. Data: 0.72s. Batch: 0.78s. Loss: 0.8128. :  67%|██████▋   | 2/3 [00:00<00:00,  2.28it/s]Finetune Epoch: 43/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8241. :  67%|██████▋   | 2/3 [00:01<00:00,  2.28it/s]Finetune Epoch: 43/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8241. : 100%|██████████| 3/3 [00:01<00:00,  2.70it/s]Finetune Epoch: 43/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8241. : 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8329. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8329. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8396. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8396. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.29it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8439. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.29it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8439. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8546. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8546. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8517. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8517. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8483. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8483. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8496. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8496. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8474. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8474. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8474. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.57s. Batch: 0.64s. Loss: 0.8204. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.57s. Batch: 0.64s. Loss: 0.8204. :  33%|███▎      | 1/3 [00:00<00:01,  1.56it/s]Finetune Epoch: 44/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8152. :  33%|███▎      | 1/3 [00:00<00:01,  1.56it/s]Finetune Epoch: 44/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8152. :  67%|██████▋   | 2/3 [00:00<00:00,  2.34it/s]Finetune Epoch: 44/70. Data: 0.89s. Batch: 0.94s. Loss: 0.8186. :  67%|██████▋   | 2/3 [00:01<00:00,  2.34it/s]Finetune Epoch: 44/70. Data: 0.89s. Batch: 0.94s. Loss: 0.8186. : 100%|██████████| 3/3 [00:01<00:00,  2.60it/s]Finetune Epoch: 44/70. Data: 0.89s. Batch: 0.94s. Loss: 0.8186. : 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8325. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8325. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8392. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8392. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8436. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8436. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8542. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8542. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8513. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8513. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.39it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8479. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.39it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8479. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.38it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8492. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.38it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8492. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8470. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8470. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8470. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.93it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.64s. Batch: 0.69s. Loss: 0.8060. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.64s. Batch: 0.69s. Loss: 0.8060. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch: 45/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8057. :  33%|███▎      | 1/3 [00:01<00:01,  1.44it/s]Finetune Epoch: 45/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8057. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 45/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8169. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 45/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8169. : 100%|██████████| 3/3 [00:01<00:00,  2.31it/s]Finetune Epoch: 45/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8169. : 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8321. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8321. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.85it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8388. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.85it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8388. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.46it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8432. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.46it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8432. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.13it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8538. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.13it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8538. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.28it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8509. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.28it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8509. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8475. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8475. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.73it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8488. top1: 94.42. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.73it/s] Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8488. top1: 94.42. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.77it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8466. top1: 94.60. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.77it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8466. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  4.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8466. top1: 94.60. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.32it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.62s. Batch: 0.69s. Loss: 0.8263. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.62s. Batch: 0.69s. Loss: 0.8263. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 46/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8139. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 46/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8139. :  67%|██████▋   | 2/3 [00:00<00:00,  2.24it/s]Finetune Epoch: 46/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8203. :  67%|██████▋   | 2/3 [00:01<00:00,  2.24it/s]Finetune Epoch: 46/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8203. : 100%|██████████| 3/3 [00:01<00:00,  2.81it/s]Finetune Epoch: 46/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8203. : 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8318. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8318. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8385. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8385. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8428. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.67it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8428. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.14it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8535. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.14it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8535. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.56it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8506. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.56it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8506. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8472. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8472. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8485. top1: 94.42. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8485. top1: 94.42. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.93it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8463. top1: 94.60. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.93it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8463. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8463. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.25it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.56s. Batch: 0.62s. Loss: 0.8528. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.56s. Batch: 0.62s. Loss: 0.8528. :  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s]Finetune Epoch: 47/70. Data: 0.69s. Batch: 0.74s. Loss: 0.8448. :  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s]Finetune Epoch: 47/70. Data: 0.69s. Batch: 0.74s. Loss: 0.8448. :  67%|██████▋   | 2/3 [00:00<00:00,  2.51it/s]Finetune Epoch: 47/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8245. :  67%|██████▋   | 2/3 [00:01<00:00,  2.51it/s]Finetune Epoch: 47/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8245. : 100%|██████████| 3/3 [00:01<00:00,  3.01it/s]Finetune Epoch: 47/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8245. : 100%|██████████| 3/3 [00:01<00:00,  2.32it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8315. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8315. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.87it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8382. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.87it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8382. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8425. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8425. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.32it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8532. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.32it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8532. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8502. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8502. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.50it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8468. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.50it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8468. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8481. top1: 94.42. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8481. top1: 94.42. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8459. top1: 94.60. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8459. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.00it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8459. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.33it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.67s. Batch: 0.72s. Loss: 0.8087. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.67s. Batch: 0.72s. Loss: 0.8087. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 48/70. Data: 0.86s. Batch: 0.91s. Loss: 0.8348. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 48/70. Data: 0.86s. Batch: 0.91s. Loss: 0.8348. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 48/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8236. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 48/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8236. : 100%|██████████| 3/3 [00:01<00:00,  2.32it/s]Finetune Epoch: 48/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8236. : 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8311. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8311. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8378. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8378. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8421. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8421. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8528. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8528. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.59it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8499. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.59it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8499. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.79it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8465. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.79it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8465. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8477. top1: 94.42. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8477. top1: 94.42. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  3.91it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8455. top1: 94.60. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.91it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8455. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8455. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.36it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.48s. Batch: 0.53s. Loss: 0.8188. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.48s. Batch: 0.53s. Loss: 0.8188. :  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s]Finetune Epoch: 49/70. Data: 0.64s. Batch: 0.69s. Loss: 0.8263. :  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s]Finetune Epoch: 49/70. Data: 0.64s. Batch: 0.69s. Loss: 0.8263. :  67%|██████▋   | 2/3 [00:00<00:00,  2.49it/s]Finetune Epoch: 49/70. Data: 0.81s. Batch: 0.85s. Loss: 0.8264. :  67%|██████▋   | 2/3 [00:01<00:00,  2.49it/s]Finetune Epoch: 49/70. Data: 0.81s. Batch: 0.85s. Loss: 0.8264. : 100%|██████████| 3/3 [00:01<00:00,  2.70it/s]Finetune Epoch: 49/70. Data: 0.81s. Batch: 0.85s. Loss: 0.8264. : 100%|██████████| 3/3 [00:01<00:00,  2.14it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8307. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8307. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8374. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8374. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8417. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8417. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8524. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8524. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8495. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8495. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.33it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8461. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.33it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8461. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8474. top1: 94.42. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8474. top1: 94.42. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.59it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8451. top1: 94.60. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.59it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8451. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.84it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8451. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.62s. Batch: 0.67s. Loss: 0.8159. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.62s. Batch: 0.67s. Loss: 0.8159. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch: 50/70. Data: 0.76s. Batch: 0.81s. Loss: 0.7970. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch: 50/70. Data: 0.76s. Batch: 0.81s. Loss: 0.7970. :  67%|██████▋   | 2/3 [00:00<00:00,  2.24it/s]Finetune Epoch: 50/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8226. :  67%|██████▋   | 2/3 [00:01<00:00,  2.24it/s]Finetune Epoch: 50/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8226. : 100%|██████████| 3/3 [00:01<00:00,  2.67it/s]Finetune Epoch: 50/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8226. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8304. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8304. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.84it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8371. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.84it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8371. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.72it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8414. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.72it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8414. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8521. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8521. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.54it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8491. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.54it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8491. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.37it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8457. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.37it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8457. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8470. top1: 94.42. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8470. top1: 94.42. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8448. top1: 94.60. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8448. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8448. top1: 94.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.21it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.67s. Batch: 0.74s. Loss: 0.8473. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.67s. Batch: 0.74s. Loss: 0.8473. :  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Finetune Epoch: 51/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8217. :  33%|███▎      | 1/3 [00:01<00:01,  1.35it/s]Finetune Epoch: 51/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8217. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 51/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8204. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 51/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8204. : 100%|██████████| 3/3 [00:01<00:00,  2.37it/s]Finetune Epoch: 51/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8204. : 100%|██████████| 3/3 [00:01<00:00,  1.89it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8301. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8301. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.62it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8368. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.62it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8368. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8411. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8411. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.97it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8518. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.97it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8518. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.28it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8488. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.28it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8488. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.31it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8454. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.31it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8454. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.54it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8467. top1: 94.48. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.54it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8467. top1: 94.48. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8444. top1: 94.65. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8444. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8444. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.66s. Batch: 0.70s. Loss: 0.8118. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.66s. Batch: 0.70s. Loss: 0.8118. :  33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]Finetune Epoch: 52/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8185. :  33%|███▎      | 1/3 [00:01<00:01,  1.42it/s]Finetune Epoch: 52/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8185. :  67%|██████▋   | 2/3 [00:01<00:00,  2.12it/s]Finetune Epoch: 52/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8208. :  67%|██████▋   | 2/3 [00:01<00:00,  2.12it/s]Finetune Epoch: 52/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8208. : 100%|██████████| 3/3 [00:01<00:00,  2.68it/s]Finetune Epoch: 52/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8208. : 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8298. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8298. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8365. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8365. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.64it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8407. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.64it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8407. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8515. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8515. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8484. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8484. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8450. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8450. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8463. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8463. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8441. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8441. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8441. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.20it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.61s. Batch: 0.69s. Loss: 0.8357. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.61s. Batch: 0.69s. Loss: 0.8357. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 53/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8221. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 53/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8221. :  67%|██████▋   | 2/3 [00:00<00:00,  2.37it/s]Finetune Epoch: 53/70. Data: 0.89s. Batch: 0.94s. Loss: 0.8277. :  67%|██████▋   | 2/3 [00:01<00:00,  2.37it/s]Finetune Epoch: 53/70. Data: 0.89s. Batch: 0.94s. Loss: 0.8277. : 100%|██████████| 3/3 [00:01<00:00,  2.73it/s]Finetune Epoch: 53/70. Data: 0.89s. Batch: 0.94s. Loss: 0.8277. : 100%|██████████| 3/3 [00:01<00:00,  2.19it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8294. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8294. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8361. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8361. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8404. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8404. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8511. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8511. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.42it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8481. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.42it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8481. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8447. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8447. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8460. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8460. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8437. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8437. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.84it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8437. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.72s. Batch: 0.77s. Loss: 0.8632. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.72s. Batch: 0.77s. Loss: 0.8632. :  33%|███▎      | 1/3 [00:00<00:01,  1.29it/s]Finetune Epoch: 54/70. Data: 0.87s. Batch: 0.92s. Loss: 0.8460. :  33%|███▎      | 1/3 [00:01<00:01,  1.29it/s]Finetune Epoch: 54/70. Data: 0.87s. Batch: 0.92s. Loss: 0.8460. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 54/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8253. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 54/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8253. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]Finetune Epoch: 54/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8253. : 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8291. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8291. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.88it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8358. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.88it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8358. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8400. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8400. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.37it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8507. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.37it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8507. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.69it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8477. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.69it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8477. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8443. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8443. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.00it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8456. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.00it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8456. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  3.73it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8434. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.73it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8434. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.76it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8434. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.17it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.70s. Batch: 0.77s. Loss: 0.8376. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.70s. Batch: 0.77s. Loss: 0.8376. :  33%|███▎      | 1/3 [00:00<00:01,  1.30it/s]Finetune Epoch: 55/70. Data: 0.95s. Batch: 1.02s. Loss: 0.8237. :  33%|███▎      | 1/3 [00:01<00:01,  1.30it/s]Finetune Epoch: 55/70. Data: 0.95s. Batch: 1.02s. Loss: 0.8237. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 55/70. Data: 1.12s. Batch: 1.19s. Loss: 0.8155. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 55/70. Data: 1.12s. Batch: 1.19s. Loss: 0.8155. : 100%|██████████| 3/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 55/70. Data: 1.12s. Batch: 1.19s. Loss: 0.8155. : 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8288. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8288. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8355. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8355. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8397. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8397. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8505. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8505. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8474. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8474. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8440. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8440. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.20it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8453. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.20it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8453. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8430. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8430. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8430. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.62s. Batch: 0.68s. Loss: 0.8301. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.62s. Batch: 0.68s. Loss: 0.8301. :  33%|███▎      | 1/3 [00:00<00:01,  1.48it/s]Finetune Epoch: 56/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8220. :  33%|███▎      | 1/3 [00:00<00:01,  1.48it/s]Finetune Epoch: 56/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8220. :  67%|██████▋   | 2/3 [00:00<00:00,  2.24it/s]Finetune Epoch: 56/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8275. :  67%|██████▋   | 2/3 [00:01<00:00,  2.24it/s]Finetune Epoch: 56/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8275. : 100%|██████████| 3/3 [00:01<00:00,  2.68it/s]Finetune Epoch: 56/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8275. : 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8285. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8285. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8352. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8352. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.46it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8394. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.46it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8394. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8501. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8501. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.42it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8471. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.42it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8471. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.61it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8437. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.61it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8437. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8449. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8449. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8427. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8427. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8427. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.74s. Batch: 0.79s. Loss: 0.8276. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.74s. Batch: 0.79s. Loss: 0.8276. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 57/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8219. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 57/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8219. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 57/70. Data: 1.13s. Batch: 1.18s. Loss: 0.8223. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 57/70. Data: 1.13s. Batch: 1.18s. Loss: 0.8223. : 100%|██████████| 3/3 [00:01<00:00,  2.11it/s]Finetune Epoch: 57/70. Data: 1.13s. Batch: 1.18s. Loss: 0.8223. : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8281. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8281. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8348. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8348. top1: 96.48. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8390. top1: 96.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8390. top1: 96.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8498. top1: 95.02. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8498. top1: 95.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8467. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8467. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8433. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8433. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8446. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8446. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8423. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8423. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8423. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.80s. Batch: 0.85s. Loss: 0.8032. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.80s. Batch: 0.85s. Loss: 0.8032. :  33%|███▎      | 1/3 [00:00<00:01,  1.18it/s]Finetune Epoch: 58/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8263. :  33%|███▎      | 1/3 [00:01<00:01,  1.18it/s]Finetune Epoch: 58/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8263. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 58/70. Data: 1.13s. Batch: 1.18s. Loss: 0.8185. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 58/70. Data: 1.13s. Batch: 1.18s. Loss: 0.8185. : 100%|██████████| 3/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 58/70. Data: 1.13s. Batch: 1.18s. Loss: 0.8185. : 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8277. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8277. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8345. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8345. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.61it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8386. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.61it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8386. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8494. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8494. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8464. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8464. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8430. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8430. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.56it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8443. top1: 94.48. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.56it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8443. top1: 94.48. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8420. top1: 94.65. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8420. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8420. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.99it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.69s. Batch: 0.73s. Loss: 0.8343. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.69s. Batch: 0.73s. Loss: 0.8343. :  33%|███▎      | 1/3 [00:00<00:01,  1.36it/s]Finetune Epoch: 59/70. Data: 0.85s. Batch: 0.90s. Loss: 0.8276. :  33%|███▎      | 1/3 [00:01<00:01,  1.36it/s]Finetune Epoch: 59/70. Data: 0.85s. Batch: 0.90s. Loss: 0.8276. :  67%|██████▋   | 2/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 59/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8242. :  67%|██████▋   | 2/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 59/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8242. : 100%|██████████| 3/3 [00:01<00:00,  2.35it/s]Finetune Epoch: 59/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8242. : 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8275. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8275. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8342. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8342. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.74it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8383. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.74it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8383. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.96it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8492. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.96it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8492. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8461. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8461. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.40it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8427. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.40it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8427. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.48it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8440. top1: 94.48. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.48it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8440. top1: 94.48. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8417. top1: 94.65. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8417. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8417. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8449. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8449. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 60/70. Data: 0.75s. Batch: 0.80s. Loss: 0.8230. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 60/70. Data: 0.75s. Batch: 0.80s. Loss: 0.8230. :  67%|██████▋   | 2/3 [00:00<00:00,  2.39it/s]Finetune Epoch: 60/70. Data: 0.90s. Batch: 0.95s. Loss: 0.8219. :  67%|██████▋   | 2/3 [00:01<00:00,  2.39it/s]Finetune Epoch: 60/70. Data: 0.90s. Batch: 0.95s. Loss: 0.8219. : 100%|██████████| 3/3 [00:01<00:00,  2.64it/s]Finetune Epoch: 60/70. Data: 0.90s. Batch: 0.95s. Loss: 0.8219. : 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8271. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8271. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.87it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8338. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.87it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8338. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8380. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8380. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.24it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8488. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.24it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8488. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8457. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8457. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.56it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8423. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.56it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8423. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.63it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8436. top1: 94.48. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.63it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8436. top1: 94.48. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.76it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8413. top1: 94.65. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.76it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8413. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8413. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.50s. Batch: 0.56s. Loss: 0.7854. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.50s. Batch: 0.56s. Loss: 0.7854. :  33%|███▎      | 1/3 [00:00<00:01,  1.77it/s]Finetune Epoch: 61/70. Data: 0.62s. Batch: 0.68s. Loss: 0.8145. :  33%|███▎      | 1/3 [00:00<00:01,  1.77it/s]Finetune Epoch: 61/70. Data: 0.62s. Batch: 0.68s. Loss: 0.8145. :  67%|██████▋   | 2/3 [00:00<00:00,  2.68it/s]Finetune Epoch: 61/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8226. :  67%|██████▋   | 2/3 [00:01<00:00,  2.68it/s]Finetune Epoch: 61/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8226. : 100%|██████████| 3/3 [00:01<00:00,  3.17it/s]Finetune Epoch: 61/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8226. : 100%|██████████| 3/3 [00:01<00:00,  2.48it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8267. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8267. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.99it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8335. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.99it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8335. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8376. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8376. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8484. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8484. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.60it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8453. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.60it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8453. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8420. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8420. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8432. top1: 94.48. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8432. top1: 94.48. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8409. top1: 94.65. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8409. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8409. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.45it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.52s. Batch: 0.58s. Loss: 0.8263. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.52s. Batch: 0.58s. Loss: 0.8263. :  33%|███▎      | 1/3 [00:00<00:01,  1.71it/s]Finetune Epoch: 62/70. Data: 0.66s. Batch: 0.71s. Loss: 0.8197. :  33%|███▎      | 1/3 [00:00<00:01,  1.71it/s]Finetune Epoch: 62/70. Data: 0.66s. Batch: 0.71s. Loss: 0.8197. :  67%|██████▋   | 2/3 [00:00<00:00,  2.56it/s]Finetune Epoch: 62/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8199. :  67%|██████▋   | 2/3 [00:01<00:00,  2.56it/s]Finetune Epoch: 62/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8199. : 100%|██████████| 3/3 [00:01<00:00,  2.66it/s]Finetune Epoch: 62/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8199. : 100%|██████████| 3/3 [00:01<00:00,  2.24it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8265. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8265. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.02it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8332. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.02it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8332. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8373. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8373. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.47it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8482. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.47it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8482. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8451. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8451. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.79it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8417. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.79it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8417. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.95it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8429. top1: 94.48. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.95it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8429. top1: 94.48. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8406. top1: 94.65. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8406. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8406. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.49it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.53s. Batch: 0.59s. Loss: 0.8383. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.53s. Batch: 0.59s. Loss: 0.8383. :  33%|███▎      | 1/3 [00:00<00:01,  1.70it/s]Finetune Epoch: 63/70. Data: 0.66s. Batch: 0.71s. Loss: 0.8233. :  33%|███▎      | 1/3 [00:00<00:01,  1.70it/s]Finetune Epoch: 63/70. Data: 0.66s. Batch: 0.71s. Loss: 0.8233. :  67%|██████▋   | 2/3 [00:00<00:00,  2.58it/s]Finetune Epoch: 63/70. Data: 0.80s. Batch: 0.85s. Loss: 0.8148. :  67%|██████▋   | 2/3 [00:01<00:00,  2.58it/s]Finetune Epoch: 63/70. Data: 0.80s. Batch: 0.85s. Loss: 0.8148. : 100%|██████████| 3/3 [00:01<00:00,  2.87it/s]Finetune Epoch: 63/70. Data: 0.80s. Batch: 0.85s. Loss: 0.8148. : 100%|██████████| 3/3 [00:01<00:00,  2.36it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8262. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8262. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.23it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8329. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.23it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8329. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8370. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8370. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.44it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8479. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.44it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8479. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.69it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8448. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.69it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8448. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8413. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8413. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8426. top1: 94.48. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  4.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8426. top1: 94.48. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.13it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8403. top1: 94.65. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.13it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8403. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.26s. Loss: 0.8403. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.57it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.52s. Batch: 0.58s. Loss: 0.8346. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.52s. Batch: 0.58s. Loss: 0.8346. :  33%|███▎      | 1/3 [00:00<00:01,  1.73it/s]Finetune Epoch: 64/70. Data: 0.65s. Batch: 0.71s. Loss: 0.8286. :  33%|███▎      | 1/3 [00:00<00:01,  1.73it/s]Finetune Epoch: 64/70. Data: 0.65s. Batch: 0.71s. Loss: 0.8286. :  67%|██████▋   | 2/3 [00:00<00:00,  2.56it/s]Finetune Epoch: 64/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8211. :  67%|██████▋   | 2/3 [00:01<00:00,  2.56it/s]Finetune Epoch: 64/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8211. : 100%|██████████| 3/3 [00:01<00:00,  2.99it/s]Finetune Epoch: 64/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8211. : 100%|██████████| 3/3 [00:01<00:00,  2.39it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8259. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8259. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.08it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8326. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.08it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8326. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.09it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8367. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.09it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8367. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.45it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8476. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.45it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8476. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8444. top1: 94.77. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8444. top1: 94.77. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.92it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8410. top1: 94.79. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.92it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8410. top1: 94.79. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8423. top1: 94.48. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8423. top1: 94.48. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  4.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8399. top1: 94.65. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  4.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8399. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8399. top1: 94.65. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.50s. Batch: 0.54s. Loss: 0.8353. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.50s. Batch: 0.54s. Loss: 0.8353. :  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s]Finetune Epoch: 65/70. Data: 0.64s. Batch: 0.68s. Loss: 0.8220. :  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s]Finetune Epoch: 65/70. Data: 0.64s. Batch: 0.68s. Loss: 0.8220. :  67%|██████▋   | 2/3 [00:00<00:00,  2.58it/s]Finetune Epoch: 65/70. Data: 0.78s. Batch: 0.82s. Loss: 0.8316. :  67%|██████▋   | 2/3 [00:01<00:00,  2.58it/s]Finetune Epoch: 65/70. Data: 0.78s. Batch: 0.82s. Loss: 0.8316. : 100%|██████████| 3/3 [00:01<00:00,  2.93it/s]Finetune Epoch: 65/70. Data: 0.78s. Batch: 0.82s. Loss: 0.8316. : 100%|██████████| 3/3 [00:01<00:00,  2.42it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8256. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8256. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.97it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8323. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.97it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8323. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.74it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8364. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.74it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8364. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.22it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8473. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.22it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8473. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.66it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8441. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.66it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8441. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8407. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8407. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.90it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8420. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.90it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8420. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8396. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8396. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8396. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.39it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.49s. Batch: 0.55s. Loss: 0.8117. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.49s. Batch: 0.55s. Loss: 0.8117. :  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s]Finetune Epoch: 66/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8166. :  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s]Finetune Epoch: 66/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8166. :  67%|██████▋   | 2/3 [00:00<00:00,  2.57it/s]Finetune Epoch: 66/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8214. :  67%|██████▋   | 2/3 [00:01<00:00,  2.57it/s]Finetune Epoch: 66/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8214. : 100%|██████████| 3/3 [00:01<00:00,  2.95it/s]Finetune Epoch: 66/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8214. : 100%|██████████| 3/3 [00:01<00:00,  2.35it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8254. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8254. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.95it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8321. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.95it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8321. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.86it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8362. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.86it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8362. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8471. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8471. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.55it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8439. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.55it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8439. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8404. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8404. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.72it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8416. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.72it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8416. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8393. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8393. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8393. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.45it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.53s. Batch: 0.59s. Loss: 0.8120. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.53s. Batch: 0.59s. Loss: 0.8120. :  33%|███▎      | 1/3 [00:00<00:01,  1.69it/s]Finetune Epoch: 67/70. Data: 0.66s. Batch: 0.72s. Loss: 0.8336. :  33%|███▎      | 1/3 [00:00<00:01,  1.69it/s]Finetune Epoch: 67/70. Data: 0.66s. Batch: 0.72s. Loss: 0.8336. :  67%|██████▋   | 2/3 [00:00<00:00,  2.57it/s]Finetune Epoch: 67/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8216. :  67%|██████▋   | 2/3 [00:01<00:00,  2.57it/s]Finetune Epoch: 67/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8216. : 100%|██████████| 3/3 [00:01<00:00,  3.09it/s]Finetune Epoch: 67/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8216. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8251. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8251. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8318. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8318. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8359. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:01,  3.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8359. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:00<00:01,  3.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8468. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8468. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8435. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8435. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8401. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8401. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.63it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8413. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.63it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8413. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:01<00:00,  3.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8390. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8390. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.8390. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.39it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 68/70. Data: 0.61s. Batch: 0.68s. Loss: 0.8166. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 68/70. Data: 0.61s. Batch: 0.68s. Loss: 0.8166. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 68/70. Data: 0.74s. Batch: 0.80s. Loss: 0.8167. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 68/70. Data: 0.74s. Batch: 0.80s. Loss: 0.8167. :  67%|██████▋   | 2/3 [00:00<00:00,  2.39it/s]Finetune Epoch: 68/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8226. :  67%|██████▋   | 2/3 [00:01<00:00,  2.39it/s]Finetune Epoch: 68/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8226. : 100%|██████████| 3/3 [00:01<00:00,  2.81it/s]Finetune Epoch: 68/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8226. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8248. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8248. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8315. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.94it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8315. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.86it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8356. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.86it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8356. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.27it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8465. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.27it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8465. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.55it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8432. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.55it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8432. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8398. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8398. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8410. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8410. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8387. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8387. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8387. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.61s. Batch: 0.67s. Loss: 0.8151. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.61s. Batch: 0.67s. Loss: 0.8151. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch: 69/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8391. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch: 69/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8391. :  67%|██████▋   | 2/3 [00:00<00:00,  2.17it/s]Finetune Epoch: 69/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8217. :  67%|██████▋   | 2/3 [00:01<00:00,  2.17it/s]Finetune Epoch: 69/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8217. : 100%|██████████| 3/3 [00:01<00:00,  2.43it/s]Finetune Epoch: 69/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8217. : 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8245. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8245. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8313. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8313. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8353. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8353. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8462. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8462. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.30it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8430. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.30it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8430. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.47it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8395. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.47it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8395. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8407. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8407. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8383. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8383. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8383. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.21it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.64s. Batch: 0.73s. Loss: 0.8372. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.64s. Batch: 0.73s. Loss: 0.8372. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 70/70. Data: 0.83s. Batch: 0.90s. Loss: 0.8329. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 70/70. Data: 0.83s. Batch: 0.90s. Loss: 0.8329. :  67%|██████▋   | 2/3 [00:01<00:00,  1.98it/s]Finetune Epoch: 70/70. Data: 1.00s. Batch: 1.06s. Loss: 0.8255. :  67%|██████▋   | 2/3 [00:01<00:00,  1.98it/s]Finetune Epoch: 70/70. Data: 1.00s. Batch: 1.06s. Loss: 0.8255. : 100%|██████████| 3/3 [00:01<00:00,  2.43it/s]Finetune Epoch: 70/70. Data: 1.00s. Batch: 1.06s. Loss: 0.8255. : 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8242. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8242. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8309. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8309. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.55it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8349. top1: 95.96. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.55it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8349. top1: 95.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8459. top1: 94.92. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8459. top1: 94.92. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.05it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8426. top1: 94.84. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.05it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8426. top1: 94.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8392. top1: 94.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8392. top1: 94.86. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.57it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8404. top1: 94.53. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.57it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8404. top1: 94.53. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8380. top1: 94.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8380. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8380. top1: 94.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.99it/s]
total 9984 correct 7951 accuracy 79.63741987179486
[INFO] main.py:349 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 10 -> 10
[INFO] main.py:357 > [2-3] Start to train under online
[INFO] main.py:372 > Train over streamed data once
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42 pseudo_stream_size 42
num_stuff 237
[INFO] rainbow_memory.py:120 > Streamed samples: 800
[INFO] rainbow_memory.py:121 > In-memory samples: 500
[INFO] rainbow_memory.py:122 > Pseudo samples: 9984
[INFO] rainbow_memory.py:128 > Train samples: 11284
[INFO] rainbow_memory.py:129 > Test samples: 4000
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([38, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
last_idx 17
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
final_idx 237
task1/train/loss 0.6990182244477152 0
task1/test/loss 5.407322778806582 0
task1/test/acc 0.40825 0
task1/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 1/1 | train_loss 0.6990 | train_acc 0.6118 | test_loss 5.4073 | test_acc 0.4083 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[INFO] finetune.py:679 > Compute uncertainty by vr_randaug!
[WARNING] finetune.py:639 > Fill the unused slots by breaking the equilibrium.
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
automobile    153
bird          133
deer          117
dog            97
Name: klass, dtype: int64
[INFO] main.py:388 > Train over memory
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42 pseudo_stream_size 42
num_stuff 0
[INFO] rainbow_memory.py:120 > Streamed samples: 0
[INFO] rainbow_memory.py:121 > In-memory samples: 500
[INFO] rainbow_memory.py:122 > Pseudo samples: 0
[INFO] rainbow_memory.py:128 > Train samples: 500
[INFO] rainbow_memory.py:129 > Test samples: 4000
last_idx 11
final_idx 0
task1/train/loss 2.602961699167887 0
task1/test/loss 1.4102685333608271 0
task1/test/acc 0.44275 0
task1/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 1/256 | train_loss 2.6030 | train_acc 0.4280 | test_loss 1.4103 | test_acc 0.4427 | lr 0.0050
last_idx 11
final_idx 0
task1/train/loss 2.338137040535609 1
task1/test/loss 5.754269679824074 1
task1/test/acc 0.3225 1
task1/train/lr 0.05 1
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 2/256 | train_loss 2.3381 | train_acc 0.3100 | test_loss 5.7543 | test_acc 0.3225 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 1.6739309926827748 2
task1/test/loss 1.6039417006157257 2
task1/test/acc 0.3295 2
task1/train/lr 0.05 2
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 3/256 | train_loss 1.6739 | train_acc 0.2620 | test_loss 1.6039 | test_acc 0.3295 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 1.3770008385181427 3
task1/test/loss 1.352015108852596 3
task1/test/acc 0.31225 3
task1/train/lr 0.02525 3
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 4/256 | train_loss 1.3770 | train_acc 0.3300 | test_loss 1.3520 | test_acc 0.3123 | lr 0.0253
last_idx 11
final_idx 0
task1/train/loss 1.422677864631017 4
task1/test/loss 1.4849277787156157 4
task1/test/acc 0.4045 4
task1/train/lr 0.05 4
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 5/256 | train_loss 1.4227 | train_acc 0.3160 | test_loss 1.4849 | test_acc 0.4045 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 1.3787216345469158 5
task1/test/loss 1.5260744160348243 5
task1/test/acc 0.3955 5
task1/train/lr 0.04275089283436705 5
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 6/256 | train_loss 1.3787 | train_acc 0.3520 | test_loss 1.5261 | test_acc 0.3955 | lr 0.0428
last_idx 11
final_idx 0
task1/train/loss 1.273021290699641 6
task1/test/loss 1.2371196910575195 6
task1/test/acc 0.3845 6
task1/train/lr 0.02525 6
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 7/256 | train_loss 1.2730 | train_acc 0.4080 | test_loss 1.2371 | test_acc 0.3845 | lr 0.0253
last_idx 11
final_idx 0
task1/train/loss 1.273768852154414 7
task1/test/loss 1.178568007526817 7
task1/test/acc 0.46 7
task1/train/lr 0.00774910716563295 7
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 8/256 | train_loss 1.2738 | train_acc 0.3980 | test_loss 1.1786 | test_acc 0.4600 | lr 0.0077
last_idx 11
final_idx 0
task1/train/loss 1.2638354102770488 8
task1/test/loss 1.1245084725893462 8
task1/test/acc 0.46825 8
task1/train/lr 0.05 8
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 9/256 | train_loss 1.2638 | train_acc 0.4060 | test_loss 1.1245 | test_acc 0.4682 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 1.2256912887096405 9
task1/test/loss 1.2408490881815062 9
task1/test/acc 0.373 9
task1/train/lr 0.04811601842965435 9
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 10/256 | train_loss 1.2257 | train_acc 0.4540 | test_loss 1.2408 | test_acc 0.3730 | lr 0.0481
last_idx 11
final_idx 0
task1/train/loss 1.19644333422184 10
task1/test/loss 1.0833149218297267 10
task1/test/acc 0.5295 10
task1/train/lr 0.04275089283436705 10
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 11/256 | train_loss 1.1964 | train_acc 0.4520 | test_loss 1.0833 | test_acc 0.5295 | lr 0.0428
last_idx 11
final_idx 0
task1/train/loss 1.2362124919891357 11
task1/test/loss 1.0715354805464272 11
task1/test/acc 0.539 11
task1/train/lr 0.03472141495103598 11
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 12/256 | train_loss 1.2362 | train_acc 0.4500 | test_loss 1.0715 | test_acc 0.5390 | lr 0.0347
last_idx 11
final_idx 0
task1/train/loss 1.1482748140891392 12
task1/test/loss 1.0328691313554952 12
task1/test/acc 0.5105 12
task1/train/lr 0.02525 12
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 13/256 | train_loss 1.1483 | train_acc 0.4900 | test_loss 1.0329 | test_acc 0.5105 | lr 0.0253
last_idx 11
final_idx 0
task1/train/loss 1.162957678238551 13
task1/test/loss 1.0921623965541085 13
task1/test/acc 0.5225 13
task1/train/lr 0.01577858504896403 13
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 14/256 | train_loss 1.1630 | train_acc 0.4760 | test_loss 1.0922 | test_acc 0.5225 | lr 0.0158
last_idx 11
final_idx 0
task1/train/loss 1.142528548836708 14
task1/test/loss 1.0858154329624805 14
task1/test/acc 0.513 14
task1/train/lr 0.00774910716563295 14
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 15/256 | train_loss 1.1425 | train_acc 0.4840 | test_loss 1.0858 | test_acc 0.5130 | lr 0.0077
last_idx 11
final_idx 0
task1/train/loss 1.1426277309656143 15
task1/test/loss 1.0634814913456256 15
task1/test/acc 0.52725 15
task1/train/lr 0.0023839815703456534 15
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 16/256 | train_loss 1.1426 | train_acc 0.4560 | test_loss 1.0635 | test_acc 0.5272 | lr 0.0024
last_idx 11
final_idx 0
task1/train/loss 1.105643053849538 16
task1/test/loss 1.0422476726573902 16
task1/test/acc 0.5445 16
task1/train/lr 0.05 16
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 17/256 | train_loss 1.1056 | train_acc 0.5120 | test_loss 1.0422 | test_acc 0.5445 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 1.216803600390752 17
task1/test/loss 1.1626347467139526 17
task1/test/acc 0.50625 17
task1/train/lr 0.049524435689979954 17
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 18/256 | train_loss 1.2168 | train_acc 0.4280 | test_loss 1.1626 | test_acc 0.5062 | lr 0.0495
last_idx 11
final_idx 0
task1/train/loss 1.2956137855847676 18
task1/test/loss 1.0815905129516519 18
task1/test/acc 0.5195 18
task1/train/lr 0.04811601842965435 18
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 19/256 | train_loss 1.2956 | train_acc 0.4000 | test_loss 1.0816 | test_acc 0.5195 | lr 0.0481
last_idx 11
final_idx 0
task1/train/loss 1.1815215547879536 19
task1/test/loss 1.0085329727812127 19
task1/test/acc 0.55625 19
task1/train/lr 0.045828872904488 19
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 20/256 | train_loss 1.1815 | train_acc 0.4860 | test_loss 1.0085 | test_acc 0.5563 | lr 0.0458
last_idx 11
final_idx 0
task1/train/loss 1.1748272279898326 20
task1/test/loss 1.068207832483145 20
task1/test/acc 0.55025 20
task1/train/lr 0.04275089283436705 20
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 21/256 | train_loss 1.1748 | train_acc 0.4940 | test_loss 1.0682 | test_acc 0.5503 | lr 0.0428
last_idx 11
final_idx 0
task1/train/loss 1.1469052682320278 21
task1/test/loss 1.0818352509330917 21
task1/test/acc 0.51825 21
task1/train/lr 0.039000363267235154 21
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 22/256 | train_loss 1.1469 | train_acc 0.5060 | test_loss 1.0818 | test_acc 0.5182 | lr 0.0390
last_idx 11
final_idx 0
task1/train/loss 1.154572586218516 22
task1/test/loss 1.0104721227844993 22
task1/test/acc 0.55775 22
task1/train/lr 0.03472141495103598 22
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 23/256 | train_loss 1.1546 | train_acc 0.4540 | test_loss 1.0105 | test_acc 0.5577 | lr 0.0347
last_idx 11
final_idx 0
task1/train/loss 1.0871155112981796 23
task1/test/loss 0.9966427244982876 23
task1/test/acc 0.5125 23
task1/train/lr 0.03007848546989918 23
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 24/256 | train_loss 1.0871 | train_acc 0.5100 | test_loss 0.9966 | test_acc 0.5125 | lr 0.0301
last_idx 11
final_idx 0
task1/train/loss 1.153786266843478 24
task1/test/loss 1.0050466912133353 24
task1/test/acc 0.56625 24
task1/train/lr 0.02525 24
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 25/256 | train_loss 1.1538 | train_acc 0.4720 | test_loss 1.0050 | test_acc 0.5663 | lr 0.0253
last_idx 11
final_idx 0
task1/train/loss 1.1955155730247498 25
task1/test/loss 0.9737176914791484 25
task1/test/acc 0.5785 25
task1/train/lr 0.02042151453010083 25
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 26/256 | train_loss 1.1955 | train_acc 0.4220 | test_loss 0.9737 | test_acc 0.5785 | lr 0.0204
last_idx 11
final_idx 0
task1/train/loss 1.1363909244537354 26
task1/test/loss 0.9524815960244818 26
task1/test/acc 0.5965 26
task1/train/lr 0.01577858504896403 26
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 27/256 | train_loss 1.1364 | train_acc 0.4940 | test_loss 0.9525 | test_acc 0.5965 | lr 0.0158
last_idx 11
final_idx 0
task1/train/loss 1.0839642783006032 27
task1/test/loss 0.9616185253138071 27
task1/test/acc 0.586 27
task1/train/lr 0.011499636732764853 27
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 28/256 | train_loss 1.0840 | train_acc 0.5160 | test_loss 0.9616 | test_acc 0.5860 | lr 0.0115
last_idx 11
final_idx 0
task1/train/loss 1.0994619925816853 28
task1/test/loss 0.9394325417476695 28
task1/test/acc 0.601 28
task1/train/lr 0.00774910716563295 28
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 29/256 | train_loss 1.0995 | train_acc 0.4760 | test_loss 0.9394 | test_acc 0.6010 | lr 0.0077
last_idx 11
final_idx 0
task1/train/loss 1.0608283082644145 29
task1/test/loss 0.9135413258285313 29
task1/test/acc 0.60375 29
task1/train/lr 0.004671127095512003 29
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 30/256 | train_loss 1.0608 | train_acc 0.5340 | test_loss 0.9135 | test_acc 0.6038 | lr 0.0047
last_idx 11
final_idx 0
task1/train/loss 1.118595411380132 30
task1/test/loss 0.9149009434731452 30
task1/test/acc 0.604 30
task1/train/lr 0.0023839815703456534 30
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 31/256 | train_loss 1.1186 | train_acc 0.4940 | test_loss 0.9149 | test_acc 0.6040 | lr 0.0024
last_idx 11
final_idx 0
task1/train/loss 1.037907545765241 31
task1/test/loss 0.9178432943401756 31
task1/test/acc 0.60275 31
task1/train/lr 0.0009755643100200469 31
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 32/256 | train_loss 1.0379 | train_acc 0.5520 | test_loss 0.9178 | test_acc 0.6028 | lr 0.0010
last_idx 11
final_idx 0
task1/train/loss 1.0564035922288895 32
task1/test/loss 1.247817222233657 32
task1/test/acc 0.467 32
task1/train/lr 0.05 32
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 33/256 | train_loss 1.0564 | train_acc 0.5440 | test_loss 1.2478 | test_acc 0.4670 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 1.094810610016187 33
task1/test/loss 0.9376754407044295 33
task1/test/acc 0.582 33
task1/train/lr 0.049880821985136874 33
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 34/256 | train_loss 1.0948 | train_acc 0.5380 | test_loss 0.9377 | test_acc 0.5820 | lr 0.0499
last_idx 11
final_idx 0
task1/train/loss 1.1243126491705577 34
task1/test/loss 1.0348421459669594 34
task1/test/acc 0.54325 34
task1/train/lr 0.049524435689979954 34
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 35/256 | train_loss 1.1243 | train_acc 0.4940 | test_loss 1.0348 | test_acc 0.5433 | lr 0.0495
last_idx 11
final_idx 0
task1/train/loss 1.1418312340974808 35
task1/test/loss 1.161991618164293 35
task1/test/acc 0.48625 35
task1/train/lr 0.048934273309372174 35
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 36/256 | train_loss 1.1418 | train_acc 0.4860 | test_loss 1.1620 | test_acc 0.4863 | lr 0.0489
last_idx 11
final_idx 0
task1/train/loss 1.1192801545063655 36
task1/test/loss 1.0717503788707021 36
task1/test/acc 0.5345 36
task1/train/lr 0.04811601842965435 36
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 37/256 | train_loss 1.1193 | train_acc 0.5040 | test_loss 1.0718 | test_acc 0.5345 | lr 0.0481
last_idx 11
final_idx 0
task1/train/loss 1.0907806406418483 37
task1/test/loss 1.0803918648552109 37
task1/test/acc 0.52875 37
task1/train/lr 0.04707755129262179 37
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 38/256 | train_loss 1.0908 | train_acc 0.4940 | test_loss 1.0804 | test_acc 0.5288 | lr 0.0471
last_idx 11
final_idx 0
task1/train/loss 1.1227204849322636 38
task1/test/loss 1.1165721102075263 38
task1/test/acc 0.506 38
task1/train/lr 0.045828872904488 38
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 39/256 | train_loss 1.1227 | train_acc 0.5340 | test_loss 1.1166 | test_acc 0.5060 | lr 0.0458
last_idx 11
final_idx 0
task1/train/loss 1.1885684033234913 39
task1/test/loss 1.0568165281316737 39
task1/test/acc 0.53125 39
task1/train/lr 0.04438200872072774 39
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 40/256 | train_loss 1.1886 | train_acc 0.4520 | test_loss 1.0568 | test_acc 0.5312 | lr 0.0444
last_idx 11
final_idx 0
task1/train/loss 1.0482824047406514 40
task1/test/loss 1.0274405354981895 40
task1/test/acc 0.55925 40
task1/train/lr 0.04275089283436705 40
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 41/256 | train_loss 1.0483 | train_acc 0.5620 | test_loss 1.0274 | test_acc 0.5593 | lr 0.0428
last_idx 11
final_idx 0
task1/train/loss 1.0534672886133194 41
task1/test/loss 1.1152682500881153 41
task1/test/acc 0.52725 41
task1/train/lr 0.040951233783050225 41
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 42/256 | train_loss 1.0535 | train_acc 0.5120 | test_loss 1.1153 | test_acc 0.5272 | lr 0.0410
last_idx 11
final_idx 0
task1/train/loss 1.1578583319981892 42
task1/test/loss 0.9720453770606072 42
task1/test/acc 0.54575 42
task1/train/lr 0.039000363267235154 42
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 43/256 | train_loss 1.1579 | train_acc 0.4860 | test_loss 0.9720 | test_acc 0.5457 | lr 0.0390
last_idx 11
final_idx 0
task1/train/loss 1.1154344479242961 43
task1/test/loss 0.9542043867347004 43
task1/test/acc 0.59 43
task1/train/lr 0.03691706923644345 43
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 44/256 | train_loss 1.1154 | train_acc 0.5020 | test_loss 0.9542 | test_acc 0.5900 | lr 0.0369
last_idx 11
final_idx 0
task1/train/loss 1.0746117134888966 44
task1/test/loss 1.0262393211270426 44
task1/test/acc 0.55 44
task1/train/lr 0.03472141495103598 44
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 45/256 | train_loss 1.0746 | train_acc 0.4940 | test_loss 1.0262 | test_acc 0.5500 | lr 0.0347
last_idx 11
final_idx 0
task1/train/loss 1.106924682855606 45
task1/test/loss 1.1776147406179827 45
task1/test/acc 0.50325 45
task1/train/lr 0.03243454576204794 45
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 46/256 | train_loss 1.1069 | train_acc 0.5140 | test_loss 1.1776 | test_acc 0.5032 | lr 0.0324
last_idx 11
final_idx 0
task1/train/loss 1.1714665691057842 46
task1/test/loss 0.9310926526457399 46
task1/test/acc 0.587 46
task1/train/lr 0.03007848546989918 46
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 47/256 | train_loss 1.1715 | train_acc 0.4480 | test_loss 0.9311 | test_acc 0.5870 | lr 0.0301
last_idx 11
final_idx 0
task1/train/loss 1.1180309504270554 47
task1/test/loss 0.9999319845503503 47
task1/test/acc 0.5665 47
task1/train/lr 0.027675924223156633 47
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 48/256 | train_loss 1.1180 | train_acc 0.4840 | test_loss 0.9999 | test_acc 0.5665 | lr 0.0277
last_idx 11
final_idx 0
task1/train/loss 1.0624373257160187 48
task1/test/loss 0.9737096979067876 48
task1/test/acc 0.572 48
task1/train/lr 0.02525 48
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 49/256 | train_loss 1.0624 | train_acc 0.5360 | test_loss 0.9737 | test_acc 0.5720 | lr 0.0253
last_idx 11
final_idx 0
task1/train/loss 1.0645025372505188 49
task1/test/loss 0.8935193262257419 49
task1/test/acc 0.61675 49
task1/train/lr 0.022824075776843374 49
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 50/256 | train_loss 1.0645 | train_acc 0.5620 | test_loss 0.8935 | test_acc 0.6168 | lr 0.0228
last_idx 11
final_idx 0
task1/train/loss 1.0554646104574203 50
task1/test/loss 0.9746716323789659 50
task1/test/acc 0.573 50
task1/train/lr 0.02042151453010083 50
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 51/256 | train_loss 1.0555 | train_acc 0.5280 | test_loss 0.9747 | test_acc 0.5730 | lr 0.0204
last_idx 11
final_idx 0
task1/train/loss 1.004790132244428 51
task1/test/loss 0.9563270057295705 51
task1/test/acc 0.58225 51
task1/train/lr 0.018065454237952062 51
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 52/256 | train_loss 1.0048 | train_acc 0.5740 | test_loss 0.9563 | test_acc 0.5823 | lr 0.0181
last_idx 11
final_idx 0
task1/train/loss 0.9828466773033142 52
task1/test/loss 0.9739330961154058 52
task1/test/acc 0.5715 52
task1/train/lr 0.01577858504896403 52
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 53/256 | train_loss 0.9828 | train_acc 0.5900 | test_loss 0.9739 | test_acc 0.5715 | lr 0.0158
last_idx 11
final_idx 0
task1/train/loss 0.995037724574407 53
task1/test/loss 0.9651563927367494 53
task1/test/acc 0.5765 53
task1/train/lr 0.013582930763556558 53
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 54/256 | train_loss 0.9950 | train_acc 0.5420 | test_loss 0.9652 | test_acc 0.5765 | lr 0.0136
last_idx 11
final_idx 0
task1/train/loss 0.9666573901971182 54
task1/test/loss 1.005424868274521 54
task1/test/acc 0.565 54
task1/train/lr 0.011499636732764853 54
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 55/256 | train_loss 0.9667 | train_acc 0.5860 | test_loss 1.0054 | test_acc 0.5650 | lr 0.0115
last_idx 11
final_idx 0
task1/train/loss 1.06497161090374 55
task1/test/loss 1.007634924335794 55
task1/test/acc 0.5635 55
task1/train/lr 0.009548766216949778 55
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 56/256 | train_loss 1.0650 | train_acc 0.5180 | test_loss 1.0076 | test_acc 0.5635 | lr 0.0095
last_idx 11
final_idx 0
task1/train/loss 1.0064909060796101 56
task1/test/loss 0.988178727391002 56
task1/test/acc 0.57325 56
task1/train/lr 0.00774910716563295 56
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 57/256 | train_loss 1.0065 | train_acc 0.5500 | test_loss 0.9882 | test_acc 0.5733 | lr 0.0077
last_idx 11
final_idx 0
task1/train/loss 1.087097446123759 57
task1/test/loss 0.9411906148051168 57
task1/test/acc 0.589 57
task1/train/lr 0.0061179912792722595 57
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 58/256 | train_loss 1.0871 | train_acc 0.5020 | test_loss 0.9412 | test_acc 0.5890 | lr 0.0061
last_idx 11
final_idx 0
task1/train/loss 0.977228432893753 58
task1/test/loss 0.924882077908778 58
task1/test/acc 0.60075 58
task1/train/lr 0.004671127095512003 58
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 59/256 | train_loss 0.9772 | train_acc 0.5720 | test_loss 0.9249 | test_acc 0.6008 | lr 0.0047
last_idx 11
final_idx 0
task1/train/loss 1.0196159730354946 59
task1/test/loss 0.9191216675789802 59
task1/test/acc 0.59675 59
task1/train/lr 0.0034224487073782153 59
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 60/256 | train_loss 1.0196 | train_acc 0.5480 | test_loss 0.9191 | test_acc 0.5968 | lr 0.0034
last_idx 11
final_idx 0
task1/train/loss 0.9730347990989685 60
task1/test/loss 0.8995768061051002 60
task1/test/acc 0.60625 60
task1/train/lr 0.0023839815703456534 60
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 61/256 | train_loss 0.9730 | train_acc 0.6060 | test_loss 0.8996 | test_acc 0.6062 | lr 0.0024
last_idx 11
final_idx 0
task1/train/loss 0.9184192816416422 61
task1/test/loss 0.9020161032676697 61
task1/test/acc 0.60325 61
task1/train/lr 0.0015657266906278318 61
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 62/256 | train_loss 0.9184 | train_acc 0.6100 | test_loss 0.9020 | test_acc 0.6032 | lr 0.0016
last_idx 11
final_idx 0
task1/train/loss 0.9595038294792175 62
task1/test/loss 0.9163216778210231 62
task1/test/acc 0.596 62
task1/train/lr 0.0009755643100200469 62
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 63/256 | train_loss 0.9595 | train_acc 0.5700 | test_loss 0.9163 | test_acc 0.5960 | lr 0.0010
last_idx 11
final_idx 0
task1/train/loss 0.9666369408369064 63
task1/test/loss 0.9087565904790229 63
task1/test/acc 0.6005 63
task1/train/lr 0.0006191780148631288 63
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 64/256 | train_loss 0.9666 | train_acc 0.5740 | test_loss 0.9088 | test_acc 0.6005 | lr 0.0006
last_idx 11
final_idx 0
task1/train/loss 1.015133132537206 64
task1/test/loss 1.014652349791684 64
task1/test/acc 0.56075 64
task1/train/lr 0.05 64
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 65/256 | train_loss 1.0151 | train_acc 0.5800 | test_loss 1.0147 | test_acc 0.5607 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 1.0769668966531754 65
task1/test/loss 1.0360591765288467 65
task1/test/acc 0.5415 65
task1/train/lr 0.04997018754107802 65
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 66/256 | train_loss 1.0770 | train_acc 0.5300 | test_loss 1.0361 | test_acc 0.5415 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 1.1864106555779774 66
task1/test/loss 1.0842836119316437 66
task1/test/acc 0.523 66
task1/train/lr 0.049880821985136874 66
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 67/256 | train_loss 1.1864 | train_acc 0.4960 | test_loss 1.0843 | test_acc 0.5230 | lr 0.0499
last_idx 11
final_idx 0
task1/train/loss 1.180515433351199 67
task1/test/loss 0.9583204499967806 67
task1/test/acc 0.58425 67
task1/train/lr 0.04973211862162834 67
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 68/256 | train_loss 1.1805 | train_acc 0.4640 | test_loss 0.9583 | test_acc 0.5843 | lr 0.0497
last_idx 11
final_idx 0
task1/train/loss 1.0812414288520813 68
task1/test/loss 0.9074386391010913 68
task1/test/acc 0.606 68
task1/train/lr 0.049524435689979954 68
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 69/256 | train_loss 1.0812 | train_acc 0.5080 | test_loss 0.9074 | test_acc 0.6060 | lr 0.0495
last_idx 11
final_idx 0
task1/train/loss 1.0978756596644719 69
task1/test/loss 0.9367775484755799 69
task1/test/acc 0.58625 69
task1/train/lr 0.04925827351656497 69
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 70/256 | train_loss 1.0979 | train_acc 0.5380 | test_loss 0.9368 | test_acc 0.5863 | lr 0.0493
last_idx 11
final_idx 0
task1/train/loss 1.0655523041884105 70
task1/test/loss 1.007574961735652 70
task1/test/acc 0.55275 70
task1/train/lr 0.048934273309372174 70
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 71/256 | train_loss 1.0656 | train_acc 0.5080 | test_loss 1.0076 | test_acc 0.5527 | lr 0.0489
last_idx 11
final_idx 0
task1/train/loss 1.104565714796384 71
task1/test/loss 0.9842083179033719 71
task1/test/acc 0.56675 71
task1/train/lr 0.048553215613279764 71
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 72/256 | train_loss 1.1046 | train_acc 0.5520 | test_loss 0.9842 | test_acc 0.5667 | lr 0.0486
last_idx 11
final_idx 0
task1/train/loss 1.0296863069136937 72
task1/test/loss 0.9454057737366184 72
task1/test/acc 0.597 72
task1/train/lr 0.04811601842965435 72
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 73/256 | train_loss 1.0297 | train_acc 0.5500 | test_loss 0.9454 | test_acc 0.5970 | lr 0.0481
last_idx 11
final_idx 0
task1/train/loss 1.031478722890218 73
task1/test/loss 1.016469915162076 73
task1/test/acc 0.54175 73
task1/train/lr 0.047623735004805226 73
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 74/256 | train_loss 1.0315 | train_acc 0.5880 | test_loss 1.0165 | test_acc 0.5417 | lr 0.0476
last_idx 11
final_idx 0
task1/train/loss 1.0745288381973903 74
task1/test/loss 1.043325757587349 74
task1/test/acc 0.53275 74
task1/train/lr 0.04707755129262179 74
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 75/256 | train_loss 1.0745 | train_acc 0.5300 | test_loss 1.0433 | test_acc 0.5327 | lr 0.0471
last_idx 11
final_idx 0
task1/train/loss 1.0437394678592682 75
task1/test/loss 1.0043377981081114 75
task1/test/acc 0.577 75
task1/train/lr 0.046478783097506735 75
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 76/256 | train_loss 1.0437 | train_acc 0.5340 | test_loss 1.0043 | test_acc 0.5770 | lr 0.0465
last_idx 11
final_idx 0
task1/train/loss 0.9681035826603571 76
task1/test/loss 0.9706354082285703 76
task1/test/acc 0.57125 76
task1/train/lr 0.045828872904488 76
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 77/256 | train_loss 0.9681 | train_acc 0.6100 | test_loss 0.9706 | test_acc 0.5713 | lr 0.0458
last_idx 11
final_idx 0
task1/train/loss 1.0535491406917572 77
task1/test/loss 0.9138561221924457 77
task1/test/acc 0.597 77
task1/train/lr 0.04512938640414596 77
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 78/256 | train_loss 1.0535 | train_acc 0.5440 | test_loss 0.9139 | test_acc 0.5970 | lr 0.0451
last_idx 11
final_idx 0
task1/train/loss 0.972808395822843 78
task1/test/loss 0.9296008482739165 78
task1/test/acc 0.5875 78
task1/train/lr 0.04438200872072774 78
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 79/256 | train_loss 0.9728 | train_acc 0.6060 | test_loss 0.9296 | test_acc 0.5875 | lr 0.0444
last_idx 11
final_idx 0
task1/train/loss 1.1899218161900837 79
task1/test/loss 0.9612741136288905 79
task1/test/acc 0.58425 79
task1/train/lr 0.043588540352535246 79
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 80/256 | train_loss 1.1899 | train_acc 0.4780 | test_loss 0.9613 | test_acc 0.5843 | lr 0.0436
last_idx 11
final_idx 0
task1/train/loss 1.054407964150111 80
task1/test/loss 0.9364244259975769 80
task1/test/acc 0.5895 80
task1/train/lr 0.04275089283436705 80
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 81/256 | train_loss 1.0544 | train_acc 0.5580 | test_loss 0.9364 | test_acc 0.5895 | lr 0.0428
last_idx 11
final_idx 0
task1/train/loss 1.0149333824714024 81
task1/test/loss 1.0639656467752143 81
task1/test/acc 0.535 81
task1/train/lr 0.04187108413246371 81
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 82/256 | train_loss 1.0149 | train_acc 0.5800 | test_loss 1.0640 | test_acc 0.5350 | lr 0.0419
last_idx 11
final_idx 0
task1/train/loss 1.006184662381808 82
task1/test/loss 1.017916996727933 82
task1/test/acc 0.5605 82
task1/train/lr 0.040951233783050225 82
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 83/256 | train_loss 1.0062 | train_acc 0.5720 | test_loss 1.0179 | test_acc 0.5605 | lr 0.0410
last_idx 11
final_idx 0
task1/train/loss 1.0923788249492645 83
task1/test/loss 0.9552713865107232 83
task1/test/acc 0.582 83
task1/train/lr 0.03999355778618773 83
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 84/256 | train_loss 1.0924 | train_acc 0.5200 | test_loss 0.9553 | test_acc 0.5820 | lr 0.0400
last_idx 11
final_idx 0
task1/train/loss 1.0466324090957642 84
task1/test/loss 0.9651680143324883 84
task1/test/acc 0.58025 84
task1/train/lr 0.039000363267235154 84
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 85/256 | train_loss 1.0466 | train_acc 0.5580 | test_loss 0.9652 | test_acc 0.5803 | lr 0.0390
last_idx 11
final_idx 0
task1/train/loss 0.9544451286395391 85
task1/test/loss 0.9890897120093252 85
task1/test/acc 0.582 85
task1/train/lr 0.03797404291878224 85
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 86/256 | train_loss 0.9544 | train_acc 0.5760 | test_loss 0.9891 | test_acc 0.5820 | lr 0.0380
last_idx 11
final_idx 0
task1/train/loss 1.051557977994283 86
task1/test/loss 1.0721345105014004 86
task1/test/acc 0.50825 86
task1/train/lr 0.03691706923644345 86
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 87/256 | train_loss 1.0516 | train_acc 0.5540 | test_loss 1.0721 | test_acc 0.5082 | lr 0.0369
last_idx 11
final_idx 0
task1/train/loss 1.042651856938998 87
task1/test/loss 1.0938786268234253 87
task1/test/acc 0.52925 87
task1/train/lr 0.03583198856239948 87
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 88/256 | train_loss 1.0427 | train_acc 0.5940 | test_loss 1.0939 | test_acc 0.5292 | lr 0.0358
last_idx 11
final_idx 0
task1/train/loss 0.9906289378801981 88
task1/test/loss 1.0156785121985845 88
task1/test/acc 0.56625 88
task1/train/lr 0.03472141495103598 88
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 89/256 | train_loss 0.9906 | train_acc 0.5840 | test_loss 1.0157 | test_acc 0.5663 | lr 0.0347
last_idx 11
final_idx 0
task1/train/loss 1.0400893092155457 89
task1/test/loss 0.9366300305822394 89
task1/test/acc 0.594 89
task1/train/lr 0.03358802387145745 89
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 90/256 | train_loss 1.0401 | train_acc 0.5900 | test_loss 0.9366 | test_acc 0.5940 | lr 0.0336
last_idx 11
final_idx 0
task1/train/loss 1.0457947502533596 90
task1/test/loss 0.8992184145109994 90
task1/test/acc 0.61375 90
task1/train/lr 0.03243454576204794 90
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 91/256 | train_loss 1.0458 | train_acc 0.5860 | test_loss 0.8992 | test_acc 0.6138 | lr 0.0324
last_idx 11
final_idx 0
task1/train/loss 0.9767090330521265 91
task1/test/loss 1.0041181556471102 91
task1/test/acc 0.5785 91
task1/train/lr 0.03126375945260579 91
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 92/256 | train_loss 0.9767 | train_acc 0.6060 | test_loss 1.0041 | test_acc 0.5785 | lr 0.0313
last_idx 11
final_idx 0
task1/train/loss 0.952545166015625 92
task1/test/loss 0.9611552672071771 92
task1/test/acc 0.5855 92
task1/train/lr 0.03007848546989918 92
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 93/256 | train_loss 0.9525 | train_acc 0.6260 | test_loss 0.9612 | test_acc 0.5855 | lr 0.0301
last_idx 11
final_idx 0
task1/train/loss 0.8969728748003641 93
task1/test/loss 0.9206203621822399 93
task1/test/acc 0.6125 93
task1/train/lr 0.028881579242770204 93
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 94/256 | train_loss 0.8970 | train_acc 0.6460 | test_loss 0.9206 | test_acc 0.6125 | lr 0.0289
last_idx 11
final_idx 0
task1/train/loss 1.008579323689143 94
task1/test/loss 1.085410747554276 94
task1/test/acc 0.5475 94
task1/train/lr 0.027675924223156633 94
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 95/256 | train_loss 1.0086 | train_acc 0.5800 | test_loss 1.0854 | test_acc 0.5475 | lr 0.0277
last_idx 11
final_idx 0
task1/train/loss 0.9921225955088934 95
task1/test/loss 0.9978588848978609 95
task1/test/acc 0.58575 95
task1/train/lr 0.0264644249396036 95
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 96/256 | train_loss 0.9921 | train_acc 0.5780 | test_loss 0.9979 | test_acc 0.5857 | lr 0.0265
last_idx 11
final_idx 0
task1/train/loss 0.9943288067976633 96
task1/test/loss 0.9105951982540089 96
task1/test/acc 0.62 96
task1/train/lr 0.02525 96
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 97/256 | train_loss 0.9943 | train_acc 0.5740 | test_loss 0.9106 | test_acc 0.6200 | lr 0.0253
last_idx 11
final_idx 0
task1/train/loss 0.901887928446134 97
task1/test/loss 0.866209423149025 97
task1/test/acc 0.6375 97
task1/train/lr 0.024035575060396407 97
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 98/256 | train_loss 0.9019 | train_acc 0.6220 | test_loss 0.8662 | test_acc 0.6375 | lr 0.0240
last_idx 11
final_idx 0
task1/train/loss 0.9101491918166479 98
task1/test/loss 0.9474493428901002 98
task1/test/acc 0.602 98
task1/train/lr 0.022824075776843374 98
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 99/256 | train_loss 0.9101 | train_acc 0.6220 | test_loss 0.9474 | test_acc 0.6020 | lr 0.0228
last_idx 11
final_idx 0
task1/train/loss 0.891127328077952 99
task1/test/loss 0.9573233176718702 99
task1/test/acc 0.606 99
task1/train/lr 0.0216184207572298 99
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 100/256 | train_loss 0.8911 | train_acc 0.6060 | test_loss 0.9573 | test_acc 0.6060 | lr 0.0216
last_idx 11
final_idx 0
task1/train/loss 0.9470648964246114 100
task1/test/loss 0.8801409968963037 100
task1/test/acc 0.62475 100
task1/train/lr 0.02042151453010083 100
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 101/256 | train_loss 0.9471 | train_acc 0.6180 | test_loss 0.8801 | test_acc 0.6248 | lr 0.0204
last_idx 11
final_idx 0
task1/train/loss 0.9699381788571676 101
task1/test/loss 0.86530741918218 101
task1/test/acc 0.63025 101
task1/train/lr 0.019236240547394222 101
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 102/256 | train_loss 0.9699 | train_acc 0.6300 | test_loss 0.8653 | test_acc 0.6302 | lr 0.0192
last_idx 11
final_idx 0
task1/train/loss 0.934499646226565 102
task1/test/loss 0.8898070605246575 102
task1/test/acc 0.63075 102
task1/train/lr 0.018065454237952062 102
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 103/256 | train_loss 0.9345 | train_acc 0.6420 | test_loss 0.8898 | test_acc 0.6308 | lr 0.0181
last_idx 11
final_idx 0
task1/train/loss 0.8858609298865 103
task1/test/loss 0.9006887890480377 103
task1/test/acc 0.6345 103
task1/train/lr 0.016911976128542557 103
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 104/256 | train_loss 0.8859 | train_acc 0.6900 | test_loss 0.9007 | test_acc 0.6345 | lr 0.0169
last_idx 11
final_idx 0
task1/train/loss 0.9331776152054468 104
task1/test/loss 0.9993063116466606 104
task1/test/acc 0.59325 104
task1/train/lr 0.01577858504896403 104
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 105/256 | train_loss 0.9332 | train_acc 0.6400 | test_loss 0.9993 | test_acc 0.5933 | lr 0.0158
last_idx 11
final_idx 0
task1/train/loss 0.8747707456350327 105
task1/test/loss 0.8705098625722822 105
task1/test/acc 0.63275 105
task1/train/lr 0.014668011437600525 105
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 106/256 | train_loss 0.8748 | train_acc 0.6420 | test_loss 0.8705 | test_acc 0.6328 | lr 0.0147
last_idx 11
final_idx 0
task1/train/loss 0.9862238417069117 106
task1/test/loss 0.9160745510688195 106
task1/test/acc 0.613 106
task1/train/lr 0.013582930763556558 106
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 107/256 | train_loss 0.9862 | train_acc 0.6160 | test_loss 0.9161 | test_acc 0.6130 | lr 0.0136
last_idx 11
final_idx 0
task1/train/loss 1.0366600006818771 107
task1/test/loss 0.9463603172328446 107
task1/test/acc 0.6095 107
task1/train/lr 0.012525957081217764 107
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 108/256 | train_loss 1.0367 | train_acc 0.5540 | test_loss 0.9464 | test_acc 0.6095 | lr 0.0125
last_idx 11
final_idx 0
task1/train/loss 0.871860682964325 108
task1/test/loss 0.9125616321196923 108
task1/test/acc 0.61 108
task1/train/lr 0.011499636732764853 108
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 109/256 | train_loss 0.8719 | train_acc 0.6300 | test_loss 0.9126 | test_acc 0.6100 | lr 0.0115
last_idx 11
final_idx 0
task1/train/loss 0.9239647636810938 109
task1/test/loss 0.8995222331403376 109
task1/test/acc 0.626 109
task1/train/lr 0.010506442213812275 109
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 110/256 | train_loss 0.9240 | train_acc 0.6400 | test_loss 0.8995 | test_acc 0.6260 | lr 0.0105
last_idx 11
final_idx 0
task1/train/loss 0.9567461510499319 110
task1/test/loss 0.888720935845113 110
task1/test/acc 0.626 110
task1/train/lr 0.009548766216949778 110
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 111/256 | train_loss 0.9567 | train_acc 0.6220 | test_loss 0.8887 | test_acc 0.6260 | lr 0.0095
last_idx 11
final_idx 0
task1/train/loss 0.9823214610417684 111
task1/test/loss 0.87692310996763 111
task1/test/acc 0.6275 111
task1/train/lr 0.008628915867536294 111
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 112/256 | train_loss 0.9823 | train_acc 0.6160 | test_loss 0.8769 | test_acc 0.6275 | lr 0.0086
last_idx 11
final_idx 0
task1/train/loss 0.9260813693205515 112
task1/test/loss 0.8410381170419546 112
task1/test/acc 0.65475 112
task1/train/lr 0.00774910716563295 112
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 113/256 | train_loss 0.9261 | train_acc 0.6060 | test_loss 0.8410 | test_acc 0.6548 | lr 0.0077
last_idx 11
final_idx 0
task1/train/loss 0.7789793511231741 113
task1/test/loss 0.9187722546713692 113
task1/test/acc 0.6185 113
task1/train/lr 0.006911459647464768 113
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 114/256 | train_loss 0.7790 | train_acc 0.6980 | test_loss 0.9188 | test_acc 0.6185 | lr 0.0069
last_idx 11
final_idx 0
task1/train/loss 0.9210536579291025 114
task1/test/loss 0.8820086570231469 114
task1/test/acc 0.63275 114
task1/train/lr 0.0061179912792722595 114
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 115/256 | train_loss 0.9211 | train_acc 0.6340 | test_loss 0.8820 | test_acc 0.6328 | lr 0.0061
last_idx 11
final_idx 0
task1/train/loss 0.947294200460116 115
task1/test/loss 0.8894624687158145 115
task1/test/acc 0.6355 115
task1/train/lr 0.005370613595854041 115
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 116/256 | train_loss 0.9473 | train_acc 0.6540 | test_loss 0.8895 | test_acc 0.6355 | lr 0.0054
last_idx 11
final_idx 0
task1/train/loss 0.8378081346551577 116
task1/test/loss 0.8832542761996552 116
task1/test/acc 0.64525 116
task1/train/lr 0.004671127095512003 116
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 117/256 | train_loss 0.8378 | train_acc 0.6520 | test_loss 0.8833 | test_acc 0.6452 | lr 0.0047
last_idx 11
final_idx 0
task1/train/loss 0.7892073318362236 117
task1/test/loss 0.8858020253233857 117
task1/test/acc 0.6445 117
task1/train/lr 0.004021216902493268 117
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 118/256 | train_loss 0.7892 | train_acc 0.7000 | test_loss 0.8858 | test_acc 0.6445 | lr 0.0040
last_idx 11
final_idx 0
task1/train/loss 0.9244365046421686 118
task1/test/loss 0.892399074939581 118
task1/test/acc 0.6345 118
task1/train/lr 0.0034224487073782153 118
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 119/256 | train_loss 0.9244 | train_acc 0.6500 | test_loss 0.8924 | test_acc 0.6345 | lr 0.0034
last_idx 11
final_idx 0
task1/train/loss 0.809362697104613 119
task1/test/loss 0.857275902897447 119
task1/test/acc 0.6535 119
task1/train/lr 0.0028762649951947776 119
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 120/256 | train_loss 0.8094 | train_acc 0.7200 | test_loss 0.8573 | test_acc 0.6535 | lr 0.0029
last_idx 11
final_idx 0
task1/train/loss 0.9389277299245199 120
task1/test/loss 0.8928778685711243 120
task1/test/acc 0.63775 120
task1/train/lr 0.0023839815703456534 120
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 121/256 | train_loss 0.9389 | train_acc 0.6460 | test_loss 0.8929 | test_acc 0.6378 | lr 0.0024
last_idx 11
final_idx 0
task1/train/loss 0.7634553064902624 121
task1/test/loss 0.9031306492103325 121
task1/test/acc 0.6335 121
task1/train/lr 0.0019467843867202379 121
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 122/256 | train_loss 0.7635 | train_acc 0.6980 | test_loss 0.9031 | test_acc 0.6335 | lr 0.0019
last_idx 11
final_idx 0
task1/train/loss 0.7543248931566874 122
task1/test/loss 0.8946909891380058 122
task1/test/acc 0.64325 122
task1/train/lr 0.0015657266906278318 122
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 123/256 | train_loss 0.7543 | train_acc 0.6860 | test_loss 0.8947 | test_acc 0.6432 | lr 0.0016
last_idx 11
final_idx 0
task1/train/loss 0.8526759395996729 123
task1/test/loss 0.9089257438759227 123
task1/test/acc 0.63925 123
task1/train/lr 0.0012417264834350366 123
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 124/256 | train_loss 0.8527 | train_acc 0.6180 | test_loss 0.9089 | test_acc 0.6392 | lr 0.0012
last_idx 11
final_idx 0
task1/train/loss 0.7982675184806188 124
task1/test/loss 0.9060323824594309 124
task1/test/acc 0.639 124
task1/train/lr 0.0009755643100200469 124
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 125/256 | train_loss 0.7983 | train_acc 0.7060 | test_loss 0.9060 | test_acc 0.6390 | lr 0.0010
last_idx 11
final_idx 0
task1/train/loss 0.8586737637718519 125
task1/test/loss 0.8839243447387611 125
task1/test/acc 0.648 125
task1/train/lr 0.0007678813783716699 125
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 126/256 | train_loss 0.8587 | train_acc 0.6640 | test_loss 0.8839 | test_acc 0.6480 | lr 0.0008
last_idx 11
final_idx 0
task1/train/loss 0.8797387952605883 126
task1/test/loss 0.8704395330214239 126
task1/test/acc 0.6515 126
task1/train/lr 0.0006191780148631288 126
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 127/256 | train_loss 0.8797 | train_acc 0.6600 | test_loss 0.8704 | test_acc 0.6515 | lr 0.0006
last_idx 11
final_idx 0
task1/train/loss 0.9495042910178503 127
task1/test/loss 0.8948692404306852 127
task1/test/acc 0.64525 127
task1/train/lr 0.0005298124589219829 127
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 128/256 | train_loss 0.9495 | train_acc 0.6280 | test_loss 0.8949 | test_acc 0.6452 | lr 0.0005
last_idx 11
final_idx 0
task1/train/loss 1.0211946417888005 128
task1/test/loss 1.148141542306313 128
task1/test/acc 0.51275 128
task1/train/lr 0.05 128
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 129/256 | train_loss 1.0212 | train_acc 0.5840 | test_loss 1.1481 | test_acc 0.5128 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 0.9750395317872366 129
task1/test/loss 0.8280248998940646 129
task1/test/acc 0.6485 129
task1/train/lr 0.04999254576273106 129
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 130/256 | train_loss 0.9750 | train_acc 0.6060 | test_loss 0.8280 | test_acc 0.6485 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 1.078416829307874 130
task1/test/loss 0.815161811044583 130
task1/test/acc 0.64775 130
task1/train/lr 0.04997018754107802 130
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 131/256 | train_loss 1.0784 | train_acc 0.5240 | test_loss 0.8152 | test_acc 0.6478 | lr 0.0500
last_idx 11
final_idx 0
task1/train/loss 0.9111874649922053 131
task1/test/loss 0.8234066406449119 131
task1/test/acc 0.6505 131
task1/train/lr 0.04993293880279759 131
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 132/256 | train_loss 0.9112 | train_acc 0.6460 | test_loss 0.8234 | test_acc 0.6505 | lr 0.0499
last_idx 11
final_idx 0
task1/train/loss 0.9310366113980612 132
task1/test/loss 1.0993339851662354 132
task1/test/acc 0.54975 132
task1/train/lr 0.049880821985136874 132
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 133/256 | train_loss 0.9310 | train_acc 0.6260 | test_loss 1.0993 | test_acc 0.5497 | lr 0.0499
last_idx 11
final_idx 0
task1/train/loss 1.0108601252237956 133
task1/test/loss 0.9053342899532764 133
task1/test/acc 0.61125 133
task1/train/lr 0.04981386848131808 133
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 134/256 | train_loss 1.0109 | train_acc 0.5580 | test_loss 0.9053 | test_acc 0.6112 | lr 0.0498
last_idx 11
final_idx 0
task1/train/loss 1.0271000663439434 134
task1/test/loss 1.070853434421204 134
task1/test/acc 0.5595 134
task1/train/lr 0.04973211862162834 134
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 135/256 | train_loss 1.0271 | train_acc 0.5440 | test_loss 1.0709 | test_acc 0.5595 | lr 0.0497
last_idx 11
final_idx 0
task1/train/loss 0.9445154964923859 135
task1/test/loss 0.8786539578667054 135
task1/test/acc 0.602 135
task1/train/lr 0.04963562164912629 135
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 136/256 | train_loss 0.9445 | train_acc 0.5760 | test_loss 0.8787 | test_acc 0.6020 | lr 0.0496
last_idx 11
final_idx 0
task1/train/loss 1.0510092029968898 136
task1/test/loss 0.9869335483718704 136
task1/test/acc 0.58125 136
task1/train/lr 0.049524435689979954 136
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 137/256 | train_loss 1.0510 | train_acc 0.5720 | test_loss 0.9869 | test_acc 0.5813 | lr 0.0495
last_idx 11
final_idx 0
task1/train/loss 0.8748481621344885 137
task1/test/loss 0.870368534719551 137
task1/test/acc 0.6265 137
task1/train/lr 0.04939862771845358 137
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 138/256 | train_loss 0.8748 | train_acc 0.6160 | test_loss 0.8704 | test_acc 0.6265 | lr 0.0494
last_idx 11
final_idx 0
task1/train/loss 1.011794279019038 138
task1/test/loss 1.0351411569249498 138
task1/test/acc 0.56225 138
task1/train/lr 0.04925827351656497 138
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 139/256 | train_loss 1.0118 | train_acc 0.5940 | test_loss 1.0351 | test_acc 0.5623 | lr 0.0493
last_idx 11
final_idx 0
task1/train/loss 1.0435481468836467 139
task1/test/loss 0.9220265215569801 139
task1/test/acc 0.60675 139
task1/train/lr 0.04910345762843714 139
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 140/256 | train_loss 1.0435 | train_acc 0.5540 | test_loss 0.9220 | test_acc 0.6068 | lr 0.0491
last_idx 11
final_idx 0
task1/train/loss 0.9172320117553076 140
task1/test/loss 0.9426732969644306 140
task1/test/acc 0.6105 140
task1/train/lr 0.048934273309372174 140
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 141/256 | train_loss 0.9172 | train_acc 0.6280 | test_loss 0.9427 | test_acc 0.6105 | lr 0.0489
last_idx 11
final_idx 0
task1/train/loss 0.9679338832696279 141
task1/test/loss 0.902226719554964 141
task1/test/acc 0.62125 141
task1/train/lr 0.04875082246967766 141
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 142/256 | train_loss 0.9679 | train_acc 0.6500 | test_loss 0.9022 | test_acc 0.6212 | lr 0.0488
last_idx 11
final_idx 0
task1/train/loss 0.9569877435763677 142
task1/test/loss 0.8461303239340311 142
task1/test/acc 0.63625 142
task1/train/lr 0.048553215613279764 142
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 143/256 | train_loss 0.9570 | train_acc 0.6000 | test_loss 0.8461 | test_acc 0.6362 | lr 0.0486
last_idx 11
final_idx 0
task1/train/loss 0.9032232811053594 143
task1/test/loss 1.2616222268277473 143
task1/test/acc 0.51475 143
task1/train/lr 0.04834157177115979 143
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 144/256 | train_loss 0.9032 | train_acc 0.6480 | test_loss 1.2616 | test_acc 0.5148 | lr 0.0483
last_idx 11
final_idx 0
task1/train/loss 1.0168200880289078 144
task1/test/loss 0.9554768250538752 144
task1/test/acc 0.63175 144
task1/train/lr 0.04811601842965435 144
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 145/256 | train_loss 1.0168 | train_acc 0.6300 | test_loss 0.9555 | test_acc 0.6318 | lr 0.0481
last_idx 11
final_idx 0
task1/train/loss 0.9170092393954595 145
task1/test/loss 0.8979369202157953 145
task1/test/acc 0.62275 145
task1/train/lr 0.047876691453662384 145
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 146/256 | train_loss 0.9170 | train_acc 0.6160 | test_loss 0.8979 | test_acc 0.6228 | lr 0.0479
last_idx 11
final_idx 0
task1/train/loss 0.953176369269689 146
task1/test/loss 1.0142877817481428 146
task1/test/acc 0.59675 146
task1/train/lr 0.047623735004805226 146
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 147/256 | train_loss 0.9532 | train_acc 0.6180 | test_loss 1.0143 | test_acc 0.5968 | lr 0.0476
last_idx 11
final_idx 0
task1/train/loss 1.0451173683007557 147
task1/test/loss 0.9619216096925212 147
task1/test/acc 0.58175 147
task1/train/lr 0.047357301454589 147
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 148/256 | train_loss 1.0451 | train_acc 0.5260 | test_loss 0.9619 | test_acc 0.5817 | lr 0.0474
last_idx 11
final_idx 0
task1/train/loss 0.9294977386792501 148
task1/test/loss 0.9122080242895818 148
task1/test/acc 0.62675 148
task1/train/lr 0.04707755129262179 148
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 149/256 | train_loss 0.9295 | train_acc 0.6340 | test_loss 0.9122 | test_acc 0.6268 | lr 0.0471
last_idx 11
final_idx 0
task1/train/loss 0.9404718627532324 149
task1/test/loss 0.9171826931146475 149
task1/test/acc 0.59325 149
task1/train/lr 0.04678465302994061 149
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 150/256 | train_loss 0.9405 | train_acc 0.6060 | test_loss 0.9172 | test_acc 0.5933 | lr 0.0468
last_idx 11
final_idx 0
task1/train/loss 1.072780321041743 150
task1/test/loss 0.9338037556671834 150
task1/test/acc 0.60425 150
task1/train/lr 0.046478783097506735 150
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 151/256 | train_loss 1.0728 | train_acc 0.5700 | test_loss 0.9338 | test_acc 0.6042 | lr 0.0465
last_idx 11
final_idx 0
task1/train/loss 0.9095240583022436 151
task1/test/loss 1.073953281392108 151
task1/test/acc 0.547 151
task1/train/lr 0.04616012573993025 151
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 152/256 | train_loss 0.9095 | train_acc 0.6060 | test_loss 1.0740 | test_acc 0.5470 | lr 0.0462
last_idx 11
final_idx 0
task1/train/loss 0.9457237621148428 152
task1/test/loss 0.8991949862831241 152
task1/test/acc 0.61825 152
task1/train/lr 0.045828872904488 152
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 153/256 | train_loss 0.9457 | train_acc 0.6400 | test_loss 0.8992 | test_acc 0.6182 | lr 0.0458
last_idx 11
final_idx 0
task1/train/loss 1.0070627530415852 153
task1/test/loss 0.8614539402526814 153
task1/test/acc 0.646 153
task1/train/lr 0.0454852241255017 153
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 154/256 | train_loss 1.0071 | train_acc 0.5800 | test_loss 0.8615 | test_acc 0.6460 | lr 0.0455
last_idx 11
final_idx 0
task1/train/loss 0.8110090345144272 154
task1/test/loss 0.8873846095341903 154
task1/test/acc 0.62525 154
task1/train/lr 0.04512938640414596 154
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 155/256 | train_loss 0.8110 | train_acc 0.6960 | test_loss 0.8874 | test_acc 0.6252 | lr 0.0451
last_idx 11
final_idx 0
task1/train/loss 0.9346267630656561 155
task1/test/loss 0.96822203875898 155
task1/test/acc 0.615 155
task1/train/lr 0.04476157408375851 155
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 156/256 | train_loss 0.9346 | train_acc 0.6540 | test_loss 0.9682 | test_acc 0.6150 | lr 0.0448
last_idx 11
final_idx 0
task1/train/loss 0.8250617186228434 156
task1/test/loss 0.969810500904754 156
task1/test/acc 0.625 156
task1/train/lr 0.04438200872072774 156
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 157/256 | train_loss 0.8251 | train_acc 0.6740 | test_loss 0.9698 | test_acc 0.6250 | lr 0.0444
last_idx 11
final_idx 0
task1/train/loss 1.0602957457304 157
task1/test/loss 1.0863500931760768 157
task1/test/acc 0.55375 157
task1/train/lr 0.0439909189510355 157
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 158/256 | train_loss 1.0603 | train_acc 0.5420 | test_loss 1.0864 | test_acc 0.5537 | lr 0.0440
last_idx 11
final_idx 0
task1/train/loss 0.8311794598897299 158
task1/test/loss 0.9261250089813065 158
task1/test/acc 0.62225 158
task1/train/lr 0.043588540352535246 158
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 159/256 | train_loss 0.8312 | train_acc 0.6940 | test_loss 0.9261 | test_acc 0.6222 | lr 0.0436
last_idx 11
final_idx 0
task1/train/loss 0.9677433595061302 159
task1/test/loss 0.8095835149288177 159
task1/test/acc 0.66425 159
task1/train/lr 0.043175115303048815 159
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 160/256 | train_loss 0.9677 | train_acc 0.6020 | test_loss 0.8096 | test_acc 0.6643 | lr 0.0432
last_idx 11
final_idx 0
task1/train/loss 0.9267025540272394 160
task1/test/loss 0.7639635134856779 160
task1/test/acc 0.69325 160
task1/train/lr 0.04275089283436705 160
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 161/256 | train_loss 0.9267 | train_acc 0.5600 | test_loss 0.7640 | test_acc 0.6933 | lr 0.0428
last_idx 11
final_idx 0
task1/train/loss 0.8921345869700114 161
task1/test/loss 0.9001887679755033 161
task1/test/acc 0.6325 161
task1/train/lr 0.042316128482242414 161
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 162/256 | train_loss 0.8921 | train_acc 0.6300 | test_loss 0.9002 | test_acc 0.6325 | lr 0.0423
last_idx 11
final_idx 0
task1/train/loss 0.8921346192558607 162
task1/test/loss 1.032087919476268 162
task1/test/acc 0.5995 162
task1/train/lr 0.04187108413246371 162
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 163/256 | train_loss 0.8921 | train_acc 0.6160 | test_loss 1.0321 | test_acc 0.5995 | lr 0.0419
last_idx 11
final_idx 0
task1/train/loss 0.8014631470044454 163
task1/test/loss 0.942061762888353 163
task1/test/acc 0.62375 163
task1/train/lr 0.04141602786310598 163
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 164/256 | train_loss 0.8015 | train_acc 0.6240 | test_loss 0.9421 | test_acc 0.6238 | lr 0.0414
last_idx 11
final_idx 0
task1/train/loss 0.8248690466086069 164
task1/test/loss 0.9108475296051948 164
task1/test/acc 0.64275 164
task1/train/lr 0.040951233783050225 164
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 165/256 | train_loss 0.8249 | train_acc 0.6840 | test_loss 0.9108 | test_acc 0.6428 | lr 0.0410
last_idx 11
final_idx 0
task1/train/loss 1.1201019833485286 165
task1/test/loss 0.8447286083803072 165
task1/test/acc 0.64375 165
task1/train/lr 0.040476981866870515 165
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 166/256 | train_loss 1.1201 | train_acc 0.4980 | test_loss 0.8447 | test_acc 0.6438 | lr 0.0405
last_idx 11
final_idx 0
task1/train/loss 0.8368093768755595 166
task1/test/loss 0.8024126795622019 166
task1/test/acc 0.67875 166
task1/train/lr 0.03999355778618773 166
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 167/256 | train_loss 0.8368 | train_acc 0.6620 | test_loss 0.8024 | test_acc 0.6787 | lr 0.0400
last_idx 11
final_idx 0
task1/train/loss 0.9014656096696854 167
task1/test/loss 0.8918633356199159 167
task1/test/acc 0.653 167
task1/train/lr 0.039501252737591676 167
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 168/256 | train_loss 0.9015 | train_acc 0.6220 | test_loss 0.8919 | test_acc 0.6530 | lr 0.0395
last_idx 11
final_idx 0
task1/train/loss 0.8595714295903841 168
task1/test/loss 0.9592529954491081 168
task1/test/acc 0.6135 168
task1/train/lr 0.039000363267235154 168
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 169/256 | train_loss 0.8596 | train_acc 0.6600 | test_loss 0.9593 | test_acc 0.6135 | lr 0.0390
last_idx 11
final_idx 0
task1/train/loss 0.8789116591215134 169
task1/test/loss 0.8834623494646051 169
task1/test/acc 0.632 169
task1/train/lr 0.03849119109220566 169
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 170/256 | train_loss 0.8789 | train_acc 0.6180 | test_loss 0.8835 | test_acc 0.6320 | lr 0.0385
last_idx 11
final_idx 0
task1/train/loss 0.8148900171120962 170
task1/test/loss 1.048712534563882 170
task1/test/acc 0.59775 170
task1/train/lr 0.03797404291878224 170
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 171/256 | train_loss 0.8149 | train_acc 0.7060 | test_loss 1.0487 | test_acc 0.5978 | lr 0.0380
last_idx 11
final_idx 0
task1/train/loss 0.7550519357124964 171
task1/test/loss 1.0027132279925295 171
task1/test/acc 0.61025 171
task1/train/lr 0.03744923025768716 171
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 172/256 | train_loss 0.7551 | train_acc 0.7320 | test_loss 1.0027 | test_acc 0.6102 | lr 0.0374
last_idx 11
final_idx 0
task1/train/loss 0.8428205475211143 172
task1/test/loss 0.8327343889645168 172
task1/test/acc 0.679 172
task1/train/lr 0.03691706923644345 172
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 173/256 | train_loss 0.8428 | train_acc 0.6840 | test_loss 0.8327 | test_acc 0.6790 | lr 0.0369
last_idx 11
final_idx 0
task1/train/loss 0.8776946018139521 173
task1/test/loss 0.8418287387588522 173
task1/test/acc 0.661 173
task1/train/lr 0.03637788040895152 173
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 174/256 | train_loss 0.8777 | train_acc 0.6480 | test_loss 0.8418 | test_acc 0.6610 | lr 0.0364
last_idx 11
final_idx 0
task1/train/loss 0.8330762137969335 174
task1/test/loss 0.9851937682091535 174
task1/test/acc 0.6305 174
task1/train/lr 0.03583198856239948 174
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 175/256 | train_loss 0.8331 | train_acc 0.6480 | test_loss 0.9852 | test_acc 0.6305 | lr 0.0358
last_idx 11
final_idx 0
task1/train/loss 0.7623152484496435 175
task1/test/loss 0.8289586318718208 175
task1/test/acc 0.6785 175
task1/train/lr 0.0352797225216235 175
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 176/256 | train_loss 0.7623 | train_acc 0.7080 | test_loss 0.8290 | test_acc 0.6785 | lr 0.0353
last_idx 11
final_idx 0
task1/train/loss 0.8494147708018621 176
task1/test/loss 0.9226549189169329 176
task1/test/acc 0.65825 176
task1/train/lr 0.03472141495103598 176
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 177/256 | train_loss 0.8494 | train_acc 0.6180 | test_loss 0.9227 | test_acc 0.6583 | lr 0.0347
last_idx 11
final_idx 0
task1/train/loss 0.9143164058526357 177
task1/test/loss 0.803861161345964 177
task1/test/acc 0.6645 177
task1/train/lr 0.034157402154240964 177
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 178/256 | train_loss 0.9143 | train_acc 0.6580 | test_loss 0.8039 | test_acc 0.6645 | lr 0.0342
last_idx 11
final_idx 0
task1/train/loss 0.8351612066229185 178
task1/test/loss 0.7344092453246588 178
task1/test/acc 0.69375 178
task1/train/lr 0.03358802387145745 178
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 179/256 | train_loss 0.8352 | train_acc 0.6680 | test_loss 0.7344 | test_acc 0.6937 | lr 0.0336
last_idx 11
final_idx 0
task1/train/loss 0.9534009546041489 179
task1/test/loss 0.8876774343815479 179
task1/test/acc 0.635 179
task1/train/lr 0.03301362307487257 179
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 180/256 | train_loss 0.9534 | train_acc 0.6180 | test_loss 0.8877 | test_acc 0.6350 | lr 0.0330
last_idx 11
final_idx 0
task1/train/loss 0.755509170393149 180
task1/test/loss 0.8382357994278709 180
task1/test/acc 0.665 180
task1/train/lr 0.03243454576204794 180
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 181/256 | train_loss 0.7555 | train_acc 0.7360 | test_loss 0.8382 | test_acc 0.6650 | lr 0.0324
last_idx 11
final_idx 0
task1/train/loss 0.9380826006333033 181
task1/test/loss 0.8912187755762876 181
task1/test/acc 0.6395 181
task1/train/lr 0.03185114074750374 181
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 182/256 | train_loss 0.9381 | train_acc 0.6020 | test_loss 0.8912 | test_acc 0.6395 | lr 0.0319
last_idx 11
final_idx 0
task1/train/loss 0.7883797883987427 182
task1/test/loss 0.8448068637114304 182
task1/test/acc 0.647 182
task1/train/lr 0.03126375945260579 182
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 183/256 | train_loss 0.7884 | train_acc 0.7040 | test_loss 0.8448 | test_acc 0.6470 | lr 0.0313
last_idx 11
final_idx 0
task1/train/loss 0.8691930944720904 183
task1/test/loss 0.9727189879823517 183
task1/test/acc 0.6225 183
task1/train/lr 0.030672755693882527 183
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 184/256 | train_loss 0.8692 | train_acc 0.6920 | test_loss 0.9727 | test_acc 0.6225 | lr 0.0307
last_idx 11
final_idx 0
task1/train/loss 0.7343927646676699 184
task1/test/loss 0.8897224667144346 184
task1/test/acc 0.6555 184
task1/train/lr 0.03007848546989918 184
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 185/256 | train_loss 0.7344 | train_acc 0.7300 | test_loss 0.8897 | test_acc 0.6555 | lr 0.0301
last_idx 11
final_idx 0
task1/train/loss 0.8263605684041977 185
task1/test/loss 0.8099870383739471 185
task1/test/acc 0.677 185
task1/train/lr 0.029481306746817457 185
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 186/256 | train_loss 0.8264 | train_acc 0.7040 | test_loss 0.8100 | test_acc 0.6770 | lr 0.0295
last_idx 11
final_idx 0
task1/train/loss 0.7713945582509041 186
task1/test/loss 0.8888134956359863 186
task1/test/acc 0.64275 186
task1/train/lr 0.028881579242770204 186
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 187/256 | train_loss 0.7714 | train_acc 0.7360 | test_loss 0.8888 | test_acc 0.6428 | lr 0.0289
last_idx 11
final_idx 0
task1/train/loss 0.7836315035820007 187
task1/test/loss 0.8397097168387947 187
task1/test/acc 0.66475 187
task1/train/lr 0.028279664211180604 187
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 188/256 | train_loss 0.7836 | train_acc 0.7020 | test_loss 0.8397 | test_acc 0.6647 | lr 0.0283
last_idx 11
final_idx 0
task1/train/loss 0.7892993241548538 188
task1/test/loss 1.0308280759132826 188
task1/test/acc 0.634 188
task1/train/lr 0.027675924223156633 188
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 189/256 | train_loss 0.7893 | train_acc 0.6860 | test_loss 1.0308 | test_acc 0.6340 | lr 0.0277
last_idx 11
final_idx 0
task1/train/loss 0.6736121277014414 189
task1/test/loss 0.8594603869286213 189
task1/test/acc 0.681 189
task1/train/lr 0.027070722949091772 189
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 190/256 | train_loss 0.6736 | train_acc 0.7640 | test_loss 0.8595 | test_acc 0.6810 | lr 0.0271
last_idx 11
final_idx 0
task1/train/loss 0.8799696216980616 190
task1/test/loss 0.8549433773035532 190
task1/test/acc 0.67275 190
task1/train/lr 0.0264644249396036 190
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 191/256 | train_loss 0.8800 | train_acc 0.6620 | test_loss 0.8549 | test_acc 0.6727 | lr 0.0265
last_idx 11
final_idx 0
task1/train/loss 0.7316414217154185 191
task1/test/loss 0.7925808085845067 191
task1/test/acc 0.68525 191
task1/train/lr 0.02585739540594208 191
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 192/256 | train_loss 0.7316 | train_acc 0.7100 | test_loss 0.7926 | test_acc 0.6853 | lr 0.0259
last_idx 11
final_idx 0
task1/train/loss 0.7860912183920542 192
task1/test/loss 1.0445739983857334 192
task1/test/acc 0.61925 192
task1/train/lr 0.02525 192
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 193/256 | train_loss 0.7861 | train_acc 0.7340 | test_loss 1.0446 | test_acc 0.6192 | lr 0.0253
last_idx 11
final_idx 0
task1/train/loss 0.7330135752757391 193
task1/test/loss 0.9099932938486666 193
task1/test/acc 0.67225 193
task1/train/lr 0.024642604594057926 193
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 194/256 | train_loss 0.7330 | train_acc 0.7040 | test_loss 0.9100 | test_acc 0.6723 | lr 0.0246
last_idx 11
final_idx 0
task1/train/loss 0.7574604625503222 194
task1/test/loss 0.8545880234994732 194
task1/test/acc 0.6695 194
task1/train/lr 0.024035575060396407 194
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 195/256 | train_loss 0.7575 | train_acc 0.7340 | test_loss 0.8546 | test_acc 0.6695 | lr 0.0240
last_idx 11
final_idx 0
task1/train/loss 0.7918257787823677 195
task1/test/loss 0.9570787676743099 195
task1/test/acc 0.63125 195
task1/train/lr 0.023429277050908234 195
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 196/256 | train_loss 0.7918 | train_acc 0.6560 | test_loss 0.9571 | test_acc 0.6312 | lr 0.0234
last_idx 11
final_idx 0
task1/train/loss 0.5338555450240771 196
task1/test/loss 0.8075889877893112 196
task1/test/acc 0.68925 196
task1/train/lr 0.022824075776843374 196
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 197/256 | train_loss 0.5339 | train_acc 0.8160 | test_loss 0.8076 | test_acc 0.6893 | lr 0.0228
last_idx 11
final_idx 0
task1/train/loss 0.8597019836306572 197
task1/test/loss 0.7892416646847358 197
task1/test/acc 0.68775 197
task1/train/lr 0.022220335788819403 197
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 198/256 | train_loss 0.8597 | train_acc 0.6540 | test_loss 0.7892 | test_acc 0.6877 | lr 0.0222
last_idx 11
final_idx 0
task1/train/loss 0.836160883307457 198
task1/test/loss 0.8576641739397258 198
task1/test/acc 0.64725 198
task1/train/lr 0.0216184207572298 198
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 199/256 | train_loss 0.8362 | train_acc 0.6700 | test_loss 0.8577 | test_acc 0.6472 | lr 0.0216
last_idx 11
final_idx 0
task1/train/loss 0.8631909886995951 199
task1/test/loss 0.9099714690512353 199
task1/test/acc 0.63525 199
task1/train/lr 0.021018693253182546 199
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 200/256 | train_loss 0.8632 | train_acc 0.6560 | test_loss 0.9100 | test_acc 0.6352 | lr 0.0210
last_idx 11
final_idx 0
task1/train/loss 0.9230262289444605 200
task1/test/loss 0.8500365225168375 200
task1/test/acc 0.656 200
task1/train/lr 0.02042151453010083 200
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 201/256 | train_loss 0.9230 | train_acc 0.6120 | test_loss 0.8500 | test_acc 0.6560 | lr 0.0204
last_idx 11
final_idx 0
task1/train/loss 0.627948967119058 201
task1/test/loss 0.8693379779438396 201
task1/test/acc 0.67025 201
task1/train/lr 0.019827244306117476 201
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 202/256 | train_loss 0.6279 | train_acc 0.7880 | test_loss 0.8693 | test_acc 0.6703 | lr 0.0198
last_idx 11
final_idx 0
task1/train/loss 0.7466882268587748 202
task1/test/loss 0.7253950799886997 202
task1/test/acc 0.715 202
task1/train/lr 0.019236240547394222 202
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 203/256 | train_loss 0.7467 | train_acc 0.6800 | test_loss 0.7254 | test_acc 0.7150 | lr 0.0192
last_idx 11
final_idx 0
task1/train/loss 0.7248013218243917 203
task1/test/loss 0.7625741297072106 203
task1/test/acc 0.69425 203
task1/train/lr 0.018648859252496267 203
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 204/256 | train_loss 0.7248 | train_acc 0.7360 | test_loss 0.7626 | test_acc 0.6943 | lr 0.0186
last_idx 11
final_idx 0
task1/train/loss 0.8834223449230194 204
task1/test/loss 0.904648903634522 204
task1/test/acc 0.6535 204
task1/train/lr 0.018065454237952062 204
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 205/256 | train_loss 0.8834 | train_acc 0.6480 | test_loss 0.9046 | test_acc 0.6535 | lr 0.0181
last_idx 11
final_idx 0
task1/train/loss 0.7999282702803612 205
task1/test/loss 0.7896460648421403 205
task1/test/acc 0.68825 205
task1/train/lr 0.017486376925127438 205
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 206/256 | train_loss 0.7999 | train_acc 0.6520 | test_loss 0.7896 | test_acc 0.6883 | lr 0.0175
last_idx 11
final_idx 0
task1/train/loss 0.6391031766931216 206
task1/test/loss 0.8633656948804855 206
task1/test/acc 0.666 206
task1/train/lr 0.016911976128542557 206
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 207/256 | train_loss 0.6391 | train_acc 0.7780 | test_loss 0.8634 | test_acc 0.6660 | lr 0.0169
last_idx 11
final_idx 0
task1/train/loss 0.7014200786749522 207
task1/test/loss 0.8450059704073183 207
task1/test/acc 0.6675 207
task1/train/lr 0.016342597845759043 207
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 208/256 | train_loss 0.7014 | train_acc 0.7680 | test_loss 0.8450 | test_acc 0.6675 | lr 0.0163
last_idx 11
final_idx 0
task1/train/loss 0.6309429084261259 208
task1/test/loss 0.8325941428378388 208
task1/test/acc 0.68325 208
task1/train/lr 0.01577858504896403 208
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 209/256 | train_loss 0.6309 | train_acc 0.7160 | test_loss 0.8326 | test_acc 0.6833 | lr 0.0158
last_idx 11
final_idx 0
task1/train/loss 0.7449811647335688 209
task1/test/loss 0.8252199167733664 209
task1/test/acc 0.694 209
task1/train/lr 0.015220277478376504 209
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 210/256 | train_loss 0.7450 | train_acc 0.7060 | test_loss 0.8252 | test_acc 0.6940 | lr 0.0152
last_idx 11
final_idx 0
task1/train/loss 0.5879464273651441 210
task1/test/loss 0.8560699473370562 210
task1/test/acc 0.6885 210
task1/train/lr 0.014668011437600525 210
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 211/256 | train_loss 0.5879 | train_acc 0.8020 | test_loss 0.8561 | test_acc 0.6885 | lr 0.0147
last_idx 11
final_idx 0
task1/train/loss 0.7772507468859354 211
task1/test/loss 0.7506448109071333 211
task1/test/acc 0.71375 211
task1/train/lr 0.014122119591048485 211
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 212/256 | train_loss 0.7773 | train_acc 0.7160 | test_loss 0.7506 | test_acc 0.7137 | lr 0.0141
last_idx 11
final_idx 0
task1/train/loss 0.8479311987757683 212
task1/test/loss 0.7955871296452952 212
task1/test/acc 0.692 212
task1/train/lr 0.013582930763556558 212
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 213/256 | train_loss 0.8479 | train_acc 0.6740 | test_loss 0.7956 | test_acc 0.6920 | lr 0.0136
last_idx 11
final_idx 0
task1/train/loss 0.8239206646879514 213
task1/test/loss 0.8011410987966663 213
task1/test/acc 0.6795 213
task1/train/lr 0.013050769742312849 213
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 214/256 | train_loss 0.8239 | train_acc 0.6980 | test_loss 0.8011 | test_acc 0.6795 | lr 0.0131
last_idx 11
final_idx 0
task1/train/loss 0.6151599908868471 214
task1/test/loss 0.7273760303691194 214
task1/test/acc 0.7195 214
task1/train/lr 0.012525957081217764 214
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 215/256 | train_loss 0.6152 | train_acc 0.7800 | test_loss 0.7274 | test_acc 0.7195 | lr 0.0125
last_idx 11
final_idx 0
task1/train/loss 0.7374386861920357 215
task1/test/loss 0.8297685915297204 215
task1/test/acc 0.66825 215
task1/train/lr 0.01200880890779435 215
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 216/256 | train_loss 0.7374 | train_acc 0.7420 | test_loss 0.8298 | test_acc 0.6683 | lr 0.0120
last_idx 11
final_idx 0
task1/train/loss 0.6499915197491646 216
task1/test/loss 0.8817447238898539 216
task1/test/acc 0.67625 216
task1/train/lr 0.011499636732764853 216
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 217/256 | train_loss 0.6500 | train_acc 0.8040 | test_loss 0.8817 | test_acc 0.6763 | lr 0.0115
last_idx 11
final_idx 0
task1/train/loss 0.5986264993747076 217
task1/test/loss 0.7975601437655124 217
task1/test/acc 0.7035 217
task1/train/lr 0.010998747262408329 217
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 218/256 | train_loss 0.5986 | train_acc 0.8280 | test_loss 0.7976 | test_acc 0.7035 | lr 0.0110
last_idx 11
final_idx 0
task1/train/loss 0.73878746231397 218
task1/test/loss 0.9295053264269462 218
task1/test/acc 0.67925 218
task1/train/lr 0.010506442213812275 218
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 219/256 | train_loss 0.7388 | train_acc 0.7580 | test_loss 0.9295 | test_acc 0.6793 | lr 0.0105
last_idx 11
final_idx 0
task1/train/loss 0.6743661736448606 219
task1/test/loss 0.8042536692304926 219
task1/test/acc 0.70625 219
task1/train/lr 0.01002301813312949 219
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 220/256 | train_loss 0.6744 | train_acc 0.7280 | test_loss 0.8043 | test_acc 0.7063 | lr 0.0100
last_idx 11
final_idx 0
task1/train/loss 0.7368329217036566 220
task1/test/loss 0.7490474912491474 220
task1/test/acc 0.70775 220
task1/train/lr 0.009548766216949778 220
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 221/256 | train_loss 0.7368 | train_acc 0.7020 | test_loss 0.7490 | test_acc 0.7077 | lr 0.0095
last_idx 11
final_idx 0
task1/train/loss 0.6094138622283936 221
task1/test/loss 0.7756030639776816 221
task1/test/acc 0.707 221
task1/train/lr 0.009083972136894032 221
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 222/256 | train_loss 0.6094 | train_acc 0.7720 | test_loss 0.7756 | test_acc 0.7070 | lr 0.0091
last_idx 11
final_idx 0
task1/train/loss 0.5448898822069168 222
task1/test/loss 0.7522378653124139 222
task1/test/acc 0.72475 222
task1/train/lr 0.008628915867536294 222
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 223/256 | train_loss 0.5449 | train_acc 0.8420 | test_loss 0.7522 | test_acc 0.7248 | lr 0.0086
last_idx 11
final_idx 0
task1/train/loss 0.6913861955205599 223
task1/test/loss 0.8526383140584924 223
task1/test/acc 0.68925 223
task1/train/lr 0.008183871517757594 223
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 224/256 | train_loss 0.6914 | train_acc 0.8080 | test_loss 0.8526 | test_acc 0.6893 | lr 0.0082
last_idx 11
final_idx 0
task1/train/loss 0.8245623757441839 224
task1/test/loss 0.7621074741358286 224
task1/test/acc 0.71175 224
task1/train/lr 0.00774910716563295 224
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 225/256 | train_loss 0.8246 | train_acc 0.6740 | test_loss 0.7621 | test_acc 0.7117 | lr 0.0077
last_idx 11
final_idx 0
task1/train/loss 0.720782627662023 225
task1/test/loss 0.7689272771169852 225
task1/test/acc 0.7085 225
task1/train/lr 0.007324884696951197 225
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 226/256 | train_loss 0.7208 | train_acc 0.7080 | test_loss 0.7689 | test_acc 0.7085 | lr 0.0073
last_idx 11
final_idx 0
task1/train/loss 0.6350285708904266 226
task1/test/loss 0.7602278089130318 226
task1/test/acc 0.7155 226
task1/train/lr 0.006911459647464768 226
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 227/256 | train_loss 0.6350 | train_acc 0.7780 | test_loss 0.7602 | test_acc 0.7155 | lr 0.0069
last_idx 11
final_idx 0
task1/train/loss 0.6536727547645569 227
task1/test/loss 0.8044558237869661 227
task1/test/acc 0.70625 227
task1/train/lr 0.006509081048964508 227
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 228/256 | train_loss 0.6537 | train_acc 0.7300 | test_loss 0.8045 | test_acc 0.7063 | lr 0.0065
last_idx 11
final_idx 0
task1/train/loss 0.6331089772284031 228
task1/test/loss 0.7370303042314865 228
task1/test/acc 0.72675 228
task1/train/lr 0.0061179912792722595 228
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 229/256 | train_loss 0.6331 | train_acc 0.8020 | test_loss 0.7370 | test_acc 0.7268 | lr 0.0061
last_idx 11
final_idx 0
task1/train/loss 0.7403801381587982 229
task1/test/loss 0.7458737251850275 229
task1/test/acc 0.7215 229
task1/train/lr 0.005738425916241496 229
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 230/256 | train_loss 0.7404 | train_acc 0.7380 | test_loss 0.7459 | test_acc 0.7215 | lr 0.0057
last_idx 11
final_idx 0
task1/train/loss 0.578339196741581 230
task1/test/loss 0.7810716170531052 230
task1/test/acc 0.714 230
task1/train/lr 0.005370613595854041 230
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 231/256 | train_loss 0.5783 | train_acc 0.7800 | test_loss 0.7811 | test_acc 0.7140 | lr 0.0054
last_idx 11
final_idx 0
task1/train/loss 0.6490789080659548 231
task1/test/loss 0.7921645936075148 231
task1/test/acc 0.70875 231
task1/train/lr 0.005014775874498306 231
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 232/256 | train_loss 0.6491 | train_acc 0.8160 | test_loss 0.7922 | test_acc 0.7087 | lr 0.0050
last_idx 11
final_idx 0
task1/train/loss 0.9087527592976888 232
task1/test/loss 0.7608467034258686 232
task1/test/acc 0.71125 232
task1/train/lr 0.004671127095512003 232
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 233/256 | train_loss 0.9088 | train_acc 0.6740 | test_loss 0.7608 | test_acc 0.7113 | lr 0.0047
last_idx 11
final_idx 0
task1/train/loss 0.4808204621076584 233
task1/test/loss 0.7394059108836311 233
task1/test/acc 0.72 233
task1/train/lr 0.004339874260069749 233
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 234/256 | train_loss 0.4808 | train_acc 0.8320 | test_loss 0.7394 | test_acc 0.7200 | lr 0.0043
last_idx 11
final_idx 0
task1/train/loss 0.5818077735602856 234
task1/test/loss 0.7610324287152552 234
task1/test/acc 0.7105 234
task1/train/lr 0.004021216902493268 234
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 235/256 | train_loss 0.5818 | train_acc 0.7640 | test_loss 0.7610 | test_acc 0.7105 | lr 0.0040
last_idx 11
final_idx 0
task1/train/loss 0.6285772398114204 235
task1/test/loss 0.7974240229679987 235
task1/test/acc 0.7095 235
task1/train/lr 0.0037153469700593944 235
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 236/256 | train_loss 0.6286 | train_acc 0.7860 | test_loss 0.7974 | test_acc 0.7095 | lr 0.0037
last_idx 11
final_idx 0
task1/train/loss 0.5926806045075258 236
task1/test/loss 0.7428311556577682 236
task1/test/acc 0.727 236
task1/train/lr 0.0034224487073782153 236
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 237/256 | train_loss 0.5927 | train_acc 0.7880 | test_loss 0.7428 | test_acc 0.7270 | lr 0.0034
last_idx 11
final_idx 0
task1/train/loss 0.8005566075444221 237
task1/test/loss 0.7900605211546133 237
task1/test/acc 0.71475 237
task1/train/lr 0.0031426985454109987 237
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 238/256 | train_loss 0.8006 | train_acc 0.6920 | test_loss 0.7901 | test_acc 0.7147 | lr 0.0031
last_idx 11
final_idx 0
task1/train/loss 0.6099751430253187 238
task1/test/loss 0.7914126040516319 238
task1/test/acc 0.708 238
task1/train/lr 0.0028762649951947776 238
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 239/256 | train_loss 0.6100 | train_acc 0.7880 | test_loss 0.7914 | test_acc 0.7080 | lr 0.0029
last_idx 11
final_idx 0
task1/train/loss 0.45890946313738823 239
task1/test/loss 0.7688603037661248 239
task1/test/acc 0.728 239
task1/train/lr 0.0026233085463376153 239
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 240/256 | train_loss 0.4589 | train_acc 0.8580 | test_loss 0.7689 | test_acc 0.7280 | lr 0.0026
last_idx 11
final_idx 0
task1/train/loss 0.7128467621902624 240
task1/test/loss 0.7748889408923767 240
task1/test/acc 0.72425 240
task1/train/lr 0.0023839815703456534 240
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 241/256 | train_loss 0.7128 | train_acc 0.7980 | test_loss 0.7749 | test_acc 0.7242 | lr 0.0024
last_idx 11
final_idx 0
task1/train/loss 0.5656176693737507 241
task1/test/loss 0.762350815993089 241
task1/test/acc 0.727 241
task1/train/lr 0.0021584282288402137 241
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 242/256 | train_loss 0.5656 | train_acc 0.8080 | test_loss 0.7624 | test_acc 0.7270 | lr 0.0022
last_idx 11
final_idx 0
task1/train/loss 0.44060827791690826 242
task1/test/loss 0.7504889586797128 242
task1/test/acc 0.73275 242
task1/train/lr 0.0019467843867202379 242
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 243/256 | train_loss 0.4406 | train_acc 0.8340 | test_loss 0.7505 | test_acc 0.7328 | lr 0.0019
last_idx 11
final_idx 0
task1/train/loss 0.5770765530566374 243
task1/test/loss 0.7514134208252142 243
task1/test/acc 0.7295 243
task1/train/lr 0.0017491775303223424 243
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 244/256 | train_loss 0.5771 | train_acc 0.8400 | test_loss 0.7514 | test_acc 0.7295 | lr 0.0017
last_idx 11
final_idx 0
task1/train/loss 0.6023066701988379 244
task1/test/loss 0.7722347850327963 244
task1/test/acc 0.72 244
task1/train/lr 0.0015657266906278318 244
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 245/256 | train_loss 0.6023 | train_acc 0.7680 | test_loss 0.7722 | test_acc 0.7200 | lr 0.0016
last_idx 11
final_idx 0
task1/train/loss 0.7493923790752888 245
task1/test/loss 0.7605328373201601 245
task1/test/acc 0.72575 245
task1/train/lr 0.001396542371562864 245
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 246/256 | train_loss 0.7494 | train_acc 0.7020 | test_loss 0.7605 | test_acc 0.7258 | lr 0.0014
last_idx 11
final_idx 0
task1/train/loss 0.5650620212157568 246
task1/test/loss 0.7745657610696751 246
task1/test/acc 0.7175 246
task1/train/lr 0.0012417264834350366 246
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 247/256 | train_loss 0.5651 | train_acc 0.8160 | test_loss 0.7746 | test_acc 0.7175 | lr 0.0012
last_idx 11
final_idx 0
task1/train/loss 0.7173499974111716 247
task1/test/loss 0.7660873884355629 247
task1/test/acc 0.71775 247
task1/train/lr 0.0011013722815464207 247
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 248/256 | train_loss 0.7173 | train_acc 0.7520 | test_loss 0.7661 | test_acc 0.7177 | lr 0.0011
last_idx 11
final_idx 0
task1/train/loss 0.6153416832288107 248
task1/test/loss 0.7538271189390958 248
task1/test/acc 0.7225 248
task1/train/lr 0.0009755643100200469 248
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 249/256 | train_loss 0.6153 | train_acc 0.7580 | test_loss 0.7538 | test_acc 0.7225 | lr 0.0010
last_idx 11
final_idx 0
task1/train/loss 0.6039670171837012 249
task1/test/loss 0.7468772373356662 249
task1/test/acc 0.72225 249
task1/train/lr 0.0008643783508737047 249
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 250/256 | train_loss 0.6040 | train_acc 0.7820 | test_loss 0.7469 | test_acc 0.7222 | lr 0.0009
last_idx 11
final_idx 0
task1/train/loss 0.5957315266132355 250
task1/test/loss 0.7505768560446225 250
task1/test/acc 0.728 250
task1/train/lr 0.0007678813783716699 250
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 251/256 | train_loss 0.5957 | train_acc 0.7820 | test_loss 0.7506 | test_acc 0.7280 | lr 0.0008
last_idx 11
final_idx 0
task1/train/loss 0.7798759614427885 251
task1/test/loss 0.7541857089970138 251
task1/test/acc 0.72325 251
task1/train/lr 0.0006861315186819283 251
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 252/256 | train_loss 0.7799 | train_acc 0.7480 | test_loss 0.7542 | test_acc 0.7232 | lr 0.0007
last_idx 11
final_idx 0
task1/train/loss 0.5271645387013754 252
task1/test/loss 0.7561559318513661 252
task1/test/acc 0.7215 252
task1/train/lr 0.0006191780148631288 252
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 253/256 | train_loss 0.5272 | train_acc 0.8200 | test_loss 0.7562 | test_acc 0.7215 | lr 0.0006
last_idx 11
final_idx 0
task1/train/loss 0.5829333638151487 253
task1/test/loss 0.7528928018860764 253
task1/test/acc 0.7255 253
task1/train/lr 0.0005670611972024174 253
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 254/256 | train_loss 0.5829 | train_acc 0.8120 | test_loss 0.7529 | test_acc 0.7255 | lr 0.0006
last_idx 11
final_idx 0
task1/train/loss 0.600976916650931 254
task1/test/loss 0.7505100020668009 254
task1/test/acc 0.72725 254
task1/train/lr 0.0005298124589219829 254
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 255/256 | train_loss 0.6010 | train_acc 0.7880 | test_loss 0.7505 | test_acc 0.7272 | lr 0.0005
last_idx 11
final_idx 0
task1/train/loss 0.4563663949569066 255
task1/test/loss 0.7544943667375125 255
task1/test/acc 0.72725 255
task1/train/lr 0.0005074542372689448 255
[INFO] rainbow_memory.py:184 > Task 1 | Epoch 256/256 | train_loss 0.4564 | train_acc 0.8400 | test_loss 0.7545 | test_acc 0.7272 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (1)
[INFO] main.py:398 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (1)
[INFO] main.py:405 > [2-5] Report task result
Metrics/TaskAcc 0.73275 1

##################################################
# Task 2 iteration
##################################################

[INFO] main.py:316 > [2-1] Prepare a datalist for the current task
total : 1000  current step :  0
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.59s. Batch: 0.75s. S_Loss: 2.3735. T_Loss: 2.4140. Mask: 0.0039. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.59s. Batch: 0.75s. S_Loss: 2.3735. T_Loss: 2.4140. Mask: 0.0039. :   2%|▏         | 1/50 [00:00<00:36,  1.33it/s]Train Iter:   2/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4187. T_Loss: 2.4626. Mask: 0.0020. :   2%|▏         | 1/50 [00:01<00:36,  1.33it/s]Train Iter:   2/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4187. T_Loss: 2.4626. Mask: 0.0020. :   4%|▍         | 2/50 [00:01<00:30,  1.59it/s]Train Iter:   3/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4282. T_Loss: 2.4543. Mask: 0.0013. :   4%|▍         | 2/50 [00:01<00:30,  1.59it/s]Train Iter:   3/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4282. T_Loss: 2.4543. Mask: 0.0013. :   6%|▌         | 3/50 [00:01<00:27,  1.70it/s]total : 1000  current step :  1
total : 1000  current step :  2
total : 1000  current step :  3
Train Iter:   4/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.4104. T_Loss: 2.4638. Mask: 0.0020. :   6%|▌         | 3/50 [00:02<00:27,  1.70it/s]Train Iter:   4/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.4104. T_Loss: 2.4638. Mask: 0.0020. :   8%|▊         | 4/50 [00:02<00:34,  1.32it/s]Train Iter:   5/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.4058. T_Loss: 2.4396. Mask: 0.0031. :   8%|▊         | 4/50 [00:03<00:34,  1.32it/s]Train Iter:   5/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.4058. T_Loss: 2.4396. Mask: 0.0031. :  10%|█         | 5/50 [00:03<00:28,  1.59it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.3970. T_Loss: 2.4342. Mask: 0.0046. :  10%|█         | 5/50 [00:03<00:28,  1.59it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.3970. T_Loss: 2.4342. Mask: 0.0046. :  12%|█▏        | 6/50 [00:03<00:22,  1.95it/s]total : 1000  current step :  4
total : 1000  current step :  5
total : 1000  current step :  6
Train Iter:   7/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.3908. T_Loss: 2.4223. Mask: 0.0050. :  12%|█▏        | 6/50 [00:04<00:22,  1.95it/s]Train Iter:   7/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.3908. T_Loss: 2.4223. Mask: 0.0050. :  14%|█▍        | 7/50 [00:04<00:29,  1.43it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.3877. T_Loss: 2.4175. Mask: 0.0054. :  14%|█▍        | 7/50 [00:04<00:29,  1.43it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.3877. T_Loss: 2.4175. Mask: 0.0054. :  16%|█▌        | 8/50 [00:04<00:24,  1.70it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.3850. T_Loss: 2.4183. Mask: 0.0052. :  16%|█▌        | 8/50 [00:05<00:24,  1.70it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.3850. T_Loss: 2.4183. Mask: 0.0052. :  18%|█▊        | 9/50 [00:05<00:20,  1.96it/s]total : 1000  current step :  7
total : 1000  current step :  8
total : 1000  current step :  9
Train Iter:  10/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3857. T_Loss: 2.4081. Mask: 0.0047. :  18%|█▊        | 9/50 [00:06<00:20,  1.96it/s]Train Iter:  10/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3857. T_Loss: 2.4081. Mask: 0.0047. :  20%|██        | 10/50 [00:06<00:26,  1.52it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.3769. T_Loss: 2.3956. Mask: 0.0046. :  20%|██        | 10/50 [00:06<00:26,  1.52it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.3769. T_Loss: 2.3956. Mask: 0.0046. :  22%|██▏       | 11/50 [00:06<00:21,  1.81it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.3758. T_Loss: 2.3762. Mask: 0.0046. :  22%|██▏       | 11/50 [00:06<00:21,  1.81it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.3758. T_Loss: 2.3762. Mask: 0.0046. :  24%|██▍       | 12/50 [00:06<00:19,  1.99it/s]total : 1000  current step :  10
total : 1000  current step :  11
total : 1000  current step :  12
Train Iter:  13/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.3692. T_Loss: 2.3658. Mask: 0.0042. :  24%|██▍       | 12/50 [00:08<00:19,  1.99it/s]Train Iter:  13/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.3692. T_Loss: 2.3658. Mask: 0.0042. :  26%|██▌       | 13/50 [00:08<00:25,  1.47it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.3680. T_Loss: 2.3476. Mask: 0.0039. :  26%|██▌       | 13/50 [00:08<00:25,  1.47it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.3680. T_Loss: 2.3476. Mask: 0.0039. :  28%|██▊       | 14/50 [00:08<00:21,  1.67it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.3620. T_Loss: 2.3285. Mask: 0.0036. :  28%|██▊       | 14/50 [00:08<00:21,  1.67it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.3620. T_Loss: 2.3285. Mask: 0.0036. :  30%|███       | 15/50 [00:08<00:17,  2.02it/s]total : 1000  current step :  13
total : 1000  current step :  14
total : 1000  current step :  15
Train Iter:  16/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3545. T_Loss: 2.3053. Mask: 0.0037. :  30%|███       | 15/50 [00:09<00:17,  2.02it/s]Train Iter:  16/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3545. T_Loss: 2.3053. Mask: 0.0037. :  32%|███▏      | 16/50 [00:09<00:24,  1.41it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3458. T_Loss: 2.2814. Mask: 0.0041. :  32%|███▏      | 16/50 [00:10<00:24,  1.41it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3458. T_Loss: 2.2814. Mask: 0.0041. :  34%|███▍      | 17/50 [00:10<00:19,  1.71it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.3422. T_Loss: 2.2545. Mask: 0.0043. :  34%|███▍      | 17/50 [00:10<00:19,  1.71it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.3422. T_Loss: 2.2545. Mask: 0.0043. :  36%|███▌      | 18/50 [00:10<00:15,  2.03it/s]total : 1000  current step :  16
total : 1000  current step :  17
total : 1000  current step :  18
Train Iter:  19/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3379. T_Loss: 2.2304. Mask: 0.0045. :  36%|███▌      | 18/50 [00:11<00:15,  2.03it/s]Train Iter:  19/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3379. T_Loss: 2.2304. Mask: 0.0045. :  38%|███▊      | 19/50 [00:11<00:20,  1.52it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.3359. T_Loss: 2.2022. Mask: 0.0064. :  38%|███▊      | 19/50 [00:12<00:20,  1.52it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.3359. T_Loss: 2.2022. Mask: 0.0064. :  40%|████      | 20/50 [00:12<00:18,  1.66it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.3357. T_Loss: 2.1775. Mask: 0.0099. :  40%|████      | 20/50 [00:12<00:18,  1.66it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.3357. T_Loss: 2.1775. Mask: 0.0099. :  42%|████▏     | 21/50 [00:12<00:15,  1.93it/s]total : 1000  current step :  19
total : 1000  current step :  20
total : 1000  current step :  21
Train Iter:  22/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3342. T_Loss: 2.1560. Mask: 0.0172. :  42%|████▏     | 21/50 [00:13<00:15,  1.93it/s]Train Iter:  22/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3342. T_Loss: 2.1560. Mask: 0.0172. :  44%|████▍     | 22/50 [00:13<00:20,  1.36it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.3337. T_Loss: 2.1345. Mask: 0.0275. :  44%|████▍     | 22/50 [00:13<00:20,  1.36it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.3337. T_Loss: 2.1345. Mask: 0.0275. :  46%|████▌     | 23/50 [00:13<00:17,  1.58it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.3325. T_Loss: 2.1125. Mask: 0.0407. :  46%|████▌     | 23/50 [00:14<00:17,  1.58it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.3325. T_Loss: 2.1125. Mask: 0.0407. :  48%|████▊     | 24/50 [00:14<00:13,  1.90it/s]total : 1000  current step :  22
total : 1000  current step :  23
total : 1000  current step :  24
Train Iter:  25/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3305. T_Loss: 2.0940. Mask: 0.0581. :  48%|████▊     | 24/50 [00:15<00:13,  1.90it/s]Train Iter:  25/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3305. T_Loss: 2.0940. Mask: 0.0581. :  50%|█████     | 25/50 [00:15<00:18,  1.38it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3290. T_Loss: 2.0803. Mask: 0.0757. :  50%|█████     | 25/50 [00:15<00:18,  1.38it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3290. T_Loss: 2.0803. Mask: 0.0757. :  52%|█████▏    | 26/50 [00:15<00:15,  1.51it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3305. T_Loss: 2.0620. Mask: 0.0958. :  52%|█████▏    | 26/50 [00:16<00:15,  1.51it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3305. T_Loss: 2.0620. Mask: 0.0958. :  54%|█████▍    | 27/50 [00:16<00:12,  1.80it/s]total : 1000  current step :  25
total : 1000  current step :  26
total : 1000  current step :  27
Train Iter:  28/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3312. T_Loss: 2.0490. Mask: 0.1155. :  54%|█████▍    | 27/50 [00:17<00:12,  1.80it/s]Train Iter:  28/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3312. T_Loss: 2.0490. Mask: 0.1155. :  56%|█████▌    | 28/50 [00:17<00:15,  1.43it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3330. T_Loss: 2.0357. Mask: 0.1354. :  56%|█████▌    | 28/50 [00:17<00:15,  1.43it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3330. T_Loss: 2.0357. Mask: 0.1354. :  58%|█████▊    | 29/50 [00:17<00:12,  1.66it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3328. T_Loss: 2.0215. Mask: 0.1561. :  58%|█████▊    | 29/50 [00:18<00:12,  1.66it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3328. T_Loss: 2.0215. Mask: 0.1561. :  60%|██████    | 30/50 [00:18<00:10,  1.93it/s]total : 1000  current step :  28
total : 1000  current step :  29
total : 1000  current step :  30
Train Iter:  31/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3337. T_Loss: 2.0112. Mask: 0.1753. :  60%|██████    | 30/50 [00:19<00:10,  1.93it/s]Train Iter:  31/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.3337. T_Loss: 2.0112. Mask: 0.1753. :  62%|██████▏   | 31/50 [00:19<00:13,  1.41it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3350. T_Loss: 1.9985. Mask: 0.1949. :  62%|██████▏   | 31/50 [00:19<00:13,  1.41it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3350. T_Loss: 1.9985. Mask: 0.1949. :  64%|██████▍   | 32/50 [00:19<00:10,  1.72it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3365. T_Loss: 1.9869. Mask: 0.2134. :  64%|██████▍   | 32/50 [00:19<00:10,  1.72it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3365. T_Loss: 1.9869. Mask: 0.2134. :  66%|██████▌   | 33/50 [00:19<00:08,  1.96it/s]total : 1000  current step :  31
total : 1000  current step :  32
total : 1000  current step :  33
Train Iter:  34/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3374. T_Loss: 1.9770. Mask: 0.2321. :  66%|██████▌   | 33/50 [00:20<00:08,  1.96it/s]Train Iter:  34/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3374. T_Loss: 1.9770. Mask: 0.2321. :  68%|██████▊   | 34/50 [00:20<00:11,  1.45it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3399. T_Loss: 1.9667. Mask: 0.2507. :  68%|██████▊   | 34/50 [00:21<00:11,  1.45it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3399. T_Loss: 1.9667. Mask: 0.2507. :  70%|███████   | 35/50 [00:21<00:08,  1.76it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3396. T_Loss: 1.9557. Mask: 0.2671. :  70%|███████   | 35/50 [00:21<00:08,  1.76it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3396. T_Loss: 1.9557. Mask: 0.2671. :  72%|███████▏  | 36/50 [00:21<00:07,  1.96it/s]total : 1000  current step :  34
total : 1000  current step :  35
total : 1000  current step :  36
Train Iter:  37/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3407. T_Loss: 1.9447. Mask: 0.2826. :  72%|███████▏  | 36/50 [00:22<00:07,  1.96it/s]Train Iter:  37/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.3407. T_Loss: 1.9447. Mask: 0.2826. :  74%|███████▍  | 37/50 [00:22<00:08,  1.47it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3424. T_Loss: 1.9339. Mask: 0.2982. :  74%|███████▍  | 37/50 [00:23<00:08,  1.47it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3424. T_Loss: 1.9339. Mask: 0.2982. :  76%|███████▌  | 38/50 [00:23<00:06,  1.73it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3440. T_Loss: 1.9238. Mask: 0.3126. :  76%|███████▌  | 38/50 [00:23<00:06,  1.73it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.3440. T_Loss: 1.9238. Mask: 0.3126. :  78%|███████▊  | 39/50 [00:23<00:05,  1.98it/s]total : 1000  current step :  37
total : 1000  current step :  38
total : 1000  current step :  39
Train Iter:  40/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3464. T_Loss: 1.9120. Mask: 0.3260. :  78%|███████▊  | 39/50 [00:25<00:05,  1.98it/s]Train Iter:  40/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3464. T_Loss: 1.9120. Mask: 0.3260. :  80%|████████  | 40/50 [00:25<00:08,  1.11it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3468. T_Loss: 1.9029. Mask: 0.3396. :  80%|████████  | 40/50 [00:25<00:08,  1.11it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3468. T_Loss: 1.9029. Mask: 0.3396. :  82%|████████▏ | 41/50 [00:25<00:07,  1.16it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3475. T_Loss: 1.8931. Mask: 0.3516. :  82%|████████▏ | 41/50 [00:26<00:07,  1.16it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3475. T_Loss: 1.8931. Mask: 0.3516. :  84%|████████▍ | 42/50 [00:26<00:06,  1.25it/s]total : 1000  current step :  40
total : 1000  current step :  41
total : 1000  current step :  42
Train Iter:  43/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.3497. T_Loss: 1.8837. Mask: 0.3630. :  84%|████████▍ | 42/50 [00:27<00:06,  1.25it/s]Train Iter:  43/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.3497. T_Loss: 1.8837. Mask: 0.3630. :  86%|████████▌ | 43/50 [00:27<00:06,  1.04it/s]Train Iter:  44/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.3504. T_Loss: 1.8742. Mask: 0.3731. :  86%|████████▌ | 43/50 [00:28<00:06,  1.04it/s]Train Iter:  44/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.3504. T_Loss: 1.8742. Mask: 0.3731. :  88%|████████▊ | 44/50 [00:28<00:04,  1.29it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3493. T_Loss: 1.8651. Mask: 0.3832. :  88%|████████▊ | 44/50 [00:28<00:04,  1.29it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.3493. T_Loss: 1.8651. Mask: 0.3832. :  90%|█████████ | 45/50 [00:28<00:03,  1.65it/s]total : 1000  current step :  43
total : 1000  current step :  44
total : 1000  current step :  45
Train Iter:  46/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.3502. T_Loss: 1.8579. Mask: 0.3933. :  90%|█████████ | 45/50 [00:29<00:03,  1.65it/s]Train Iter:  46/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.3502. T_Loss: 1.8579. Mask: 0.3933. :  92%|█████████▏| 46/50 [00:29<00:03,  1.20it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.3509. T_Loss: 1.8508. Mask: 0.4028. :  92%|█████████▏| 46/50 [00:30<00:03,  1.20it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.3509. T_Loss: 1.8508. Mask: 0.4028. :  94%|█████████▍| 47/50 [00:30<00:02,  1.42it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.3508. T_Loss: 1.8428. Mask: 0.4123. :  94%|█████████▍| 47/50 [00:30<00:02,  1.42it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.3508. T_Loss: 1.8428. Mask: 0.4123. :  96%|█████████▌| 48/50 [00:30<00:01,  1.62it/s]total : 1000  current step :  46
total : 1000  current step :  47
total : 1000  current step :  48
Train Iter:  49/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.3524. T_Loss: 1.8348. Mask: 0.4208. :  96%|█████████▌| 48/50 [00:31<00:01,  1.62it/s]Train Iter:  49/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.3524. T_Loss: 1.8348. Mask: 0.4208. :  98%|█████████▊| 49/50 [00:31<00:00,  1.27it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.3533. T_Loss: 1.8284. Mask: 0.4305. :  98%|█████████▊| 49/50 [00:32<00:00,  1.27it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.3533. T_Loss: 1.8284. Mask: 0.4305. : 100%|██████████| 50/50 [00:32<00:00,  1.49it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.3533. T_Loss: 1.8284. Mask: 0.4305. : 100%|██████████| 50/50 [00:32<00:00,  1.55it/s]
total : 1000  current step :  49
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 1.2015. top1: 98.44. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 1.2015. top1: 98.44. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.22it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.1939. top1: 98.63. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.22it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.1939. top1: 98.63. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.93it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.2013. top1: 98.83. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.93it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.2013. top1: 98.83. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.21it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.3552. top1: 96.39. top5: 98.63. :  38%|███▊      | 3/8 [00:01<00:02,  2.21it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.3552. top1: 96.39. top5: 98.63. :  50%|█████     | 4/8 [00:01<00:01,  2.43it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.46s. Loss: 2.8173. top1: 77.11. top5: 88.36. :  50%|█████     | 4/8 [00:02<00:01,  2.43it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.46s. Loss: 2.8173. top1: 77.11. top5: 88.36. :  62%|██████▎   | 5/8 [00:02<00:01,  2.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.45s. Loss: 3.7727. top1: 64.26. top5: 81.38. :  62%|██████▎   | 5/8 [00:02<00:01,  2.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.45s. Loss: 3.7727. top1: 64.26. top5: 81.38. :  75%|███████▌  | 6/8 [00:02<00:00,  2.39it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.43s. Loss: 4.4657. top1: 55.08. top5: 76.17. :  75%|███████▌  | 6/8 [00:02<00:00,  2.39it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.43s. Loss: 4.4657. top1: 55.08. top5: 76.17. :  88%|████████▊ | 7/8 [00:02<00:00,  2.67it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.41s. Loss: 4.8703. top1: 49.35. top5: 73.70. :  88%|████████▊ | 7/8 [00:03<00:00,  2.67it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.41s. Loss: 4.8703. top1: 49.35. top5: 73.70. : 100%|██████████| 8/8 [00:03<00:00,  2.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.41s. Loss: 4.8703. top1: 49.35. top5: 73.70. : 100%|██████████| 8/8 [00:03<00:00,  2.29it/s]
total : 1000  current step :  50
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.01s. Batch: 0.23s. S_Loss: 2.3974. T_Loss: 1.5631. Mask: 0.8906. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.01s. Batch: 0.23s. S_Loss: 2.3974. T_Loss: 1.5631. Mask: 0.8906. :   2%|▏         | 1/50 [00:00<00:11,  4.25it/s]total : 1000  current step :  51
Train Iter:  52/1000. LR: 0.0000. Data: 0.64s. Batch: 0.88s. S_Loss: 2.3948. T_Loss: 1.5191. Mask: 0.8750. :   2%|▏         | 1/50 [00:01<00:11,  4.25it/s]Train Iter:  52/1000. LR: 0.0000. Data: 0.64s. Batch: 0.88s. S_Loss: 2.3948. T_Loss: 1.5191. Mask: 0.8750. :   4%|▍         | 2/50 [00:01<00:47,  1.01it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.52s. Batch: 0.73s. S_Loss: 2.3978. T_Loss: 1.5262. Mask: 0.8763. :   4%|▍         | 2/50 [00:02<00:47,  1.01it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.52s. Batch: 0.73s. S_Loss: 2.3978. T_Loss: 1.5262. Mask: 0.8763. :   6%|▌         | 3/50 [00:02<00:34,  1.35it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.3991. T_Loss: 1.5286. Mask: 0.8799. :   6%|▌         | 3/50 [00:02<00:34,  1.35it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.3991. T_Loss: 1.5286. Mask: 0.8799. :   8%|▊         | 4/50 [00:02<00:29,  1.54it/s]total : 1000  current step :  52
total : 1000  current step :  53
total : 1000  current step :  54
Train Iter:  55/1000. LR: 0.0000. Data: 0.56s. Batch: 0.78s. S_Loss: 2.4066. T_Loss: 1.5238. Mask: 0.8820. :   8%|▊         | 4/50 [00:03<00:29,  1.54it/s]Train Iter:  55/1000. LR: 0.0000. Data: 0.56s. Batch: 0.78s. S_Loss: 2.4066. T_Loss: 1.5238. Mask: 0.8820. :  10%|█         | 5/50 [00:03<00:38,  1.18it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.4112. T_Loss: 1.5275. Mask: 0.8880. :  10%|█         | 5/50 [00:04<00:38,  1.18it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.4112. T_Loss: 1.5275. Mask: 0.8880. :  12%|█▏        | 6/50 [00:04<00:29,  1.48it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.4162. T_Loss: 1.5358. Mask: 0.8878. :  12%|█▏        | 6/50 [00:04<00:29,  1.48it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.4162. T_Loss: 1.5358. Mask: 0.8878. :  14%|█▍        | 7/50 [00:04<00:24,  1.75it/s]total : 1000  current step :  55
total : 1000  current step :  56
total : 1000  current step :  57
Train Iter:  58/1000. LR: 0.0000. Data: 0.52s. Batch: 0.73s. S_Loss: 2.4058. T_Loss: 1.5391. Mask: 0.8901. :  14%|█▍        | 7/50 [00:05<00:24,  1.75it/s]Train Iter:  58/1000. LR: 0.0000. Data: 0.52s. Batch: 0.73s. S_Loss: 2.4058. T_Loss: 1.5391. Mask: 0.8901. :  16%|█▌        | 8/50 [00:05<00:33,  1.25it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4054. T_Loss: 1.5356. Mask: 0.8919. :  16%|█▌        | 8/50 [00:06<00:33,  1.25it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4054. T_Loss: 1.5356. Mask: 0.8919. :  18%|█▊        | 9/50 [00:06<00:27,  1.50it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.3986. T_Loss: 1.5354. Mask: 0.8938. :  18%|█▊        | 9/50 [00:06<00:27,  1.50it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.3986. T_Loss: 1.5354. Mask: 0.8938. :  20%|██        | 10/50 [00:06<00:22,  1.79it/s]total : 1000  current step :  58
total : 1000  current step :  59
total : 1000  current step :  60
Train Iter:  61/1000. LR: 0.0000. Data: 0.51s. Batch: 0.72s. S_Loss: 2.4032. T_Loss: 1.5322. Mask: 0.8935. :  20%|██        | 10/50 [00:07<00:22,  1.79it/s]Train Iter:  61/1000. LR: 0.0000. Data: 0.51s. Batch: 0.72s. S_Loss: 2.4032. T_Loss: 1.5322. Mask: 0.8935. :  22%|██▏       | 11/50 [00:07<00:31,  1.24it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4090. T_Loss: 1.5367. Mask: 0.8975. :  22%|██▏       | 11/50 [00:08<00:31,  1.24it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4090. T_Loss: 1.5367. Mask: 0.8975. :  24%|██▍       | 12/50 [00:08<00:25,  1.47it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4073. T_Loss: 1.5335. Mask: 0.8972. :  24%|██▍       | 12/50 [00:08<00:25,  1.47it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4073. T_Loss: 1.5335. Mask: 0.8972. :  26%|██▌       | 13/50 [00:08<00:21,  1.75it/s]total : 1000  current step :  61
total : 1000  current step :  62
total : 1000  current step :  63
Train Iter:  64/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.4059. T_Loss: 1.5363. Mask: 0.9004. :  26%|██▌       | 13/50 [00:10<00:21,  1.75it/s]Train Iter:  64/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.4059. T_Loss: 1.5363. Mask: 0.9004. :  28%|██▊       | 14/50 [00:10<00:29,  1.22it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.4109. T_Loss: 1.5334. Mask: 0.8992. :  28%|██▊       | 14/50 [00:10<00:29,  1.22it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.4109. T_Loss: 1.5334. Mask: 0.8992. :  30%|███       | 15/50 [00:10<00:22,  1.52it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4147. T_Loss: 1.5339. Mask: 0.9011. :  30%|███       | 15/50 [00:10<00:22,  1.52it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4147. T_Loss: 1.5339. Mask: 0.9011. :  32%|███▏      | 16/50 [00:10<00:20,  1.64it/s]total : 1000  current step :  64
total : 1000  current step :  65
total : 1000  current step :  66
Train Iter:  67/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.4158. T_Loss: 1.5359. Mask: 0.9028. :  32%|███▏      | 16/50 [00:11<00:20,  1.64it/s]Train Iter:  67/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.4158. T_Loss: 1.5359. Mask: 0.9028. :  34%|███▍      | 17/50 [00:11<00:25,  1.31it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4157. T_Loss: 1.5409. Mask: 0.9047. :  34%|███▍      | 17/50 [00:12<00:25,  1.31it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4157. T_Loss: 1.5409. Mask: 0.9047. :  36%|███▌      | 18/50 [00:12<00:20,  1.53it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4155. T_Loss: 1.5434. Mask: 0.9054. :  36%|███▌      | 18/50 [00:12<00:20,  1.53it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4155. T_Loss: 1.5434. Mask: 0.9054. :  38%|███▊      | 19/50 [00:12<00:16,  1.89it/s]total : 1000  current step :  67
total : 1000  current step :  68
total : 1000  current step :  69
Train Iter:  70/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4170. T_Loss: 1.5439. Mask: 0.9057. :  38%|███▊      | 19/50 [00:13<00:16,  1.89it/s]Train Iter:  70/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4170. T_Loss: 1.5439. Mask: 0.9057. :  40%|████      | 20/50 [00:13<00:22,  1.36it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4154. T_Loss: 1.5463. Mask: 0.9064. :  40%|████      | 20/50 [00:14<00:22,  1.36it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4154. T_Loss: 1.5463. Mask: 0.9064. :  42%|████▏     | 21/50 [00:14<00:18,  1.57it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4171. T_Loss: 1.5449. Mask: 0.9071. :  42%|████▏     | 21/50 [00:14<00:18,  1.57it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4171. T_Loss: 1.5449. Mask: 0.9071. :  44%|████▍     | 22/50 [00:14<00:15,  1.85it/s]total : 1000  current step :  70
total : 1000  current step :  71
total : 1000  current step :  72
Train Iter:  73/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4185. T_Loss: 1.5456. Mask: 0.9076. :  44%|████▍     | 22/50 [00:15<00:15,  1.85it/s]Train Iter:  73/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4185. T_Loss: 1.5456. Mask: 0.9076. :  46%|████▌     | 23/50 [00:15<00:20,  1.35it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4204. T_Loss: 1.5453. Mask: 0.9077. :  46%|████▌     | 23/50 [00:16<00:20,  1.35it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4204. T_Loss: 1.5453. Mask: 0.9077. :  48%|████▊     | 24/50 [00:16<00:16,  1.61it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4214. T_Loss: 1.5450. Mask: 0.9080. :  48%|████▊     | 24/50 [00:16<00:16,  1.61it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4214. T_Loss: 1.5450. Mask: 0.9080. :  50%|█████     | 25/50 [00:16<00:13,  1.86it/s]total : 1000  current step :  73
total : 1000  current step :  74
total : 1000  current step :  75
Train Iter:  76/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4233. T_Loss: 1.5476. Mask: 0.9090. :  50%|█████     | 25/50 [00:17<00:13,  1.86it/s]Train Iter:  76/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4233. T_Loss: 1.5476. Mask: 0.9090. :  52%|█████▏    | 26/50 [00:17<00:17,  1.40it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4236. T_Loss: 1.5485. Mask: 0.9102. :  52%|█████▏    | 26/50 [00:17<00:17,  1.40it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4236. T_Loss: 1.5485. Mask: 0.9102. :  54%|█████▍    | 27/50 [00:17<00:13,  1.68it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.4236. T_Loss: 1.5490. Mask: 0.9104. :  54%|█████▍    | 27/50 [00:18<00:13,  1.68it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.4236. T_Loss: 1.5490. Mask: 0.9104. :  56%|█████▌    | 28/50 [00:18<00:11,  1.87it/s]total : 1000  current step :  76
total : 1000  current step :  77
total : 1000  current step :  78
Train Iter:  79/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4213. T_Loss: 1.5515. Mask: 0.9118. :  56%|█████▌    | 28/50 [00:19<00:11,  1.87it/s]Train Iter:  79/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4213. T_Loss: 1.5515. Mask: 0.9118. :  58%|█████▊    | 29/50 [00:19<00:15,  1.34it/s]total : 1000  current step :  79
Train Iter:  80/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4204. T_Loss: 1.5527. Mask: 0.9116. :  58%|█████▊    | 29/50 [00:20<00:15,  1.34it/s]Train Iter:  80/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4204. T_Loss: 1.5527. Mask: 0.9116. :  60%|██████    | 30/50 [00:20<00:14,  1.34it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4178. T_Loss: 1.5562. Mask: 0.9133. :  60%|██████    | 30/50 [00:21<00:14,  1.34it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4178. T_Loss: 1.5562. Mask: 0.9133. :  62%|██████▏   | 31/50 [00:21<00:14,  1.28it/s]total : 1000  current step :  80
total : 1000  current step :  81
Train Iter:  82/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.4160. T_Loss: 1.5598. Mask: 0.9138. :  62%|██████▏   | 31/50 [00:22<00:14,  1.28it/s]Train Iter:  82/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.4160. T_Loss: 1.5598. Mask: 0.9138. :  64%|██████▍   | 32/50 [00:22<00:17,  1.05it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.4148. T_Loss: 1.5613. Mask: 0.9141. :  64%|██████▍   | 32/50 [00:22<00:17,  1.05it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.4148. T_Loss: 1.5613. Mask: 0.9141. :  66%|██████▌   | 33/50 [00:22<00:12,  1.34it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.4137. T_Loss: 1.5607. Mask: 0.9135. :  66%|██████▌   | 33/50 [00:23<00:12,  1.34it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.4137. T_Loss: 1.5607. Mask: 0.9135. :  68%|██████▊   | 34/50 [00:23<00:10,  1.52it/s]total : 1000  current step :  82
total : 1000  current step :  83
total : 1000  current step :  84
Train Iter:  85/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4132. T_Loss: 1.5639. Mask: 0.9137. :  68%|██████▊   | 34/50 [00:24<00:10,  1.52it/s]Train Iter:  85/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4132. T_Loss: 1.5639. Mask: 0.9137. :  70%|███████   | 35/50 [00:24<00:12,  1.25it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4120. T_Loss: 1.5645. Mask: 0.9138. :  70%|███████   | 35/50 [00:24<00:12,  1.25it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4120. T_Loss: 1.5645. Mask: 0.9138. :  72%|███████▏  | 36/50 [00:24<00:09,  1.47it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.4105. T_Loss: 1.5656. Mask: 0.9147. :  72%|███████▏  | 36/50 [00:25<00:09,  1.47it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.4105. T_Loss: 1.5656. Mask: 0.9147. :  74%|███████▍  | 37/50 [00:25<00:07,  1.67it/s]total : 1000  current step :  85
total : 1000  current step :  86
total : 1000  current step :  87
Train Iter:  88/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4094. T_Loss: 1.5678. Mask: 0.9147. :  74%|███████▍  | 37/50 [00:26<00:07,  1.67it/s]Train Iter:  88/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4094. T_Loss: 1.5678. Mask: 0.9147. :  76%|███████▌  | 38/50 [00:26<00:09,  1.22it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4092. T_Loss: 1.5676. Mask: 0.9148. :  76%|███████▌  | 38/50 [00:26<00:09,  1.22it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4092. T_Loss: 1.5676. Mask: 0.9148. :  78%|███████▊  | 39/50 [00:26<00:07,  1.48it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4099. T_Loss: 1.5693. Mask: 0.9153. :  78%|███████▊  | 39/50 [00:27<00:07,  1.48it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4099. T_Loss: 1.5693. Mask: 0.9153. :  80%|████████  | 40/50 [00:27<00:05,  1.78it/s]total : 1000  current step :  88
total : 1000  current step :  89
total : 1000  current step :  90
Train Iter:  91/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4091. T_Loss: 1.5723. Mask: 0.9157. :  80%|████████  | 40/50 [00:28<00:05,  1.78it/s]Train Iter:  91/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4091. T_Loss: 1.5723. Mask: 0.9157. :  82%|████████▏ | 41/50 [00:28<00:06,  1.39it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4082. T_Loss: 1.5742. Mask: 0.9163. :  82%|████████▏ | 41/50 [00:28<00:06,  1.39it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4082. T_Loss: 1.5742. Mask: 0.9163. :  84%|████████▍ | 42/50 [00:28<00:04,  1.74it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4092. T_Loss: 1.5748. Mask: 0.9170. :  84%|████████▍ | 42/50 [00:28<00:04,  1.74it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4092. T_Loss: 1.5748. Mask: 0.9170. :  86%|████████▌ | 43/50 [00:28<00:03,  1.87it/s]total : 1000  current step :  91
total : 1000  current step :  92
total : 1000  current step :  93
Train Iter:  94/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4099. T_Loss: 1.5755. Mask: 0.9178. :  86%|████████▌ | 43/50 [00:30<00:03,  1.87it/s]Train Iter:  94/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4099. T_Loss: 1.5755. Mask: 0.9178. :  88%|████████▊ | 44/50 [00:30<00:04,  1.28it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.4109. T_Loss: 1.5774. Mask: 0.9187. :  88%|████████▊ | 44/50 [00:30<00:04,  1.28it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.4109. T_Loss: 1.5774. Mask: 0.9187. :  90%|█████████ | 45/50 [00:30<00:03,  1.50it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4120. T_Loss: 1.5794. Mask: 0.9191. :  90%|█████████ | 45/50 [00:30<00:03,  1.50it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4120. T_Loss: 1.5794. Mask: 0.9191. :  92%|█████████▏| 46/50 [00:30<00:02,  1.82it/s]total : 1000  current step :  94
total : 1000  current step :  95
total : 1000  current step :  96
Train Iter:  97/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4117. T_Loss: 1.5817. Mask: 0.9190. :  92%|█████████▏| 46/50 [00:32<00:02,  1.82it/s]Train Iter:  97/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4117. T_Loss: 1.5817. Mask: 0.9190. :  94%|█████████▍| 47/50 [00:32<00:02,  1.36it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4117. T_Loss: 1.5842. Mask: 0.9196. :  94%|█████████▍| 47/50 [00:32<00:02,  1.36it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4117. T_Loss: 1.5842. Mask: 0.9196. :  96%|█████████▌| 48/50 [00:32<00:01,  1.60it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4113. T_Loss: 1.5857. Mask: 0.9201. :  96%|█████████▌| 48/50 [00:32<00:01,  1.60it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4113. T_Loss: 1.5857. Mask: 0.9201. :  98%|█████████▊| 49/50 [00:32<00:00,  1.96it/s]total : 1000  current step :  97
total : 1000  current step :  98
total : 1000  current step :  99
Train Iter: 100/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4106. T_Loss: 1.5870. Mask: 0.9205. :  98%|█████████▊| 49/50 [00:33<00:00,  1.96it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4106. T_Loss: 1.5870. Mask: 0.9205. : 100%|██████████| 50/50 [00:33<00:00,  1.43it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4106. T_Loss: 1.5870. Mask: 0.9205. : 100%|██████████| 50/50 [00:33<00:00,  1.48it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.1909. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.1909. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.1952. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.1952. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.1947. top1: 96.88. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.1947. top1: 96.88. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.2644. top1: 94.63. top5: 98.14. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.2644. top1: 94.63. top5: 98.14. :  50%|█████     | 4/8 [00:01<00:01,  3.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.9392. top1: 75.70. top5: 84.61. :  50%|█████     | 4/8 [00:01<00:01,  3.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.9392. top1: 75.70. top5: 84.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.3772. top1: 63.09. top5: 76.24. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.3772. top1: 63.09. top5: 76.24. :  75%|███████▌  | 6/8 [00:01<00:00,  3.44it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.6928. top1: 54.07. top5: 70.26. :  75%|███████▌  | 6/8 [00:02<00:00,  3.44it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.6928. top1: 54.07. top5: 70.26. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.8790. top1: 48.45. top5: 66.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.8790. top1: 48.45. top5: 66.85. : 100%|██████████| 8/8 [00:02<00:00,  3.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.8790. top1: 48.45. top5: 66.85. : 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]
total : 1000  current step :  100
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.17s. S_Loss: 2.4609. T_Loss: 1.6623. Mask: 0.9531. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.17s. S_Loss: 2.4609. T_Loss: 1.6623. Mask: 0.9531. :   2%|▏         | 1/50 [00:00<00:08,  5.63it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.23s. S_Loss: 2.4179. T_Loss: 1.6625. Mask: 0.9590. :   2%|▏         | 1/50 [00:00<00:08,  5.63it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.23s. S_Loss: 2.4179. T_Loss: 1.6625. Mask: 0.9590. :   4%|▍         | 2/50 [00:00<00:11,  4.18it/s]total : 1000  current step :  101
total : 1000  current step :  102
Train Iter: 103/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.4452. T_Loss: 1.6676. Mask: 0.9505. :   4%|▍         | 2/50 [00:01<00:11,  4.18it/s]Train Iter: 103/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.4452. T_Loss: 1.6676. Mask: 0.9505. :   6%|▌         | 3/50 [00:01<00:34,  1.36it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.4404. T_Loss: 1.6618. Mask: 0.9492. :   6%|▌         | 3/50 [00:02<00:34,  1.36it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.4404. T_Loss: 1.6618. Mask: 0.9492. :   8%|▊         | 4/50 [00:02<00:27,  1.69it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.4294. T_Loss: 1.6833. Mask: 0.9461. :   8%|▊         | 4/50 [00:02<00:27,  1.69it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.4294. T_Loss: 1.6833. Mask: 0.9461. :  10%|█         | 5/50 [00:02<00:23,  1.89it/s]total : 1000  current step :  103
total : 1000  current step :  104
total : 1000  current step :  105
Train Iter: 106/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4278. T_Loss: 1.6986. Mask: 0.9434. :  10%|█         | 5/50 [00:03<00:23,  1.89it/s]Train Iter: 106/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4278. T_Loss: 1.6986. Mask: 0.9434. :  12%|█▏        | 6/50 [00:03<00:33,  1.32it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 2.4300. T_Loss: 1.7131. Mask: 0.9408. :  12%|█▏        | 6/50 [00:04<00:33,  1.32it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 2.4300. T_Loss: 1.7131. Mask: 0.9408. :  14%|█▍        | 7/50 [00:04<00:27,  1.54it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.4232. T_Loss: 1.7017. Mask: 0.9429. :  14%|█▍        | 7/50 [00:04<00:27,  1.54it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.4232. T_Loss: 1.7017. Mask: 0.9429. :  16%|█▌        | 8/50 [00:04<00:24,  1.74it/s]total : 1000  current step :  106
total : 1000  current step :  107
total : 1000  current step :  108
Train Iter: 109/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4255. T_Loss: 1.7015. Mask: 0.9427. :  16%|█▌        | 8/50 [00:05<00:24,  1.74it/s]Train Iter: 109/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4255. T_Loss: 1.7015. Mask: 0.9427. :  18%|█▊        | 9/50 [00:05<00:30,  1.35it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.4188. T_Loss: 1.6884. Mask: 0.9422. :  18%|█▊        | 9/50 [00:06<00:30,  1.35it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.4188. T_Loss: 1.6884. Mask: 0.9422. :  20%|██        | 10/50 [00:06<00:25,  1.60it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.4168. T_Loss: 1.6913. Mask: 0.9453. :  20%|██        | 10/50 [00:06<00:25,  1.60it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.4168. T_Loss: 1.6913. Mask: 0.9453. :  22%|██▏       | 11/50 [00:06<00:21,  1.85it/s]total : 1000  current step :  109
total : 1000  current step :  110
total : 1000  current step :  111
Train Iter: 112/1000. LR: 0.0000. Data: 0.45s. Batch: 0.64s. S_Loss: 2.4169. T_Loss: 1.6963. Mask: 0.9447. :  22%|██▏       | 11/50 [00:07<00:21,  1.85it/s]Train Iter: 112/1000. LR: 0.0000. Data: 0.45s. Batch: 0.64s. S_Loss: 2.4169. T_Loss: 1.6963. Mask: 0.9447. :  24%|██▍       | 12/50 [00:07<00:28,  1.32it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4144. T_Loss: 1.7019. Mask: 0.9450. :  24%|██▍       | 12/50 [00:08<00:28,  1.32it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4144. T_Loss: 1.7019. Mask: 0.9450. :  26%|██▌       | 13/50 [00:08<00:23,  1.58it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4163. T_Loss: 1.7054. Mask: 0.9448. :  26%|██▌       | 13/50 [00:08<00:23,  1.58it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4163. T_Loss: 1.7054. Mask: 0.9448. :  28%|██▊       | 14/50 [00:08<00:20,  1.79it/s]total : 1000  current step :  112
total : 1000  current step :  113
total : 1000  current step :  114
Train Iter: 115/1000. LR: 0.0000. Data: 0.45s. Batch: 0.64s. S_Loss: 2.4144. T_Loss: 1.7124. Mask: 0.9453. :  28%|██▊       | 14/50 [00:09<00:20,  1.79it/s]Train Iter: 115/1000. LR: 0.0000. Data: 0.45s. Batch: 0.64s. S_Loss: 2.4144. T_Loss: 1.7124. Mask: 0.9453. :  30%|███       | 15/50 [00:09<00:26,  1.30it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4137. T_Loss: 1.7179. Mask: 0.9443. :  30%|███       | 15/50 [00:10<00:26,  1.30it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4137. T_Loss: 1.7179. Mask: 0.9443. :  32%|███▏      | 16/50 [00:10<00:21,  1.55it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4141. T_Loss: 1.7231. Mask: 0.9444. :  32%|███▏      | 16/50 [00:10<00:21,  1.55it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4141. T_Loss: 1.7231. Mask: 0.9444. :  34%|███▍      | 17/50 [00:10<00:19,  1.72it/s]total : 1000  current step :  115
total : 1000  current step :  116
total : 1000  current step :  117
Train Iter: 118/1000. LR: 0.0000. Data: 0.46s. Batch: 0.65s. S_Loss: 2.4147. T_Loss: 1.7292. Mask: 0.9429. :  34%|███▍      | 17/50 [00:11<00:19,  1.72it/s]Train Iter: 118/1000. LR: 0.0000. Data: 0.46s. Batch: 0.65s. S_Loss: 2.4147. T_Loss: 1.7292. Mask: 0.9429. :  36%|███▌      | 18/50 [00:11<00:24,  1.29it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4143. T_Loss: 1.7284. Mask: 0.9428. :  36%|███▌      | 18/50 [00:12<00:24,  1.29it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4143. T_Loss: 1.7284. Mask: 0.9428. :  38%|███▊      | 19/50 [00:12<00:20,  1.50it/s]total : 1000  current step :  118
total : 1000  current step :  119
Train Iter: 120/1000. LR: 0.0000. Data: 0.45s. Batch: 0.64s. S_Loss: 2.4149. T_Loss: 1.7290. Mask: 0.9432. :  38%|███▊      | 19/50 [00:12<00:20,  1.50it/s]Train Iter: 120/1000. LR: 0.0000. Data: 0.45s. Batch: 0.64s. S_Loss: 2.4149. T_Loss: 1.7290. Mask: 0.9432. :  40%|████      | 20/50 [00:12<00:20,  1.43it/s]total : 1000  current step :  120
Train Iter: 121/1000. LR: 0.0000. Data: 0.48s. Batch: 0.67s. S_Loss: 2.4132. T_Loss: 1.7316. Mask: 0.9442. :  40%|████      | 20/50 [00:14<00:20,  1.43it/s]Train Iter: 121/1000. LR: 0.0000. Data: 0.48s. Batch: 0.67s. S_Loss: 2.4132. T_Loss: 1.7316. Mask: 0.9442. :  42%|████▏     | 21/50 [00:14<00:25,  1.16it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4127. T_Loss: 1.7343. Mask: 0.9446. :  42%|████▏     | 21/50 [00:14<00:25,  1.16it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4127. T_Loss: 1.7343. Mask: 0.9446. :  44%|████▍     | 22/50 [00:14<00:19,  1.43it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.46s. Batch: 0.65s. S_Loss: 2.4099. T_Loss: 1.7335. Mask: 0.9445. :  44%|████▍     | 22/50 [00:15<00:19,  1.43it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.46s. Batch: 0.65s. S_Loss: 2.4099. T_Loss: 1.7335. Mask: 0.9445. :  46%|████▌     | 23/50 [00:15<00:18,  1.47it/s]total : 1000  current step :  121
total : 1000  current step :  122
total : 1000  current step :  123
Train Iter: 124/1000. LR: 0.0000. Data: 0.48s. Batch: 0.68s. S_Loss: 2.4091. T_Loss: 1.7358. Mask: 0.9453. :  46%|████▌     | 23/50 [00:16<00:18,  1.47it/s]Train Iter: 124/1000. LR: 0.0000. Data: 0.48s. Batch: 0.68s. S_Loss: 2.4091. T_Loss: 1.7358. Mask: 0.9453. :  48%|████▊     | 24/50 [00:16<00:22,  1.18it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4086. T_Loss: 1.7394. Mask: 0.9456. :  48%|████▊     | 24/50 [00:16<00:22,  1.18it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4086. T_Loss: 1.7394. Mask: 0.9456. :  50%|█████     | 25/50 [00:16<00:17,  1.40it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.46s. Batch: 0.65s. S_Loss: 2.4104. T_Loss: 1.7396. Mask: 0.9449. :  50%|█████     | 25/50 [00:17<00:17,  1.40it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.46s. Batch: 0.65s. S_Loss: 2.4104. T_Loss: 1.7396. Mask: 0.9449. :  52%|█████▏    | 26/50 [00:17<00:14,  1.66it/s]total : 1000  current step :  124
total : 1000  current step :  125
total : 1000  current step :  126
Train Iter: 127/1000. LR: 0.0000. Data: 0.48s. Batch: 0.67s. S_Loss: 2.4104. T_Loss: 1.7457. Mask: 0.9449. :  52%|█████▏    | 26/50 [00:18<00:14,  1.66it/s]Train Iter: 127/1000. LR: 0.0000. Data: 0.48s. Batch: 0.67s. S_Loss: 2.4104. T_Loss: 1.7457. Mask: 0.9449. :  54%|█████▍    | 27/50 [00:18<00:17,  1.29it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4106. T_Loss: 1.7475. Mask: 0.9455. :  54%|█████▍    | 27/50 [00:18<00:17,  1.29it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4106. T_Loss: 1.7475. Mask: 0.9455. :  56%|█████▌    | 28/50 [00:18<00:14,  1.55it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4138. T_Loss: 1.7496. Mask: 0.9450. :  56%|█████▌    | 28/50 [00:18<00:14,  1.55it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4138. T_Loss: 1.7496. Mask: 0.9450. :  58%|█████▊    | 29/50 [00:18<00:11,  1.80it/s]total : 1000  current step :  127
total : 1000  current step :  128
total : 1000  current step :  129
Train Iter: 130/1000. LR: 0.0000. Data: 0.48s. Batch: 0.67s. S_Loss: 2.4164. T_Loss: 1.7488. Mask: 0.9452. :  58%|█████▊    | 29/50 [00:20<00:11,  1.80it/s]Train Iter: 130/1000. LR: 0.0000. Data: 0.48s. Batch: 0.67s. S_Loss: 2.4164. T_Loss: 1.7488. Mask: 0.9452. :  60%|██████    | 30/50 [00:20<00:15,  1.26it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.47s. Batch: 0.66s. S_Loss: 2.4174. T_Loss: 1.7521. Mask: 0.9459. :  60%|██████    | 30/50 [00:20<00:15,  1.26it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.47s. Batch: 0.66s. S_Loss: 2.4174. T_Loss: 1.7521. Mask: 0.9459. :  62%|██████▏   | 31/50 [00:20<00:12,  1.50it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4199. T_Loss: 1.7583. Mask: 0.9458. :  62%|██████▏   | 31/50 [00:21<00:12,  1.50it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4199. T_Loss: 1.7583. Mask: 0.9458. :  64%|██████▍   | 32/50 [00:21<00:10,  1.72it/s]total : 1000  current step :  130
total : 1000  current step :  131
total : 1000  current step :  132
Train Iter: 133/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4195. T_Loss: 1.7684. Mask: 0.9458. :  64%|██████▍   | 32/50 [00:22<00:10,  1.72it/s]Train Iter: 133/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4195. T_Loss: 1.7684. Mask: 0.9458. :  66%|██████▌   | 33/50 [00:22<00:12,  1.31it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4223. T_Loss: 1.7696. Mask: 0.9455. :  66%|██████▌   | 33/50 [00:22<00:12,  1.31it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4223. T_Loss: 1.7696. Mask: 0.9455. :  68%|██████▊   | 34/50 [00:22<00:10,  1.51it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4225. T_Loss: 1.7671. Mask: 0.9455. :  68%|██████▊   | 34/50 [00:22<00:10,  1.51it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4225. T_Loss: 1.7671. Mask: 0.9455. :  70%|███████   | 35/50 [00:22<00:08,  1.78it/s]total : 1000  current step :  133
total : 1000  current step :  134
total : 1000  current step :  135
Train Iter: 136/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4224. T_Loss: 1.7693. Mask: 0.9460. :  70%|███████   | 35/50 [00:24<00:08,  1.78it/s]Train Iter: 136/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4224. T_Loss: 1.7693. Mask: 0.9460. :  72%|███████▏  | 36/50 [00:24<00:10,  1.33it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4229. T_Loss: 1.7746. Mask: 0.9462. :  72%|███████▏  | 36/50 [00:24<00:10,  1.33it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4229. T_Loss: 1.7746. Mask: 0.9462. :  74%|███████▍  | 37/50 [00:24<00:08,  1.53it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4230. T_Loss: 1.7760. Mask: 0.9466. :  74%|███████▍  | 37/50 [00:24<00:08,  1.53it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4230. T_Loss: 1.7760. Mask: 0.9466. :  76%|███████▌  | 38/50 [00:24<00:06,  1.77it/s]total : 1000  current step :  136
total : 1000  current step :  137
total : 1000  current step :  138
Train Iter: 139/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4220. T_Loss: 1.7812. Mask: 0.9467. :  76%|███████▌  | 38/50 [00:26<00:06,  1.77it/s]Train Iter: 139/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4220. T_Loss: 1.7812. Mask: 0.9467. :  78%|███████▊  | 39/50 [00:26<00:08,  1.33it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4220. T_Loss: 1.7835. Mask: 0.9468. :  78%|███████▊  | 39/50 [00:26<00:08,  1.33it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4220. T_Loss: 1.7835. Mask: 0.9468. :  80%|████████  | 40/50 [00:26<00:06,  1.46it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4219. T_Loss: 1.7862. Mask: 0.9467. :  80%|████████  | 40/50 [00:27<00:06,  1.46it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4219. T_Loss: 1.7862. Mask: 0.9467. :  82%|████████▏ | 41/50 [00:27<00:05,  1.66it/s]total : 1000  current step :  139
total : 1000  current step :  140
total : 1000  current step :  141
Train Iter: 142/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4211. T_Loss: 1.7885. Mask: 0.9469. :  82%|████████▏ | 41/50 [00:28<00:05,  1.66it/s]Train Iter: 142/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.4211. T_Loss: 1.7885. Mask: 0.9469. :  84%|████████▍ | 42/50 [00:28<00:06,  1.27it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4212. T_Loss: 1.7925. Mask: 0.9464. :  84%|████████▍ | 42/50 [00:28<00:06,  1.27it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4212. T_Loss: 1.7925. Mask: 0.9464. :  86%|████████▌ | 43/50 [00:28<00:04,  1.46it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4199. T_Loss: 1.7942. Mask: 0.9463. :  86%|████████▌ | 43/50 [00:29<00:04,  1.46it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4199. T_Loss: 1.7942. Mask: 0.9463. :  88%|████████▊ | 44/50 [00:29<00:03,  1.75it/s]total : 1000  current step :  142
total : 1000  current step :  143
total : 1000  current step :  144
Train Iter: 145/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4198. T_Loss: 1.7968. Mask: 0.9457. :  88%|████████▊ | 44/50 [00:30<00:03,  1.75it/s]Train Iter: 145/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4198. T_Loss: 1.7968. Mask: 0.9457. :  90%|█████████ | 45/50 [00:30<00:03,  1.34it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4195. T_Loss: 1.7981. Mask: 0.9451. :  90%|█████████ | 45/50 [00:30<00:03,  1.34it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4195. T_Loss: 1.7981. Mask: 0.9451. :  92%|█████████▏| 46/50 [00:30<00:02,  1.60it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4184. T_Loss: 1.8014. Mask: 0.9453. :  92%|█████████▏| 46/50 [00:30<00:02,  1.60it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4184. T_Loss: 1.8014. Mask: 0.9453. :  94%|█████████▍| 47/50 [00:30<00:01,  1.89it/s]total : 1000  current step :  145
total : 1000  current step :  146
total : 1000  current step :  147
Train Iter: 148/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4190. T_Loss: 1.8028. Mask: 0.9456. :  94%|█████████▍| 47/50 [00:32<00:01,  1.89it/s]Train Iter: 148/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4190. T_Loss: 1.8028. Mask: 0.9456. :  96%|█████████▌| 48/50 [00:32<00:01,  1.40it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4177. T_Loss: 1.8048. Mask: 0.9453. :  96%|█████████▌| 48/50 [00:32<00:01,  1.40it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4177. T_Loss: 1.8048. Mask: 0.9453. :  98%|█████████▊| 49/50 [00:32<00:00,  1.57it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.4163. T_Loss: 1.8075. Mask: 0.9452. :  98%|█████████▊| 49/50 [00:32<00:00,  1.57it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.4163. T_Loss: 1.8075. Mask: 0.9452. : 100%|██████████| 50/50 [00:32<00:00,  1.91it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.4163. T_Loss: 1.8075. Mask: 0.9452. : 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]
total : 1000  current step :  148
total : 1000  current step :  149
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.4951. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.4951. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.4985. top1: 91.80. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.4985. top1: 91.80. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.4951. top1: 92.19. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.4951. top1: 92.19. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.89it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.5339. top1: 90.33. top5: 98.14. :  38%|███▊      | 3/8 [00:01<00:01,  2.89it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.5339. top1: 90.33. top5: 98.14. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.9165. top1: 72.27. top5: 82.34. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.9165. top1: 72.27. top5: 82.34. :  62%|██████▎   | 5/8 [00:01<00:00,  3.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.1634. top1: 60.22. top5: 71.68. :  62%|██████▎   | 5/8 [00:02<00:00,  3.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.1634. top1: 60.22. top5: 71.68. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.3403. top1: 51.62. top5: 64.06. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.3403. top1: 51.62. top5: 64.06. :  88%|████████▊ | 7/8 [00:02<00:00,  3.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.4466. top1: 46.25. top5: 59.40. :  88%|████████▊ | 7/8 [00:02<00:00,  3.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.4466. top1: 46.25. top5: 59.40. : 100%|██████████| 8/8 [00:02<00:00,  3.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 2.4466. top1: 46.25. top5: 59.40. : 100%|██████████| 8/8 [00:02<00:00,  2.86it/s]
total : 1000  current step :  150
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 1.01s. Batch: 1.19s. S_Loss: 2.4520. T_Loss: 2.3683. Mask: 0.9727. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 1.01s. Batch: 1.19s. S_Loss: 2.4520. T_Loss: 2.3683. Mask: 0.9727. :   2%|▏         | 1/50 [00:01<00:58,  1.19s/it]Train Iter: 152/1000. LR: 0.0000. Data: 0.61s. Batch: 0.78s. S_Loss: 2.4487. T_Loss: 2.3712. Mask: 0.9512. :   2%|▏         | 1/50 [00:01<00:58,  1.19s/it]Train Iter: 152/1000. LR: 0.0000. Data: 0.61s. Batch: 0.78s. S_Loss: 2.4487. T_Loss: 2.3712. Mask: 0.9512. :   4%|▍         | 2/50 [00:01<00:34,  1.40it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.47s. Batch: 0.65s. S_Loss: 2.4411. T_Loss: 2.3024. Mask: 0.9544. :   4%|▍         | 2/50 [00:01<00:34,  1.40it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.47s. Batch: 0.65s. S_Loss: 2.4411. T_Loss: 2.3024. Mask: 0.9544. :   6%|▌         | 3/50 [00:01<00:26,  1.75it/s]total : 1000  current step :  151
total : 1000  current step :  152
total : 1000  current step :  153
Train Iter: 154/1000. LR: 0.0000. Data: 0.58s. Batch: 0.76s. S_Loss: 2.4422. T_Loss: 2.2273. Mask: 0.9541. :   6%|▌         | 3/50 [00:03<00:26,  1.75it/s]Train Iter: 154/1000. LR: 0.0000. Data: 0.58s. Batch: 0.76s. S_Loss: 2.4422. T_Loss: 2.2273. Mask: 0.9541. :   8%|▊         | 4/50 [00:03<00:35,  1.31it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.49s. Batch: 0.67s. S_Loss: 2.4354. T_Loss: 2.1924. Mask: 0.9523. :   8%|▊         | 4/50 [00:03<00:35,  1.31it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.49s. Batch: 0.67s. S_Loss: 2.4354. T_Loss: 2.1924. Mask: 0.9523. :  10%|█         | 5/50 [00:03<00:27,  1.64it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.44s. Batch: 0.62s. S_Loss: 2.4356. T_Loss: 2.1725. Mask: 0.9525. :  10%|█         | 5/50 [00:03<00:27,  1.64it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.44s. Batch: 0.62s. S_Loss: 2.4356. T_Loss: 2.1725. Mask: 0.9525. :  12%|█▏        | 6/50 [00:03<00:22,  1.92it/s]total : 1000  current step :  154
total : 1000  current step :  155
total : 1000  current step :  156
Train Iter: 157/1000. LR: 0.0000. Data: 0.52s. Batch: 0.70s. S_Loss: 2.4291. T_Loss: 2.1529. Mask: 0.9520. :  12%|█▏        | 6/50 [00:04<00:22,  1.92it/s]Train Iter: 157/1000. LR: 0.0000. Data: 0.52s. Batch: 0.70s. S_Loss: 2.4291. T_Loss: 2.1529. Mask: 0.9520. :  14%|█▍        | 7/50 [00:04<00:32,  1.33it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.48s. Batch: 0.67s. S_Loss: 2.4297. T_Loss: 2.1437. Mask: 0.9531. :  14%|█▍        | 7/50 [00:05<00:32,  1.33it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.48s. Batch: 0.67s. S_Loss: 2.4297. T_Loss: 2.1437. Mask: 0.9531. :  16%|█▌        | 8/50 [00:05<00:27,  1.53it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4260. T_Loss: 2.1307. Mask: 0.9527. :  16%|█▌        | 8/50 [00:05<00:27,  1.53it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.4260. T_Loss: 2.1307. Mask: 0.9527. :  18%|█▊        | 9/50 [00:05<00:24,  1.67it/s]total : 1000  current step :  157
total : 1000  current step :  158
total : 1000  current step :  159
Train Iter: 160/1000. LR: 0.0000. Data: 0.54s. Batch: 0.74s. S_Loss: 2.4211. T_Loss: 2.1164. Mask: 0.9520. :  18%|█▊        | 9/50 [00:07<00:24,  1.67it/s]Train Iter: 160/1000. LR: 0.0000. Data: 0.54s. Batch: 0.74s. S_Loss: 2.4211. T_Loss: 2.1164. Mask: 0.9520. :  20%|██        | 10/50 [00:07<00:36,  1.11it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.56s. Batch: 0.76s. S_Loss: 2.4247. T_Loss: 2.1059. Mask: 0.9506. :  20%|██        | 10/50 [00:08<00:36,  1.11it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.56s. Batch: 0.76s. S_Loss: 2.4247. T_Loss: 2.1059. Mask: 0.9506. :  22%|██▏       | 11/50 [00:08<00:35,  1.11it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.55s. Batch: 0.74s. S_Loss: 2.4225. T_Loss: 2.1071. Mask: 0.9502. :  22%|██▏       | 11/50 [00:08<00:35,  1.11it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.55s. Batch: 0.74s. S_Loss: 2.4225. T_Loss: 2.1071. Mask: 0.9502. :  24%|██▍       | 12/50 [00:08<00:30,  1.24it/s]total : 1000  current step :  160
total : 1000  current step :  161
total : 1000  current step :  162
Train Iter: 163/1000. LR: 0.0000. Data: 0.58s. Batch: 0.78s. S_Loss: 2.4241. T_Loss: 2.1027. Mask: 0.9498. :  24%|██▍       | 12/50 [00:10<00:30,  1.24it/s]Train Iter: 163/1000. LR: 0.0000. Data: 0.58s. Batch: 0.78s. S_Loss: 2.4241. T_Loss: 2.1027. Mask: 0.9498. :  26%|██▌       | 13/50 [00:10<00:34,  1.07it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.55s. Batch: 0.75s. S_Loss: 2.4248. T_Loss: 2.0985. Mask: 0.9498. :  26%|██▌       | 13/50 [00:10<00:34,  1.07it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.55s. Batch: 0.75s. S_Loss: 2.4248. T_Loss: 2.0985. Mask: 0.9498. :  28%|██▊       | 14/50 [00:10<00:26,  1.34it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.52s. Batch: 0.72s. S_Loss: 2.4273. T_Loss: 2.0920. Mask: 0.9508. :  28%|██▊       | 14/50 [00:10<00:26,  1.34it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.52s. Batch: 0.72s. S_Loss: 2.4273. T_Loss: 2.0920. Mask: 0.9508. :  30%|███       | 15/50 [00:10<00:21,  1.60it/s]total : 1000  current step :  163
total : 1000  current step :  164
total : 1000  current step :  165
Train Iter: 166/1000. LR: 0.0000. Data: 0.54s. Batch: 0.74s. S_Loss: 2.4267. T_Loss: 2.0832. Mask: 0.9519. :  30%|███       | 15/50 [00:11<00:21,  1.60it/s]Train Iter: 166/1000. LR: 0.0000. Data: 0.54s. Batch: 0.74s. S_Loss: 2.4267. T_Loss: 2.0832. Mask: 0.9519. :  32%|███▏      | 16/50 [00:11<00:25,  1.32it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.52s. Batch: 0.72s. S_Loss: 2.4280. T_Loss: 2.0757. Mask: 0.9531. :  32%|███▏      | 16/50 [00:12<00:25,  1.32it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.52s. Batch: 0.72s. S_Loss: 2.4280. T_Loss: 2.0757. Mask: 0.9531. :  34%|███▍      | 17/50 [00:12<00:21,  1.52it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.50s. Batch: 0.70s. S_Loss: 2.4291. T_Loss: 2.0644. Mask: 0.9533. :  34%|███▍      | 17/50 [00:12<00:21,  1.52it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.50s. Batch: 0.70s. S_Loss: 2.4291. T_Loss: 2.0644. Mask: 0.9533. :  36%|███▌      | 18/50 [00:12<00:17,  1.79it/s]total : 1000  current step :  166
total : 1000  current step :  167
total : 1000  current step :  168
Train Iter: 169/1000. LR: 0.0000. Data: 0.52s. Batch: 0.72s. S_Loss: 2.4291. T_Loss: 2.0624. Mask: 0.9529. :  36%|███▌      | 18/50 [00:13<00:17,  1.79it/s]Train Iter: 169/1000. LR: 0.0000. Data: 0.52s. Batch: 0.72s. S_Loss: 2.4291. T_Loss: 2.0624. Mask: 0.9529. :  38%|███▊      | 19/50 [00:13<00:21,  1.44it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.50s. Batch: 0.70s. S_Loss: 2.4273. T_Loss: 2.0635. Mask: 0.9527. :  38%|███▊      | 19/50 [00:14<00:21,  1.44it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.50s. Batch: 0.70s. S_Loss: 2.4273. T_Loss: 2.0635. Mask: 0.9527. :  40%|████      | 20/50 [00:14<00:18,  1.64it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4279. T_Loss: 2.0688. Mask: 0.9533. :  40%|████      | 20/50 [00:14<00:18,  1.64it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.48s. Batch: 0.69s. S_Loss: 2.4279. T_Loss: 2.0688. Mask: 0.9533. :  42%|████▏     | 21/50 [00:14<00:15,  1.84it/s]total : 1000  current step :  169
total : 1000  current step :  170
total : 1000  current step :  171
Train Iter: 172/1000. LR: 0.0000. Data: 0.50s. Batch: 0.70s. S_Loss: 2.4266. T_Loss: 2.0677. Mask: 0.9529. :  42%|████▏     | 21/50 [00:15<00:15,  1.84it/s]Train Iter: 172/1000. LR: 0.0000. Data: 0.50s. Batch: 0.70s. S_Loss: 2.4266. T_Loss: 2.0677. Mask: 0.9529. :  44%|████▍     | 22/50 [00:15<00:19,  1.42it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.49s. Batch: 0.69s. S_Loss: 2.4258. T_Loss: 2.0655. Mask: 0.9535. :  44%|████▍     | 22/50 [00:15<00:19,  1.42it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.49s. Batch: 0.69s. S_Loss: 2.4258. T_Loss: 2.0655. Mask: 0.9535. :  46%|████▌     | 23/50 [00:15<00:16,  1.59it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4277. T_Loss: 2.0596. Mask: 0.9535. :  46%|████▌     | 23/50 [00:16<00:16,  1.59it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4277. T_Loss: 2.0596. Mask: 0.9535. :  48%|████▊     | 24/50 [00:16<00:14,  1.84it/s]total : 1000  current step :  172
total : 1000  current step :  173
total : 1000  current step :  174
Train Iter: 175/1000. LR: 0.0000. Data: 0.49s. Batch: 0.69s. S_Loss: 2.4270. T_Loss: 2.0577. Mask: 0.9539. :  48%|████▊     | 24/50 [00:17<00:14,  1.84it/s]Train Iter: 175/1000. LR: 0.0000. Data: 0.49s. Batch: 0.69s. S_Loss: 2.4270. T_Loss: 2.0577. Mask: 0.9539. :  50%|█████     | 25/50 [00:17<00:17,  1.46it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4257. T_Loss: 2.0648. Mask: 0.9524. :  50%|█████     | 25/50 [00:17<00:17,  1.46it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4257. T_Loss: 2.0648. Mask: 0.9524. :  52%|█████▏    | 26/50 [00:17<00:13,  1.73it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4245. T_Loss: 2.0687. Mask: 0.9530. :  52%|█████▏    | 26/50 [00:17<00:13,  1.73it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.4245. T_Loss: 2.0687. Mask: 0.9530. :  54%|█████▍    | 27/50 [00:17<00:11,  2.00it/s]total : 1000  current step :  175
total : 1000  current step :  176
total : 1000  current step :  177
Train Iter: 178/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4235. T_Loss: 2.0741. Mask: 0.9524. :  54%|█████▍    | 27/50 [00:19<00:11,  2.00it/s]Train Iter: 178/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.4235. T_Loss: 2.0741. Mask: 0.9524. :  56%|█████▌    | 28/50 [00:19<00:14,  1.52it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4223. T_Loss: 2.0753. Mask: 0.9520. :  56%|█████▌    | 28/50 [00:19<00:14,  1.52it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4223. T_Loss: 2.0753. Mask: 0.9520. :  58%|█████▊    | 29/50 [00:19<00:12,  1.68it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4202. T_Loss: 2.0820. Mask: 0.9516. :  58%|█████▊    | 29/50 [00:19<00:12,  1.68it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4202. T_Loss: 2.0820. Mask: 0.9516. :  60%|██████    | 30/50 [00:19<00:10,  1.93it/s]total : 1000  current step :  178
total : 1000  current step :  179
total : 1000  current step :  180
Train Iter: 181/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4201. T_Loss: 2.0860. Mask: 0.9519. :  60%|██████    | 30/50 [00:20<00:10,  1.93it/s]Train Iter: 181/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.4201. T_Loss: 2.0860. Mask: 0.9519. :  62%|██████▏   | 31/50 [00:20<00:13,  1.40it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4183. T_Loss: 2.0915. Mask: 0.9525. :  62%|██████▏   | 31/50 [00:21<00:13,  1.40it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4183. T_Loss: 2.0915. Mask: 0.9525. :  64%|██████▍   | 32/50 [00:21<00:10,  1.67it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.4195. T_Loss: 2.0964. Mask: 0.9528. :  64%|██████▍   | 32/50 [00:21<00:10,  1.67it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.4195. T_Loss: 2.0964. Mask: 0.9528. :  66%|██████▌   | 33/50 [00:21<00:08,  2.05it/s]total : 1000  current step :  181
total : 1000  current step :  182
total : 1000  current step :  183
Train Iter: 184/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.4193. T_Loss: 2.0984. Mask: 0.9529. :  66%|██████▌   | 33/50 [00:22<00:08,  2.05it/s]Train Iter: 184/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.4193. T_Loss: 2.0984. Mask: 0.9529. :  68%|██████▊   | 34/50 [00:22<00:11,  1.37it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.4187. T_Loss: 2.1007. Mask: 0.9529. :  68%|██████▊   | 34/50 [00:23<00:11,  1.37it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.4187. T_Loss: 2.1007. Mask: 0.9529. :  70%|███████   | 35/50 [00:23<00:09,  1.59it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.4179. T_Loss: 2.1014. Mask: 0.9535. :  70%|███████   | 35/50 [00:23<00:09,  1.59it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.4179. T_Loss: 2.1014. Mask: 0.9535. :  72%|███████▏  | 36/50 [00:23<00:08,  1.69it/s]total : 1000  current step :  184
total : 1000  current step :  185
total : 1000  current step :  186
Train Iter: 187/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.4185. T_Loss: 2.1043. Mask: 0.9538. :  72%|███████▏  | 36/50 [00:24<00:08,  1.69it/s]Train Iter: 187/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.4185. T_Loss: 2.1043. Mask: 0.9538. :  74%|███████▍  | 37/50 [00:24<00:10,  1.27it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.4184. T_Loss: 2.1082. Mask: 0.9536. :  74%|███████▍  | 37/50 [00:25<00:10,  1.27it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.4184. T_Loss: 2.1082. Mask: 0.9536. :  76%|███████▌  | 38/50 [00:25<00:08,  1.47it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.4180. T_Loss: 2.1129. Mask: 0.9528. :  76%|███████▌  | 38/50 [00:25<00:08,  1.47it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.4180. T_Loss: 2.1129. Mask: 0.9528. :  78%|███████▊  | 39/50 [00:25<00:06,  1.79it/s]total : 1000  current step :  187
total : 1000  current step :  188
total : 1000  current step :  189
Train Iter: 190/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.4179. T_Loss: 2.1169. Mask: 0.9525. :  78%|███████▊  | 39/50 [00:26<00:06,  1.79it/s]Train Iter: 190/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.4179. T_Loss: 2.1169. Mask: 0.9525. :  80%|████████  | 40/50 [00:26<00:07,  1.40it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.4176. T_Loss: 2.1175. Mask: 0.9527. :  80%|████████  | 40/50 [00:27<00:07,  1.40it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.4176. T_Loss: 2.1175. Mask: 0.9527. :  82%|████████▏ | 41/50 [00:27<00:05,  1.71it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.4171. T_Loss: 2.1166. Mask: 0.9528. :  82%|████████▏ | 41/50 [00:27<00:05,  1.71it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.4171. T_Loss: 2.1166. Mask: 0.9528. :  84%|████████▍ | 42/50 [00:27<00:04,  2.00it/s]total : 1000  current step :  190
total : 1000  current step :  191
total : 1000  current step :  192
Train Iter: 193/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.4177. T_Loss: 2.1140. Mask: 0.9526. :  84%|████████▍ | 42/50 [00:28<00:04,  2.00it/s]Train Iter: 193/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.4177. T_Loss: 2.1140. Mask: 0.9526. :  86%|████████▌ | 43/50 [00:28<00:04,  1.43it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.4193. T_Loss: 2.1120. Mask: 0.9525. :  86%|████████▌ | 43/50 [00:28<00:04,  1.43it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.4193. T_Loss: 2.1120. Mask: 0.9525. :  88%|████████▊ | 44/50 [00:28<00:03,  1.80it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.4205. T_Loss: 2.1162. Mask: 0.9530. :  88%|████████▊ | 44/50 [00:29<00:03,  1.80it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.4205. T_Loss: 2.1162. Mask: 0.9530. :  90%|█████████ | 45/50 [00:29<00:02,  2.10it/s]total : 1000  current step :  193
total : 1000  current step :  194
total : 1000  current step :  195
Train Iter: 196/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.4206. T_Loss: 2.1166. Mask: 0.9526. :  90%|█████████ | 45/50 [00:29<00:02,  2.10it/s]Train Iter: 196/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.4206. T_Loss: 2.1166. Mask: 0.9526. :  92%|█████████▏| 46/50 [00:29<00:02,  1.60it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.4204. T_Loss: 2.1201. Mask: 0.9530. :  92%|█████████▏| 46/50 [00:30<00:02,  1.60it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.4204. T_Loss: 2.1201. Mask: 0.9530. :  94%|█████████▍| 47/50 [00:30<00:01,  1.93it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.4207. T_Loss: 2.1254. Mask: 0.9533. :  94%|█████████▍| 47/50 [00:30<00:01,  1.93it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.4207. T_Loss: 2.1254. Mask: 0.9533. :  96%|█████████▌| 48/50 [00:30<00:00,  2.13it/s]total : 1000  current step :  196
total : 1000  current step :  197
total : 1000  current step :  198
Train Iter: 199/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.4203. T_Loss: 2.1215. Mask: 0.9535. :  96%|█████████▌| 48/50 [00:31<00:00,  2.13it/s]Train Iter: 199/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.4203. T_Loss: 2.1215. Mask: 0.9535. :  98%|█████████▊| 49/50 [00:31<00:00,  1.68it/s]total : 1000  current step :  199
Train Iter: 200/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.4197. T_Loss: 2.1181. Mask: 0.9537. :  98%|█████████▊| 49/50 [00:32<00:00,  1.68it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.4197. T_Loss: 2.1181. Mask: 0.9537. : 100%|██████████| 50/50 [00:32<00:00,  1.63it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.4197. T_Loss: 2.1181. Mask: 0.9537. : 100%|██████████| 50/50 [00:32<00:00,  1.55it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.7208. top1: 84.77. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.7208. top1: 84.77. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.7203. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.7203. top1: 83.59. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.74it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.7165. top1: 84.24. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.74it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.7165. top1: 84.24. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.20it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.7430. top1: 82.71. top5: 97.85. :  38%|███▊      | 3/8 [00:01<00:01,  3.20it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.7430. top1: 82.71. top5: 97.85. :  50%|█████     | 4/8 [00:01<00:01,  3.33it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.9999. top1: 66.17. top5: 80.94. :  50%|█████     | 4/8 [00:01<00:01,  3.33it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.9999. top1: 66.17. top5: 80.94. :  62%|██████▎   | 5/8 [00:01<00:00,  3.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.1649. top1: 55.14. top5: 69.92. :  62%|██████▎   | 5/8 [00:01<00:00,  3.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.1649. top1: 55.14. top5: 69.92. :  75%|███████▌  | 6/8 [00:01<00:00,  3.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2832. top1: 47.27. top5: 61.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.2832. top1: 47.27. top5: 61.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.3548. top1: 42.35. top5: 56.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.3548. top1: 42.35. top5: 56.60. : 100%|██████████| 8/8 [00:02<00:00,  4.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 2.3548. top1: 42.35. top5: 56.60. : 100%|██████████| 8/8 [00:02<00:00,  3.23it/s]
total : 1000  current step :  200
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.00s. Batch: 0.30s. S_Loss: 2.4440. T_Loss: 2.1621. Mask: 0.9727. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.00s. Batch: 0.30s. S_Loss: 2.4440. T_Loss: 2.1621. Mask: 0.9727. :   2%|▏         | 1/50 [00:00<00:14,  3.27it/s]total : 1000  current step :  201
Train Iter: 202/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.4294. T_Loss: 2.2108. Mask: 0.9707. :   2%|▏         | 1/50 [00:01<00:14,  3.27it/s]Train Iter: 202/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.4294. T_Loss: 2.2108. Mask: 0.9707. :   4%|▍         | 2/50 [00:01<00:30,  1.56it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.28s. Batch: 0.51s. S_Loss: 2.4410. T_Loss: 2.1712. Mask: 0.9701. :   4%|▍         | 2/50 [00:01<00:30,  1.56it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.28s. Batch: 0.51s. S_Loss: 2.4410. T_Loss: 2.1712. Mask: 0.9701. :   6%|▌         | 3/50 [00:01<00:23,  1.97it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.4311. T_Loss: 2.1664. Mask: 0.9639. :   6%|▌         | 3/50 [00:01<00:23,  1.97it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.25s. Batch: 0.47s. S_Loss: 2.4311. T_Loss: 2.1664. Mask: 0.9639. :   8%|▊         | 4/50 [00:01<00:20,  2.22it/s]total : 1000  current step :  202
total : 1000  current step :  203
total : 1000  current step :  204
Train Iter: 205/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.4318. T_Loss: 2.1937. Mask: 0.9633. :   8%|▊         | 4/50 [00:02<00:20,  2.22it/s]Train Iter: 205/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.4318. T_Loss: 2.1937. Mask: 0.9633. :  10%|█         | 5/50 [00:02<00:28,  1.56it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.4317. T_Loss: 2.1850. Mask: 0.9629. :  10%|█         | 5/50 [00:03<00:28,  1.56it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.4317. T_Loss: 2.1850. Mask: 0.9629. :  12%|█▏        | 6/50 [00:03<00:23,  1.90it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.28s. Batch: 0.49s. S_Loss: 2.4305. T_Loss: 2.1792. Mask: 0.9654. :  12%|█▏        | 6/50 [00:03<00:23,  1.90it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.28s. Batch: 0.49s. S_Loss: 2.4305. T_Loss: 2.1792. Mask: 0.9654. :  14%|█▍        | 7/50 [00:03<00:19,  2.22it/s]total : 1000  current step :  205
total : 1000  current step :  206
total : 1000  current step :  207
Train Iter: 208/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.4307. T_Loss: 2.1628. Mask: 0.9673. :  14%|█▍        | 7/50 [00:04<00:19,  2.22it/s]Train Iter: 208/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.4307. T_Loss: 2.1628. Mask: 0.9673. :  16%|█▌        | 8/50 [00:04<00:27,  1.52it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4310. T_Loss: 2.1845. Mask: 0.9648. :  16%|█▌        | 8/50 [00:04<00:27,  1.52it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4310. T_Loss: 2.1845. Mask: 0.9648. :  18%|█▊        | 9/50 [00:04<00:23,  1.74it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.4329. T_Loss: 2.1851. Mask: 0.9637. :  18%|█▊        | 9/50 [00:05<00:23,  1.74it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.4329. T_Loss: 2.1851. Mask: 0.9637. :  20%|██        | 10/50 [00:05<00:21,  1.85it/s]total : 1000  current step :  208
total : 1000  current step :  209
total : 1000  current step :  210
Train Iter: 211/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.4364. T_Loss: 2.2052. Mask: 0.9648. :  20%|██        | 10/50 [00:06<00:21,  1.85it/s]Train Iter: 211/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.4364. T_Loss: 2.2052. Mask: 0.9648. :  22%|██▏       | 11/50 [00:06<00:27,  1.40it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4353. T_Loss: 2.2113. Mask: 0.9648. :  22%|██▏       | 11/50 [00:06<00:27,  1.40it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4353. T_Loss: 2.2113. Mask: 0.9648. :  24%|██▍       | 12/50 [00:06<00:23,  1.63it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.4338. T_Loss: 2.2234. Mask: 0.9663. :  24%|██▍       | 12/50 [00:07<00:23,  1.63it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.4338. T_Loss: 2.2234. Mask: 0.9663. :  26%|██▌       | 13/50 [00:07<00:20,  1.81it/s]total : 1000  current step :  211
total : 1000  current step :  212
total : 1000  current step :  213
Train Iter: 214/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 2.4330. T_Loss: 2.2414. Mask: 0.9654. :  26%|██▌       | 13/50 [00:08<00:20,  1.81it/s]Train Iter: 214/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 2.4330. T_Loss: 2.2414. Mask: 0.9654. :  28%|██▊       | 14/50 [00:08<00:26,  1.35it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.4323. T_Loss: 2.2695. Mask: 0.9641. :  28%|██▊       | 14/50 [00:08<00:26,  1.35it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.4323. T_Loss: 2.2695. Mask: 0.9641. :  30%|███       | 15/50 [00:08<00:21,  1.62it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.4340. T_Loss: 2.2748. Mask: 0.9646. :  30%|███       | 15/50 [00:09<00:21,  1.62it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.4340. T_Loss: 2.2748. Mask: 0.9646. :  32%|███▏      | 16/50 [00:09<00:19,  1.77it/s]total : 1000  current step :  214
total : 1000  current step :  215
total : 1000  current step :  216
Train Iter: 217/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4365. T_Loss: 2.2915. Mask: 0.9644. :  32%|███▏      | 16/50 [00:10<00:19,  1.77it/s]Train Iter: 217/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4365. T_Loss: 2.2915. Mask: 0.9644. :  34%|███▍      | 17/50 [00:10<00:22,  1.44it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.4347. T_Loss: 2.3040. Mask: 0.9648. :  34%|███▍      | 17/50 [00:10<00:22,  1.44it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.4347. T_Loss: 2.3040. Mask: 0.9648. :  36%|███▌      | 18/50 [00:10<00:17,  1.79it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.4342. T_Loss: 2.3087. Mask: 0.9644. :  36%|███▌      | 18/50 [00:11<00:17,  1.79it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.4342. T_Loss: 2.3087. Mask: 0.9644. :  38%|███▊      | 19/50 [00:11<00:18,  1.67it/s]total : 1000  current step :  217
total : 1000  current step :  218
total : 1000  current step :  219
Train Iter: 220/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4351. T_Loss: 2.3241. Mask: 0.9641. :  38%|███▊      | 19/50 [00:12<00:18,  1.67it/s]Train Iter: 220/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4351. T_Loss: 2.3241. Mask: 0.9641. :  40%|████      | 20/50 [00:12<00:23,  1.27it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.4321. T_Loss: 2.3250. Mask: 0.9637. :  40%|████      | 20/50 [00:12<00:23,  1.27it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.4321. T_Loss: 2.3250. Mask: 0.9637. :  42%|████▏     | 21/50 [00:12<00:18,  1.55it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 2.4325. T_Loss: 2.3281. Mask: 0.9632. :  42%|████▏     | 21/50 [00:13<00:18,  1.55it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 2.4325. T_Loss: 2.3281. Mask: 0.9632. :  44%|████▍     | 22/50 [00:13<00:15,  1.76it/s]total : 1000  current step :  220
total : 1000  current step :  221
total : 1000  current step :  222
Train Iter: 223/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4305. T_Loss: 2.3389. Mask: 0.9631. :  44%|████▍     | 22/50 [00:14<00:15,  1.76it/s]Train Iter: 223/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4305. T_Loss: 2.3389. Mask: 0.9631. :  46%|████▌     | 23/50 [00:14<00:20,  1.31it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4282. T_Loss: 2.3458. Mask: 0.9631. :  46%|████▌     | 23/50 [00:14<00:20,  1.31it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4282. T_Loss: 2.3458. Mask: 0.9631. :  48%|████▊     | 24/50 [00:14<00:16,  1.57it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4278. T_Loss: 2.3493. Mask: 0.9627. :  48%|████▊     | 24/50 [00:15<00:16,  1.57it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4278. T_Loss: 2.3493. Mask: 0.9627. :  50%|█████     | 25/50 [00:15<00:13,  1.81it/s]total : 1000  current step :  223
total : 1000  current step :  224
total : 1000  current step :  225
Train Iter: 226/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4265. T_Loss: 2.3563. Mask: 0.9630. :  50%|█████     | 25/50 [00:16<00:13,  1.81it/s]Train Iter: 226/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4265. T_Loss: 2.3563. Mask: 0.9630. :  52%|█████▏    | 26/50 [00:16<00:18,  1.30it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4278. T_Loss: 2.3541. Mask: 0.9628. :  52%|█████▏    | 26/50 [00:16<00:18,  1.30it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4278. T_Loss: 2.3541. Mask: 0.9628. :  54%|█████▍    | 27/50 [00:16<00:14,  1.54it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4266. T_Loss: 2.3510. Mask: 0.9622. :  54%|█████▍    | 27/50 [00:16<00:14,  1.54it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4266. T_Loss: 2.3510. Mask: 0.9622. :  56%|█████▌    | 28/50 [00:16<00:11,  1.86it/s]total : 1000  current step :  226
total : 1000  current step :  227
total : 1000  current step :  228
Train Iter: 229/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4257. T_Loss: 2.3490. Mask: 0.9627. :  56%|█████▌    | 28/50 [00:18<00:11,  1.86it/s]Train Iter: 229/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4257. T_Loss: 2.3490. Mask: 0.9627. :  58%|█████▊    | 29/50 [00:18<00:16,  1.31it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4263. T_Loss: 2.3464. Mask: 0.9626. :  58%|█████▊    | 29/50 [00:18<00:16,  1.31it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4263. T_Loss: 2.3464. Mask: 0.9626. :  60%|██████    | 30/50 [00:18<00:12,  1.57it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4262. T_Loss: 2.3419. Mask: 0.9626. :  60%|██████    | 30/50 [00:18<00:12,  1.57it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4262. T_Loss: 2.3419. Mask: 0.9626. :  62%|██████▏   | 31/50 [00:18<00:10,  1.80it/s]total : 1000  current step :  229
total : 1000  current step :  230
total : 1000  current step :  231
Train Iter: 232/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4269. T_Loss: 2.3406. Mask: 0.9626. :  62%|██████▏   | 31/50 [00:20<00:10,  1.80it/s]Train Iter: 232/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4269. T_Loss: 2.3406. Mask: 0.9626. :  64%|██████▍   | 32/50 [00:20<00:12,  1.43it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4277. T_Loss: 2.3366. Mask: 0.9628. :  64%|██████▍   | 32/50 [00:20<00:12,  1.43it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4277. T_Loss: 2.3366. Mask: 0.9628. :  66%|██████▌   | 33/50 [00:20<00:09,  1.74it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4289. T_Loss: 2.3382. Mask: 0.9629. :  66%|██████▌   | 33/50 [00:20<00:09,  1.74it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4289. T_Loss: 2.3382. Mask: 0.9629. :  68%|██████▊   | 34/50 [00:20<00:07,  2.02it/s]total : 1000  current step :  232
total : 1000  current step :  233
total : 1000  current step :  234
Train Iter: 235/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4301. T_Loss: 2.3432. Mask: 0.9633. :  68%|██████▊   | 34/50 [00:21<00:07,  2.02it/s]Train Iter: 235/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4301. T_Loss: 2.3432. Mask: 0.9633. :  70%|███████   | 35/50 [00:21<00:10,  1.49it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4293. T_Loss: 2.3579. Mask: 0.9628. :  70%|███████   | 35/50 [00:22<00:10,  1.49it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4293. T_Loss: 2.3579. Mask: 0.9628. :  72%|███████▏  | 36/50 [00:22<00:08,  1.70it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4286. T_Loss: 2.3576. Mask: 0.9622. :  72%|███████▏  | 36/50 [00:22<00:08,  1.70it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 2.4286. T_Loss: 2.3576. Mask: 0.9622. :  74%|███████▍  | 37/50 [00:22<00:07,  1.82it/s]total : 1000  current step :  235
total : 1000  current step :  236
total : 1000  current step :  237
Train Iter: 238/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4287. T_Loss: 2.3636. Mask: 0.9620. :  74%|███████▍  | 37/50 [00:23<00:07,  1.82it/s]Train Iter: 238/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4287. T_Loss: 2.3636. Mask: 0.9620. :  76%|███████▌  | 38/50 [00:23<00:09,  1.33it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4287. T_Loss: 2.3669. Mask: 0.9617. :  76%|███████▌  | 38/50 [00:24<00:09,  1.33it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4287. T_Loss: 2.3669. Mask: 0.9617. :  78%|███████▊  | 39/50 [00:24<00:07,  1.51it/s]total : 1000  current step :  238
total : 1000  current step :  239
Train Iter: 240/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4286. T_Loss: 2.3687. Mask: 0.9613. :  78%|███████▊  | 39/50 [00:24<00:07,  1.51it/s]Train Iter: 240/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4286. T_Loss: 2.3687. Mask: 0.9613. :  80%|████████  | 40/50 [00:24<00:06,  1.44it/s]total : 1000  current step :  240
Train Iter: 241/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4278. T_Loss: 2.3771. Mask: 0.9614. :  80%|████████  | 40/50 [00:26<00:06,  1.44it/s]Train Iter: 241/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4278. T_Loss: 2.3771. Mask: 0.9614. :  82%|████████▏ | 41/50 [00:26<00:07,  1.19it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4257. T_Loss: 2.3832. Mask: 0.9616. :  82%|████████▏ | 41/50 [00:26<00:07,  1.19it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4257. T_Loss: 2.3832. Mask: 0.9616. :  84%|████████▍ | 42/50 [00:26<00:05,  1.43it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4249. T_Loss: 2.3862. Mask: 0.9616. :  84%|████████▍ | 42/50 [00:27<00:05,  1.43it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4249. T_Loss: 2.3862. Mask: 0.9616. :  86%|████████▌ | 43/50 [00:27<00:04,  1.57it/s]total : 1000  current step :  241
total : 1000  current step :  242
total : 1000  current step :  243
Train Iter: 244/1000. LR: 0.0000. Data: 0.45s. Batch: 0.64s. S_Loss: 2.4245. T_Loss: 2.3884. Mask: 0.9614. :  86%|████████▌ | 43/50 [00:28<00:04,  1.57it/s]Train Iter: 244/1000. LR: 0.0000. Data: 0.45s. Batch: 0.64s. S_Loss: 2.4245. T_Loss: 2.3884. Mask: 0.9614. :  88%|████████▊ | 44/50 [00:28<00:04,  1.27it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4225. T_Loss: 2.3895. Mask: 0.9609. :  88%|████████▊ | 44/50 [00:28<00:04,  1.27it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4225. T_Loss: 2.3895. Mask: 0.9609. :  90%|█████████ | 45/50 [00:28<00:03,  1.50it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4222. T_Loss: 2.3940. Mask: 0.9612. :  90%|█████████ | 45/50 [00:28<00:03,  1.50it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4222. T_Loss: 2.3940. Mask: 0.9612. :  92%|█████████▏| 46/50 [00:28<00:02,  1.77it/s]total : 1000  current step :  244
total : 1000  current step :  245
total : 1000  current step :  246
Train Iter: 247/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4219. T_Loss: 2.3937. Mask: 0.9611. :  92%|█████████▏| 46/50 [00:29<00:02,  1.77it/s]Train Iter: 247/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4219. T_Loss: 2.3937. Mask: 0.9611. :  94%|█████████▍| 47/50 [00:29<00:02,  1.40it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4211. T_Loss: 2.3929. Mask: 0.9611. :  94%|█████████▍| 47/50 [00:30<00:02,  1.40it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4211. T_Loss: 2.3929. Mask: 0.9611. :  96%|█████████▌| 48/50 [00:30<00:01,  1.71it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4200. T_Loss: 2.3943. Mask: 0.9613. :  96%|█████████▌| 48/50 [00:30<00:01,  1.71it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4200. T_Loss: 2.3943. Mask: 0.9613. :  98%|█████████▊| 49/50 [00:30<00:00,  1.94it/s]total : 1000  current step :  247
total : 1000  current step :  248
total : 1000  current step :  249
Train Iter: 250/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4193. T_Loss: 2.3927. Mask: 0.9613. :  98%|█████████▊| 49/50 [00:31<00:00,  1.94it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4193. T_Loss: 2.3927. Mask: 0.9613. : 100%|██████████| 50/50 [00:31<00:00,  1.39it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.4193. T_Loss: 2.3927. Mask: 0.9613. : 100%|██████████| 50/50 [00:31<00:00,  1.57it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 1.8537. top1: 71.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 1.8537. top1: 71.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.8514. top1: 74.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.8514. top1: 74.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.8480. top1: 74.48. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.8480. top1: 74.48. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.44it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.8689. top1: 73.44. top5: 97.85. :  38%|███▊      | 3/8 [00:01<00:02,  2.44it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.8689. top1: 73.44. top5: 97.85. :  50%|█████     | 4/8 [00:01<00:01,  2.66it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 2.0648. top1: 58.75. top5: 81.02. :  50%|█████     | 4/8 [00:02<00:01,  2.66it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 2.0648. top1: 58.75. top5: 81.02. :  62%|██████▎   | 5/8 [00:02<00:01,  2.72it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.1902. top1: 48.96. top5: 69.73. :  62%|██████▎   | 5/8 [00:02<00:01,  2.72it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.1902. top1: 48.96. top5: 69.73. :  75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.2801. top1: 41.96. top5: 61.50. :  75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.2801. top1: 41.96. top5: 61.50. :  88%|████████▊ | 7/8 [00:02<00:00,  2.89it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.3348. top1: 37.60. top5: 56.50. :  88%|████████▊ | 7/8 [00:02<00:00,  2.89it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.3348. top1: 37.60. top5: 56.50. : 100%|██████████| 8/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.3348. top1: 37.60. top5: 56.50. : 100%|██████████| 8/8 [00:03<00:00,  2.59it/s]
total : 1000  current step :  250
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.22s. S_Loss: 2.4142. T_Loss: 2.3608. Mask: 0.9648. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.22s. S_Loss: 2.4142. T_Loss: 2.3608. Mask: 0.9648. :   2%|▏         | 1/50 [00:00<00:10,  4.47it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.22s. S_Loss: 2.4323. T_Loss: 2.2833. Mask: 0.9570. :   2%|▏         | 1/50 [00:00<00:10,  4.47it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.22s. S_Loss: 2.4323. T_Loss: 2.2833. Mask: 0.9570. :   4%|▍         | 2/50 [00:00<00:10,  4.59it/s]total : 1000  current step :  251
total : 1000  current step :  252
Train Iter: 253/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.4547. T_Loss: 2.3041. Mask: 0.9583. :   4%|▍         | 2/50 [00:01<00:10,  4.59it/s]Train Iter: 253/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.4547. T_Loss: 2.3041. Mask: 0.9583. :   6%|▌         | 3/50 [00:01<00:26,  1.74it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.23s. Batch: 0.43s. S_Loss: 2.4469. T_Loss: 2.3496. Mask: 0.9590. :   6%|▌         | 3/50 [00:01<00:26,  1.74it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.23s. Batch: 0.43s. S_Loss: 2.4469. T_Loss: 2.3496. Mask: 0.9590. :   8%|▊         | 4/50 [00:01<00:21,  2.15it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.21s. Batch: 0.42s. S_Loss: 2.4501. T_Loss: 2.3434. Mask: 0.9641. :   8%|▊         | 4/50 [00:02<00:21,  2.15it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.21s. Batch: 0.42s. S_Loss: 2.4501. T_Loss: 2.3434. Mask: 0.9641. :  10%|█         | 5/50 [00:02<00:19,  2.35it/s]total : 1000  current step :  253
total : 1000  current step :  254
total : 1000  current step :  255
Train Iter: 256/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.4500. T_Loss: 2.3755. Mask: 0.9635. :  10%|█         | 5/50 [00:03<00:19,  2.35it/s]Train Iter: 256/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.4500. T_Loss: 2.3755. Mask: 0.9635. :  12%|█▏        | 6/50 [00:03<00:29,  1.49it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.4510. T_Loss: 2.3554. Mask: 0.9648. :  12%|█▏        | 6/50 [00:03<00:29,  1.49it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.4510. T_Loss: 2.3554. Mask: 0.9648. :  14%|█▍        | 7/50 [00:03<00:24,  1.79it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.4501. T_Loss: 2.3497. Mask: 0.9634. :  14%|█▍        | 7/50 [00:03<00:24,  1.79it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.4501. T_Loss: 2.3497. Mask: 0.9634. :  16%|█▌        | 8/50 [00:03<00:20,  2.09it/s]total : 1000  current step :  256
total : 1000  current step :  257
total : 1000  current step :  258
Train Iter: 259/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4414. T_Loss: 2.3646. Mask: 0.9640. :  16%|█▌        | 8/50 [00:04<00:20,  2.09it/s]Train Iter: 259/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4414. T_Loss: 2.3646. Mask: 0.9640. :  18%|█▊        | 9/50 [00:04<00:27,  1.49it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.33s. Batch: 0.53s. S_Loss: 2.4371. T_Loss: 2.3850. Mask: 0.9648. :  18%|█▊        | 9/50 [00:05<00:27,  1.49it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.33s. Batch: 0.53s. S_Loss: 2.4371. T_Loss: 2.3850. Mask: 0.9648. :  20%|██        | 10/50 [00:05<00:22,  1.78it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.32s. Batch: 0.51s. S_Loss: 2.4351. T_Loss: 2.3899. Mask: 0.9631. :  20%|██        | 10/50 [00:05<00:22,  1.78it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.32s. Batch: 0.51s. S_Loss: 2.4351. T_Loss: 2.3899. Mask: 0.9631. :  22%|██▏       | 11/50 [00:05<00:19,  1.97it/s]total : 1000  current step :  259
total : 1000  current step :  260
total : 1000  current step :  261
Train Iter: 262/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.4337. T_Loss: 2.3911. Mask: 0.9593. :  22%|██▏       | 11/50 [00:06<00:19,  1.97it/s]Train Iter: 262/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.4337. T_Loss: 2.3911. Mask: 0.9593. :  24%|██▍       | 12/50 [00:06<00:25,  1.47it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.35s. Batch: 0.54s. S_Loss: 2.4353. T_Loss: 2.3987. Mask: 0.9594. :  24%|██▍       | 12/50 [00:07<00:25,  1.47it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.35s. Batch: 0.54s. S_Loss: 2.4353. T_Loss: 2.3987. Mask: 0.9594. :  26%|██▌       | 13/50 [00:07<00:21,  1.71it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.33s. Batch: 0.53s. S_Loss: 2.4333. T_Loss: 2.4313. Mask: 0.9604. :  26%|██▌       | 13/50 [00:07<00:21,  1.71it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.33s. Batch: 0.53s. S_Loss: 2.4333. T_Loss: 2.4313. Mask: 0.9604. :  28%|██▊       | 14/50 [00:07<00:17,  2.03it/s]total : 1000  current step :  262
total : 1000  current step :  263
total : 1000  current step :  264
Train Iter: 265/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.4331. T_Loss: 2.4520. Mask: 0.9602. :  28%|██▊       | 14/50 [00:08<00:17,  2.03it/s]Train Iter: 265/1000. LR: 0.0000. Data: 0.37s. Batch: 0.56s. S_Loss: 2.4331. T_Loss: 2.4520. Mask: 0.9602. :  30%|███       | 15/50 [00:08<00:22,  1.53it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4329. T_Loss: 2.4622. Mask: 0.9604. :  30%|███       | 15/50 [00:08<00:22,  1.53it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4329. T_Loss: 2.4622. Mask: 0.9604. :  32%|███▏      | 16/50 [00:08<00:19,  1.76it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.34s. Batch: 0.53s. S_Loss: 2.4277. T_Loss: 2.4792. Mask: 0.9614. :  32%|███▏      | 16/50 [00:09<00:19,  1.76it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.34s. Batch: 0.53s. S_Loss: 2.4277. T_Loss: 2.4792. Mask: 0.9614. :  34%|███▍      | 17/50 [00:09<00:16,  2.05it/s]total : 1000  current step :  265
total : 1000  current step :  266
total : 1000  current step :  267
Train Iter: 268/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4242. T_Loss: 2.4946. Mask: 0.9618. :  34%|███▍      | 17/50 [00:10<00:16,  2.05it/s]Train Iter: 268/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4242. T_Loss: 2.4946. Mask: 0.9618. :  36%|███▌      | 18/50 [00:10<00:21,  1.46it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4285. T_Loss: 2.5086. Mask: 0.9618. :  36%|███▌      | 18/50 [00:10<00:21,  1.46it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4285. T_Loss: 2.5086. Mask: 0.9618. :  38%|███▊      | 19/50 [00:10<00:17,  1.77it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.4286. T_Loss: 2.5092. Mask: 0.9619. :  38%|███▊      | 19/50 [00:10<00:17,  1.77it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.4286. T_Loss: 2.5092. Mask: 0.9619. :  40%|████      | 20/50 [00:10<00:15,  1.92it/s]total : 1000  current step :  268
total : 1000  current step :  269
total : 1000  current step :  270
Train Iter: 271/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.4301. T_Loss: 2.5103. Mask: 0.9628. :  40%|████      | 20/50 [00:11<00:15,  1.92it/s]Train Iter: 271/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.4301. T_Loss: 2.5103. Mask: 0.9628. :  42%|████▏     | 21/50 [00:11<00:19,  1.51it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.4282. T_Loss: 2.5061. Mask: 0.9631. :  42%|████▏     | 21/50 [00:12<00:19,  1.51it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.4282. T_Loss: 2.5061. Mask: 0.9631. :  44%|████▍     | 22/50 [00:12<00:15,  1.76it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4273. T_Loss: 2.5105. Mask: 0.9631. :  44%|████▍     | 22/50 [00:12<00:15,  1.76it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4273. T_Loss: 2.5105. Mask: 0.9631. :  46%|████▌     | 23/50 [00:12<00:13,  1.96it/s]total : 1000  current step :  271
total : 1000  current step :  272
total : 1000  current step :  273
Train Iter: 274/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4253. T_Loss: 2.5024. Mask: 0.9634. :  46%|████▌     | 23/50 [00:13<00:13,  1.96it/s]Train Iter: 274/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4253. T_Loss: 2.5024. Mask: 0.9634. :  48%|████▊     | 24/50 [00:13<00:17,  1.50it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.4245. T_Loss: 2.5050. Mask: 0.9642. :  48%|████▊     | 24/50 [00:13<00:17,  1.50it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.4245. T_Loss: 2.5050. Mask: 0.9642. :  50%|█████     | 25/50 [00:13<00:13,  1.80it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4255. T_Loss: 2.5094. Mask: 0.9648. :  50%|█████     | 25/50 [00:14<00:13,  1.80it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.4255. T_Loss: 2.5094. Mask: 0.9648. :  52%|█████▏    | 26/50 [00:14<00:12,  1.90it/s]total : 1000  current step :  274
total : 1000  current step :  275
total : 1000  current step :  276
Train Iter: 277/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4238. T_Loss: 2.5156. Mask: 0.9641. :  52%|█████▏    | 26/50 [00:15<00:12,  1.90it/s]Train Iter: 277/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4238. T_Loss: 2.5156. Mask: 0.9641. :  54%|█████▍    | 27/50 [00:15<00:15,  1.48it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4250. T_Loss: 2.5181. Mask: 0.9637. :  54%|█████▍    | 27/50 [00:15<00:15,  1.48it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.4250. T_Loss: 2.5181. Mask: 0.9637. :  56%|█████▌    | 28/50 [00:15<00:13,  1.67it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.4241. T_Loss: 2.5237. Mask: 0.9638. :  56%|█████▌    | 28/50 [00:16<00:13,  1.67it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.4241. T_Loss: 2.5237. Mask: 0.9638. :  58%|█████▊    | 29/50 [00:16<00:11,  1.86it/s]total : 1000  current step :  277
total : 1000  current step :  278
total : 1000  current step :  279
Train Iter: 280/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.4224. T_Loss: 2.5259. Mask: 0.9641. :  58%|█████▊    | 29/50 [00:17<00:11,  1.86it/s]Train Iter: 280/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 2.4224. T_Loss: 2.5259. Mask: 0.9641. :  60%|██████    | 30/50 [00:17<00:17,  1.16it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4208. T_Loss: 2.5305. Mask: 0.9637. :  60%|██████    | 30/50 [00:18<00:17,  1.16it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4208. T_Loss: 2.5305. Mask: 0.9637. :  62%|██████▏   | 31/50 [00:18<00:16,  1.16it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4199. T_Loss: 2.5324. Mask: 0.9639. :  62%|██████▏   | 31/50 [00:19<00:16,  1.16it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 2.4199. T_Loss: 2.5324. Mask: 0.9639. :  64%|██████▍   | 32/50 [00:19<00:14,  1.26it/s]total : 1000  current step :  280
total : 1000  current step :  281
total : 1000  current step :  282
Train Iter: 283/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4210. T_Loss: 2.5401. Mask: 0.9638. :  64%|██████▍   | 32/50 [00:20<00:14,  1.26it/s]Train Iter: 283/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.4210. T_Loss: 2.5401. Mask: 0.9638. :  66%|██████▌   | 33/50 [00:20<00:15,  1.07it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.4236. T_Loss: 2.5429. Mask: 0.9642. :  66%|██████▌   | 33/50 [00:21<00:15,  1.07it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.4236. T_Loss: 2.5429. Mask: 0.9642. :  68%|██████▊   | 34/50 [00:21<00:12,  1.31it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.4239. T_Loss: 2.5438. Mask: 0.9645. :  68%|██████▊   | 34/50 [00:21<00:12,  1.31it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.4239. T_Loss: 2.5438. Mask: 0.9645. :  70%|███████   | 35/50 [00:21<00:10,  1.44it/s]total : 1000  current step :  283
total : 1000  current step :  284
total : 1000  current step :  285
Train Iter: 286/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4251. T_Loss: 2.5547. Mask: 0.9650. :  70%|███████   | 35/50 [00:22<00:10,  1.44it/s]Train Iter: 286/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4251. T_Loss: 2.5547. Mask: 0.9650. :  72%|███████▏  | 36/50 [00:22<00:12,  1.14it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.4244. T_Loss: 2.5568. Mask: 0.9644. :  72%|███████▏  | 36/50 [00:23<00:12,  1.14it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.4244. T_Loss: 2.5568. Mask: 0.9644. :  74%|███████▍  | 37/50 [00:23<00:09,  1.42it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.4250. T_Loss: 2.5571. Mask: 0.9648. :  74%|███████▍  | 37/50 [00:23<00:09,  1.42it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.4250. T_Loss: 2.5571. Mask: 0.9648. :  76%|███████▌  | 38/50 [00:23<00:07,  1.65it/s]total : 1000  current step :  286
total : 1000  current step :  287
total : 1000  current step :  288
Train Iter: 289/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4234. T_Loss: 2.5639. Mask: 0.9649. :  76%|███████▌  | 38/50 [00:24<00:07,  1.65it/s]Train Iter: 289/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4234. T_Loss: 2.5639. Mask: 0.9649. :  78%|███████▊  | 39/50 [00:24<00:09,  1.19it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4222. T_Loss: 2.5674. Mask: 0.9646. :  78%|███████▊  | 39/50 [00:25<00:09,  1.19it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4222. T_Loss: 2.5674. Mask: 0.9646. :  80%|████████  | 40/50 [00:25<00:07,  1.42it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.4215. T_Loss: 2.5723. Mask: 0.9649. :  80%|████████  | 40/50 [00:25<00:07,  1.42it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.4215. T_Loss: 2.5723. Mask: 0.9649. :  82%|████████▏ | 41/50 [00:25<00:05,  1.65it/s]total : 1000  current step :  289
total : 1000  current step :  290
total : 1000  current step :  291
Train Iter: 292/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4211. T_Loss: 2.5767. Mask: 0.9654. :  82%|████████▏ | 41/50 [00:27<00:05,  1.65it/s]Train Iter: 292/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.4211. T_Loss: 2.5767. Mask: 0.9654. :  84%|████████▍ | 42/50 [00:27<00:06,  1.17it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.4208. T_Loss: 2.5823. Mask: 0.9658. :  84%|████████▍ | 42/50 [00:27<00:06,  1.17it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.4208. T_Loss: 2.5823. Mask: 0.9658. :  86%|████████▌ | 43/50 [00:27<00:05,  1.39it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4203. T_Loss: 2.5859. Mask: 0.9659. :  86%|████████▌ | 43/50 [00:27<00:05,  1.39it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4203. T_Loss: 2.5859. Mask: 0.9659. :  88%|████████▊ | 44/50 [00:27<00:03,  1.65it/s]total : 1000  current step :  292
total : 1000  current step :  293
total : 1000  current step :  294
Train Iter: 295/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.4209. T_Loss: 2.5896. Mask: 0.9659. :  88%|████████▊ | 44/50 [00:29<00:03,  1.65it/s]Train Iter: 295/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.4209. T_Loss: 2.5896. Mask: 0.9659. :  90%|█████████ | 45/50 [00:29<00:04,  1.22it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.4222. T_Loss: 2.5903. Mask: 0.9654. :  90%|█████████ | 45/50 [00:29<00:04,  1.22it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.4222. T_Loss: 2.5903. Mask: 0.9654. :  92%|█████████▏| 46/50 [00:29<00:02,  1.48it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4221. T_Loss: 2.5895. Mask: 0.9653. :  92%|█████████▏| 46/50 [00:29<00:02,  1.48it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4221. T_Loss: 2.5895. Mask: 0.9653. :  94%|█████████▍| 47/50 [00:29<00:01,  1.73it/s]total : 1000  current step :  295
total : 1000  current step :  296
total : 1000  current step :  297
Train Iter: 298/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.4220. T_Loss: 2.5916. Mask: 0.9656. :  94%|█████████▍| 47/50 [00:31<00:01,  1.73it/s]Train Iter: 298/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.4220. T_Loss: 2.5916. Mask: 0.9656. :  96%|█████████▌| 48/50 [00:31<00:01,  1.28it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.4211. T_Loss: 2.5926. Mask: 0.9659. :  96%|█████████▌| 48/50 [00:31<00:01,  1.28it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.4211. T_Loss: 2.5926. Mask: 0.9659. :  98%|█████████▊| 49/50 [00:31<00:00,  1.55it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4207. T_Loss: 2.5938. Mask: 0.9659. :  98%|█████████▊| 49/50 [00:31<00:00,  1.55it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4207. T_Loss: 2.5938. Mask: 0.9659. : 100%|██████████| 50/50 [00:31<00:00,  1.77it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.43s. Batch: 0.63s. S_Loss: 2.4207. T_Loss: 2.5938. Mask: 0.9659. : 100%|██████████| 50/50 [00:31<00:00,  1.57it/s]
total : 1000  current step :  298
total : 1000  current step :  299
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.9296. top1: 67.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.9296. top1: 67.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.9266. top1: 69.73. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.9266. top1: 69.73. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.9237. top1: 70.83. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.9237. top1: 70.83. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.9416. top1: 68.65. top5: 97.85. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.9416. top1: 68.65. top5: 97.85. :  50%|█████     | 4/8 [00:01<00:01,  3.16it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.1056. top1: 54.92. top5: 80.86. :  50%|█████     | 4/8 [00:01<00:01,  3.16it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.1056. top1: 54.92. top5: 80.86. :  62%|██████▎   | 5/8 [00:01<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.2097. top1: 45.77. top5: 69.53. :  62%|██████▎   | 5/8 [00:02<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.2097. top1: 45.77. top5: 69.53. :  75%|███████▌  | 6/8 [00:02<00:00,  3.33it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.2846. top1: 39.23. top5: 61.66. :  75%|███████▌  | 6/8 [00:02<00:00,  3.33it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.2846. top1: 39.23. top5: 61.66. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.3301. top1: 35.15. top5: 56.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.3301. top1: 35.15. top5: 56.80. : 100%|██████████| 8/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.3301. top1: 35.15. top5: 56.80. : 100%|██████████| 8/8 [00:02<00:00,  2.76it/s]
total : 1000  current step :  300
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.98s. Batch: 1.15s. S_Loss: 2.4566. T_Loss: 2.6120. Mask: 0.9844. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.98s. Batch: 1.15s. S_Loss: 2.4566. T_Loss: 2.6120. Mask: 0.9844. :   2%|▏         | 1/50 [00:01<00:56,  1.15s/it]Train Iter: 302/1000. LR: 0.0189. Data: 0.58s. Batch: 0.79s. S_Loss: 2.0176. T_Loss: 2.6083. Mask: 0.9707. :   2%|▏         | 1/50 [00:01<00:56,  1.15s/it]Train Iter: 302/1000. LR: 0.0189. Data: 0.58s. Batch: 0.79s. S_Loss: 2.0176. T_Loss: 2.6083. Mask: 0.9707. :   4%|▍         | 2/50 [00:01<00:34,  1.37it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.43s. Batch: 0.64s. S_Loss: 1.8013. T_Loss: 2.6465. Mask: 0.9727. :   4%|▍         | 2/50 [00:01<00:34,  1.37it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.43s. Batch: 0.64s. S_Loss: 1.8013. T_Loss: 2.6465. Mask: 0.9727. :   6%|▌         | 3/50 [00:01<00:26,  1.80it/s]total : 1000  current step :  301
total : 1000  current step :  302
total : 1000  current step :  303
Train Iter: 304/1000. LR: 0.0190. Data: 0.60s. Batch: 0.80s. S_Loss: 1.6667. T_Loss: 2.6369. Mask: 0.9756. :   6%|▌         | 3/50 [00:03<00:26,  1.80it/s]Train Iter: 304/1000. LR: 0.0190. Data: 0.60s. Batch: 0.80s. S_Loss: 1.6667. T_Loss: 2.6369. Mask: 0.9756. :   8%|▊         | 4/50 [00:03<00:38,  1.19it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.51s. Batch: 0.71s. S_Loss: 1.6125. T_Loss: 2.6239. Mask: 0.9680. :   8%|▊         | 4/50 [00:03<00:38,  1.19it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.51s. Batch: 0.71s. S_Loss: 1.6125. T_Loss: 2.6239. Mask: 0.9680. :  10%|█         | 5/50 [00:03<00:29,  1.53it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.47s. Batch: 0.67s. S_Loss: 1.5870. T_Loss: 2.6571. Mask: 0.9694. :  10%|█         | 5/50 [00:04<00:29,  1.53it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.47s. Batch: 0.67s. S_Loss: 1.5870. T_Loss: 2.6571. Mask: 0.9694. :  12%|█▏        | 6/50 [00:04<00:26,  1.68it/s]total : 1000  current step :  304
total : 1000  current step :  305
total : 1000  current step :  306
Train Iter: 307/1000. LR: 0.0192. Data: 0.55s. Batch: 0.74s. S_Loss: 1.5575. T_Loss: 2.6423. Mask: 0.9693. :  12%|█▏        | 6/50 [00:05<00:26,  1.68it/s]Train Iter: 307/1000. LR: 0.0192. Data: 0.55s. Batch: 0.74s. S_Loss: 1.5575. T_Loss: 2.6423. Mask: 0.9693. :  14%|█▍        | 7/50 [00:05<00:34,  1.26it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.51s. Batch: 0.70s. S_Loss: 1.5437. T_Loss: 2.6613. Mask: 0.9697. :  14%|█▍        | 7/50 [00:05<00:34,  1.26it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.51s. Batch: 0.70s. S_Loss: 1.5437. T_Loss: 2.6613. Mask: 0.9697. :  16%|█▌        | 8/50 [00:05<00:27,  1.51it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.49s. Batch: 0.68s. S_Loss: 1.5429. T_Loss: 2.6923. Mask: 0.9705. :  16%|█▌        | 8/50 [00:06<00:27,  1.51it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.49s. Batch: 0.68s. S_Loss: 1.5429. T_Loss: 2.6923. Mask: 0.9705. :  18%|█▊        | 9/50 [00:06<00:25,  1.63it/s]total : 1000  current step :  307
total : 1000  current step :  308
total : 1000  current step :  309
Train Iter: 310/1000. LR: 0.0194. Data: 0.52s. Batch: 0.72s. S_Loss: 1.5431. T_Loss: 2.7326. Mask: 0.9699. :  18%|█▊        | 9/50 [00:07<00:25,  1.63it/s]Train Iter: 310/1000. LR: 0.0194. Data: 0.52s. Batch: 0.72s. S_Loss: 1.5431. T_Loss: 2.7326. Mask: 0.9699. :  20%|██        | 10/50 [00:07<00:30,  1.29it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.49s. Batch: 0.69s. S_Loss: 1.5432. T_Loss: 2.7465. Mask: 0.9663. :  20%|██        | 10/50 [00:07<00:30,  1.29it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.49s. Batch: 0.69s. S_Loss: 1.5432. T_Loss: 2.7465. Mask: 0.9663. :  22%|██▏       | 11/50 [00:07<00:24,  1.57it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.47s. Batch: 0.66s. S_Loss: 1.5421. T_Loss: 2.7418. Mask: 0.9671. :  22%|██▏       | 11/50 [00:07<00:24,  1.57it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.47s. Batch: 0.66s. S_Loss: 1.5421. T_Loss: 2.7418. Mask: 0.9671. :  24%|██▍       | 12/50 [00:07<00:21,  1.75it/s]total : 1000  current step :  310
total : 1000  current step :  311
total : 1000  current step :  312
Train Iter: 313/1000. LR: 0.0196. Data: 0.49s. Batch: 0.69s. S_Loss: 1.5438. T_Loss: 2.7740. Mask: 0.9663. :  24%|██▍       | 12/50 [00:09<00:21,  1.75it/s]Train Iter: 313/1000. LR: 0.0196. Data: 0.49s. Batch: 0.69s. S_Loss: 1.5438. T_Loss: 2.7740. Mask: 0.9663. :  26%|██▌       | 13/50 [00:09<00:26,  1.42it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.47s. Batch: 0.67s. S_Loss: 1.5436. T_Loss: 2.7634. Mask: 0.9657. :  26%|██▌       | 13/50 [00:09<00:26,  1.42it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.47s. Batch: 0.67s. S_Loss: 1.5436. T_Loss: 2.7634. Mask: 0.9657. :  28%|██▊       | 14/50 [00:09<00:21,  1.64it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.46s. Batch: 0.67s. S_Loss: 1.5432. T_Loss: 2.7709. Mask: 0.9654. :  28%|██▊       | 14/50 [00:10<00:21,  1.64it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.46s. Batch: 0.67s. S_Loss: 1.5432. T_Loss: 2.7709. Mask: 0.9654. :  30%|███       | 15/50 [00:10<00:21,  1.63it/s]total : 1000  current step :  313
total : 1000  current step :  314
total : 1000  current step :  315
Train Iter: 316/1000. LR: 0.0198. Data: 0.49s. Batch: 0.69s. S_Loss: 1.5414. T_Loss: 2.7825. Mask: 0.9644. :  30%|███       | 15/50 [00:11<00:21,  1.63it/s]Train Iter: 316/1000. LR: 0.0198. Data: 0.49s. Batch: 0.69s. S_Loss: 1.5414. T_Loss: 2.7825. Mask: 0.9644. :  32%|███▏      | 16/50 [00:11<00:25,  1.34it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.47s. Batch: 0.68s. S_Loss: 1.5404. T_Loss: 2.8060. Mask: 0.9639. :  32%|███▏      | 16/50 [00:11<00:25,  1.34it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.47s. Batch: 0.68s. S_Loss: 1.5404. T_Loss: 2.8060. Mask: 0.9639. :  34%|███▍      | 17/50 [00:11<00:21,  1.53it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.45s. Batch: 0.65s. S_Loss: 1.5384. T_Loss: 2.8081. Mask: 0.9648. :  34%|███▍      | 17/50 [00:11<00:21,  1.53it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.45s. Batch: 0.65s. S_Loss: 1.5384. T_Loss: 2.8081. Mask: 0.9648. :  36%|███▌      | 18/50 [00:11<00:17,  1.85it/s]total : 1000  current step :  316
total : 1000  current step :  317
total : 1000  current step :  318
Train Iter: 319/1000. LR: 0.0199. Data: 0.47s. Batch: 0.67s. S_Loss: 1.5332. T_Loss: 2.8126. Mask: 0.9657. :  36%|███▌      | 18/50 [00:12<00:17,  1.85it/s]Train Iter: 319/1000. LR: 0.0199. Data: 0.47s. Batch: 0.67s. S_Loss: 1.5332. T_Loss: 2.8126. Mask: 0.9657. :  38%|███▊      | 19/50 [00:12<00:21,  1.45it/s]total : 1000  current step :  319
Train Iter: 320/1000. LR: 0.0200. Data: 0.47s. Batch: 0.68s. S_Loss: 1.5279. T_Loss: 2.8035. Mask: 0.9658. :  38%|███▊      | 19/50 [00:13<00:21,  1.45it/s]Train Iter: 320/1000. LR: 0.0200. Data: 0.47s. Batch: 0.68s. S_Loss: 1.5279. T_Loss: 2.8035. Mask: 0.9658. :  40%|████      | 20/50 [00:13<00:21,  1.41it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.47s. Batch: 0.68s. S_Loss: 1.5255. T_Loss: 2.8032. Mask: 0.9656. :  40%|████      | 20/50 [00:14<00:21,  1.41it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.47s. Batch: 0.68s. S_Loss: 1.5255. T_Loss: 2.8032. Mask: 0.9656. :  42%|████▏     | 21/50 [00:14<00:20,  1.45it/s]total : 1000  current step :  320
total : 1000  current step :  321
Train Iter: 322/1000. LR: 0.0201. Data: 0.49s. Batch: 0.70s. S_Loss: 1.5183. T_Loss: 2.8066. Mask: 0.9661. :  42%|████▏     | 21/50 [00:15<00:20,  1.45it/s]Train Iter: 322/1000. LR: 0.0201. Data: 0.49s. Batch: 0.70s. S_Loss: 1.5183. T_Loss: 2.8066. Mask: 0.9661. :  44%|████▍     | 22/50 [00:15<00:23,  1.21it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.48s. Batch: 0.68s. S_Loss: 1.5122. T_Loss: 2.8048. Mask: 0.9665. :  44%|████▍     | 22/50 [00:15<00:23,  1.21it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.48s. Batch: 0.68s. S_Loss: 1.5122. T_Loss: 2.8048. Mask: 0.9665. :  46%|████▌     | 23/50 [00:15<00:18,  1.49it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.46s. Batch: 0.67s. S_Loss: 1.5060. T_Loss: 2.7911. Mask: 0.9660. :  46%|████▌     | 23/50 [00:16<00:18,  1.49it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.46s. Batch: 0.67s. S_Loss: 1.5060. T_Loss: 2.7911. Mask: 0.9660. :  48%|████▊     | 24/50 [00:16<00:14,  1.75it/s]total : 1000  current step :  322
total : 1000  current step :  323
total : 1000  current step :  324
Train Iter: 325/1000. LR: 0.0203. Data: 0.48s. Batch: 0.68s. S_Loss: 1.4989. T_Loss: 2.7844. Mask: 0.9667. :  48%|████▊     | 24/50 [00:17<00:14,  1.75it/s]Train Iter: 325/1000. LR: 0.0203. Data: 0.48s. Batch: 0.68s. S_Loss: 1.4989. T_Loss: 2.7844. Mask: 0.9667. :  50%|█████     | 25/50 [00:17<00:18,  1.35it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.47s. Batch: 0.67s. S_Loss: 1.4919. T_Loss: 2.7771. Mask: 0.9665. :  50%|█████     | 25/50 [00:17<00:18,  1.35it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.47s. Batch: 0.67s. S_Loss: 1.4919. T_Loss: 2.7771. Mask: 0.9665. :  52%|█████▏    | 26/50 [00:17<00:15,  1.60it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.46s. Batch: 0.66s. S_Loss: 1.4853. T_Loss: 2.7781. Mask: 0.9666. :  52%|█████▏    | 26/50 [00:17<00:15,  1.60it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.46s. Batch: 0.66s. S_Loss: 1.4853. T_Loss: 2.7781. Mask: 0.9666. :  54%|█████▍    | 27/50 [00:17<00:12,  1.89it/s]total : 1000  current step :  325
total : 1000  current step :  326
total : 1000  current step :  327
Train Iter: 328/1000. LR: 0.0205. Data: 0.48s. Batch: 0.68s. S_Loss: 1.4788. T_Loss: 2.7795. Mask: 0.9667. :  54%|█████▍    | 27/50 [00:19<00:12,  1.89it/s]Train Iter: 328/1000. LR: 0.0205. Data: 0.48s. Batch: 0.68s. S_Loss: 1.4788. T_Loss: 2.7795. Mask: 0.9667. :  56%|█████▌    | 28/50 [00:19<00:16,  1.36it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.47s. Batch: 0.67s. S_Loss: 1.4709. T_Loss: 2.7974. Mask: 0.9669. :  56%|█████▌    | 28/50 [00:19<00:16,  1.36it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.47s. Batch: 0.67s. S_Loss: 1.4709. T_Loss: 2.7974. Mask: 0.9669. :  58%|█████▊    | 29/50 [00:19<00:13,  1.54it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.46s. Batch: 0.66s. S_Loss: 1.4636. T_Loss: 2.8035. Mask: 0.9674. :  58%|█████▊    | 29/50 [00:19<00:13,  1.54it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.46s. Batch: 0.66s. S_Loss: 1.4636. T_Loss: 2.8035. Mask: 0.9674. :  60%|██████    | 30/50 [00:19<00:11,  1.76it/s]total : 1000  current step :  328
total : 1000  current step :  329
total : 1000  current step :  330
Train Iter: 331/1000. LR: 0.0207. Data: 0.47s. Batch: 0.67s. S_Loss: 1.4553. T_Loss: 2.7943. Mask: 0.9672. :  60%|██████    | 30/50 [00:20<00:11,  1.76it/s]Train Iter: 331/1000. LR: 0.0207. Data: 0.47s. Batch: 0.67s. S_Loss: 1.4553. T_Loss: 2.7943. Mask: 0.9672. :  62%|██████▏   | 31/50 [00:20<00:13,  1.39it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.46s. Batch: 0.66s. S_Loss: 1.4477. T_Loss: 2.7893. Mask: 0.9673. :  62%|██████▏   | 31/50 [00:21<00:13,  1.39it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.46s. Batch: 0.66s. S_Loss: 1.4477. T_Loss: 2.7893. Mask: 0.9673. :  64%|██████▍   | 32/50 [00:21<00:11,  1.62it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.45s. Batch: 0.65s. S_Loss: 1.4399. T_Loss: 2.7896. Mask: 0.9670. :  64%|██████▍   | 32/50 [00:21<00:11,  1.62it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.45s. Batch: 0.65s. S_Loss: 1.4399. T_Loss: 2.7896. Mask: 0.9670. :  66%|██████▌   | 33/50 [00:21<00:09,  1.86it/s]total : 1000  current step :  331
total : 1000  current step :  332
total : 1000  current step :  333
Train Iter: 334/1000. LR: 0.0209. Data: 0.46s. Batch: 0.67s. S_Loss: 1.4324. T_Loss: 2.7829. Mask: 0.9671. :  66%|██████▌   | 33/50 [00:22<00:09,  1.86it/s]Train Iter: 334/1000. LR: 0.0209. Data: 0.46s. Batch: 0.67s. S_Loss: 1.4324. T_Loss: 2.7829. Mask: 0.9671. :  68%|██████▊   | 34/50 [00:22<00:10,  1.47it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.45s. Batch: 0.65s. S_Loss: 1.4242. T_Loss: 2.7806. Mask: 0.9673. :  68%|██████▊   | 34/50 [00:22<00:10,  1.47it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.45s. Batch: 0.65s. S_Loss: 1.4242. T_Loss: 2.7806. Mask: 0.9673. :  70%|███████   | 35/50 [00:22<00:08,  1.77it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.45s. Batch: 0.65s. S_Loss: 1.4165. T_Loss: 2.7730. Mask: 0.9671. :  70%|███████   | 35/50 [00:23<00:08,  1.77it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.45s. Batch: 0.65s. S_Loss: 1.4165. T_Loss: 2.7730. Mask: 0.9671. :  72%|███████▏  | 36/50 [00:23<00:07,  1.87it/s]total : 1000  current step :  334
total : 1000  current step :  335
total : 1000  current step :  336
Train Iter: 337/1000. LR: 0.0211. Data: 0.46s. Batch: 0.66s. S_Loss: 1.4104. T_Loss: 2.7842. Mask: 0.9672. :  72%|███████▏  | 36/50 [00:24<00:07,  1.87it/s]Train Iter: 337/1000. LR: 0.0211. Data: 0.46s. Batch: 0.66s. S_Loss: 1.4104. T_Loss: 2.7842. Mask: 0.9672. :  74%|███████▍  | 37/50 [00:24<00:09,  1.43it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.45s. Batch: 0.65s. S_Loss: 1.4038. T_Loss: 2.8000. Mask: 0.9672. :  74%|███████▍  | 37/50 [00:24<00:09,  1.43it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.45s. Batch: 0.65s. S_Loss: 1.4038. T_Loss: 2.8000. Mask: 0.9672. :  76%|███████▌  | 38/50 [00:24<00:06,  1.72it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.44s. Batch: 0.64s. S_Loss: 1.3973. T_Loss: 2.8070. Mask: 0.9671. :  76%|███████▌  | 38/50 [00:25<00:06,  1.72it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.44s. Batch: 0.64s. S_Loss: 1.3973. T_Loss: 2.8070. Mask: 0.9671. :  78%|███████▊  | 39/50 [00:25<00:05,  2.00it/s]total : 1000  current step :  337
total : 1000  current step :  338
total : 1000  current step :  339
Train Iter: 340/1000. LR: 0.0213. Data: 0.45s. Batch: 0.65s. S_Loss: 1.3906. T_Loss: 2.8115. Mask: 0.9675. :  78%|███████▊  | 39/50 [00:25<00:05,  2.00it/s]Train Iter: 340/1000. LR: 0.0213. Data: 0.45s. Batch: 0.65s. S_Loss: 1.3906. T_Loss: 2.8115. Mask: 0.9675. :  80%|████████  | 40/50 [00:25<00:06,  1.65it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.44s. Batch: 0.64s. S_Loss: 1.3843. T_Loss: 2.8174. Mask: 0.9676. :  80%|████████  | 40/50 [00:26<00:06,  1.65it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.44s. Batch: 0.64s. S_Loss: 1.3843. T_Loss: 2.8174. Mask: 0.9676. :  82%|████████▏ | 41/50 [00:26<00:04,  1.97it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.43s. Batch: 0.63s. S_Loss: 1.3774. T_Loss: 2.8238. Mask: 0.9678. :  82%|████████▏ | 41/50 [00:26<00:04,  1.97it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.43s. Batch: 0.63s. S_Loss: 1.3774. T_Loss: 2.8238. Mask: 0.9678. :  84%|████████▍ | 42/50 [00:26<00:03,  2.29it/s]total : 1000  current step :  340
total : 1000  current step :  341
total : 1000  current step :  342
Train Iter: 343/1000. LR: 0.0214. Data: 0.44s. Batch: 0.64s. S_Loss: 1.3710. T_Loss: 2.8335. Mask: 0.9675. :  84%|████████▍ | 42/50 [00:27<00:03,  2.29it/s]Train Iter: 343/1000. LR: 0.0214. Data: 0.44s. Batch: 0.64s. S_Loss: 1.3710. T_Loss: 2.8335. Mask: 0.9675. :  86%|████████▌ | 43/50 [00:27<00:04,  1.60it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.44s. Batch: 0.63s. S_Loss: 1.3657. T_Loss: 2.8449. Mask: 0.9671. :  86%|████████▌ | 43/50 [00:27<00:04,  1.60it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.44s. Batch: 0.63s. S_Loss: 1.3657. T_Loss: 2.8449. Mask: 0.9671. :  88%|████████▊ | 44/50 [00:27<00:03,  1.83it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.43s. Batch: 0.63s. S_Loss: 1.3597. T_Loss: 2.8502. Mask: 0.9671. :  88%|████████▊ | 44/50 [00:28<00:03,  1.83it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.43s. Batch: 0.63s. S_Loss: 1.3597. T_Loss: 2.8502. Mask: 0.9671. :  90%|█████████ | 45/50 [00:28<00:02,  2.04it/s]total : 1000  current step :  343
total : 1000  current step :  344
total : 1000  current step :  345
Train Iter: 346/1000. LR: 0.0216. Data: 0.44s. Batch: 0.63s. S_Loss: 1.3534. T_Loss: 2.8456. Mask: 0.9672. :  90%|█████████ | 45/50 [00:29<00:02,  2.04it/s]Train Iter: 346/1000. LR: 0.0216. Data: 0.44s. Batch: 0.63s. S_Loss: 1.3534. T_Loss: 2.8456. Mask: 0.9672. :  92%|█████████▏| 46/50 [00:29<00:02,  1.58it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.43s. Batch: 0.63s. S_Loss: 1.3480. T_Loss: 2.8477. Mask: 0.9671. :  92%|█████████▏| 46/50 [00:29<00:02,  1.58it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.43s. Batch: 0.63s. S_Loss: 1.3480. T_Loss: 2.8477. Mask: 0.9671. :  94%|█████████▍| 47/50 [00:29<00:01,  1.89it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.42s. Batch: 0.62s. S_Loss: 1.3418. T_Loss: 2.8431. Mask: 0.9674. :  94%|█████████▍| 47/50 [00:29<00:01,  1.89it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.42s. Batch: 0.62s. S_Loss: 1.3418. T_Loss: 2.8431. Mask: 0.9674. :  96%|█████████▌| 48/50 [00:29<00:00,  2.14it/s]total : 1000  current step :  346
total : 1000  current step :  347
total : 1000  current step :  348
Train Iter: 349/1000. LR: 0.0218. Data: 0.43s. Batch: 0.63s. S_Loss: 1.3358. T_Loss: 2.8466. Mask: 0.9673. :  96%|█████████▌| 48/50 [00:30<00:00,  2.14it/s]Train Iter: 349/1000. LR: 0.0218. Data: 0.43s. Batch: 0.63s. S_Loss: 1.3358. T_Loss: 2.8466. Mask: 0.9673. :  98%|█████████▊| 49/50 [00:30<00:00,  1.59it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.42s. Batch: 0.62s. S_Loss: 1.3297. T_Loss: 2.8414. Mask: 0.9671. :  98%|█████████▊| 49/50 [00:31<00:00,  1.59it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.42s. Batch: 0.62s. S_Loss: 1.3297. T_Loss: 2.8414. Mask: 0.9671. : 100%|██████████| 50/50 [00:31<00:00,  1.89it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.42s. Batch: 0.62s. S_Loss: 1.3297. T_Loss: 2.8414. Mask: 0.9671. : 100%|██████████| 50/50 [00:31<00:00,  1.60it/s]
total : 1000  current step :  349
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.5309. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.5309. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.5428. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.5428. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.5423. top1: 89.97. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.5423. top1: 89.97. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.86it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.5432. top1: 88.96. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.86it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.5432. top1: 88.96. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.22it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.5594. top1: 84.38. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.22it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.5594. top1: 84.38. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.42it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.5668. top1: 82.23. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.42it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.5668. top1: 82.23. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.5733. top1: 79.85. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.5733. top1: 79.85. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.5783. top1: 79.00. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.5783. top1: 79.00. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.77it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.5783. top1: 79.00. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]
total : 1000  current step :  350
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.23s. S_Loss: 1.0393. T_Loss: 2.7886. Mask: 0.9648. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.23s. S_Loss: 1.0393. T_Loss: 2.7886. Mask: 0.9648. :   2%|▏         | 1/50 [00:00<00:11,  4.27it/s]total : 1000  current step :  351
Train Iter: 352/1000. LR: 0.0220. Data: 0.39s. Batch: 0.60s. S_Loss: 1.0469. T_Loss: 2.7903. Mask: 0.9648. :   2%|▏         | 1/50 [00:01<00:11,  4.27it/s]Train Iter: 352/1000. LR: 0.0220. Data: 0.39s. Batch: 0.60s. S_Loss: 1.0469. T_Loss: 2.7903. Mask: 0.9648. :   4%|▍         | 2/50 [00:01<00:32,  1.49it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.31s. Batch: 0.50s. S_Loss: 1.0497. T_Loss: 2.9485. Mask: 0.9661. :   4%|▍         | 2/50 [00:01<00:32,  1.49it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.31s. Batch: 0.50s. S_Loss: 1.0497. T_Loss: 2.9485. Mask: 0.9661. :   6%|▌         | 3/50 [00:01<00:23,  1.98it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.27s. Batch: 0.46s. S_Loss: 1.0575. T_Loss: 3.0257. Mask: 0.9629. :   6%|▌         | 3/50 [00:01<00:23,  1.98it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.27s. Batch: 0.46s. S_Loss: 1.0575. T_Loss: 3.0257. Mask: 0.9629. :   8%|▊         | 4/50 [00:01<00:19,  2.34it/s]total : 1000  current step :  352
total : 1000  current step :  353
total : 1000  current step :  354
Train Iter: 355/1000. LR: 0.0222. Data: 0.39s. Batch: 0.58s. S_Loss: 1.0547. T_Loss: 3.0478. Mask: 0.9648. :   8%|▊         | 4/50 [00:02<00:19,  2.34it/s]Train Iter: 355/1000. LR: 0.0222. Data: 0.39s. Batch: 0.58s. S_Loss: 1.0547. T_Loss: 3.0478. Mask: 0.9648. :  10%|█         | 5/50 [00:02<00:29,  1.50it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0494. T_Loss: 3.0533. Mask: 0.9648. :  10%|█         | 5/50 [00:03<00:29,  1.50it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0494. T_Loss: 3.0533. Mask: 0.9648. :  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.32s. Batch: 0.50s. S_Loss: 1.0425. T_Loss: 3.0307. Mask: 0.9632. :  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.32s. Batch: 0.50s. S_Loss: 1.0425. T_Loss: 3.0307. Mask: 0.9632. :  14%|█▍        | 7/50 [00:03<00:19,  2.17it/s]total : 1000  current step :  355
total : 1000  current step :  356
total : 1000  current step :  357
Train Iter: 358/1000. LR: 0.0224. Data: 0.38s. Batch: 0.57s. S_Loss: 1.0423. T_Loss: 2.9882. Mask: 0.9658. :  14%|█▍        | 7/50 [00:04<00:19,  2.17it/s]Train Iter: 358/1000. LR: 0.0224. Data: 0.38s. Batch: 0.57s. S_Loss: 1.0423. T_Loss: 2.9882. Mask: 0.9658. :  16%|█▌        | 8/50 [00:04<00:27,  1.53it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.34s. Batch: 0.54s. S_Loss: 1.0412. T_Loss: 3.0149. Mask: 0.9683. :  16%|█▌        | 8/50 [00:04<00:27,  1.53it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.34s. Batch: 0.54s. S_Loss: 1.0412. T_Loss: 3.0149. Mask: 0.9683. :  18%|█▊        | 9/50 [00:04<00:21,  1.91it/s]total : 1000  current step :  358
total : 1000  current step :  359
Train Iter: 360/1000. LR: 0.0225. Data: 0.34s. Batch: 0.54s. S_Loss: 1.0406. T_Loss: 3.0026. Mask: 0.9680. :  18%|█▊        | 9/50 [00:05<00:21,  1.91it/s]Train Iter: 360/1000. LR: 0.0225. Data: 0.34s. Batch: 0.54s. S_Loss: 1.0406. T_Loss: 3.0026. Mask: 0.9680. :  20%|██        | 10/50 [00:05<00:21,  1.89it/s]total : 1000  current step :  360
Train Iter: 361/1000. LR: 0.0226. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0386. T_Loss: 2.9933. Mask: 0.9680. :  20%|██        | 10/50 [00:06<00:21,  1.89it/s]Train Iter: 361/1000. LR: 0.0226. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0386. T_Loss: 2.9933. Mask: 0.9680. :  22%|██▏       | 11/50 [00:06<00:26,  1.50it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0444. T_Loss: 3.0045. Mask: 0.9681. :  22%|██▏       | 11/50 [00:06<00:26,  1.50it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0444. T_Loss: 3.0045. Mask: 0.9681. :  24%|██▍       | 12/50 [00:06<00:20,  1.81it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0456. T_Loss: 3.0164. Mask: 0.9666. :  24%|██▍       | 12/50 [00:07<00:20,  1.81it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0456. T_Loss: 3.0164. Mask: 0.9666. :  26%|██▌       | 13/50 [00:07<00:19,  1.91it/s]total : 1000  current step :  361
total : 1000  current step :  362
total : 1000  current step :  363
Train Iter: 364/1000. LR: 0.0228. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0465. T_Loss: 2.9907. Mask: 0.9665. :  26%|██▌       | 13/50 [00:08<00:19,  1.91it/s]Train Iter: 364/1000. LR: 0.0228. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0465. T_Loss: 2.9907. Mask: 0.9665. :  28%|██▊       | 14/50 [00:08<00:24,  1.48it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0436. T_Loss: 2.9931. Mask: 0.9667. :  28%|██▊       | 14/50 [00:08<00:24,  1.48it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0436. T_Loss: 2.9931. Mask: 0.9667. :  30%|███       | 15/50 [00:08<00:19,  1.78it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0415. T_Loss: 2.9792. Mask: 0.9670. :  30%|███       | 15/50 [00:08<00:19,  1.78it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0415. T_Loss: 2.9792. Mask: 0.9670. :  32%|███▏      | 16/50 [00:08<00:17,  1.91it/s]total : 1000  current step :  364
total : 1000  current step :  365
total : 1000  current step :  366
Train Iter: 367/1000. LR: 0.0229. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0396. T_Loss: 2.9744. Mask: 0.9671. :  32%|███▏      | 16/50 [00:09<00:17,  1.91it/s]Train Iter: 367/1000. LR: 0.0229. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0396. T_Loss: 2.9744. Mask: 0.9671. :  34%|███▍      | 17/50 [00:09<00:21,  1.54it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0391. T_Loss: 2.9657. Mask: 0.9664. :  34%|███▍      | 17/50 [00:10<00:21,  1.54it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0391. T_Loss: 2.9657. Mask: 0.9664. :  36%|███▌      | 18/50 [00:10<00:17,  1.84it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0367. T_Loss: 2.9543. Mask: 0.9667. :  36%|███▌      | 18/50 [00:10<00:17,  1.84it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0367. T_Loss: 2.9543. Mask: 0.9667. :  38%|███▊      | 19/50 [00:10<00:15,  2.06it/s]total : 1000  current step :  367
total : 1000  current step :  368
total : 1000  current step :  369
Train Iter: 370/1000. LR: 0.0231. Data: 0.38s. Batch: 0.57s. S_Loss: 1.0335. T_Loss: 2.9371. Mask: 0.9672. :  38%|███▊      | 19/50 [00:11<00:15,  2.06it/s]Train Iter: 370/1000. LR: 0.0231. Data: 0.38s. Batch: 0.57s. S_Loss: 1.0335. T_Loss: 2.9371. Mask: 0.9672. :  40%|████      | 20/50 [00:11<00:19,  1.57it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0311. T_Loss: 2.9398. Mask: 0.9680. :  40%|████      | 20/50 [00:11<00:19,  1.57it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0311. T_Loss: 2.9398. Mask: 0.9680. :  42%|████▏     | 21/50 [00:11<00:15,  1.90it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0315. T_Loss: 2.9562. Mask: 0.9680. :  42%|████▏     | 21/50 [00:11<00:15,  1.90it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0315. T_Loss: 2.9562. Mask: 0.9680. :  44%|████▍     | 22/50 [00:11<00:12,  2.25it/s]total : 1000  current step :  370
total : 1000  current step :  371
total : 1000  current step :  372
Train Iter: 373/1000. LR: 0.0233. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0305. T_Loss: 2.9682. Mask: 0.9688. :  44%|████▍     | 22/50 [00:12<00:12,  2.25it/s]Train Iter: 373/1000. LR: 0.0233. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0305. T_Loss: 2.9682. Mask: 0.9688. :  46%|████▌     | 23/50 [00:12<00:16,  1.64it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0286. T_Loss: 2.9588. Mask: 0.9689. :  46%|████▌     | 23/50 [00:13<00:16,  1.64it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0286. T_Loss: 2.9588. Mask: 0.9689. :  48%|████▊     | 24/50 [00:13<00:13,  1.93it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0270. T_Loss: 2.9587. Mask: 0.9695. :  48%|████▊     | 24/50 [00:13<00:13,  1.93it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0270. T_Loss: 2.9587. Mask: 0.9695. :  50%|█████     | 25/50 [00:13<00:10,  2.29it/s]total : 1000  current step :  373
total : 1000  current step :  374
total : 1000  current step :  375
Train Iter: 376/1000. LR: 0.0235. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0256. T_Loss: 2.9514. Mask: 0.9686. :  50%|█████     | 25/50 [00:14<00:10,  2.29it/s]Train Iter: 376/1000. LR: 0.0235. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0256. T_Loss: 2.9514. Mask: 0.9686. :  52%|█████▏    | 26/50 [00:14<00:14,  1.61it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0242. T_Loss: 2.9504. Mask: 0.9683. :  52%|█████▏    | 26/50 [00:14<00:14,  1.61it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0242. T_Loss: 2.9504. Mask: 0.9683. :  54%|█████▍    | 27/50 [00:14<00:12,  1.81it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0227. T_Loss: 2.9597. Mask: 0.9681. :  54%|█████▍    | 27/50 [00:15<00:12,  1.81it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0227. T_Loss: 2.9597. Mask: 0.9681. :  56%|█████▌    | 28/50 [00:15<00:10,  2.04it/s]total : 1000  current step :  376
total : 1000  current step :  377
total : 1000  current step :  378
Train Iter: 379/1000. LR: 0.0237. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0232. T_Loss: 2.9819. Mask: 0.9675. :  56%|█████▌    | 28/50 [00:16<00:10,  2.04it/s]Train Iter: 379/1000. LR: 0.0237. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0232. T_Loss: 2.9819. Mask: 0.9675. :  58%|█████▊    | 29/50 [00:16<00:13,  1.57it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0229. T_Loss: 2.9959. Mask: 0.9669. :  58%|█████▊    | 29/50 [00:16<00:13,  1.57it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0229. T_Loss: 2.9959. Mask: 0.9669. :  60%|██████    | 30/50 [00:16<00:10,  1.82it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0219. T_Loss: 3.0054. Mask: 0.9664. :  60%|██████    | 30/50 [00:16<00:10,  1.82it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0219. T_Loss: 3.0054. Mask: 0.9664. :  62%|██████▏   | 31/50 [00:16<00:08,  2.12it/s]total : 1000  current step :  379
total : 1000  current step :  380
total : 1000  current step :  381
Train Iter: 382/1000. LR: 0.0239. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0225. T_Loss: 3.0146. Mask: 0.9661. :  62%|██████▏   | 31/50 [00:18<00:08,  2.12it/s]Train Iter: 382/1000. LR: 0.0239. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0225. T_Loss: 3.0146. Mask: 0.9661. :  64%|██████▍   | 32/50 [00:18<00:11,  1.51it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0229. T_Loss: 3.0190. Mask: 0.9663. :  64%|██████▍   | 32/50 [00:18<00:11,  1.51it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0229. T_Loss: 3.0190. Mask: 0.9663. :  66%|██████▌   | 33/50 [00:18<00:09,  1.70it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.35s. Batch: 0.55s. S_Loss: 1.0226. T_Loss: 3.0211. Mask: 0.9655. :  66%|██████▌   | 33/50 [00:18<00:09,  1.70it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.35s. Batch: 0.55s. S_Loss: 1.0226. T_Loss: 3.0211. Mask: 0.9655. :  68%|██████▊   | 34/50 [00:18<00:07,  2.05it/s]total : 1000  current step :  382
total : 1000  current step :  383
total : 1000  current step :  384
Train Iter: 385/1000. LR: 0.0241. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0228. T_Loss: 3.0250. Mask: 0.9648. :  68%|██████▊   | 34/50 [00:19<00:07,  2.05it/s]Train Iter: 385/1000. LR: 0.0241. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0228. T_Loss: 3.0250. Mask: 0.9648. :  70%|███████   | 35/50 [00:19<00:09,  1.59it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0217. T_Loss: 3.0262. Mask: 0.9650. :  70%|███████   | 35/50 [00:19<00:09,  1.59it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0217. T_Loss: 3.0262. Mask: 0.9650. :  72%|███████▏  | 36/50 [00:19<00:07,  1.91it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0215. T_Loss: 3.0361. Mask: 0.9655. :  72%|███████▏  | 36/50 [00:20<00:07,  1.91it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.35s. Batch: 0.54s. S_Loss: 1.0215. T_Loss: 3.0361. Mask: 0.9655. :  74%|███████▍  | 37/50 [00:20<00:05,  2.21it/s]total : 1000  current step :  385
total : 1000  current step :  386
total : 1000  current step :  387
Train Iter: 388/1000. LR: 0.0243. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0190. T_Loss: 3.0235. Mask: 0.9656. :  74%|███████▍  | 37/50 [00:21<00:05,  2.21it/s]Train Iter: 388/1000. LR: 0.0243. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0190. T_Loss: 3.0235. Mask: 0.9656. :  76%|███████▌  | 38/50 [00:21<00:07,  1.53it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0173. T_Loss: 3.0149. Mask: 0.9660. :  76%|███████▌  | 38/50 [00:21<00:07,  1.53it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0173. T_Loss: 3.0149. Mask: 0.9660. :  78%|███████▊  | 39/50 [00:21<00:06,  1.82it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.35s. Batch: 0.55s. S_Loss: 1.0162. T_Loss: 3.0115. Mask: 0.9668. :  78%|███████▊  | 39/50 [00:21<00:06,  1.82it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.35s. Batch: 0.55s. S_Loss: 1.0162. T_Loss: 3.0115. Mask: 0.9668. :  80%|████████  | 40/50 [00:21<00:04,  2.04it/s]total : 1000  current step :  388
total : 1000  current step :  389
total : 1000  current step :  390
Train Iter: 391/1000. LR: 0.0244. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0161. T_Loss: 3.0093. Mask: 0.9664. :  80%|████████  | 40/50 [00:23<00:04,  2.04it/s]Train Iter: 391/1000. LR: 0.0244. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0161. T_Loss: 3.0093. Mask: 0.9664. :  82%|████████▏ | 41/50 [00:23<00:06,  1.45it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0159. T_Loss: 3.0168. Mask: 0.9667. :  82%|████████▏ | 41/50 [00:23<00:06,  1.45it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0159. T_Loss: 3.0168. Mask: 0.9667. :  84%|████████▍ | 42/50 [00:23<00:04,  1.68it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0160. T_Loss: 3.0217. Mask: 0.9668. :  84%|████████▍ | 42/50 [00:23<00:04,  1.68it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.36s. Batch: 0.55s. S_Loss: 1.0160. T_Loss: 3.0217. Mask: 0.9668. :  86%|████████▌ | 43/50 [00:23<00:03,  1.96it/s]total : 1000  current step :  391
total : 1000  current step :  392
total : 1000  current step :  393
Train Iter: 394/1000. LR: 0.0246. Data: 0.37s. Batch: 0.57s. S_Loss: 1.0155. T_Loss: 3.0276. Mask: 0.9670. :  86%|████████▌ | 43/50 [00:25<00:03,  1.96it/s]Train Iter: 394/1000. LR: 0.0246. Data: 0.37s. Batch: 0.57s. S_Loss: 1.0155. T_Loss: 3.0276. Mask: 0.9670. :  88%|████████▊ | 44/50 [00:25<00:04,  1.40it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0146. T_Loss: 3.0268. Mask: 0.9668. :  88%|████████▊ | 44/50 [00:25<00:04,  1.40it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0146. T_Loss: 3.0268. Mask: 0.9668. :  90%|█████████ | 45/50 [00:25<00:03,  1.60it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0140. T_Loss: 3.0315. Mask: 0.9673. :  90%|█████████ | 45/50 [00:25<00:03,  1.60it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0140. T_Loss: 3.0315. Mask: 0.9673. :  92%|█████████▏| 46/50 [00:25<00:02,  1.77it/s]total : 1000  current step :  394
total : 1000  current step :  395
total : 1000  current step :  396
Train Iter: 397/1000. LR: 0.0248. Data: 0.38s. Batch: 0.57s. S_Loss: 1.0130. T_Loss: 3.0341. Mask: 0.9673. :  92%|█████████▏| 46/50 [00:27<00:02,  1.77it/s]Train Iter: 397/1000. LR: 0.0248. Data: 0.38s. Batch: 0.57s. S_Loss: 1.0130. T_Loss: 3.0341. Mask: 0.9673. :  94%|█████████▍| 47/50 [00:27<00:02,  1.32it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.37s. Batch: 0.57s. S_Loss: 1.0119. T_Loss: 3.0419. Mask: 0.9675. :  94%|█████████▍| 47/50 [00:27<00:02,  1.32it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.37s. Batch: 0.57s. S_Loss: 1.0119. T_Loss: 3.0419. Mask: 0.9675. :  96%|█████████▌| 48/50 [00:27<00:01,  1.63it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0116. T_Loss: 3.0499. Mask: 0.9672. :  96%|█████████▌| 48/50 [00:27<00:01,  1.63it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.37s. Batch: 0.56s. S_Loss: 1.0116. T_Loss: 3.0499. Mask: 0.9672. :  98%|█████████▊| 49/50 [00:27<00:00,  1.81it/s]total : 1000  current step :  397
total : 1000  current step :  398
total : 1000  current step :  399
Train Iter: 400/1000. LR: 0.0250. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0107. T_Loss: 3.0511. Mask: 0.9676. :  98%|█████████▊| 49/50 [00:29<00:00,  1.81it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0107. T_Loss: 3.0511. Mask: 0.9676. : 100%|██████████| 50/50 [00:29<00:00,  1.22it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0107. T_Loss: 3.0511. Mask: 0.9676. : 100%|██████████| 50/50 [00:29<00:00,  1.71it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 1.3142. top1: 90.23. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 1.3142. top1: 90.23. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.3345. top1: 89.65. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.3345. top1: 89.65. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.08it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.3348. top1: 88.80. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.08it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.3348. top1: 88.80. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.3372. top1: 88.18. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.3372. top1: 88.18. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.67it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.3675. top1: 86.25. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.67it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.3675. top1: 86.25. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.3828. top1: 85.55. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.3828. top1: 85.55. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.3941. top1: 84.38. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.3941. top1: 84.38. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.4032. top1: 83.85. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.4032. top1: 83.85. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.4032. top1: 83.85. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.57it/s]
total : 1000  current step :  400
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.20s. S_Loss: 0.9389. T_Loss: 2.8450. Mask: 0.9766. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.20s. S_Loss: 0.9389. T_Loss: 2.8450. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:09,  4.93it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.21s. S_Loss: 0.9529. T_Loss: 2.7959. Mask: 0.9707. :   2%|▏         | 1/50 [00:00<00:09,  4.93it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.21s. S_Loss: 0.9529. T_Loss: 2.7959. Mask: 0.9707. :   4%|▍         | 2/50 [00:00<00:10,  4.71it/s]total : 1000  current step :  401
total : 1000  current step :  402
Train Iter: 403/1000. LR: 0.0252. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9626. T_Loss: 2.8566. Mask: 0.9753. :   4%|▍         | 2/50 [00:01<00:10,  4.71it/s]Train Iter: 403/1000. LR: 0.0252. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9626. T_Loss: 2.8566. Mask: 0.9753. :   6%|▌         | 3/50 [00:01<00:31,  1.51it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.31s. Batch: 0.51s. S_Loss: 0.9676. T_Loss: 2.9137. Mask: 0.9736. :   6%|▌         | 3/50 [00:02<00:31,  1.51it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.31s. Batch: 0.51s. S_Loss: 0.9676. T_Loss: 2.9137. Mask: 0.9736. :   8%|▊         | 4/50 [00:02<00:26,  1.75it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.30s. Batch: 0.51s. S_Loss: 0.9730. T_Loss: 3.0518. Mask: 0.9719. :   8%|▊         | 4/50 [00:02<00:26,  1.75it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.30s. Batch: 0.51s. S_Loss: 0.9730. T_Loss: 3.0518. Mask: 0.9719. :  10%|█         | 5/50 [00:02<00:24,  1.82it/s]total : 1000  current step :  403
total : 1000  current step :  404
total : 1000  current step :  405
Train Iter: 406/1000. LR: 0.0254. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9715. T_Loss: 3.0441. Mask: 0.9733. :  10%|█         | 5/50 [00:03<00:24,  1.82it/s]Train Iter: 406/1000. LR: 0.0254. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9715. T_Loss: 3.0441. Mask: 0.9733. :  12%|█▏        | 6/50 [00:03<00:35,  1.23it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9712. T_Loss: 3.0943. Mask: 0.9743. :  12%|█▏        | 6/50 [00:04<00:35,  1.23it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9712. T_Loss: 3.0943. Mask: 0.9743. :  14%|█▍        | 7/50 [00:04<00:28,  1.53it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9689. T_Loss: 3.1164. Mask: 0.9736. :  14%|█▍        | 7/50 [00:04<00:28,  1.53it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9689. T_Loss: 3.1164. Mask: 0.9736. :  16%|█▌        | 8/50 [00:04<00:24,  1.74it/s]total : 1000  current step :  406
total : 1000  current step :  407
total : 1000  current step :  408
Train Iter: 409/1000. LR: 0.0256. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9649. T_Loss: 3.1118. Mask: 0.9753. :  16%|█▌        | 8/50 [00:05<00:24,  1.74it/s]Train Iter: 409/1000. LR: 0.0256. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9649. T_Loss: 3.1118. Mask: 0.9753. :  18%|█▊        | 9/50 [00:05<00:30,  1.34it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9708. T_Loss: 3.1252. Mask: 0.9738. :  18%|█▊        | 9/50 [00:06<00:30,  1.34it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9708. T_Loss: 3.1252. Mask: 0.9738. :  20%|██        | 10/50 [00:06<00:26,  1.52it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9705. T_Loss: 3.1007. Mask: 0.9734. :  20%|██        | 10/50 [00:06<00:26,  1.52it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9705. T_Loss: 3.1007. Mask: 0.9734. :  22%|██▏       | 11/50 [00:06<00:21,  1.79it/s]total : 1000  current step :  409
total : 1000  current step :  410
total : 1000  current step :  411
Train Iter: 412/1000. LR: 0.0258. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9688. T_Loss: 3.0962. Mask: 0.9723. :  22%|██▏       | 11/50 [00:07<00:21,  1.79it/s]Train Iter: 412/1000. LR: 0.0258. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9688. T_Loss: 3.0962. Mask: 0.9723. :  24%|██▍       | 12/50 [00:07<00:29,  1.31it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9697. T_Loss: 3.1056. Mask: 0.9712. :  24%|██▍       | 12/50 [00:08<00:29,  1.31it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9697. T_Loss: 3.1056. Mask: 0.9712. :  26%|██▌       | 13/50 [00:08<00:23,  1.57it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9746. T_Loss: 3.1181. Mask: 0.9690. :  26%|██▌       | 13/50 [00:08<00:23,  1.57it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9746. T_Loss: 3.1181. Mask: 0.9690. :  28%|██▊       | 14/50 [00:08<00:18,  1.92it/s]total : 1000  current step :  412
total : 1000  current step :  413
total : 1000  current step :  414
Train Iter: 415/1000. LR: 0.0259. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9754. T_Loss: 3.1632. Mask: 0.9688. :  28%|██▊       | 14/50 [00:09<00:18,  1.92it/s]Train Iter: 415/1000. LR: 0.0259. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9754. T_Loss: 3.1632. Mask: 0.9688. :  30%|███       | 15/50 [00:09<00:24,  1.41it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9765. T_Loss: 3.1849. Mask: 0.9683. :  30%|███       | 15/50 [00:09<00:24,  1.41it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9765. T_Loss: 3.1849. Mask: 0.9683. :  32%|███▏      | 16/50 [00:09<00:20,  1.69it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9767. T_Loss: 3.2169. Mask: 0.9681. :  32%|███▏      | 16/50 [00:10<00:20,  1.69it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9767. T_Loss: 3.2169. Mask: 0.9681. :  34%|███▍      | 17/50 [00:10<00:17,  1.85it/s]total : 1000  current step :  415
total : 1000  current step :  416
total : 1000  current step :  417
Train Iter: 418/1000. LR: 0.0261. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9763. T_Loss: 3.2341. Mask: 0.9681. :  34%|███▍      | 17/50 [00:11<00:17,  1.85it/s]Train Iter: 418/1000. LR: 0.0261. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9763. T_Loss: 3.2341. Mask: 0.9681. :  36%|███▌      | 18/50 [00:11<00:22,  1.42it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9755. T_Loss: 3.2331. Mask: 0.9677. :  36%|███▌      | 18/50 [00:11<00:22,  1.42it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9755. T_Loss: 3.2331. Mask: 0.9677. :  38%|███▊      | 19/50 [00:11<00:17,  1.73it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9741. T_Loss: 3.2307. Mask: 0.9686. :  38%|███▊      | 19/50 [00:12<00:17,  1.73it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9741. T_Loss: 3.2307. Mask: 0.9686. :  40%|████      | 20/50 [00:12<00:16,  1.78it/s]total : 1000  current step :  418
total : 1000  current step :  419
total : 1000  current step :  420
Train Iter: 421/1000. LR: 0.0263. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9719. T_Loss: 3.2230. Mask: 0.9688. :  40%|████      | 20/50 [00:13<00:16,  1.78it/s]Train Iter: 421/1000. LR: 0.0263. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9719. T_Loss: 3.2230. Mask: 0.9688. :  42%|████▏     | 21/50 [00:13<00:23,  1.25it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9724. T_Loss: 3.2276. Mask: 0.9689. :  42%|████▏     | 21/50 [00:13<00:23,  1.25it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9724. T_Loss: 3.2276. Mask: 0.9689. :  44%|████▍     | 22/50 [00:13<00:18,  1.49it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9729. T_Loss: 3.2394. Mask: 0.9696. :  44%|████▍     | 22/50 [00:14<00:18,  1.49it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9729. T_Loss: 3.2394. Mask: 0.9696. :  46%|████▌     | 23/50 [00:14<00:15,  1.77it/s]total : 1000  current step :  421
total : 1000  current step :  422
total : 1000  current step :  423
Train Iter: 424/1000. LR: 0.0265. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9736. T_Loss: 3.2295. Mask: 0.9699. :  46%|████▌     | 23/50 [00:15<00:15,  1.77it/s]Train Iter: 424/1000. LR: 0.0265. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9736. T_Loss: 3.2295. Mask: 0.9699. :  48%|████▊     | 24/50 [00:15<00:18,  1.37it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9736. T_Loss: 3.2239. Mask: 0.9702. :  48%|████▊     | 24/50 [00:15<00:18,  1.37it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9736. T_Loss: 3.2239. Mask: 0.9702. :  50%|█████     | 25/50 [00:15<00:15,  1.62it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9728. T_Loss: 3.2176. Mask: 0.9704. :  50%|█████     | 25/50 [00:16<00:15,  1.62it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9728. T_Loss: 3.2176. Mask: 0.9704. :  52%|█████▏    | 26/50 [00:16<00:12,  1.86it/s]total : 1000  current step :  424
total : 1000  current step :  425
total : 1000  current step :  426
Train Iter: 427/1000. LR: 0.0267. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9718. T_Loss: 3.2153. Mask: 0.9703. :  52%|█████▏    | 26/50 [00:17<00:12,  1.86it/s]Train Iter: 427/1000. LR: 0.0267. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9718. T_Loss: 3.2153. Mask: 0.9703. :  54%|█████▍    | 27/50 [00:17<00:16,  1.39it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9717. T_Loss: 3.2192. Mask: 0.9704. :  54%|█████▍    | 27/50 [00:17<00:16,  1.39it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9717. T_Loss: 3.2192. Mask: 0.9704. :  56%|█████▌    | 28/50 [00:17<00:14,  1.54it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9723. T_Loss: 3.2177. Mask: 0.9710. :  56%|█████▌    | 28/50 [00:17<00:14,  1.54it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9723. T_Loss: 3.2177. Mask: 0.9710. :  58%|█████▊    | 29/50 [00:17<00:11,  1.82it/s]total : 1000  current step :  427
total : 1000  current step :  428
total : 1000  current step :  429
Train Iter: 430/1000. LR: 0.0269. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9718. T_Loss: 3.2133. Mask: 0.9710. :  58%|█████▊    | 29/50 [00:19<00:11,  1.82it/s]Train Iter: 430/1000. LR: 0.0269. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9718. T_Loss: 3.2133. Mask: 0.9710. :  60%|██████    | 30/50 [00:19<00:14,  1.39it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9730. T_Loss: 3.2175. Mask: 0.9706. :  60%|██████    | 30/50 [00:19<00:14,  1.39it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9730. T_Loss: 3.2175. Mask: 0.9706. :  62%|██████▏   | 31/50 [00:19<00:11,  1.69it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9737. T_Loss: 3.2434. Mask: 0.9705. :  62%|██████▏   | 31/50 [00:19<00:11,  1.69it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9737. T_Loss: 3.2434. Mask: 0.9705. :  64%|██████▍   | 32/50 [00:19<00:09,  1.82it/s]total : 1000  current step :  430
total : 1000  current step :  431
total : 1000  current step :  432
Train Iter: 433/1000. LR: 0.0271. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9746. T_Loss: 3.2577. Mask: 0.9701. :  64%|██████▍   | 32/50 [00:20<00:09,  1.82it/s]Train Iter: 433/1000. LR: 0.0271. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9746. T_Loss: 3.2577. Mask: 0.9701. :  66%|██████▌   | 33/50 [00:20<00:12,  1.37it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9767. T_Loss: 3.2711. Mask: 0.9696. :  66%|██████▌   | 33/50 [00:21<00:12,  1.37it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9767. T_Loss: 3.2711. Mask: 0.9696. :  68%|██████▊   | 34/50 [00:21<00:09,  1.67it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9777. T_Loss: 3.2822. Mask: 0.9690. :  68%|██████▊   | 34/50 [00:21<00:09,  1.67it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9777. T_Loss: 3.2822. Mask: 0.9690. :  70%|███████   | 35/50 [00:21<00:07,  1.89it/s]total : 1000  current step :  433
total : 1000  current step :  434
total : 1000  current step :  435
Train Iter: 436/1000. LR: 0.0273. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9778. T_Loss: 3.2933. Mask: 0.9689. :  70%|███████   | 35/50 [00:22<00:07,  1.89it/s]Train Iter: 436/1000. LR: 0.0273. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9778. T_Loss: 3.2933. Mask: 0.9689. :  72%|███████▏  | 36/50 [00:22<00:10,  1.38it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9769. T_Loss: 3.2901. Mask: 0.9694. :  72%|███████▏  | 36/50 [00:23<00:10,  1.38it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9769. T_Loss: 3.2901. Mask: 0.9694. :  74%|███████▍  | 37/50 [00:23<00:07,  1.64it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9780. T_Loss: 3.2938. Mask: 0.9690. :  74%|███████▍  | 37/50 [00:23<00:07,  1.64it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9780. T_Loss: 3.2938. Mask: 0.9690. :  76%|███████▌  | 38/50 [00:23<00:06,  1.87it/s]total : 1000  current step :  436
total : 1000  current step :  437
total : 1000  current step :  438
Train Iter: 439/1000. LR: 0.0274. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9775. T_Loss: 3.3011. Mask: 0.9693. :  76%|███████▌  | 38/50 [00:24<00:06,  1.87it/s]Train Iter: 439/1000. LR: 0.0274. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9775. T_Loss: 3.3011. Mask: 0.9693. :  78%|███████▊  | 39/50 [00:24<00:07,  1.43it/s]total : 1000  current step :  439
Train Iter: 440/1000. LR: 0.0275. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9763. T_Loss: 3.2991. Mask: 0.9694. :  78%|███████▊  | 39/50 [00:25<00:07,  1.43it/s]Train Iter: 440/1000. LR: 0.0275. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9763. T_Loss: 3.2991. Mask: 0.9694. :  80%|████████  | 40/50 [00:25<00:07,  1.39it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9764. T_Loss: 3.2974. Mask: 0.9694. :  80%|████████  | 40/50 [00:25<00:07,  1.39it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9764. T_Loss: 3.2974. Mask: 0.9694. :  82%|████████▏ | 41/50 [00:25<00:06,  1.46it/s]total : 1000  current step :  440
total : 1000  current step :  441
Train Iter: 442/1000. LR: 0.0276. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9763. T_Loss: 3.2958. Mask: 0.9693. :  82%|████████▏ | 41/50 [00:26<00:06,  1.46it/s]Train Iter: 442/1000. LR: 0.0276. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9763. T_Loss: 3.2958. Mask: 0.9693. :  84%|████████▍ | 42/50 [00:26<00:06,  1.31it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9758. T_Loss: 3.2939. Mask: 0.9693. :  84%|████████▍ | 42/50 [00:27<00:06,  1.31it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9758. T_Loss: 3.2939. Mask: 0.9693. :  86%|████████▌ | 43/50 [00:27<00:04,  1.64it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9751. T_Loss: 3.2900. Mask: 0.9694. :  86%|████████▌ | 43/50 [00:27<00:04,  1.64it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9751. T_Loss: 3.2900. Mask: 0.9694. :  88%|████████▊ | 44/50 [00:27<00:03,  1.84it/s]total : 1000  current step :  442
total : 1000  current step :  443
total : 1000  current step :  444
Train Iter: 445/1000. LR: 0.0278. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9746. T_Loss: 3.2835. Mask: 0.9696. :  88%|████████▊ | 44/50 [00:28<00:03,  1.84it/s]Train Iter: 445/1000. LR: 0.0278. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9746. T_Loss: 3.2835. Mask: 0.9696. :  90%|█████████ | 45/50 [00:28<00:03,  1.40it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9739. T_Loss: 3.2768. Mask: 0.9699. :  90%|█████████ | 45/50 [00:29<00:03,  1.40it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9739. T_Loss: 3.2768. Mask: 0.9699. :  92%|█████████▏| 46/50 [00:29<00:02,  1.59it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9737. T_Loss: 3.2789. Mask: 0.9702. :  92%|█████████▏| 46/50 [00:29<00:02,  1.59it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9737. T_Loss: 3.2789. Mask: 0.9702. :  94%|█████████▍| 47/50 [00:29<00:01,  1.82it/s]total : 1000  current step :  445
total : 1000  current step :  446
total : 1000  current step :  447
Train Iter: 448/1000. LR: 0.0280. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9729. T_Loss: 3.2764. Mask: 0.9701. :  94%|█████████▍| 47/50 [00:30<00:01,  1.82it/s]Train Iter: 448/1000. LR: 0.0280. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9729. T_Loss: 3.2764. Mask: 0.9701. :  96%|█████████▌| 48/50 [00:30<00:01,  1.44it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9732. T_Loss: 3.2908. Mask: 0.9699. :  96%|█████████▌| 48/50 [00:30<00:01,  1.44it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9732. T_Loss: 3.2908. Mask: 0.9699. :  98%|█████████▊| 49/50 [00:30<00:00,  1.68it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9733. T_Loss: 3.3003. Mask: 0.9701. :  98%|█████████▊| 49/50 [00:31<00:00,  1.68it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9733. T_Loss: 3.3003. Mask: 0.9701. : 100%|██████████| 50/50 [00:31<00:00,  1.94it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9733. T_Loss: 3.3003. Mask: 0.9701. : 100%|██████████| 50/50 [00:31<00:00,  1.60it/s]
total : 1000  current step :  448
total : 1000  current step :  449
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 1.1511. top1: 91.02. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 1.1511. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.1720. top1: 90.43. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.1720. top1: 90.43. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.1727. top1: 89.71. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.1727. top1: 89.71. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.01it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1764. top1: 89.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.01it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1764. top1: 89.36. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2098. top1: 88.12. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2098. top1: 88.12. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2249. top1: 87.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2249. top1: 87.70. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.2360. top1: 87.05. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.2360. top1: 87.05. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.2454. top1: 86.60. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.2454. top1: 86.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.2454. top1: 86.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.15it/s]
total : 1000  current step :  450
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.74s. Batch: 1.01s. S_Loss: 0.9975. T_Loss: 3.3330. Mask: 0.9531. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.74s. Batch: 1.01s. S_Loss: 0.9975. T_Loss: 3.3330. Mask: 0.9531. :   2%|▏         | 1/50 [00:01<00:49,  1.01s/it]Train Iter: 452/1000. LR: 0.0282. Data: 0.40s. Batch: 0.63s. S_Loss: 1.0119. T_Loss: 3.6346. Mask: 0.9609. :   2%|▏         | 1/50 [00:01<00:49,  1.01s/it]Train Iter: 452/1000. LR: 0.0282. Data: 0.40s. Batch: 0.63s. S_Loss: 1.0119. T_Loss: 3.6346. Mask: 0.9609. :   4%|▍         | 2/50 [00:01<00:27,  1.77it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.29s. Batch: 0.51s. S_Loss: 1.0107. T_Loss: 3.8292. Mask: 0.9661. :   4%|▍         | 2/50 [00:01<00:27,  1.77it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.29s. Batch: 0.51s. S_Loss: 1.0107. T_Loss: 3.8292. Mask: 0.9661. :   6%|▌         | 3/50 [00:01<00:19,  2.35it/s]total : 1000  current step :  451
total : 1000  current step :  452
total : 1000  current step :  453
Train Iter: 454/1000. LR: 0.0284. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9965. T_Loss: 3.7684. Mask: 0.9697. :   6%|▌         | 3/50 [00:02<00:19,  2.35it/s]Train Iter: 454/1000. LR: 0.0284. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9965. T_Loss: 3.7684. Mask: 0.9697. :   8%|▊         | 4/50 [00:02<00:32,  1.43it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9926. T_Loss: 3.7220. Mask: 0.9695. :   8%|▊         | 4/50 [00:03<00:32,  1.43it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9926. T_Loss: 3.7220. Mask: 0.9695. :  10%|█         | 5/50 [00:03<00:27,  1.65it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.33s. Batch: 0.56s. S_Loss: 0.9933. T_Loss: 3.6546. Mask: 0.9701. :  10%|█         | 5/50 [00:03<00:27,  1.65it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.33s. Batch: 0.56s. S_Loss: 0.9933. T_Loss: 3.6546. Mask: 0.9701. :  12%|█▏        | 6/50 [00:03<00:21,  2.00it/s]total : 1000  current step :  454
total : 1000  current step :  455
total : 1000  current step :  456
Train Iter: 457/1000. LR: 0.0286. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9912. T_Loss: 3.6109. Mask: 0.9715. :  12%|█▏        | 6/50 [00:04<00:21,  2.00it/s]Train Iter: 457/1000. LR: 0.0286. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9912. T_Loss: 3.6109. Mask: 0.9715. :  14%|█▍        | 7/50 [00:04<00:29,  1.46it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9845. T_Loss: 3.5366. Mask: 0.9731. :  14%|█▍        | 7/50 [00:04<00:29,  1.46it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9845. T_Loss: 3.5366. Mask: 0.9731. :  16%|█▌        | 8/50 [00:04<00:24,  1.70it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9776. T_Loss: 3.4985. Mask: 0.9744. :  16%|█▌        | 8/50 [00:05<00:24,  1.70it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9776. T_Loss: 3.4985. Mask: 0.9744. :  18%|█▊        | 9/50 [00:05<00:22,  1.85it/s]total : 1000  current step :  457
total : 1000  current step :  458
total : 1000  current step :  459
Train Iter: 460/1000. LR: 0.0287. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9727. T_Loss: 3.4842. Mask: 0.9742. :  18%|█▊        | 9/50 [00:06<00:22,  1.85it/s]Train Iter: 460/1000. LR: 0.0287. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9727. T_Loss: 3.4842. Mask: 0.9742. :  20%|██        | 10/50 [00:06<00:28,  1.40it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9719. T_Loss: 3.4690. Mask: 0.9741. :  20%|██        | 10/50 [00:06<00:28,  1.40it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9719. T_Loss: 3.4690. Mask: 0.9741. :  22%|██▏       | 11/50 [00:06<00:23,  1.64it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.38s. Batch: 0.60s. S_Loss: 0.9701. T_Loss: 3.4332. Mask: 0.9746. :  22%|██▏       | 11/50 [00:07<00:23,  1.64it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.38s. Batch: 0.60s. S_Loss: 0.9701. T_Loss: 3.4332. Mask: 0.9746. :  24%|██▍       | 12/50 [00:07<00:21,  1.76it/s]total : 1000  current step :  460
total : 1000  current step :  461
total : 1000  current step :  462
Train Iter: 463/1000. LR: 0.0289. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9678. T_Loss: 3.4128. Mask: 0.9742. :  24%|██▍       | 12/50 [00:08<00:21,  1.76it/s]Train Iter: 463/1000. LR: 0.0289. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9678. T_Loss: 3.4128. Mask: 0.9742. :  26%|██▌       | 13/50 [00:08<00:27,  1.36it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9653. T_Loss: 3.3926. Mask: 0.9746. :  26%|██▌       | 13/50 [00:08<00:27,  1.36it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9653. T_Loss: 3.3926. Mask: 0.9746. :  28%|██▊       | 14/50 [00:08<00:23,  1.53it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9669. T_Loss: 3.3651. Mask: 0.9740. :  28%|██▊       | 14/50 [00:09<00:23,  1.53it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9669. T_Loss: 3.3651. Mask: 0.9740. :  30%|███       | 15/50 [00:09<00:19,  1.80it/s]total : 1000  current step :  463
total : 1000  current step :  464
total : 1000  current step :  465
Train Iter: 466/1000. LR: 0.0291. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9657. T_Loss: 3.3635. Mask: 0.9749. :  30%|███       | 15/50 [00:10<00:19,  1.80it/s]Train Iter: 466/1000. LR: 0.0291. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9657. T_Loss: 3.3635. Mask: 0.9749. :  32%|███▏      | 16/50 [00:10<00:27,  1.25it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9671. T_Loss: 3.4092. Mask: 0.9745. :  32%|███▏      | 16/50 [00:10<00:27,  1.25it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9671. T_Loss: 3.4092. Mask: 0.9745. :  34%|███▍      | 17/50 [00:10<00:20,  1.59it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9683. T_Loss: 3.4249. Mask: 0.9740. :  34%|███▍      | 17/50 [00:11<00:20,  1.59it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9683. T_Loss: 3.4249. Mask: 0.9740. :  36%|███▌      | 18/50 [00:11<00:17,  1.86it/s]total : 1000  current step :  466
total : 1000  current step :  467
total : 1000  current step :  468
Train Iter: 469/1000. LR: 0.0293. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9677. T_Loss: 3.4057. Mask: 0.9741. :  36%|███▌      | 18/50 [00:12<00:17,  1.86it/s]Train Iter: 469/1000. LR: 0.0293. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9677. T_Loss: 3.4057. Mask: 0.9741. :  38%|███▊      | 19/50 [00:12<00:22,  1.40it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9663. T_Loss: 3.4270. Mask: 0.9742. :  38%|███▊      | 19/50 [00:12<00:22,  1.40it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9663. T_Loss: 3.4270. Mask: 0.9742. :  40%|████      | 20/50 [00:12<00:19,  1.57it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9644. T_Loss: 3.4316. Mask: 0.9741. :  40%|████      | 20/50 [00:13<00:19,  1.57it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9644. T_Loss: 3.4316. Mask: 0.9741. :  42%|████▏     | 21/50 [00:13<00:16,  1.78it/s]total : 1000  current step :  469
total : 1000  current step :  470
total : 1000  current step :  471
Train Iter: 472/1000. LR: 0.0295. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9651. T_Loss: 3.4382. Mask: 0.9744. :  42%|████▏     | 21/50 [00:14<00:16,  1.78it/s]Train Iter: 472/1000. LR: 0.0295. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9651. T_Loss: 3.4382. Mask: 0.9744. :  44%|████▍     | 22/50 [00:14<00:20,  1.36it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9643. T_Loss: 3.4296. Mask: 0.9745. :  44%|████▍     | 22/50 [00:14<00:20,  1.36it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9643. T_Loss: 3.4296. Mask: 0.9745. :  46%|████▌     | 23/50 [00:14<00:17,  1.57it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9626. T_Loss: 3.4451. Mask: 0.9744. :  46%|████▌     | 23/50 [00:14<00:17,  1.57it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9626. T_Loss: 3.4451. Mask: 0.9744. :  48%|████▊     | 24/50 [00:14<00:13,  1.88it/s]total : 1000  current step :  472
total : 1000  current step :  473
total : 1000  current step :  474
Train Iter: 475/1000. LR: 0.0297. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9603. T_Loss: 3.4349. Mask: 0.9752. :  48%|████▊     | 24/50 [00:15<00:13,  1.88it/s]Train Iter: 475/1000. LR: 0.0297. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9603. T_Loss: 3.4349. Mask: 0.9752. :  50%|█████     | 25/50 [00:15<00:17,  1.40it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9593. T_Loss: 3.4287. Mask: 0.9755. :  50%|█████     | 25/50 [00:16<00:17,  1.40it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9593. T_Loss: 3.4287. Mask: 0.9755. :  52%|█████▏    | 26/50 [00:16<00:14,  1.61it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9604. T_Loss: 3.4204. Mask: 0.9737. :  52%|█████▏    | 26/50 [00:16<00:14,  1.61it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9604. T_Loss: 3.4204. Mask: 0.9737. :  54%|█████▍    | 27/50 [00:16<00:12,  1.84it/s]total : 1000  current step :  475
total : 1000  current step :  476
total : 1000  current step :  477
Train Iter: 478/1000. LR: 0.0299. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9605. T_Loss: 3.4243. Mask: 0.9736. :  54%|█████▍    | 27/50 [00:17<00:12,  1.84it/s]Train Iter: 478/1000. LR: 0.0299. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9605. T_Loss: 3.4243. Mask: 0.9736. :  56%|█████▌    | 28/50 [00:17<00:16,  1.37it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9592. T_Loss: 3.4187. Mask: 0.9731. :  56%|█████▌    | 28/50 [00:18<00:16,  1.37it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9592. T_Loss: 3.4187. Mask: 0.9731. :  58%|█████▊    | 29/50 [00:18<00:12,  1.65it/s]total : 1000  current step :  478
total : 1000  current step :  479
Train Iter: 480/1000. LR: 0.0300. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9585. T_Loss: 3.4200. Mask: 0.9730. :  58%|█████▊    | 29/50 [00:19<00:12,  1.65it/s]Train Iter: 480/1000. LR: 0.0300. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9585. T_Loss: 3.4200. Mask: 0.9730. :  60%|██████    | 30/50 [00:19<00:13,  1.45it/s]total : 1000  current step :  480
Train Iter: 481/1000. LR: 0.0301. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9583. T_Loss: 3.4374. Mask: 0.9732. :  60%|██████    | 30/50 [00:20<00:13,  1.45it/s]Train Iter: 481/1000. LR: 0.0301. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9583. T_Loss: 3.4374. Mask: 0.9732. :  62%|██████▏   | 31/50 [00:20<00:16,  1.17it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9590. T_Loss: 3.4558. Mask: 0.9733. :  62%|██████▏   | 31/50 [00:20<00:16,  1.17it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9590. T_Loss: 3.4558. Mask: 0.9733. :  64%|██████▍   | 32/50 [00:20<00:12,  1.45it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9584. T_Loss: 3.4695. Mask: 0.9735. :  64%|██████▍   | 32/50 [00:21<00:12,  1.45it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9584. T_Loss: 3.4695. Mask: 0.9735. :  66%|██████▌   | 33/50 [00:21<00:10,  1.63it/s]total : 1000  current step :  481
total : 1000  current step :  482
total : 1000  current step :  483
Train Iter: 484/1000. LR: 0.0302. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9576. T_Loss: 3.4730. Mask: 0.9733. :  66%|██████▌   | 33/50 [00:22<00:10,  1.63it/s]Train Iter: 484/1000. LR: 0.0302. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9576. T_Loss: 3.4730. Mask: 0.9733. :  68%|██████▊   | 34/50 [00:22<00:12,  1.24it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9579. T_Loss: 3.4824. Mask: 0.9734. :  68%|██████▊   | 34/50 [00:22<00:12,  1.24it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9579. T_Loss: 3.4824. Mask: 0.9734. :  70%|███████   | 35/50 [00:22<00:10,  1.47it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9563. T_Loss: 3.4773. Mask: 0.9738. :  70%|███████   | 35/50 [00:23<00:10,  1.47it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9563. T_Loss: 3.4773. Mask: 0.9738. :  72%|███████▏  | 36/50 [00:23<00:08,  1.64it/s]total : 1000  current step :  484
total : 1000  current step :  485
total : 1000  current step :  486
Train Iter: 487/1000. LR: 0.0304. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9564. T_Loss: 3.4796. Mask: 0.9739. :  72%|███████▏  | 36/50 [00:24<00:08,  1.64it/s]Train Iter: 487/1000. LR: 0.0304. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9564. T_Loss: 3.4796. Mask: 0.9739. :  74%|███████▍  | 37/50 [00:24<00:10,  1.29it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9552. T_Loss: 3.4783. Mask: 0.9735. :  74%|███████▍  | 37/50 [00:24<00:10,  1.29it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9552. T_Loss: 3.4783. Mask: 0.9735. :  76%|███████▌  | 38/50 [00:24<00:08,  1.46it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9551. T_Loss: 3.4867. Mask: 0.9736. :  76%|███████▌  | 38/50 [00:25<00:08,  1.46it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9551. T_Loss: 3.4867. Mask: 0.9736. :  78%|███████▊  | 39/50 [00:25<00:06,  1.73it/s]total : 1000  current step :  487
total : 1000  current step :  488
total : 1000  current step :  489
Train Iter: 490/1000. LR: 0.0306. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9540. T_Loss: 3.4823. Mask: 0.9734. :  78%|███████▊  | 39/50 [00:26<00:06,  1.73it/s]Train Iter: 490/1000. LR: 0.0306. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9540. T_Loss: 3.4823. Mask: 0.9734. :  80%|████████  | 40/50 [00:26<00:07,  1.42it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9539. T_Loss: 3.4820. Mask: 0.9736. :  80%|████████  | 40/50 [00:26<00:07,  1.42it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9539. T_Loss: 3.4820. Mask: 0.9736. :  82%|████████▏ | 41/50 [00:26<00:05,  1.56it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9526. T_Loss: 3.4779. Mask: 0.9734. :  82%|████████▏ | 41/50 [00:26<00:05,  1.56it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9526. T_Loss: 3.4779. Mask: 0.9734. :  84%|████████▍ | 42/50 [00:26<00:04,  1.79it/s]total : 1000  current step :  490
total : 1000  current step :  491
total : 1000  current step :  492
Train Iter: 493/1000. LR: 0.0308. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9525. T_Loss: 3.4696. Mask: 0.9733. :  84%|████████▍ | 42/50 [00:28<00:04,  1.79it/s]Train Iter: 493/1000. LR: 0.0308. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9525. T_Loss: 3.4696. Mask: 0.9733. :  86%|████████▌ | 43/50 [00:28<00:05,  1.32it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9523. T_Loss: 3.4669. Mask: 0.9737. :  86%|████████▌ | 43/50 [00:28<00:05,  1.32it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9523. T_Loss: 3.4669. Mask: 0.9737. :  88%|████████▊ | 44/50 [00:28<00:03,  1.58it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9525. T_Loss: 3.4673. Mask: 0.9735. :  88%|████████▊ | 44/50 [00:29<00:03,  1.58it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9525. T_Loss: 3.4673. Mask: 0.9735. :  90%|█████████ | 45/50 [00:29<00:02,  1.74it/s]total : 1000  current step :  493
total : 1000  current step :  494
total : 1000  current step :  495
Train Iter: 496/1000. LR: 0.0310. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9527. T_Loss: 3.4697. Mask: 0.9738. :  90%|█████████ | 45/50 [00:30<00:02,  1.74it/s]Train Iter: 496/1000. LR: 0.0310. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9527. T_Loss: 3.4697. Mask: 0.9738. :  92%|█████████▏| 46/50 [00:30<00:03,  1.32it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9540. T_Loss: 3.4643. Mask: 0.9735. :  92%|█████████▏| 46/50 [00:30<00:03,  1.32it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9540. T_Loss: 3.4643. Mask: 0.9735. :  94%|█████████▍| 47/50 [00:30<00:02,  1.47it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9545. T_Loss: 3.4739. Mask: 0.9736. :  94%|█████████▍| 47/50 [00:31<00:02,  1.47it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9545. T_Loss: 3.4739. Mask: 0.9736. :  96%|█████████▌| 48/50 [00:31<00:01,  1.73it/s]total : 1000  current step :  496
total : 1000  current step :  497
total : 1000  current step :  498
Train Iter: 499/1000. LR: 0.0312. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9543. T_Loss: 3.4797. Mask: 0.9736. :  96%|█████████▌| 48/50 [00:32<00:01,  1.73it/s]Train Iter: 499/1000. LR: 0.0312. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9543. T_Loss: 3.4797. Mask: 0.9736. :  98%|█████████▊| 49/50 [00:32<00:00,  1.25it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9540. T_Loss: 3.4855. Mask: 0.9740. :  98%|█████████▊| 49/50 [00:32<00:00,  1.25it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9540. T_Loss: 3.4855. Mask: 0.9740. : 100%|██████████| 50/50 [00:32<00:00,  1.43it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9540. T_Loss: 3.4855. Mask: 0.9740. : 100%|██████████| 50/50 [00:32<00:00,  1.52it/s]
total : 1000  current step :  499
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 1.0675. top1: 90.62. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 1.0675. top1: 90.62. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0890. top1: 90.04. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0890. top1: 90.04. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0907. top1: 88.80. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0907. top1: 88.80. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0939. top1: 88.38. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0939. top1: 88.38. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.1141. top1: 87.58. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.1141. top1: 87.58. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1189. top1: 87.57. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1189. top1: 87.57. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.48it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1227. top1: 87.56. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.48it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1227. top1: 87.56. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1283. top1: 87.35. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1283. top1: 87.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.90it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1283. top1: 87.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]
total : 1000  current step :  500
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.17s. S_Loss: 0.9554. T_Loss: 3.8176. Mask: 0.9805. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.17s. S_Loss: 0.9554. T_Loss: 3.8176. Mask: 0.9805. :   2%|▏         | 1/50 [00:00<00:08,  5.69it/s]total : 1000  current step :  501
Train Iter: 502/1000. LR: 0.0314. Data: 0.55s. Batch: 0.75s. S_Loss: 0.9536. T_Loss: 3.7320. Mask: 0.9746. :   2%|▏         | 1/50 [00:01<00:08,  5.69it/s]Train Iter: 502/1000. LR: 0.0314. Data: 0.55s. Batch: 0.75s. S_Loss: 0.9536. T_Loss: 3.7320. Mask: 0.9746. :   4%|▍         | 2/50 [00:01<00:40,  1.17it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9560. T_Loss: 3.8807. Mask: 0.9701. :   4%|▍         | 2/50 [00:01<00:40,  1.17it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9560. T_Loss: 3.8807. Mask: 0.9701. :   6%|▌         | 3/50 [00:01<00:31,  1.47it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9535. T_Loss: 3.8078. Mask: 0.9727. :   6%|▌         | 3/50 [00:02<00:31,  1.47it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9535. T_Loss: 3.8078. Mask: 0.9727. :   8%|▊         | 4/50 [00:02<00:24,  1.85it/s]total : 1000  current step :  502
total : 1000  current step :  503
total : 1000  current step :  504
Train Iter: 505/1000. LR: 0.0316. Data: 0.50s. Batch: 0.70s. S_Loss: 0.9674. T_Loss: 3.8949. Mask: 0.9742. :   8%|▊         | 4/50 [00:03<00:24,  1.85it/s]Train Iter: 505/1000. LR: 0.0316. Data: 0.50s. Batch: 0.70s. S_Loss: 0.9674. T_Loss: 3.8949. Mask: 0.9742. :  10%|█         | 5/50 [00:03<00:34,  1.30it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9641. T_Loss: 3.8710. Mask: 0.9733. :  10%|█         | 5/50 [00:03<00:34,  1.30it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9641. T_Loss: 3.8710. Mask: 0.9733. :  12%|█▏        | 6/50 [00:03<00:28,  1.56it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9642. T_Loss: 3.8132. Mask: 0.9721. :  12%|█▏        | 6/50 [00:04<00:28,  1.56it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9642. T_Loss: 3.8132. Mask: 0.9721. :  14%|█▍        | 7/50 [00:04<00:23,  1.80it/s]total : 1000  current step :  505
total : 1000  current step :  506
total : 1000  current step :  507
Train Iter: 508/1000. LR: 0.0318. Data: 0.47s. Batch: 0.68s. S_Loss: 0.9585. T_Loss: 3.7708. Mask: 0.9717. :  14%|█▍        | 7/50 [00:05<00:23,  1.80it/s]Train Iter: 508/1000. LR: 0.0318. Data: 0.47s. Batch: 0.68s. S_Loss: 0.9585. T_Loss: 3.7708. Mask: 0.9717. :  16%|█▌        | 8/50 [00:05<00:31,  1.33it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9590. T_Loss: 3.7080. Mask: 0.9709. :  16%|█▌        | 8/50 [00:05<00:31,  1.33it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9590. T_Loss: 3.7080. Mask: 0.9709. :  18%|█▊        | 9/50 [00:05<00:25,  1.58it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9587. T_Loss: 3.6770. Mask: 0.9684. :  18%|█▊        | 9/50 [00:06<00:25,  1.58it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9587. T_Loss: 3.6770. Mask: 0.9684. :  20%|██        | 10/50 [00:06<00:21,  1.88it/s]total : 1000  current step :  508
total : 1000  current step :  509
total : 1000  current step :  510
Train Iter: 511/1000. LR: 0.0319. Data: 0.47s. Batch: 0.66s. S_Loss: 0.9598. T_Loss: 3.6412. Mask: 0.9684. :  20%|██        | 10/50 [00:07<00:21,  1.88it/s]Train Iter: 511/1000. LR: 0.0319. Data: 0.47s. Batch: 0.66s. S_Loss: 0.9598. T_Loss: 3.6412. Mask: 0.9684. :  22%|██▏       | 11/50 [00:07<00:28,  1.34it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9551. T_Loss: 3.5748. Mask: 0.9684. :  22%|██▏       | 11/50 [00:07<00:28,  1.34it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9551. T_Loss: 3.5748. Mask: 0.9684. :  24%|██▍       | 12/50 [00:07<00:24,  1.53it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9592. T_Loss: 3.6381. Mask: 0.9697. :  24%|██▍       | 12/50 [00:08<00:24,  1.53it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9592. T_Loss: 3.6381. Mask: 0.9697. :  26%|██▌       | 13/50 [00:08<00:20,  1.81it/s]total : 1000  current step :  511
total : 1000  current step :  512
total : 1000  current step :  513
Train Iter: 514/1000. LR: 0.0321. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9626. T_Loss: 3.7373. Mask: 0.9707. :  26%|██▌       | 13/50 [00:09<00:20,  1.81it/s]Train Iter: 514/1000. LR: 0.0321. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9626. T_Loss: 3.7373. Mask: 0.9707. :  28%|██▊       | 14/50 [00:09<00:26,  1.38it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9637. T_Loss: 3.7483. Mask: 0.9711. :  28%|██▊       | 14/50 [00:09<00:26,  1.38it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9637. T_Loss: 3.7483. Mask: 0.9711. :  30%|███       | 15/50 [00:09<00:20,  1.71it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9606. T_Loss: 3.7142. Mask: 0.9705. :  30%|███       | 15/50 [00:09<00:20,  1.71it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9606. T_Loss: 3.7142. Mask: 0.9705. :  32%|███▏      | 16/50 [00:09<00:17,  1.95it/s]total : 1000  current step :  514
total : 1000  current step :  515
total : 1000  current step :  516
Train Iter: 517/1000. LR: 0.0323. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9580. T_Loss: 3.6944. Mask: 0.9713. :  32%|███▏      | 16/50 [00:10<00:17,  1.95it/s]Train Iter: 517/1000. LR: 0.0323. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9580. T_Loss: 3.6944. Mask: 0.9713. :  34%|███▍      | 17/50 [00:10<00:23,  1.42it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9562. T_Loss: 3.6701. Mask: 0.9711. :  34%|███▍      | 17/50 [00:11<00:23,  1.42it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.42s. Batch: 0.63s. S_Loss: 0.9562. T_Loss: 3.6701. Mask: 0.9711. :  36%|███▌      | 18/50 [00:11<00:19,  1.63it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9549. T_Loss: 3.6622. Mask: 0.9716. :  36%|███▌      | 18/50 [00:11<00:19,  1.63it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9549. T_Loss: 3.6622. Mask: 0.9716. :  38%|███▊      | 19/50 [00:11<00:16,  1.83it/s]total : 1000  current step :  517
total : 1000  current step :  518
total : 1000  current step :  519
Train Iter: 520/1000. LR: 0.0325. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9525. T_Loss: 3.6277. Mask: 0.9717. :  38%|███▊      | 19/50 [00:13<00:16,  1.83it/s]Train Iter: 520/1000. LR: 0.0325. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9525. T_Loss: 3.6277. Mask: 0.9717. :  40%|████      | 20/50 [00:13<00:22,  1.32it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9541. T_Loss: 3.6196. Mask: 0.9712. :  40%|████      | 20/50 [00:13<00:22,  1.32it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9541. T_Loss: 3.6196. Mask: 0.9712. :  42%|████▏     | 21/50 [00:13<00:21,  1.37it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9516. T_Loss: 3.5963. Mask: 0.9718. :  42%|████▏     | 21/50 [00:14<00:21,  1.37it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9516. T_Loss: 3.5963. Mask: 0.9718. :  44%|████▍     | 22/50 [00:14<00:18,  1.55it/s]total : 1000  current step :  520
total : 1000  current step :  521
total : 1000  current step :  522
Train Iter: 523/1000. LR: 0.0327. Data: 0.46s. Batch: 0.66s. S_Loss: 0.9513. T_Loss: 3.5840. Mask: 0.9718. :  44%|████▍     | 22/50 [00:15<00:18,  1.55it/s]Train Iter: 523/1000. LR: 0.0327. Data: 0.46s. Batch: 0.66s. S_Loss: 0.9513. T_Loss: 3.5840. Mask: 0.9718. :  46%|████▌     | 23/50 [00:15<00:20,  1.32it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9507. T_Loss: 3.5706. Mask: 0.9725. :  46%|████▌     | 23/50 [00:15<00:20,  1.32it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9507. T_Loss: 3.5706. Mask: 0.9725. :  48%|████▊     | 24/50 [00:15<00:15,  1.65it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9486. T_Loss: 3.5427. Mask: 0.9728. :  48%|████▊     | 24/50 [00:15<00:15,  1.65it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9486. T_Loss: 3.5427. Mask: 0.9728. :  50%|█████     | 25/50 [00:15<00:12,  1.95it/s]total : 1000  current step :  523
total : 1000  current step :  524
total : 1000  current step :  525
Train Iter: 526/1000. LR: 0.0329. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9485. T_Loss: 3.5485. Mask: 0.9733. :  50%|█████     | 25/50 [00:16<00:12,  1.95it/s]Train Iter: 526/1000. LR: 0.0329. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9485. T_Loss: 3.5485. Mask: 0.9733. :  52%|█████▏    | 26/50 [00:16<00:14,  1.62it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9491. T_Loss: 3.5764. Mask: 0.9734. :  52%|█████▏    | 26/50 [00:16<00:14,  1.62it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9491. T_Loss: 3.5764. Mask: 0.9734. :  54%|█████▍    | 27/50 [00:16<00:12,  1.85it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9490. T_Loss: 3.5973. Mask: 0.9738. :  54%|█████▍    | 27/50 [00:17<00:12,  1.85it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9490. T_Loss: 3.5973. Mask: 0.9738. :  56%|█████▌    | 28/50 [00:17<00:10,  2.19it/s]total : 1000  current step :  526
total : 1000  current step :  527
total : 1000  current step :  528
Train Iter: 529/1000. LR: 0.0331. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9481. T_Loss: 3.6076. Mask: 0.9732. :  56%|█████▌    | 28/50 [00:18<00:10,  2.19it/s]Train Iter: 529/1000. LR: 0.0331. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9481. T_Loss: 3.6076. Mask: 0.9732. :  58%|█████▊    | 29/50 [00:18<00:12,  1.67it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9473. T_Loss: 3.6260. Mask: 0.9729. :  58%|█████▊    | 29/50 [00:18<00:12,  1.67it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9473. T_Loss: 3.6260. Mask: 0.9729. :  60%|██████    | 30/50 [00:18<00:10,  1.95it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9461. T_Loss: 3.6333. Mask: 0.9730. :  60%|██████    | 30/50 [00:18<00:10,  1.95it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9461. T_Loss: 3.6333. Mask: 0.9730. :  62%|██████▏   | 31/50 [00:18<00:08,  2.26it/s]total : 1000  current step :  529
total : 1000  current step :  530
total : 1000  current step :  531
Train Iter: 532/1000. LR: 0.0333. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9450. T_Loss: 3.6319. Mask: 0.9733. :  62%|██████▏   | 31/50 [00:19<00:08,  2.26it/s]Train Iter: 532/1000. LR: 0.0333. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9450. T_Loss: 3.6319. Mask: 0.9733. :  64%|██████▍   | 32/50 [00:19<00:10,  1.79it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9447. T_Loss: 3.6348. Mask: 0.9736. :  64%|██████▍   | 32/50 [00:19<00:10,  1.79it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9447. T_Loss: 3.6348. Mask: 0.9736. :  66%|██████▌   | 33/50 [00:19<00:08,  2.07it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9449. T_Loss: 3.6282. Mask: 0.9729. :  66%|██████▌   | 33/50 [00:20<00:08,  2.07it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9449. T_Loss: 3.6282. Mask: 0.9729. :  68%|██████▊   | 34/50 [00:20<00:07,  2.24it/s]total : 1000  current step :  532
total : 1000  current step :  533
total : 1000  current step :  534
Train Iter: 535/1000. LR: 0.0334. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9451. T_Loss: 3.6208. Mask: 0.9729. :  68%|██████▊   | 34/50 [00:21<00:07,  2.24it/s]Train Iter: 535/1000. LR: 0.0334. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9451. T_Loss: 3.6208. Mask: 0.9729. :  70%|███████   | 35/50 [00:21<00:08,  1.72it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9444. T_Loss: 3.6328. Mask: 0.9728. :  70%|███████   | 35/50 [00:21<00:08,  1.72it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9444. T_Loss: 3.6328. Mask: 0.9728. :  72%|███████▏  | 36/50 [00:21<00:06,  2.00it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9445. T_Loss: 3.6289. Mask: 0.9726. :  72%|███████▏  | 36/50 [00:21<00:06,  2.00it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9445. T_Loss: 3.6289. Mask: 0.9726. :  74%|███████▍  | 37/50 [00:21<00:05,  2.18it/s]total : 1000  current step :  535
total : 1000  current step :  536
total : 1000  current step :  537
Train Iter: 538/1000. LR: 0.0336. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9451. T_Loss: 3.6129. Mask: 0.9723. :  74%|███████▍  | 37/50 [00:22<00:05,  2.18it/s]Train Iter: 538/1000. LR: 0.0336. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9451. T_Loss: 3.6129. Mask: 0.9723. :  76%|███████▌  | 38/50 [00:22<00:07,  1.63it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9442. T_Loss: 3.6047. Mask: 0.9724. :  76%|███████▌  | 38/50 [00:22<00:07,  1.63it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9442. T_Loss: 3.6047. Mask: 0.9724. :  78%|███████▊  | 39/50 [00:22<00:05,  2.00it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9430. T_Loss: 3.5974. Mask: 0.9725. :  78%|███████▊  | 39/50 [00:23<00:05,  2.00it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9430. T_Loss: 3.5974. Mask: 0.9725. :  80%|████████  | 40/50 [00:23<00:04,  2.26it/s]total : 1000  current step :  538
total : 1000  current step :  539
total : 1000  current step :  540
Train Iter: 541/1000. LR: 0.0338. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9430. T_Loss: 3.5975. Mask: 0.9727. :  80%|████████  | 40/50 [00:24<00:04,  2.26it/s]Train Iter: 541/1000. LR: 0.0338. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9430. T_Loss: 3.5975. Mask: 0.9727. :  82%|████████▏ | 41/50 [00:24<00:05,  1.74it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9427. T_Loss: 3.5975. Mask: 0.9728. :  82%|████████▏ | 41/50 [00:24<00:05,  1.74it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9427. T_Loss: 3.5975. Mask: 0.9728. :  84%|████████▍ | 42/50 [00:24<00:03,  2.11it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9423. T_Loss: 3.5970. Mask: 0.9727. :  84%|████████▍ | 42/50 [00:24<00:03,  2.11it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9423. T_Loss: 3.5970. Mask: 0.9727. :  86%|████████▌ | 43/50 [00:24<00:02,  2.52it/s]total : 1000  current step :  541
total : 1000  current step :  542
total : 1000  current step :  543
Train Iter: 544/1000. LR: 0.0340. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9424. T_Loss: 3.5969. Mask: 0.9727. :  86%|████████▌ | 43/50 [00:25<00:02,  2.52it/s]Train Iter: 544/1000. LR: 0.0340. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9424. T_Loss: 3.5969. Mask: 0.9727. :  88%|████████▊ | 44/50 [00:25<00:03,  1.77it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9422. T_Loss: 3.5944. Mask: 0.9729. :  88%|████████▊ | 44/50 [00:25<00:03,  1.77it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9422. T_Loss: 3.5944. Mask: 0.9729. :  90%|█████████ | 45/50 [00:25<00:02,  2.08it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9424. T_Loss: 3.5967. Mask: 0.9728. :  90%|█████████ | 45/50 [00:26<00:02,  2.08it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9424. T_Loss: 3.5967. Mask: 0.9728. :  92%|█████████▏| 46/50 [00:26<00:01,  2.50it/s]total : 1000  current step :  544
total : 1000  current step :  545
total : 1000  current step :  546
Train Iter: 547/1000. LR: 0.0342. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9420. T_Loss: 3.5958. Mask: 0.9731. :  92%|█████████▏| 46/50 [00:27<00:01,  2.50it/s]Train Iter: 547/1000. LR: 0.0342. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9420. T_Loss: 3.5958. Mask: 0.9731. :  94%|█████████▍| 47/50 [00:27<00:01,  1.67it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9423. T_Loss: 3.5922. Mask: 0.9728. :  94%|█████████▍| 47/50 [00:27<00:01,  1.67it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9423. T_Loss: 3.5922. Mask: 0.9728. :  96%|█████████▌| 48/50 [00:27<00:01,  1.95it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.37s. Batch: 0.56s. S_Loss: 0.9425. T_Loss: 3.5866. Mask: 0.9729. :  96%|█████████▌| 48/50 [00:27<00:01,  1.95it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.37s. Batch: 0.56s. S_Loss: 0.9425. T_Loss: 3.5866. Mask: 0.9729. :  98%|█████████▊| 49/50 [00:27<00:00,  2.22it/s]total : 1000  current step :  547
total : 1000  current step :  548
total : 1000  current step :  549
Train Iter: 550/1000. LR: 0.0344. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9424. T_Loss: 3.5766. Mask: 0.9729. :  98%|█████████▊| 49/50 [00:28<00:00,  2.22it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9424. T_Loss: 3.5766. Mask: 0.9729. : 100%|██████████| 50/50 [00:28<00:00,  1.59it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9424. T_Loss: 3.5766. Mask: 0.9729. : 100%|██████████| 50/50 [00:28<00:00,  1.74it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0324. top1: 88.67. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0324. top1: 88.67. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.40it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.0539. top1: 87.30. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:04,  1.40it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.0539. top1: 87.30. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0563. top1: 86.85. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0563. top1: 86.85. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0578. top1: 87.21. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0578. top1: 87.21. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0612. top1: 86.95. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0612. top1: 86.95. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.31it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0548. top1: 87.37. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.31it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0548. top1: 87.37. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.56it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0505. top1: 87.95. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.56it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0505. top1: 87.95. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0515. top1: 88.10. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.70it/s] Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0515. top1: 88.10. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0515. top1: 88.10. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.99it/s]
total : 1000  current step :  550
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.20s. S_Loss: 0.9056. T_Loss: 3.3166. Mask: 0.9688. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.20s. S_Loss: 0.9056. T_Loss: 3.3166. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:09,  5.07it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.19s. S_Loss: 0.9218. T_Loss: 3.2331. Mask: 0.9668. :   2%|▏         | 1/50 [00:00<00:09,  5.07it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.19s. S_Loss: 0.9218. T_Loss: 3.2331. Mask: 0.9668. :   4%|▍         | 2/50 [00:00<00:09,  5.27it/s]total : 1000  current step :  551
total : 1000  current step :  552
Train Iter: 553/1000. LR: 0.0346. Data: 0.28s. Batch: 0.46s. S_Loss: 0.9341. T_Loss: 3.3993. Mask: 0.9674. :   4%|▍         | 2/50 [00:01<00:09,  5.27it/s]Train Iter: 553/1000. LR: 0.0346. Data: 0.28s. Batch: 0.46s. S_Loss: 0.9341. T_Loss: 3.3993. Mask: 0.9674. :   6%|▌         | 3/50 [00:01<00:26,  1.80it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.25s. Batch: 0.42s. S_Loss: 0.9529. T_Loss: 3.5084. Mask: 0.9658. :   6%|▌         | 3/50 [00:01<00:26,  1.80it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.25s. Batch: 0.42s. S_Loss: 0.9529. T_Loss: 3.5084. Mask: 0.9658. :   8%|▊         | 4/50 [00:01<00:21,  2.16it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.23s. Batch: 0.41s. S_Loss: 0.9615. T_Loss: 3.7378. Mask: 0.9625. :   8%|▊         | 4/50 [00:02<00:21,  2.16it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.23s. Batch: 0.41s. S_Loss: 0.9615. T_Loss: 3.7378. Mask: 0.9625. :  10%|█         | 5/50 [00:02<00:19,  2.31it/s]total : 1000  current step :  553
total : 1000  current step :  554
total : 1000  current step :  555
Train Iter: 556/1000. LR: 0.0347. Data: 0.35s. Batch: 0.52s. S_Loss: 0.9683. T_Loss: 3.8441. Mask: 0.9642. :  10%|█         | 5/50 [00:03<00:19,  2.31it/s]Train Iter: 556/1000. LR: 0.0347. Data: 0.35s. Batch: 0.52s. S_Loss: 0.9683. T_Loss: 3.8441. Mask: 0.9642. :  12%|█▏        | 6/50 [00:03<00:28,  1.54it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9593. T_Loss: 3.8711. Mask: 0.9676. :  12%|█▏        | 6/50 [00:03<00:28,  1.54it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9593. T_Loss: 3.8711. Mask: 0.9676. :  14%|█▍        | 7/50 [00:03<00:24,  1.76it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.29s. Batch: 0.48s. S_Loss: 0.9558. T_Loss: 3.8745. Mask: 0.9688. :  14%|█▍        | 7/50 [00:03<00:24,  1.76it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.29s. Batch: 0.48s. S_Loss: 0.9558. T_Loss: 3.8745. Mask: 0.9688. :  16%|█▌        | 8/50 [00:03<00:20,  2.04it/s]total : 1000  current step :  556
total : 1000  current step :  557
total : 1000  current step :  558
Train Iter: 559/1000. LR: 0.0349. Data: 0.35s. Batch: 0.53s. S_Loss: 0.9530. T_Loss: 3.8686. Mask: 0.9701. :  16%|█▌        | 8/50 [00:04<00:20,  2.04it/s]Train Iter: 559/1000. LR: 0.0349. Data: 0.35s. Batch: 0.53s. S_Loss: 0.9530. T_Loss: 3.8686. Mask: 0.9701. :  18%|█▊        | 9/50 [00:04<00:25,  1.59it/s]total : 1000  current step :  559
Train Iter: 560/1000. LR: 0.0350. Data: 0.36s. Batch: 0.55s. S_Loss: 0.9494. T_Loss: 3.8469. Mask: 0.9723. :  18%|█▊        | 9/50 [00:05<00:25,  1.59it/s]Train Iter: 560/1000. LR: 0.0350. Data: 0.36s. Batch: 0.55s. S_Loss: 0.9494. T_Loss: 3.8469. Mask: 0.9723. :  20%|██        | 10/50 [00:05<00:25,  1.55it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.37s. Batch: 0.56s. S_Loss: 0.9473. T_Loss: 3.8107. Mask: 0.9712. :  20%|██        | 10/50 [00:06<00:25,  1.55it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.37s. Batch: 0.56s. S_Loss: 0.9473. T_Loss: 3.8107. Mask: 0.9712. :  22%|██▏       | 11/50 [00:06<00:25,  1.54it/s]total : 1000  current step :  560
total : 1000  current step :  561
Train Iter: 562/1000. LR: 0.0351. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9469. T_Loss: 3.7754. Mask: 0.9704. :  22%|██▏       | 11/50 [00:07<00:25,  1.54it/s]Train Iter: 562/1000. LR: 0.0351. Data: 0.40s. Batch: 0.59s. S_Loss: 0.9469. T_Loss: 3.7754. Mask: 0.9704. :  24%|██▍       | 12/50 [00:07<00:28,  1.34it/s]Train Iter: 563/1000. LR: 0.0352. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9466. T_Loss: 3.8073. Mask: 0.9718. :  24%|██▍       | 12/50 [00:07<00:28,  1.34it/s]Train Iter: 563/1000. LR: 0.0352. Data: 0.38s. Batch: 0.57s. S_Loss: 0.9466. T_Loss: 3.8073. Mask: 0.9718. :  26%|██▌       | 13/50 [00:07<00:23,  1.57it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9459. T_Loss: 3.7961. Mask: 0.9724. :  26%|██▌       | 13/50 [00:07<00:23,  1.57it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9459. T_Loss: 3.7961. Mask: 0.9724. :  28%|██▊       | 14/50 [00:07<00:19,  1.83it/s]total : 1000  current step :  562
total : 1000  current step :  563
total : 1000  current step :  564
Train Iter: 565/1000. LR: 0.0353. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9449. T_Loss: 3.7834. Mask: 0.9724. :  28%|██▊       | 14/50 [00:08<00:19,  1.83it/s]Train Iter: 565/1000. LR: 0.0353. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9449. T_Loss: 3.7834. Mask: 0.9724. :  30%|███       | 15/50 [00:08<00:25,  1.37it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9436. T_Loss: 3.8019. Mask: 0.9734. :  30%|███       | 15/50 [00:09<00:25,  1.37it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9436. T_Loss: 3.8019. Mask: 0.9734. :  32%|███▏      | 16/50 [00:09<00:21,  1.60it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9452. T_Loss: 3.8076. Mask: 0.9713. :  32%|███▏      | 16/50 [00:09<00:21,  1.60it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9452. T_Loss: 3.8076. Mask: 0.9713. :  34%|███▍      | 17/50 [00:09<00:19,  1.72it/s]total : 1000  current step :  565
total : 1000  current step :  566
total : 1000  current step :  567
Train Iter: 568/1000. LR: 0.0355. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9431. T_Loss: 3.7834. Mask: 0.9718. :  34%|███▍      | 17/50 [00:10<00:19,  1.72it/s]Train Iter: 568/1000. LR: 0.0355. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9431. T_Loss: 3.7834. Mask: 0.9718. :  36%|███▌      | 18/50 [00:10<00:23,  1.39it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9400. T_Loss: 3.7560. Mask: 0.9722. :  36%|███▌      | 18/50 [00:11<00:23,  1.39it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9400. T_Loss: 3.7560. Mask: 0.9722. :  38%|███▊      | 19/50 [00:11<00:19,  1.58it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9424. T_Loss: 3.7582. Mask: 0.9729. :  38%|███▊      | 19/50 [00:11<00:19,  1.58it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9424. T_Loss: 3.7582. Mask: 0.9729. :  40%|████      | 20/50 [00:11<00:16,  1.81it/s]total : 1000  current step :  568
total : 1000  current step :  569
total : 1000  current step :  570
Train Iter: 571/1000. LR: 0.0357. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9423. T_Loss: 3.7477. Mask: 0.9736. :  40%|████      | 20/50 [00:12<00:16,  1.81it/s]Train Iter: 571/1000. LR: 0.0357. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9423. T_Loss: 3.7477. Mask: 0.9736. :  42%|████▏     | 21/50 [00:12<00:21,  1.34it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9395. T_Loss: 3.7063. Mask: 0.9739. :  42%|████▏     | 21/50 [00:13<00:21,  1.34it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9395. T_Loss: 3.7063. Mask: 0.9739. :  44%|████▍     | 22/50 [00:13<00:17,  1.61it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9394. T_Loss: 3.6805. Mask: 0.9733. :  44%|████▍     | 22/50 [00:13<00:17,  1.61it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9394. T_Loss: 3.6805. Mask: 0.9733. :  46%|████▌     | 23/50 [00:13<00:15,  1.76it/s]total : 1000  current step :  571
total : 1000  current step :  572
total : 1000  current step :  573
Train Iter: 574/1000. LR: 0.0359. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9376. T_Loss: 3.6528. Mask: 0.9733. :  46%|████▌     | 23/50 [00:14<00:15,  1.76it/s]Train Iter: 574/1000. LR: 0.0359. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9376. T_Loss: 3.6528. Mask: 0.9733. :  48%|████▊     | 24/50 [00:14<00:18,  1.37it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9375. T_Loss: 3.6359. Mask: 0.9728. :  48%|████▊     | 24/50 [00:15<00:18,  1.37it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9375. T_Loss: 3.6359. Mask: 0.9728. :  50%|█████     | 25/50 [00:15<00:15,  1.64it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9359. T_Loss: 3.6149. Mask: 0.9730. :  50%|█████     | 25/50 [00:15<00:15,  1.64it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9359. T_Loss: 3.6149. Mask: 0.9730. :  52%|█████▏    | 26/50 [00:15<00:12,  1.92it/s]total : 1000  current step :  574
total : 1000  current step :  575
total : 1000  current step :  576
Train Iter: 577/1000. LR: 0.0361. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9334. T_Loss: 3.5905. Mask: 0.9735. :  52%|█████▏    | 26/50 [00:16<00:12,  1.92it/s]Train Iter: 577/1000. LR: 0.0361. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9334. T_Loss: 3.5905. Mask: 0.9735. :  54%|█████▍    | 27/50 [00:16<00:16,  1.36it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9324. T_Loss: 3.5714. Mask: 0.9736. :  54%|█████▍    | 27/50 [00:16<00:16,  1.36it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9324. T_Loss: 3.5714. Mask: 0.9736. :  56%|█████▌    | 28/50 [00:16<00:13,  1.62it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9313. T_Loss: 3.5599. Mask: 0.9737. :  56%|█████▌    | 28/50 [00:17<00:13,  1.62it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9313. T_Loss: 3.5599. Mask: 0.9737. :  58%|█████▊    | 29/50 [00:17<00:11,  1.85it/s]total : 1000  current step :  577
total : 1000  current step :  578
total : 1000  current step :  579
Train Iter: 580/1000. LR: 0.0362. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9318. T_Loss: 3.5662. Mask: 0.9742. :  58%|█████▊    | 29/50 [00:18<00:11,  1.85it/s]Train Iter: 580/1000. LR: 0.0362. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9318. T_Loss: 3.5662. Mask: 0.9742. :  60%|██████    | 30/50 [00:18<00:14,  1.34it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9320. T_Loss: 3.5784. Mask: 0.9733. :  60%|██████    | 30/50 [00:18<00:14,  1.34it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9320. T_Loss: 3.5784. Mask: 0.9733. :  62%|██████▏   | 31/50 [00:18<00:11,  1.63it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9321. T_Loss: 3.5831. Mask: 0.9731. :  62%|██████▏   | 31/50 [00:19<00:11,  1.63it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9321. T_Loss: 3.5831. Mask: 0.9731. :  64%|██████▍   | 32/50 [00:19<00:09,  1.88it/s]total : 1000  current step :  580
total : 1000  current step :  581
total : 1000  current step :  582
Train Iter: 583/1000. LR: 0.0364. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9322. T_Loss: 3.5981. Mask: 0.9734. :  64%|██████▍   | 32/50 [00:20<00:09,  1.88it/s]Train Iter: 583/1000. LR: 0.0364. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9322. T_Loss: 3.5981. Mask: 0.9734. :  66%|██████▌   | 33/50 [00:20<00:11,  1.42it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9337. T_Loss: 3.6174. Mask: 0.9729. :  66%|██████▌   | 33/50 [00:20<00:11,  1.42it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9337. T_Loss: 3.6174. Mask: 0.9729. :  68%|██████▊   | 34/50 [00:20<00:09,  1.64it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9350. T_Loss: 3.6364. Mask: 0.9728. :  68%|██████▊   | 34/50 [00:21<00:09,  1.64it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9350. T_Loss: 3.6364. Mask: 0.9728. :  70%|███████   | 35/50 [00:21<00:07,  1.92it/s]total : 1000  current step :  583
total : 1000  current step :  584
total : 1000  current step :  585
Train Iter: 586/1000. LR: 0.0366. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9358. T_Loss: 3.6373. Mask: 0.9729. :  70%|███████   | 35/50 [00:22<00:07,  1.92it/s]Train Iter: 586/1000. LR: 0.0366. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9358. T_Loss: 3.6373. Mask: 0.9729. :  72%|███████▏  | 36/50 [00:22<00:09,  1.42it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9352. T_Loss: 3.6378. Mask: 0.9730. :  72%|███████▏  | 36/50 [00:22<00:09,  1.42it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9352. T_Loss: 3.6378. Mask: 0.9730. :  74%|███████▍  | 37/50 [00:22<00:07,  1.72it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9347. T_Loss: 3.6315. Mask: 0.9732. :  74%|███████▍  | 37/50 [00:22<00:07,  1.72it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9347. T_Loss: 3.6315. Mask: 0.9732. :  76%|███████▌  | 38/50 [00:22<00:06,  1.95it/s]total : 1000  current step :  586
total : 1000  current step :  587
total : 1000  current step :  588
Train Iter: 589/1000. LR: 0.0368. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9350. T_Loss: 3.6260. Mask: 0.9733. :  76%|███████▌  | 38/50 [00:23<00:06,  1.95it/s]Train Iter: 589/1000. LR: 0.0368. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9350. T_Loss: 3.6260. Mask: 0.9733. :  78%|███████▊  | 39/50 [00:23<00:07,  1.41it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9345. T_Loss: 3.6193. Mask: 0.9736. :  78%|███████▊  | 39/50 [00:24<00:07,  1.41it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9345. T_Loss: 3.6193. Mask: 0.9736. :  80%|████████  | 40/50 [00:24<00:06,  1.58it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9345. T_Loss: 3.6167. Mask: 0.9739. :  80%|████████  | 40/50 [00:24<00:06,  1.58it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9345. T_Loss: 3.6167. Mask: 0.9739. :  82%|████████▏ | 41/50 [00:24<00:05,  1.79it/s]total : 1000  current step :  589
total : 1000  current step :  590
total : 1000  current step :  591
Train Iter: 592/1000. LR: 0.0370. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9344. T_Loss: 3.6248. Mask: 0.9741. :  82%|████████▏ | 41/50 [00:26<00:05,  1.79it/s]Train Iter: 592/1000. LR: 0.0370. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9344. T_Loss: 3.6248. Mask: 0.9741. :  84%|████████▍ | 42/50 [00:26<00:05,  1.34it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9341. T_Loss: 3.6206. Mask: 0.9745. :  84%|████████▍ | 42/50 [00:26<00:05,  1.34it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9341. T_Loss: 3.6206. Mask: 0.9745. :  86%|████████▌ | 43/50 [00:26<00:04,  1.64it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9338. T_Loss: 3.6109. Mask: 0.9747. :  86%|████████▌ | 43/50 [00:26<00:04,  1.64it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9338. T_Loss: 3.6109. Mask: 0.9747. :  88%|████████▊ | 44/50 [00:26<00:03,  1.87it/s]total : 1000  current step :  592
total : 1000  current step :  593
total : 1000  current step :  594
Train Iter: 595/1000. LR: 0.0372. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9331. T_Loss: 3.6024. Mask: 0.9746. :  88%|████████▊ | 44/50 [00:27<00:03,  1.87it/s]Train Iter: 595/1000. LR: 0.0372. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9331. T_Loss: 3.6024. Mask: 0.9746. :  90%|█████████ | 45/50 [00:27<00:03,  1.48it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9334. T_Loss: 3.6027. Mask: 0.9747. :  90%|█████████ | 45/50 [00:28<00:03,  1.48it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9334. T_Loss: 3.6027. Mask: 0.9747. :  92%|█████████▏| 46/50 [00:28<00:02,  1.68it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9335. T_Loss: 3.5964. Mask: 0.9747. :  92%|█████████▏| 46/50 [00:28<00:02,  1.68it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9335. T_Loss: 3.5964. Mask: 0.9747. :  94%|█████████▍| 47/50 [00:28<00:01,  1.85it/s]total : 1000  current step :  595
total : 1000  current step :  596
total : 1000  current step :  597
Train Iter: 598/1000. LR: 0.0374. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9336. T_Loss: 3.5885. Mask: 0.9745. :  94%|█████████▍| 47/50 [00:29<00:01,  1.85it/s]Train Iter: 598/1000. LR: 0.0374. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9336. T_Loss: 3.5885. Mask: 0.9745. :  96%|█████████▌| 48/50 [00:29<00:01,  1.40it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9332. T_Loss: 3.5891. Mask: 0.9743. :  96%|█████████▌| 48/50 [00:29<00:01,  1.40it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9332. T_Loss: 3.5891. Mask: 0.9743. :  98%|█████████▊| 49/50 [00:29<00:00,  1.65it/s]total : 1000  current step :  598
total : 1000  current step :  599
Train Iter: 600/1000. LR: 0.0375. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9329. T_Loss: 3.5874. Mask: 0.9741. :  98%|█████████▊| 49/50 [00:30<00:00,  1.65it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9329. T_Loss: 3.5874. Mask: 0.9741. : 100%|██████████| 50/50 [00:30<00:00,  1.48it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9329. T_Loss: 3.5874. Mask: 0.9741. : 100%|██████████| 50/50 [00:30<00:00,  1.62it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.0107. top1: 88.67. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.0107. top1: 88.67. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0326. top1: 87.11. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0326. top1: 87.11. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0356. top1: 86.59. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0356. top1: 86.59. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0357. top1: 86.91. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0357. top1: 86.91. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0293. top1: 87.11. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0293. top1: 87.11. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0167. top1: 87.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0167. top1: 87.70. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0083. top1: 88.50. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0083. top1: 88.50. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0067. top1: 88.80. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s] Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0067. top1: 88.80. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0067. top1: 88.80. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
total : 1000  current step :  600
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.88s. Batch: 1.08s. S_Loss: 0.9225. T_Loss: 4.1657. Mask: 0.9883. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.88s. Batch: 1.08s. S_Loss: 0.9225. T_Loss: 4.1657. Mask: 0.9883. :   2%|▏         | 1/50 [00:01<00:52,  1.08s/it]Train Iter: 602/1000. LR: 0.0376. Data: 0.54s. Batch: 0.75s. S_Loss: 0.9220. T_Loss: 4.0075. Mask: 0.9805. :   2%|▏         | 1/50 [00:01<00:52,  1.08s/it]Train Iter: 602/1000. LR: 0.0376. Data: 0.54s. Batch: 0.75s. S_Loss: 0.9220. T_Loss: 4.0075. Mask: 0.9805. :   4%|▍         | 2/50 [00:01<00:33,  1.44it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9122. T_Loss: 3.8470. Mask: 0.9805. :   4%|▍         | 2/50 [00:01<00:33,  1.44it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9122. T_Loss: 3.8470. Mask: 0.9805. :   6%|▌         | 3/50 [00:01<00:24,  1.92it/s]total : 1000  current step :  601
total : 1000  current step :  602
total : 1000  current step :  603
Train Iter: 604/1000. LR: 0.0378. Data: 0.54s. Batch: 0.74s. S_Loss: 0.9091. T_Loss: 3.7147. Mask: 0.9775. :   6%|▌         | 3/50 [00:02<00:24,  1.92it/s]Train Iter: 604/1000. LR: 0.0378. Data: 0.54s. Batch: 0.74s. S_Loss: 0.9091. T_Loss: 3.7147. Mask: 0.9775. :   8%|▊         | 4/50 [00:02<00:35,  1.30it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9075. T_Loss: 3.6322. Mask: 0.9711. :   8%|▊         | 4/50 [00:03<00:35,  1.30it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9075. T_Loss: 3.6322. Mask: 0.9711. :  10%|█         | 5/50 [00:03<00:27,  1.66it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9137. T_Loss: 3.5968. Mask: 0.9727. :  10%|█         | 5/50 [00:03<00:27,  1.66it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9137. T_Loss: 3.5968. Mask: 0.9727. :  12%|█▏        | 6/50 [00:03<00:22,  1.93it/s]total : 1000  current step :  604
total : 1000  current step :  605
total : 1000  current step :  606
Train Iter: 607/1000. LR: 0.0379. Data: 0.46s. Batch: 0.66s. S_Loss: 0.9138. T_Loss: 3.5589. Mask: 0.9721. :  12%|█▏        | 6/50 [00:04<00:22,  1.93it/s]Train Iter: 607/1000. LR: 0.0379. Data: 0.46s. Batch: 0.66s. S_Loss: 0.9138. T_Loss: 3.5589. Mask: 0.9721. :  14%|█▍        | 7/50 [00:04<00:29,  1.47it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9194. T_Loss: 3.5839. Mask: 0.9707. :  14%|█▍        | 7/50 [00:05<00:29,  1.47it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9194. T_Loss: 3.5839. Mask: 0.9707. :  16%|█▌        | 8/50 [00:05<00:24,  1.72it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9211. T_Loss: 3.5540. Mask: 0.9718. :  16%|█▌        | 8/50 [00:05<00:24,  1.72it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9211. T_Loss: 3.5540. Mask: 0.9718. :  18%|█▊        | 9/50 [00:05<00:23,  1.72it/s]total : 1000  current step :  607
total : 1000  current step :  608
total : 1000  current step :  609
Train Iter: 610/1000. LR: 0.0381. Data: 0.47s. Batch: 0.67s. S_Loss: 0.9194. T_Loss: 3.5461. Mask: 0.9711. :  18%|█▊        | 9/50 [00:06<00:23,  1.72it/s]Train Iter: 610/1000. LR: 0.0381. Data: 0.47s. Batch: 0.67s. S_Loss: 0.9194. T_Loss: 3.5461. Mask: 0.9711. :  20%|██        | 10/50 [00:06<00:29,  1.34it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9220. T_Loss: 3.5535. Mask: 0.9712. :  20%|██        | 10/50 [00:07<00:29,  1.34it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9220. T_Loss: 3.5535. Mask: 0.9712. :  22%|██▏       | 11/50 [00:07<00:24,  1.58it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9239. T_Loss: 3.5360. Mask: 0.9710. :  22%|██▏       | 11/50 [00:07<00:24,  1.58it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9239. T_Loss: 3.5360. Mask: 0.9710. :  24%|██▍       | 12/50 [00:07<00:20,  1.88it/s]total : 1000  current step :  610
total : 1000  current step :  611
total : 1000  current step :  612
Train Iter: 613/1000. LR: 0.0383. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9261. T_Loss: 3.5087. Mask: 0.9706. :  24%|██▍       | 12/50 [00:08<00:20,  1.88it/s]Train Iter: 613/1000. LR: 0.0383. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9261. T_Loss: 3.5087. Mask: 0.9706. :  26%|██▌       | 13/50 [00:08<00:25,  1.48it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9262. T_Loss: 3.4879. Mask: 0.9718. :  26%|██▌       | 13/50 [00:08<00:25,  1.48it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9262. T_Loss: 3.4879. Mask: 0.9718. :  28%|██▊       | 14/50 [00:08<00:21,  1.67it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9275. T_Loss: 3.4686. Mask: 0.9719. :  28%|██▊       | 14/50 [00:09<00:21,  1.67it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9275. T_Loss: 3.4686. Mask: 0.9719. :  30%|███       | 15/50 [00:09<00:18,  1.93it/s]total : 1000  current step :  613
total : 1000  current step :  614
total : 1000  current step :  615
Train Iter: 616/1000. LR: 0.0385. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9311. T_Loss: 3.4479. Mask: 0.9724. :  30%|███       | 15/50 [00:10<00:18,  1.93it/s]Train Iter: 616/1000. LR: 0.0385. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9311. T_Loss: 3.4479. Mask: 0.9724. :  32%|███▏      | 16/50 [00:10<00:22,  1.48it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9307. T_Loss: 3.4298. Mask: 0.9724. :  32%|███▏      | 16/50 [00:10<00:22,  1.48it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9307. T_Loss: 3.4298. Mask: 0.9724. :  34%|███▍      | 17/50 [00:10<00:19,  1.72it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9307. T_Loss: 3.4052. Mask: 0.9729. :  34%|███▍      | 17/50 [00:10<00:19,  1.72it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9307. T_Loss: 3.4052. Mask: 0.9729. :  36%|███▌      | 18/50 [00:10<00:16,  1.93it/s]total : 1000  current step :  616
total : 1000  current step :  617
total : 1000  current step :  618
Train Iter: 619/1000. LR: 0.0387. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9291. T_Loss: 3.3887. Mask: 0.9727. :  36%|███▌      | 18/50 [00:11<00:16,  1.93it/s]Train Iter: 619/1000. LR: 0.0387. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9291. T_Loss: 3.3887. Mask: 0.9727. :  38%|███▊      | 19/50 [00:11<00:20,  1.51it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9286. T_Loss: 3.3897. Mask: 0.9736. :  38%|███▊      | 19/50 [00:12<00:20,  1.51it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9286. T_Loss: 3.3897. Mask: 0.9736. :  40%|████      | 20/50 [00:12<00:16,  1.83it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9290. T_Loss: 3.4034. Mask: 0.9740. :  40%|████      | 20/50 [00:12<00:16,  1.83it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9290. T_Loss: 3.4034. Mask: 0.9740. :  42%|████▏     | 21/50 [00:12<00:15,  1.82it/s]total : 1000  current step :  619
total : 1000  current step :  620
total : 1000  current step :  621
Train Iter: 622/1000. LR: 0.0389. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9306. T_Loss: 3.4205. Mask: 0.9737. :  42%|████▏     | 21/50 [00:13<00:15,  1.82it/s]Train Iter: 622/1000. LR: 0.0389. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9306. T_Loss: 3.4205. Mask: 0.9737. :  44%|████▍     | 22/50 [00:13<00:19,  1.41it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9298. T_Loss: 3.4401. Mask: 0.9740. :  44%|████▍     | 22/50 [00:14<00:19,  1.41it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9298. T_Loss: 3.4401. Mask: 0.9740. :  46%|████▌     | 23/50 [00:14<00:15,  1.71it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9284. T_Loss: 3.4461. Mask: 0.9748. :  46%|████▌     | 23/50 [00:14<00:15,  1.71it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9284. T_Loss: 3.4461. Mask: 0.9748. :  48%|████▊     | 24/50 [00:14<00:13,  1.88it/s]total : 1000  current step :  622
total : 1000  current step :  623
total : 1000  current step :  624
Train Iter: 625/1000. LR: 0.0391. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9289. T_Loss: 3.4594. Mask: 0.9753. :  48%|████▊     | 24/50 [00:15<00:13,  1.88it/s]Train Iter: 625/1000. LR: 0.0391. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9289. T_Loss: 3.4594. Mask: 0.9753. :  50%|█████     | 25/50 [00:15<00:18,  1.38it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9289. T_Loss: 3.4786. Mask: 0.9752. :  50%|█████     | 25/50 [00:16<00:18,  1.38it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9289. T_Loss: 3.4786. Mask: 0.9752. :  52%|█████▏    | 26/50 [00:16<00:14,  1.65it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9298. T_Loss: 3.5062. Mask: 0.9750. :  52%|█████▏    | 26/50 [00:16<00:14,  1.65it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9298. T_Loss: 3.5062. Mask: 0.9750. :  54%|█████▍    | 27/50 [00:16<00:11,  1.94it/s]total : 1000  current step :  625
total : 1000  current step :  626
total : 1000  current step :  627
Train Iter: 628/1000. LR: 0.0393. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9287. T_Loss: 3.5242. Mask: 0.9750. :  54%|█████▍    | 27/50 [00:17<00:11,  1.94it/s]Train Iter: 628/1000. LR: 0.0393. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9287. T_Loss: 3.5242. Mask: 0.9750. :  56%|█████▌    | 28/50 [00:17<00:14,  1.47it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9280. T_Loss: 3.5350. Mask: 0.9754. :  56%|█████▌    | 28/50 [00:17<00:14,  1.47it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9280. T_Loss: 3.5350. Mask: 0.9754. :  58%|█████▊    | 29/50 [00:17<00:11,  1.80it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9277. T_Loss: 3.5362. Mask: 0.9755. :  58%|█████▊    | 29/50 [00:18<00:11,  1.80it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9277. T_Loss: 3.5362. Mask: 0.9755. :  60%|██████    | 30/50 [00:18<00:09,  2.03it/s]total : 1000  current step :  628
total : 1000  current step :  629
total : 1000  current step :  630
Train Iter: 631/1000. LR: 0.0394. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9274. T_Loss: 3.5255. Mask: 0.9754. :  60%|██████    | 30/50 [00:19<00:09,  2.03it/s]Train Iter: 631/1000. LR: 0.0394. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9274. T_Loss: 3.5255. Mask: 0.9754. :  62%|██████▏   | 31/50 [00:19<00:13,  1.42it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9268. T_Loss: 3.5228. Mask: 0.9751. :  62%|██████▏   | 31/50 [00:19<00:13,  1.42it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9268. T_Loss: 3.5228. Mask: 0.9751. :  64%|██████▍   | 32/50 [00:19<00:10,  1.68it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9267. T_Loss: 3.5157. Mask: 0.9751. :  64%|██████▍   | 32/50 [00:19<00:10,  1.68it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9267. T_Loss: 3.5157. Mask: 0.9751. :  66%|██████▌   | 33/50 [00:19<00:08,  1.96it/s]total : 1000  current step :  631
total : 1000  current step :  632
total : 1000  current step :  633
Train Iter: 634/1000. LR: 0.0396. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9265. T_Loss: 3.5046. Mask: 0.9751. :  66%|██████▌   | 33/50 [00:20<00:08,  1.96it/s]Train Iter: 634/1000. LR: 0.0396. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9265. T_Loss: 3.5046. Mask: 0.9751. :  68%|██████▊   | 34/50 [00:20<00:10,  1.46it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9281. T_Loss: 3.4959. Mask: 0.9746. :  68%|██████▊   | 34/50 [00:21<00:10,  1.46it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9281. T_Loss: 3.4959. Mask: 0.9746. :  70%|███████   | 35/50 [00:21<00:08,  1.72it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9277. T_Loss: 3.4811. Mask: 0.9744. :  70%|███████   | 35/50 [00:21<00:08,  1.72it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9277. T_Loss: 3.4811. Mask: 0.9744. :  72%|███████▏  | 36/50 [00:21<00:07,  1.99it/s]total : 1000  current step :  634
total : 1000  current step :  635
total : 1000  current step :  636
Train Iter: 637/1000. LR: 0.0398. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9277. T_Loss: 3.4730. Mask: 0.9748. :  72%|███████▏  | 36/50 [00:22<00:07,  1.99it/s]Train Iter: 637/1000. LR: 0.0398. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9277. T_Loss: 3.4730. Mask: 0.9748. :  74%|███████▍  | 37/50 [00:22<00:09,  1.43it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9273. T_Loss: 3.4625. Mask: 0.9747. :  74%|███████▍  | 37/50 [00:23<00:09,  1.43it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9273. T_Loss: 3.4625. Mask: 0.9747. :  76%|███████▌  | 38/50 [00:23<00:06,  1.73it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9274. T_Loss: 3.4541. Mask: 0.9747. :  76%|███████▌  | 38/50 [00:23<00:06,  1.73it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9274. T_Loss: 3.4541. Mask: 0.9747. :  78%|███████▊  | 39/50 [00:23<00:05,  1.85it/s]total : 1000  current step :  637
total : 1000  current step :  638
total : 1000  current step :  639
Train Iter: 640/1000. LR: 0.0400. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9265. T_Loss: 3.4420. Mask: 0.9747. :  78%|███████▊  | 39/50 [00:25<00:05,  1.85it/s]Train Iter: 640/1000. LR: 0.0400. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9265. T_Loss: 3.4420. Mask: 0.9747. :  80%|████████  | 40/50 [00:25<00:08,  1.13it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9268. T_Loss: 3.4327. Mask: 0.9748. :  80%|████████  | 40/50 [00:26<00:08,  1.13it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9268. T_Loss: 3.4327. Mask: 0.9748. :  82%|████████▏ | 41/50 [00:26<00:07,  1.14it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9259. T_Loss: 3.4267. Mask: 0.9750. :  82%|████████▏ | 41/50 [00:26<00:07,  1.14it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9259. T_Loss: 3.4267. Mask: 0.9750. :  84%|████████▍ | 42/50 [00:26<00:05,  1.35it/s]total : 1000  current step :  640
total : 1000  current step :  641
total : 1000  current step :  642
Train Iter: 643/1000. LR: 0.0402. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9269. T_Loss: 3.4370. Mask: 0.9749. :  84%|████████▍ | 42/50 [00:27<00:05,  1.35it/s]Train Iter: 643/1000. LR: 0.0402. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9269. T_Loss: 3.4370. Mask: 0.9749. :  86%|████████▌ | 43/50 [00:27<00:05,  1.22it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9277. T_Loss: 3.4472. Mask: 0.9749. :  86%|████████▌ | 43/50 [00:27<00:05,  1.22it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9277. T_Loss: 3.4472. Mask: 0.9749. :  88%|████████▊ | 44/50 [00:27<00:04,  1.40it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9282. T_Loss: 3.4641. Mask: 0.9752. :  88%|████████▊ | 44/50 [00:28<00:04,  1.40it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9282. T_Loss: 3.4641. Mask: 0.9752. :  90%|█████████ | 45/50 [00:28<00:03,  1.67it/s]total : 1000  current step :  643
total : 1000  current step :  644
total : 1000  current step :  645
Train Iter: 646/1000. LR: 0.0404. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9286. T_Loss: 3.4749. Mask: 0.9750. :  90%|█████████ | 45/50 [00:29<00:03,  1.67it/s]Train Iter: 646/1000. LR: 0.0404. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9286. T_Loss: 3.4749. Mask: 0.9750. :  92%|█████████▏| 46/50 [00:29<00:02,  1.34it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9278. T_Loss: 3.4757. Mask: 0.9751. :  92%|█████████▏| 46/50 [00:29<00:02,  1.34it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9278. T_Loss: 3.4757. Mask: 0.9751. :  94%|█████████▍| 47/50 [00:29<00:01,  1.59it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9280. T_Loss: 3.4801. Mask: 0.9749. :  94%|█████████▍| 47/50 [00:30<00:01,  1.59it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9280. T_Loss: 3.4801. Mask: 0.9749. :  96%|█████████▌| 48/50 [00:30<00:01,  1.84it/s]total : 1000  current step :  646
total : 1000  current step :  647
total : 1000  current step :  648
Train Iter: 649/1000. LR: 0.0406. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9276. T_Loss: 3.4861. Mask: 0.9750. :  96%|█████████▌| 48/50 [00:31<00:01,  1.84it/s]Train Iter: 649/1000. LR: 0.0406. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9276. T_Loss: 3.4861. Mask: 0.9750. :  98%|█████████▊| 49/50 [00:31<00:00,  1.41it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9279. T_Loss: 3.4901. Mask: 0.9749. :  98%|█████████▊| 49/50 [00:31<00:00,  1.41it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9279. T_Loss: 3.4901. Mask: 0.9749. : 100%|██████████| 50/50 [00:31<00:00,  1.66it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9279. T_Loss: 3.4901. Mask: 0.9749. : 100%|██████████| 50/50 [00:31<00:00,  1.59it/s]
total : 1000  current step :  649
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.9984. top1: 88.28. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.9984. top1: 88.28. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0213. top1: 86.52. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0213. top1: 86.52. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0245. top1: 86.20. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0245. top1: 86.20. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0237. top1: 86.72. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0237. top1: 86.72. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.19it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0114. top1: 87.27. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.19it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0114. top1: 87.27. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9953. top1: 88.28. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9953. top1: 88.28. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9846. top1: 89.01. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9846. top1: 89.01. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9813. top1: 89.35. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.68it/s] Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9813. top1: 89.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9813. top1: 89.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]
total : 1000  current step :  650
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.01s. Batch: 0.16s. S_Loss: 0.8625. T_Loss: 3.1308. Mask: 0.9844. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.01s. Batch: 0.16s. S_Loss: 0.8625. T_Loss: 3.1308. Mask: 0.9844. :   2%|▏         | 1/50 [00:00<00:08,  6.00it/s]total : 1000  current step :  651
Train Iter: 652/1000. LR: 0.0408. Data: 0.44s. Batch: 0.62s. S_Loss: 0.8883. T_Loss: 3.4968. Mask: 0.9883. :   2%|▏         | 1/50 [00:01<00:08,  6.00it/s]Train Iter: 652/1000. LR: 0.0408. Data: 0.44s. Batch: 0.62s. S_Loss: 0.8883. T_Loss: 3.4968. Mask: 0.9883. :   4%|▍         | 2/50 [00:01<00:33,  1.42it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9026. T_Loss: 3.5618. Mask: 0.9883. :   4%|▍         | 2/50 [00:01<00:33,  1.42it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9026. T_Loss: 3.5618. Mask: 0.9883. :   6%|▌         | 3/50 [00:01<00:24,  1.91it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.28s. Batch: 0.48s. S_Loss: 0.9219. T_Loss: 3.7297. Mask: 0.9863. :   6%|▌         | 3/50 [00:01<00:24,  1.91it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.28s. Batch: 0.48s. S_Loss: 0.9219. T_Loss: 3.7297. Mask: 0.9863. :   8%|▊         | 4/50 [00:01<00:21,  2.17it/s]total : 1000  current step :  652
total : 1000  current step :  653
total : 1000  current step :  654
Train Iter: 655/1000. LR: 0.0409. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9239. T_Loss: 3.7095. Mask: 0.9852. :   8%|▊         | 4/50 [00:03<00:21,  2.17it/s]Train Iter: 655/1000. LR: 0.0409. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9239. T_Loss: 3.7095. Mask: 0.9852. :  10%|█         | 5/50 [00:03<00:31,  1.41it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9217. T_Loss: 3.6067. Mask: 0.9831. :  10%|█         | 5/50 [00:03<00:31,  1.41it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9217. T_Loss: 3.6067. Mask: 0.9831. :  12%|█▏        | 6/50 [00:03<00:27,  1.63it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9176. T_Loss: 3.5269. Mask: 0.9838. :  12%|█▏        | 6/50 [00:03<00:27,  1.63it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9176. T_Loss: 3.5269. Mask: 0.9838. :  14%|█▍        | 7/50 [00:03<00:23,  1.85it/s]total : 1000  current step :  655
total : 1000  current step :  656
total : 1000  current step :  657
Train Iter: 658/1000. LR: 0.0411. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9120. T_Loss: 3.4479. Mask: 0.9844. :  14%|█▍        | 7/50 [00:04<00:23,  1.85it/s]Train Iter: 658/1000. LR: 0.0411. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9120. T_Loss: 3.4479. Mask: 0.9844. :  16%|█▌        | 8/50 [00:04<00:29,  1.43it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.38s. Batch: 0.59s. S_Loss: 0.9097. T_Loss: 3.3631. Mask: 0.9848. :  16%|█▌        | 8/50 [00:05<00:29,  1.43it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.38s. Batch: 0.59s. S_Loss: 0.9097. T_Loss: 3.3631. Mask: 0.9848. :  18%|█▊        | 9/50 [00:05<00:24,  1.66it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9112. T_Loss: 3.3284. Mask: 0.9844. :  18%|█▊        | 9/50 [00:05<00:24,  1.66it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9112. T_Loss: 3.3284. Mask: 0.9844. :  20%|██        | 10/50 [00:05<00:21,  1.85it/s]total : 1000  current step :  658
total : 1000  current step :  659
total : 1000  current step :  660
Train Iter: 661/1000. LR: 0.0413. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9113. T_Loss: 3.2877. Mask: 0.9826. :  20%|██        | 10/50 [00:06<00:21,  1.85it/s]Train Iter: 661/1000. LR: 0.0413. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9113. T_Loss: 3.2877. Mask: 0.9826. :  22%|██▏       | 11/50 [00:06<00:26,  1.45it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.38s. Batch: 0.59s. S_Loss: 0.9113. T_Loss: 3.2597. Mask: 0.9827. :  22%|██▏       | 11/50 [00:07<00:26,  1.45it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.38s. Batch: 0.59s. S_Loss: 0.9113. T_Loss: 3.2597. Mask: 0.9827. :  24%|██▍       | 12/50 [00:07<00:22,  1.72it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9119. T_Loss: 3.2579. Mask: 0.9835. :  24%|██▍       | 12/50 [00:07<00:22,  1.72it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9119. T_Loss: 3.2579. Mask: 0.9835. :  26%|██▌       | 13/50 [00:07<00:18,  2.04it/s]total : 1000  current step :  661
total : 1000  current step :  662
total : 1000  current step :  663
Train Iter: 664/1000. LR: 0.0415. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9096. T_Loss: 3.2229. Mask: 0.9830. :  26%|██▌       | 13/50 [00:08<00:18,  2.04it/s]Train Iter: 664/1000. LR: 0.0415. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9096. T_Loss: 3.2229. Mask: 0.9830. :  28%|██▊       | 14/50 [00:08<00:22,  1.62it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9099. T_Loss: 3.2360. Mask: 0.9828. :  28%|██▊       | 14/50 [00:08<00:22,  1.62it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9099. T_Loss: 3.2360. Mask: 0.9828. :  30%|███       | 15/50 [00:08<00:18,  1.91it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9091. T_Loss: 3.2274. Mask: 0.9827. :  30%|███       | 15/50 [00:08<00:18,  1.91it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9091. T_Loss: 3.2274. Mask: 0.9827. :  32%|███▏      | 16/50 [00:08<00:16,  2.10it/s]total : 1000  current step :  664
total : 1000  current step :  665
total : 1000  current step :  666
Train Iter: 667/1000. LR: 0.0417. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9093. T_Loss: 3.2274. Mask: 0.9828. :  32%|███▏      | 16/50 [00:10<00:16,  2.10it/s]Train Iter: 667/1000. LR: 0.0417. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9093. T_Loss: 3.2274. Mask: 0.9828. :  34%|███▍      | 17/50 [00:10<00:21,  1.51it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9104. T_Loss: 3.2555. Mask: 0.9833. :  34%|███▍      | 17/50 [00:10<00:21,  1.51it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9104. T_Loss: 3.2555. Mask: 0.9833. :  36%|███▌      | 18/50 [00:10<00:17,  1.79it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9078. T_Loss: 3.2489. Mask: 0.9825. :  36%|███▌      | 18/50 [00:10<00:17,  1.79it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9078. T_Loss: 3.2489. Mask: 0.9825. :  38%|███▊      | 19/50 [00:10<00:15,  1.98it/s]total : 1000  current step :  667
total : 1000  current step :  668
total : 1000  current step :  669
Train Iter: 670/1000. LR: 0.0419. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9068. T_Loss: 3.2594. Mask: 0.9820. :  38%|███▊      | 19/50 [00:11<00:15,  1.98it/s]Train Iter: 670/1000. LR: 0.0419. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9068. T_Loss: 3.2594. Mask: 0.9820. :  40%|████      | 20/50 [00:11<00:20,  1.45it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9056. T_Loss: 3.2581. Mask: 0.9818. :  40%|████      | 20/50 [00:12<00:20,  1.45it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9056. T_Loss: 3.2581. Mask: 0.9818. :  42%|████▏     | 21/50 [00:12<00:15,  1.82it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9077. T_Loss: 3.2595. Mask: 0.9817. :  42%|████▏     | 21/50 [00:12<00:15,  1.82it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9077. T_Loss: 3.2595. Mask: 0.9817. :  44%|████▍     | 22/50 [00:12<00:13,  2.09it/s]total : 1000  current step :  670
total : 1000  current step :  671
total : 1000  current step :  672
Train Iter: 673/1000. LR: 0.0421. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9099. T_Loss: 3.2807. Mask: 0.9815. :  44%|████▍     | 22/50 [00:13<00:13,  2.09it/s]Train Iter: 673/1000. LR: 0.0421. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9099. T_Loss: 3.2807. Mask: 0.9815. :  46%|████▌     | 23/50 [00:13<00:17,  1.52it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9093. T_Loss: 3.2895. Mask: 0.9808. :  46%|████▌     | 23/50 [00:13<00:17,  1.52it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9093. T_Loss: 3.2895. Mask: 0.9808. :  48%|████▊     | 24/50 [00:13<00:14,  1.84it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9098. T_Loss: 3.2948. Mask: 0.9803. :  48%|████▊     | 24/50 [00:13<00:14,  1.84it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9098. T_Loss: 3.2948. Mask: 0.9803. :  50%|█████     | 25/50 [00:13<00:11,  2.20it/s]total : 1000  current step :  673
total : 1000  current step :  674
total : 1000  current step :  675
Train Iter: 676/1000. LR: 0.0423. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9114. T_Loss: 3.3066. Mask: 0.9805. :  50%|█████     | 25/50 [00:15<00:11,  2.20it/s]Train Iter: 676/1000. LR: 0.0423. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9114. T_Loss: 3.3066. Mask: 0.9805. :  52%|█████▏    | 26/50 [00:15<00:15,  1.54it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9132. T_Loss: 3.3438. Mask: 0.9803. :  52%|█████▏    | 26/50 [00:15<00:15,  1.54it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9132. T_Loss: 3.3438. Mask: 0.9803. :  54%|█████▍    | 27/50 [00:15<00:13,  1.70it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9115. T_Loss: 3.3566. Mask: 0.9807. :  54%|█████▍    | 27/50 [00:15<00:13,  1.70it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.37s. Batch: 0.57s. S_Loss: 0.9115. T_Loss: 3.3566. Mask: 0.9807. :  56%|█████▌    | 28/50 [00:15<00:11,  1.90it/s]total : 1000  current step :  676
total : 1000  current step :  677
total : 1000  current step :  678
Train Iter: 679/1000. LR: 0.0424. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9113. T_Loss: 3.3606. Mask: 0.9806. :  56%|█████▌    | 28/50 [00:16<00:11,  1.90it/s]Train Iter: 679/1000. LR: 0.0424. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9113. T_Loss: 3.3606. Mask: 0.9806. :  58%|█████▊    | 29/50 [00:16<00:14,  1.46it/s]total : 1000  current step :  679
Train Iter: 680/1000. LR: 0.0425. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9099. T_Loss: 3.3635. Mask: 0.9810. :  58%|█████▊    | 29/50 [00:17<00:14,  1.46it/s]Train Iter: 680/1000. LR: 0.0425. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9099. T_Loss: 3.3635. Mask: 0.9810. :  60%|██████    | 30/50 [00:17<00:13,  1.51it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9112. T_Loss: 3.3846. Mask: 0.9810. :  60%|██████    | 30/50 [00:18<00:13,  1.51it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9112. T_Loss: 3.3846. Mask: 0.9810. :  62%|██████▏   | 31/50 [00:18<00:12,  1.48it/s]total : 1000  current step :  680
total : 1000  current step :  681
Train Iter: 682/1000. LR: 0.0426. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9114. T_Loss: 3.3882. Mask: 0.9811. :  62%|██████▏   | 31/50 [00:19<00:12,  1.48it/s]Train Iter: 682/1000. LR: 0.0426. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9114. T_Loss: 3.3882. Mask: 0.9811. :  64%|██████▍   | 32/50 [00:19<00:13,  1.30it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9117. T_Loss: 3.3922. Mask: 0.9812. :  64%|██████▍   | 32/50 [00:19<00:13,  1.30it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9117. T_Loss: 3.3922. Mask: 0.9812. :  66%|██████▌   | 33/50 [00:19<00:10,  1.58it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9126. T_Loss: 3.3905. Mask: 0.9809. :  66%|██████▌   | 33/50 [00:19<00:10,  1.58it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.39s. Batch: 0.58s. S_Loss: 0.9126. T_Loss: 3.3905. Mask: 0.9809. :  68%|██████▊   | 34/50 [00:19<00:08,  1.83it/s]total : 1000  current step :  682
total : 1000  current step :  683
total : 1000  current step :  684
Train Iter: 685/1000. LR: 0.0428. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9117. T_Loss: 3.3940. Mask: 0.9810. :  68%|██████▊   | 34/50 [00:21<00:08,  1.83it/s]Train Iter: 685/1000. LR: 0.0428. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9117. T_Loss: 3.3940. Mask: 0.9810. :  70%|███████   | 35/50 [00:21<00:10,  1.39it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9110. T_Loss: 3.3865. Mask: 0.9813. :  70%|███████   | 35/50 [00:21<00:10,  1.39it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9110. T_Loss: 3.3865. Mask: 0.9813. :  72%|███████▏  | 36/50 [00:21<00:08,  1.63it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9115. T_Loss: 3.3821. Mask: 0.9813. :  72%|███████▏  | 36/50 [00:21<00:08,  1.63it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9115. T_Loss: 3.3821. Mask: 0.9813. :  74%|███████▍  | 37/50 [00:21<00:06,  1.92it/s]total : 1000  current step :  685
total : 1000  current step :  686
total : 1000  current step :  687
Train Iter: 688/1000. LR: 0.0430. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9117. T_Loss: 3.3776. Mask: 0.9812. :  74%|███████▍  | 37/50 [00:22<00:06,  1.92it/s]Train Iter: 688/1000. LR: 0.0430. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9117. T_Loss: 3.3776. Mask: 0.9812. :  76%|███████▌  | 38/50 [00:22<00:08,  1.35it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9102. T_Loss: 3.3645. Mask: 0.9811. :  76%|███████▌  | 38/50 [00:23<00:08,  1.35it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9102. T_Loss: 3.3645. Mask: 0.9811. :  78%|███████▊  | 39/50 [00:23<00:06,  1.58it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9093. T_Loss: 3.3463. Mask: 0.9812. :  78%|███████▊  | 39/50 [00:23<00:06,  1.58it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9093. T_Loss: 3.3463. Mask: 0.9812. :  80%|████████  | 40/50 [00:23<00:05,  1.80it/s]total : 1000  current step :  688
total : 1000  current step :  689
total : 1000  current step :  690
Train Iter: 691/1000. LR: 0.0432. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9082. T_Loss: 3.3348. Mask: 0.9810. :  80%|████████  | 40/50 [00:25<00:05,  1.80it/s]Train Iter: 691/1000. LR: 0.0432. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9082. T_Loss: 3.3348. Mask: 0.9810. :  82%|████████▏ | 41/50 [00:25<00:06,  1.30it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9073. T_Loss: 3.3261. Mask: 0.9812. :  82%|████████▏ | 41/50 [00:25<00:06,  1.30it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9073. T_Loss: 3.3261. Mask: 0.9812. :  84%|████████▍ | 42/50 [00:25<00:05,  1.56it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9059. T_Loss: 3.3110. Mask: 0.9816. :  84%|████████▍ | 42/50 [00:25<00:05,  1.56it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9059. T_Loss: 3.3110. Mask: 0.9816. :  86%|████████▌ | 43/50 [00:25<00:04,  1.72it/s]total : 1000  current step :  691
total : 1000  current step :  692
total : 1000  current step :  693
Train Iter: 694/1000. LR: 0.0434. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9061. T_Loss: 3.3014. Mask: 0.9814. :  86%|████████▌ | 43/50 [00:26<00:04,  1.72it/s]Train Iter: 694/1000. LR: 0.0434. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9061. T_Loss: 3.3014. Mask: 0.9814. :  88%|████████▊ | 44/50 [00:26<00:04,  1.37it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9067. T_Loss: 3.3052. Mask: 0.9813. :  88%|████████▊ | 44/50 [00:27<00:04,  1.37it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9067. T_Loss: 3.3052. Mask: 0.9813. :  90%|█████████ | 45/50 [00:27<00:03,  1.56it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9059. T_Loss: 3.3047. Mask: 0.9813. :  90%|█████████ | 45/50 [00:27<00:03,  1.56it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9059. T_Loss: 3.3047. Mask: 0.9813. :  92%|█████████▏| 46/50 [00:27<00:02,  1.79it/s]total : 1000  current step :  694
total : 1000  current step :  695
total : 1000  current step :  696
Train Iter: 697/1000. LR: 0.0436. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9067. T_Loss: 3.3143. Mask: 0.9813. :  92%|█████████▏| 46/50 [00:28<00:02,  1.79it/s]Train Iter: 697/1000. LR: 0.0436. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9067. T_Loss: 3.3143. Mask: 0.9813. :  94%|█████████▍| 47/50 [00:28<00:02,  1.38it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9067. T_Loss: 3.3143. Mask: 0.9814. :  94%|█████████▍| 47/50 [00:29<00:02,  1.38it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9067. T_Loss: 3.3143. Mask: 0.9814. :  96%|█████████▌| 48/50 [00:29<00:01,  1.61it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9069. T_Loss: 3.3235. Mask: 0.9813. :  96%|█████████▌| 48/50 [00:29<00:01,  1.61it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9069. T_Loss: 3.3235. Mask: 0.9813. :  98%|█████████▊| 49/50 [00:29<00:00,  1.81it/s]total : 1000  current step :  697
total : 1000  current step :  698
total : 1000  current step :  699
Train Iter: 700/1000. LR: 0.0438. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9069. T_Loss: 3.3270. Mask: 0.9811. :  98%|█████████▊| 49/50 [00:30<00:00,  1.81it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9069. T_Loss: 3.3270. Mask: 0.9811. : 100%|██████████| 50/50 [00:30<00:00,  1.33it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9069. T_Loss: 3.3270. Mask: 0.9811. : 100%|██████████| 50/50 [00:30<00:00,  1.62it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.9581. top1: 90.23. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.9581. top1: 90.23. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.38it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.9789. top1: 88.87. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.38it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.9789. top1: 88.87. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.9821. top1: 88.41. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.9821. top1: 88.41. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9813. top1: 88.67. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9813. top1: 88.67. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9781. top1: 88.98. top5: 99.92. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9781. top1: 88.98. top5: 99.92. :  62%|██████▎   | 5/8 [00:01<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9683. top1: 89.84. top5: 99.93. :  62%|██████▎   | 5/8 [00:02<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9683. top1: 89.84. top5: 99.93. :  75%|███████▌  | 6/8 [00:02<00:00,  3.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9618. top1: 90.46. top5: 99.89. :  75%|███████▌  | 6/8 [00:02<00:00,  3.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9618. top1: 90.46. top5: 99.89. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9613. top1: 90.75. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9613. top1: 90.75. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9613. top1: 90.75. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
total : 1000  current step :  700
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9250. T_Loss: 3.6191. Mask: 0.9766. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9250. T_Loss: 3.6191. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:14,  3.32it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.29s. S_Loss: 0.9205. T_Loss: 3.6591. Mask: 0.9785. :   2%|▏         | 1/50 [00:00<00:14,  3.32it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.29s. S_Loss: 0.9205. T_Loss: 3.6591. Mask: 0.9785. :   4%|▍         | 2/50 [00:00<00:14,  3.38it/s]total : 1000  current step :  701
total : 1000  current step :  702
Train Iter: 703/1000. LR: 0.0439. Data: 0.29s. Batch: 0.55s. S_Loss: 0.9061. T_Loss: 3.5308. Mask: 0.9766. :   4%|▍         | 2/50 [00:01<00:14,  3.38it/s]Train Iter: 703/1000. LR: 0.0439. Data: 0.29s. Batch: 0.55s. S_Loss: 0.9061. T_Loss: 3.5308. Mask: 0.9766. :   6%|▌         | 3/50 [00:01<00:30,  1.54it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9086. T_Loss: 3.5354. Mask: 0.9795. :   6%|▌         | 3/50 [00:02<00:30,  1.54it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.26s. Batch: 0.51s. S_Loss: 0.9086. T_Loss: 3.5354. Mask: 0.9795. :   8%|▊         | 4/50 [00:02<00:25,  1.81it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.23s. Batch: 0.48s. S_Loss: 0.9119. T_Loss: 3.5615. Mask: 0.9805. :   8%|▊         | 4/50 [00:02<00:25,  1.81it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.23s. Batch: 0.48s. S_Loss: 0.9119. T_Loss: 3.5615. Mask: 0.9805. :  10%|█         | 5/50 [00:02<00:21,  2.08it/s]total : 1000  current step :  703
total : 1000  current step :  704
total : 1000  current step :  705
Train Iter: 706/1000. LR: 0.0441. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9092. T_Loss: 3.5143. Mask: 0.9811. :  10%|█         | 5/50 [00:03<00:21,  2.08it/s]Train Iter: 706/1000. LR: 0.0441. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9092. T_Loss: 3.5143. Mask: 0.9811. :  12%|█▏        | 6/50 [00:03<00:30,  1.45it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9059. T_Loss: 3.4709. Mask: 0.9810. :  12%|█▏        | 6/50 [00:03<00:30,  1.45it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9059. T_Loss: 3.4709. Mask: 0.9810. :  14%|█▍        | 7/50 [00:03<00:24,  1.74it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9040. T_Loss: 3.4533. Mask: 0.9810. :  14%|█▍        | 7/50 [00:04<00:24,  1.74it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.27s. Batch: 0.51s. S_Loss: 0.9040. T_Loss: 3.4533. Mask: 0.9810. :  16%|█▌        | 8/50 [00:04<00:19,  2.12it/s]total : 1000  current step :  706
total : 1000  current step :  707
total : 1000  current step :  708
Train Iter: 709/1000. LR: 0.0443. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9002. T_Loss: 3.3972. Mask: 0.9822. :  16%|█▌        | 8/50 [00:05<00:19,  2.12it/s]Train Iter: 709/1000. LR: 0.0443. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9002. T_Loss: 3.3972. Mask: 0.9822. :  18%|█▊        | 9/50 [00:05<00:28,  1.45it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.30s. Batch: 0.55s. S_Loss: 0.8989. T_Loss: 3.3680. Mask: 0.9801. :  18%|█▊        | 9/50 [00:05<00:28,  1.45it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.30s. Batch: 0.55s. S_Loss: 0.8989. T_Loss: 3.3680. Mask: 0.9801. :  20%|██        | 10/50 [00:05<00:22,  1.78it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.27s. Batch: 0.53s. S_Loss: 0.8951. T_Loss: 3.3296. Mask: 0.9808. :  20%|██        | 10/50 [00:05<00:22,  1.78it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.27s. Batch: 0.53s. S_Loss: 0.8951. T_Loss: 3.3296. Mask: 0.9808. :  22%|██▏       | 11/50 [00:05<00:18,  2.08it/s]total : 1000  current step :  709
total : 1000  current step :  710
total : 1000  current step :  711
Train Iter: 712/1000. LR: 0.0445. Data: 0.32s. Batch: 0.58s. S_Loss: 0.8976. T_Loss: 3.3220. Mask: 0.9801. :  22%|██▏       | 11/50 [00:06<00:18,  2.08it/s]Train Iter: 712/1000. LR: 0.0445. Data: 0.32s. Batch: 0.58s. S_Loss: 0.8976. T_Loss: 3.3220. Mask: 0.9801. :  24%|██▍       | 12/50 [00:06<00:25,  1.50it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.31s. Batch: 0.56s. S_Loss: 0.8989. T_Loss: 3.3122. Mask: 0.9799. :  24%|██▍       | 12/50 [00:07<00:25,  1.50it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.31s. Batch: 0.56s. S_Loss: 0.8989. T_Loss: 3.3122. Mask: 0.9799. :  26%|██▌       | 13/50 [00:07<00:20,  1.77it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.29s. Batch: 0.54s. S_Loss: 0.8994. T_Loss: 3.3118. Mask: 0.9802. :  26%|██▌       | 13/50 [00:07<00:20,  1.77it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.29s. Batch: 0.54s. S_Loss: 0.8994. T_Loss: 3.3118. Mask: 0.9802. :  28%|██▊       | 14/50 [00:07<00:17,  2.02it/s]total : 1000  current step :  712
total : 1000  current step :  713
total : 1000  current step :  714
Train Iter: 715/1000. LR: 0.0447. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8975. T_Loss: 3.2890. Mask: 0.9794. :  28%|██▊       | 14/50 [00:08<00:17,  2.02it/s]Train Iter: 715/1000. LR: 0.0447. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8975. T_Loss: 3.2890. Mask: 0.9794. :  30%|███       | 15/50 [00:08<00:24,  1.41it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.33s. Batch: 0.57s. S_Loss: 0.8963. T_Loss: 3.2745. Mask: 0.9800. :  30%|███       | 15/50 [00:09<00:24,  1.41it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.33s. Batch: 0.57s. S_Loss: 0.8963. T_Loss: 3.2745. Mask: 0.9800. :  32%|███▏      | 16/50 [00:09<00:20,  1.63it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.32s. Batch: 0.56s. S_Loss: 0.8979. T_Loss: 3.2778. Mask: 0.9798. :  32%|███▏      | 16/50 [00:09<00:20,  1.63it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.32s. Batch: 0.56s. S_Loss: 0.8979. T_Loss: 3.2778. Mask: 0.9798. :  34%|███▍      | 17/50 [00:09<00:17,  1.91it/s]total : 1000  current step :  715
total : 1000  current step :  716
total : 1000  current step :  717
Train Iter: 718/1000. LR: 0.0449. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8997. T_Loss: 3.2887. Mask: 0.9796. :  34%|███▍      | 17/50 [00:10<00:17,  1.91it/s]Train Iter: 718/1000. LR: 0.0449. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8997. T_Loss: 3.2887. Mask: 0.9796. :  36%|███▌      | 18/50 [00:10<00:21,  1.47it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8966. T_Loss: 3.2672. Mask: 0.9799. :  36%|███▌      | 18/50 [00:10<00:21,  1.47it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8966. T_Loss: 3.2672. Mask: 0.9799. :  38%|███▊      | 19/50 [00:10<00:18,  1.72it/s]total : 1000  current step :  718
total : 1000  current step :  719
Train Iter: 720/1000. LR: 0.0450. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8925. T_Loss: 3.2380. Mask: 0.9797. :  38%|███▊      | 19/50 [00:11<00:18,  1.72it/s]Train Iter: 720/1000. LR: 0.0450. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8925. T_Loss: 3.2380. Mask: 0.9797. :  40%|████      | 20/50 [00:11<00:19,  1.52it/s]total : 1000  current step :  720
Train Iter: 721/1000. LR: 0.0451. Data: 0.38s. Batch: 0.61s. S_Loss: 0.8925. T_Loss: 3.2231. Mask: 0.9790. :  40%|████      | 20/50 [00:12<00:19,  1.52it/s]Train Iter: 721/1000. LR: 0.0451. Data: 0.38s. Batch: 0.61s. S_Loss: 0.8925. T_Loss: 3.2231. Mask: 0.9790. :  42%|████▏     | 21/50 [00:12<00:23,  1.26it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8935. T_Loss: 3.2353. Mask: 0.9794. :  42%|████▏     | 21/50 [00:13<00:23,  1.26it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8935. T_Loss: 3.2353. Mask: 0.9794. :  44%|████▍     | 22/50 [00:13<00:18,  1.50it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8941. T_Loss: 3.2270. Mask: 0.9796. :  44%|████▍     | 22/50 [00:13<00:18,  1.50it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8941. T_Loss: 3.2270. Mask: 0.9796. :  46%|████▌     | 23/50 [00:13<00:16,  1.67it/s]total : 1000  current step :  721
total : 1000  current step :  722
total : 1000  current step :  723
Train Iter: 724/1000. LR: 0.0453. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8924. T_Loss: 3.2103. Mask: 0.9792. :  46%|████▌     | 23/50 [00:14<00:16,  1.67it/s]Train Iter: 724/1000. LR: 0.0453. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8924. T_Loss: 3.2103. Mask: 0.9792. :  48%|████▊     | 24/50 [00:14<00:19,  1.31it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8939. T_Loss: 3.2251. Mask: 0.9791. :  48%|████▊     | 24/50 [00:15<00:19,  1.31it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8939. T_Loss: 3.2251. Mask: 0.9791. :  50%|█████     | 25/50 [00:15<00:16,  1.56it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8953. T_Loss: 3.2405. Mask: 0.9791. :  50%|█████     | 25/50 [00:15<00:16,  1.56it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8953. T_Loss: 3.2405. Mask: 0.9791. :  52%|█████▏    | 26/50 [00:15<00:13,  1.74it/s]total : 1000  current step :  724
total : 1000  current step :  725
total : 1000  current step :  726
Train Iter: 727/1000. LR: 0.0454. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8960. T_Loss: 3.2413. Mask: 0.9793. :  52%|█████▏    | 26/50 [00:16<00:13,  1.74it/s]Train Iter: 727/1000. LR: 0.0454. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8960. T_Loss: 3.2413. Mask: 0.9793. :  54%|█████▍    | 27/50 [00:16<00:17,  1.33it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.38s. Batch: 0.61s. S_Loss: 0.8959. T_Loss: 3.2469. Mask: 0.9795. :  54%|█████▍    | 27/50 [00:17<00:17,  1.33it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.38s. Batch: 0.61s. S_Loss: 0.8959. T_Loss: 3.2469. Mask: 0.9795. :  56%|█████▌    | 28/50 [00:17<00:14,  1.56it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8964. T_Loss: 3.2485. Mask: 0.9799. :  56%|█████▌    | 28/50 [00:17<00:14,  1.56it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8964. T_Loss: 3.2485. Mask: 0.9799. :  58%|█████▊    | 29/50 [00:17<00:11,  1.79it/s]total : 1000  current step :  727
total : 1000  current step :  728
total : 1000  current step :  729
Train Iter: 730/1000. LR: 0.0456. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8969. T_Loss: 3.2556. Mask: 0.9797. :  58%|█████▊    | 29/50 [00:18<00:11,  1.79it/s]Train Iter: 730/1000. LR: 0.0456. Data: 0.39s. Batch: 0.62s. S_Loss: 0.8969. T_Loss: 3.2556. Mask: 0.9797. :  60%|██████    | 30/50 [00:18<00:14,  1.36it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8965. T_Loss: 3.2568. Mask: 0.9798. :  60%|██████    | 30/50 [00:19<00:14,  1.36it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8965. T_Loss: 3.2568. Mask: 0.9798. :  62%|██████▏   | 31/50 [00:19<00:11,  1.59it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8973. T_Loss: 3.2697. Mask: 0.9795. :  62%|██████▏   | 31/50 [00:19<00:11,  1.59it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8973. T_Loss: 3.2697. Mask: 0.9795. :  64%|██████▍   | 32/50 [00:19<00:09,  1.81it/s]total : 1000  current step :  730
total : 1000  current step :  731
total : 1000  current step :  732
Train Iter: 733/1000. LR: 0.0458. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8985. T_Loss: 3.2807. Mask: 0.9793. :  64%|██████▍   | 32/50 [00:20<00:09,  1.81it/s]Train Iter: 733/1000. LR: 0.0458. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8985. T_Loss: 3.2807. Mask: 0.9793. :  66%|██████▌   | 33/50 [00:20<00:12,  1.39it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8997. T_Loss: 3.2863. Mask: 0.9792. :  66%|██████▌   | 33/50 [00:20<00:12,  1.39it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8997. T_Loss: 3.2863. Mask: 0.9792. :  68%|██████▊   | 34/50 [00:20<00:09,  1.62it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8999. T_Loss: 3.2918. Mask: 0.9789. :  68%|██████▊   | 34/50 [00:21<00:09,  1.62it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8999. T_Loss: 3.2918. Mask: 0.9789. :  70%|███████   | 35/50 [00:21<00:07,  1.96it/s]total : 1000  current step :  733
total : 1000  current step :  734
total : 1000  current step :  735
Train Iter: 736/1000. LR: 0.0460. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8996. T_Loss: 3.2811. Mask: 0.9793. :  70%|███████   | 35/50 [00:22<00:07,  1.96it/s]Train Iter: 736/1000. LR: 0.0460. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8996. T_Loss: 3.2811. Mask: 0.9793. :  72%|███████▏  | 36/50 [00:22<00:09,  1.51it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8986. T_Loss: 3.2763. Mask: 0.9794. :  72%|███████▏  | 36/50 [00:22<00:09,  1.51it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8986. T_Loss: 3.2763. Mask: 0.9794. :  74%|███████▍  | 37/50 [00:22<00:07,  1.82it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8984. T_Loss: 3.2635. Mask: 0.9793. :  74%|███████▍  | 37/50 [00:22<00:07,  1.82it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8984. T_Loss: 3.2635. Mask: 0.9793. :  76%|███████▌  | 38/50 [00:22<00:05,  2.01it/s]total : 1000  current step :  736
total : 1000  current step :  737
total : 1000  current step :  738
Train Iter: 739/1000. LR: 0.0462. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8976. T_Loss: 3.2568. Mask: 0.9798. :  76%|███████▌  | 38/50 [00:23<00:05,  2.01it/s]Train Iter: 739/1000. LR: 0.0462. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8976. T_Loss: 3.2568. Mask: 0.9798. :  78%|███████▊  | 39/50 [00:23<00:07,  1.51it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8973. T_Loss: 3.2466. Mask: 0.9796. :  78%|███████▊  | 39/50 [00:24<00:07,  1.51it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.38s. Batch: 0.60s. S_Loss: 0.8973. T_Loss: 3.2466. Mask: 0.9796. :  80%|████████  | 40/50 [00:24<00:05,  1.86it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8971. T_Loss: 3.2351. Mask: 0.9800. :  80%|████████  | 40/50 [00:24<00:05,  1.86it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8971. T_Loss: 3.2351. Mask: 0.9800. :  82%|████████▏ | 41/50 [00:24<00:05,  1.79it/s]total : 1000  current step :  739
total : 1000  current step :  740
total : 1000  current step :  741
Train Iter: 742/1000. LR: 0.0464. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8967. T_Loss: 3.2292. Mask: 0.9801. :  82%|████████▏ | 41/50 [00:25<00:05,  1.79it/s]Train Iter: 742/1000. LR: 0.0464. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8967. T_Loss: 3.2292. Mask: 0.9801. :  84%|████████▍ | 42/50 [00:25<00:05,  1.34it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8967. T_Loss: 3.2210. Mask: 0.9800. :  84%|████████▍ | 42/50 [00:26<00:05,  1.34it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8967. T_Loss: 3.2210. Mask: 0.9800. :  86%|████████▌ | 43/50 [00:26<00:04,  1.57it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8963. T_Loss: 3.2128. Mask: 0.9801. :  86%|████████▌ | 43/50 [00:26<00:04,  1.57it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8963. T_Loss: 3.2128. Mask: 0.9801. :  88%|████████▊ | 44/50 [00:26<00:03,  1.55it/s]total : 1000  current step :  742
total : 1000  current step :  743
total : 1000  current step :  744
Train Iter: 745/1000. LR: 0.0466. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8974. T_Loss: 3.2204. Mask: 0.9804. :  88%|████████▊ | 44/50 [00:28<00:03,  1.55it/s]Train Iter: 745/1000. LR: 0.0466. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8974. T_Loss: 3.2204. Mask: 0.9804. :  90%|█████████ | 45/50 [00:28<00:04,  1.16it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8977. T_Loss: 3.2181. Mask: 0.9806. :  90%|█████████ | 45/50 [00:28<00:04,  1.16it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8977. T_Loss: 3.2181. Mask: 0.9806. :  92%|█████████▏| 46/50 [00:28<00:02,  1.38it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8975. T_Loss: 3.2124. Mask: 0.9806. :  92%|█████████▏| 46/50 [00:29<00:02,  1.38it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8975. T_Loss: 3.2124. Mask: 0.9806. :  94%|█████████▍| 47/50 [00:29<00:01,  1.65it/s]total : 1000  current step :  745
total : 1000  current step :  746
total : 1000  current step :  747
Train Iter: 748/1000. LR: 0.0468. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8978. T_Loss: 3.2098. Mask: 0.9806. :  94%|█████████▍| 47/50 [00:30<00:01,  1.65it/s]Train Iter: 748/1000. LR: 0.0468. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8978. T_Loss: 3.2098. Mask: 0.9806. :  96%|█████████▌| 48/50 [00:30<00:01,  1.22it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8985. T_Loss: 3.2132. Mask: 0.9806. :  96%|█████████▌| 48/50 [00:30<00:01,  1.22it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8985. T_Loss: 3.2132. Mask: 0.9806. :  98%|█████████▊| 49/50 [00:30<00:00,  1.43it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8983. T_Loss: 3.2108. Mask: 0.9805. :  98%|█████████▊| 49/50 [00:31<00:00,  1.43it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8983. T_Loss: 3.2108. Mask: 0.9805. : 100%|██████████| 50/50 [00:31<00:00,  1.66it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8983. T_Loss: 3.2108. Mask: 0.9805. : 100%|██████████| 50/50 [00:31<00:00,  1.60it/s]
total : 1000  current step :  748
total : 1000  current step :  749
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.9128. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.9128. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.9310. top1: 91.80. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.9310. top1: 91.80. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.9342. top1: 91.15. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.9342. top1: 91.15. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.34it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.9337. top1: 91.11. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.34it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.9337. top1: 91.11. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.53it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9415. top1: 90.86. top5: 99.92. :  50%|█████     | 4/8 [00:02<00:01,  2.53it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9415. top1: 90.86. top5: 99.92. :  62%|██████▎   | 5/8 [00:02<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9399. top1: 91.34. top5: 99.93. :  62%|██████▎   | 5/8 [00:02<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9399. top1: 91.34. top5: 99.93. :  75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9390. top1: 91.69. top5: 99.89. :  75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9390. top1: 91.69. top5: 99.89. :  88%|████████▊ | 7/8 [00:02<00:00,  2.84it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9419. top1: 91.75. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  2.84it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9419. top1: 91.75. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9419. top1: 91.75. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.49it/s]
total : 1000  current step :  750
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 1.06s. Batch: 1.28s. S_Loss: 0.8992. T_Loss: 3.7175. Mask: 0.9844. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 1.06s. Batch: 1.28s. S_Loss: 0.8992. T_Loss: 3.7175. Mask: 0.9844. :   2%|▏         | 1/50 [00:01<01:02,  1.28s/it]Train Iter: 752/1000. LR: 0.0470. Data: 0.65s. Batch: 0.87s. S_Loss: 0.9096. T_Loss: 3.5907. Mask: 0.9805. :   2%|▏         | 1/50 [00:01<01:02,  1.28s/it]Train Iter: 752/1000. LR: 0.0470. Data: 0.65s. Batch: 0.87s. S_Loss: 0.9096. T_Loss: 3.5907. Mask: 0.9805. :   4%|▍         | 2/50 [00:01<00:38,  1.26it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.49s. Batch: 0.70s. S_Loss: 0.8999. T_Loss: 3.5703. Mask: 0.9805. :   4%|▍         | 2/50 [00:02<00:38,  1.26it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.49s. Batch: 0.70s. S_Loss: 0.8999. T_Loss: 3.5703. Mask: 0.9805. :   6%|▌         | 3/50 [00:02<00:28,  1.65it/s]total : 1000  current step :  751
total : 1000  current step :  752
total : 1000  current step :  753
Train Iter: 754/1000. LR: 0.0471. Data: 0.62s. Batch: 0.82s. S_Loss: 0.9046. T_Loss: 3.5237. Mask: 0.9834. :   6%|▌         | 3/50 [00:03<00:28,  1.65it/s]Train Iter: 754/1000. LR: 0.0471. Data: 0.62s. Batch: 0.82s. S_Loss: 0.9046. T_Loss: 3.5237. Mask: 0.9834. :   8%|▊         | 4/50 [00:03<00:38,  1.20it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.52s. Batch: 0.72s. S_Loss: 0.8944. T_Loss: 3.4429. Mask: 0.9852. :   8%|▊         | 4/50 [00:03<00:38,  1.20it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.52s. Batch: 0.72s. S_Loss: 0.8944. T_Loss: 3.4429. Mask: 0.9852. :  10%|█         | 5/50 [00:03<00:29,  1.54it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.51s. Batch: 0.72s. S_Loss: 0.9000. T_Loss: 3.4606. Mask: 0.9844. :  10%|█         | 5/50 [00:04<00:29,  1.54it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.51s. Batch: 0.72s. S_Loss: 0.9000. T_Loss: 3.4606. Mask: 0.9844. :  12%|█▏        | 6/50 [00:04<00:29,  1.50it/s]total : 1000  current step :  754
total : 1000  current step :  755
total : 1000  current step :  756
Train Iter: 757/1000. LR: 0.0473. Data: 0.58s. Batch: 0.79s. S_Loss: 0.9010. T_Loss: 3.4260. Mask: 0.9833. :  12%|█▏        | 6/50 [00:05<00:29,  1.50it/s]Train Iter: 757/1000. LR: 0.0473. Data: 0.58s. Batch: 0.79s. S_Loss: 0.9010. T_Loss: 3.4260. Mask: 0.9833. :  14%|█▍        | 7/50 [00:05<00:36,  1.19it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.54s. Batch: 0.75s. S_Loss: 0.8976. T_Loss: 3.3886. Mask: 0.9829. :  14%|█▍        | 7/50 [00:05<00:36,  1.19it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.54s. Batch: 0.75s. S_Loss: 0.8976. T_Loss: 3.3886. Mask: 0.9829. :  16%|█▌        | 8/50 [00:05<00:30,  1.38it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.49s. Batch: 0.71s. S_Loss: 0.8961. T_Loss: 3.3810. Mask: 0.9844. :  16%|█▌        | 8/50 [00:06<00:30,  1.38it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.49s. Batch: 0.71s. S_Loss: 0.8961. T_Loss: 3.3810. Mask: 0.9844. :  18%|█▊        | 9/50 [00:06<00:25,  1.62it/s]total : 1000  current step :  757
total : 1000  current step :  758
total : 1000  current step :  759
Train Iter: 760/1000. LR: 0.0475. Data: 0.58s. Batch: 0.80s. S_Loss: 0.8933. T_Loss: 3.3361. Mask: 0.9844. :  18%|█▊        | 9/50 [00:07<00:25,  1.62it/s]Train Iter: 760/1000. LR: 0.0475. Data: 0.58s. Batch: 0.80s. S_Loss: 0.8933. T_Loss: 3.3361. Mask: 0.9844. :  20%|██        | 10/50 [00:07<00:37,  1.08it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.60s. Batch: 0.82s. S_Loss: 0.8937. T_Loss: 3.3000. Mask: 0.9851. :  20%|██        | 10/50 [00:08<00:37,  1.08it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.60s. Batch: 0.82s. S_Loss: 0.8937. T_Loss: 3.3000. Mask: 0.9851. :  22%|██▏       | 11/50 [00:08<00:36,  1.06it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.58s. Batch: 0.80s. S_Loss: 0.8920. T_Loss: 3.2979. Mask: 0.9850. :  22%|██▏       | 11/50 [00:09<00:36,  1.06it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.58s. Batch: 0.80s. S_Loss: 0.8920. T_Loss: 3.2979. Mask: 0.9850. :  24%|██▍       | 12/50 [00:09<00:31,  1.19it/s]total : 1000  current step :  760
total : 1000  current step :  761
total : 1000  current step :  762
Train Iter: 763/1000. LR: 0.0477. Data: 0.62s. Batch: 0.84s. S_Loss: 0.8959. T_Loss: 3.2736. Mask: 0.9835. :  24%|██▍       | 12/50 [00:10<00:31,  1.19it/s]Train Iter: 763/1000. LR: 0.0477. Data: 0.62s. Batch: 0.84s. S_Loss: 0.8959. T_Loss: 3.2736. Mask: 0.9835. :  26%|██▌       | 13/50 [00:10<00:36,  1.01it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.58s. Batch: 0.80s. S_Loss: 0.8949. T_Loss: 3.2683. Mask: 0.9844. :  26%|██▌       | 13/50 [00:11<00:36,  1.01it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.58s. Batch: 0.80s. S_Loss: 0.8949. T_Loss: 3.2683. Mask: 0.9844. :  28%|██▊       | 14/50 [00:11<00:27,  1.30it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.56s. Batch: 0.78s. S_Loss: 0.8975. T_Loss: 3.2774. Mask: 0.9846. :  28%|██▊       | 14/50 [00:11<00:27,  1.30it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.56s. Batch: 0.78s. S_Loss: 0.8975. T_Loss: 3.2774. Mask: 0.9846. :  30%|███       | 15/50 [00:11<00:24,  1.44it/s]total : 1000  current step :  763
total : 1000  current step :  764
total : 1000  current step :  765
Train Iter: 766/1000. LR: 0.0479. Data: 0.59s. Batch: 0.81s. S_Loss: 0.8994. T_Loss: 3.2780. Mask: 0.9854. :  30%|███       | 15/50 [00:12<00:24,  1.44it/s]Train Iter: 766/1000. LR: 0.0479. Data: 0.59s. Batch: 0.81s. S_Loss: 0.8994. T_Loss: 3.2780. Mask: 0.9854. :  32%|███▏      | 16/50 [00:12<00:29,  1.15it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.56s. Batch: 0.78s. S_Loss: 0.8985. T_Loss: 3.2764. Mask: 0.9853. :  32%|███▏      | 16/50 [00:13<00:29,  1.15it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.56s. Batch: 0.78s. S_Loss: 0.8985. T_Loss: 3.2764. Mask: 0.9853. :  34%|███▍      | 17/50 [00:13<00:22,  1.43it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.54s. Batch: 0.75s. S_Loss: 0.8978. T_Loss: 3.2644. Mask: 0.9850. :  34%|███▍      | 17/50 [00:13<00:22,  1.43it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.54s. Batch: 0.75s. S_Loss: 0.8978. T_Loss: 3.2644. Mask: 0.9850. :  36%|███▌      | 18/50 [00:13<00:18,  1.71it/s]total : 1000  current step :  766
total : 1000  current step :  767
total : 1000  current step :  768
Train Iter: 769/1000. LR: 0.0481. Data: 0.56s. Batch: 0.77s. S_Loss: 0.8996. T_Loss: 3.2620. Mask: 0.9850. :  36%|███▌      | 18/50 [00:14<00:18,  1.71it/s]Train Iter: 769/1000. LR: 0.0481. Data: 0.56s. Batch: 0.77s. S_Loss: 0.8996. T_Loss: 3.2620. Mask: 0.9850. :  38%|███▊      | 19/50 [00:14<00:23,  1.33it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.54s. Batch: 0.76s. S_Loss: 0.8982. T_Loss: 3.2698. Mask: 0.9840. :  38%|███▊      | 19/50 [00:15<00:23,  1.33it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.54s. Batch: 0.76s. S_Loss: 0.8982. T_Loss: 3.2698. Mask: 0.9840. :  40%|████      | 20/50 [00:15<00:19,  1.51it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.52s. Batch: 0.74s. S_Loss: 0.8990. T_Loss: 3.2542. Mask: 0.9829. :  40%|████      | 20/50 [00:15<00:19,  1.51it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.52s. Batch: 0.74s. S_Loss: 0.8990. T_Loss: 3.2542. Mask: 0.9829. :  42%|████▏     | 21/50 [00:15<00:16,  1.75it/s]total : 1000  current step :  769
total : 1000  current step :  770
total : 1000  current step :  771
Train Iter: 772/1000. LR: 0.0483. Data: 0.54s. Batch: 0.76s. S_Loss: 0.8991. T_Loss: 3.2524. Mask: 0.9835. :  42%|████▏     | 21/50 [00:16<00:16,  1.75it/s]Train Iter: 772/1000. LR: 0.0483. Data: 0.54s. Batch: 0.76s. S_Loss: 0.8991. T_Loss: 3.2524. Mask: 0.9835. :  44%|████▍     | 22/50 [00:16<00:22,  1.27it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.52s. Batch: 0.75s. S_Loss: 0.9009. T_Loss: 3.2311. Mask: 0.9832. :  44%|████▍     | 22/50 [00:17<00:22,  1.27it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.52s. Batch: 0.75s. S_Loss: 0.9009. T_Loss: 3.2311. Mask: 0.9832. :  46%|████▌     | 23/50 [00:17<00:17,  1.50it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.51s. Batch: 0.73s. S_Loss: 0.9022. T_Loss: 3.2327. Mask: 0.9834. :  46%|████▌     | 23/50 [00:17<00:17,  1.50it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.51s. Batch: 0.73s. S_Loss: 0.9022. T_Loss: 3.2327. Mask: 0.9834. :  48%|████▊     | 24/50 [00:17<00:14,  1.77it/s]total : 1000  current step :  772
total : 1000  current step :  773
total : 1000  current step :  774
Train Iter: 775/1000. LR: 0.0484. Data: 0.52s. Batch: 0.75s. S_Loss: 0.9052. T_Loss: 3.2287. Mask: 0.9833. :  48%|████▊     | 24/50 [00:18<00:14,  1.77it/s]Train Iter: 775/1000. LR: 0.0484. Data: 0.52s. Batch: 0.75s. S_Loss: 0.9052. T_Loss: 3.2287. Mask: 0.9833. :  50%|█████     | 25/50 [00:18<00:18,  1.34it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.51s. Batch: 0.73s. S_Loss: 0.9061. T_Loss: 3.2269. Mask: 0.9832. :  50%|█████     | 25/50 [00:19<00:18,  1.34it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.51s. Batch: 0.73s. S_Loss: 0.9061. T_Loss: 3.2269. Mask: 0.9832. :  52%|█████▏    | 26/50 [00:19<00:14,  1.63it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.49s. Batch: 0.72s. S_Loss: 0.9073. T_Loss: 3.2256. Mask: 0.9834. :  52%|█████▏    | 26/50 [00:19<00:14,  1.63it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.49s. Batch: 0.72s. S_Loss: 0.9073. T_Loss: 3.2256. Mask: 0.9834. :  54%|█████▍    | 27/50 [00:19<00:12,  1.87it/s]total : 1000  current step :  775
total : 1000  current step :  776
total : 1000  current step :  777
Train Iter: 778/1000. LR: 0.0486. Data: 0.51s. Batch: 0.73s. S_Loss: 0.9068. T_Loss: 3.2322. Mask: 0.9837. :  54%|█████▍    | 27/50 [00:20<00:12,  1.87it/s]Train Iter: 778/1000. LR: 0.0486. Data: 0.51s. Batch: 0.73s. S_Loss: 0.9068. T_Loss: 3.2322. Mask: 0.9837. :  56%|█████▌    | 28/50 [00:20<00:16,  1.37it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.50s. Batch: 0.72s. S_Loss: 0.9064. T_Loss: 3.2433. Mask: 0.9841. :  56%|█████▌    | 28/50 [00:20<00:16,  1.37it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.50s. Batch: 0.72s. S_Loss: 0.9064. T_Loss: 3.2433. Mask: 0.9841. :  58%|█████▊    | 29/50 [00:20<00:12,  1.64it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9068. T_Loss: 3.2434. Mask: 0.9842. :  58%|█████▊    | 29/50 [00:21<00:12,  1.64it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9068. T_Loss: 3.2434. Mask: 0.9842. :  60%|██████    | 30/50 [00:21<00:10,  1.93it/s]total : 1000  current step :  778
total : 1000  current step :  779
total : 1000  current step :  780
Train Iter: 781/1000. LR: 0.0488. Data: 0.50s. Batch: 0.72s. S_Loss: 0.9071. T_Loss: 3.2468. Mask: 0.9842. :  60%|██████    | 30/50 [00:22<00:10,  1.93it/s]Train Iter: 781/1000. LR: 0.0488. Data: 0.50s. Batch: 0.72s. S_Loss: 0.9071. T_Loss: 3.2468. Mask: 0.9842. :  62%|██████▏   | 31/50 [00:22<00:14,  1.34it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.49s. Batch: 0.71s. S_Loss: 0.9064. T_Loss: 3.2504. Mask: 0.9846. :  62%|██████▏   | 31/50 [00:22<00:14,  1.34it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.49s. Batch: 0.71s. S_Loss: 0.9064. T_Loss: 3.2504. Mask: 0.9846. :  64%|██████▍   | 32/50 [00:22<00:11,  1.63it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9042. T_Loss: 3.2430. Mask: 0.9847. :  64%|██████▍   | 32/50 [00:23<00:11,  1.63it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9042. T_Loss: 3.2430. Mask: 0.9847. :  66%|██████▌   | 33/50 [00:23<00:08,  1.98it/s]total : 1000  current step :  781
total : 1000  current step :  782
total : 1000  current step :  783
Train Iter: 784/1000. LR: 0.0490. Data: 0.49s. Batch: 0.71s. S_Loss: 0.9038. T_Loss: 3.2476. Mask: 0.9851. :  66%|██████▌   | 33/50 [00:24<00:08,  1.98it/s]Train Iter: 784/1000. LR: 0.0490. Data: 0.49s. Batch: 0.71s. S_Loss: 0.9038. T_Loss: 3.2476. Mask: 0.9851. :  68%|██████▊   | 34/50 [00:24<00:11,  1.34it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9029. T_Loss: 3.2436. Mask: 0.9855. :  68%|██████▊   | 34/50 [00:24<00:11,  1.34it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9029. T_Loss: 3.2436. Mask: 0.9855. :  70%|███████   | 35/50 [00:24<00:09,  1.60it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9039. T_Loss: 3.2436. Mask: 0.9850. :  70%|███████   | 35/50 [00:24<00:09,  1.60it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9039. T_Loss: 3.2436. Mask: 0.9850. :  72%|███████▏  | 36/50 [00:24<00:07,  1.89it/s]total : 1000  current step :  784
total : 1000  current step :  785
total : 1000  current step :  786
Train Iter: 787/1000. LR: 0.0492. Data: 0.49s. Batch: 0.71s. S_Loss: 0.9028. T_Loss: 3.2370. Mask: 0.9851. :  72%|███████▏  | 36/50 [00:26<00:07,  1.89it/s]Train Iter: 787/1000. LR: 0.0492. Data: 0.49s. Batch: 0.71s. S_Loss: 0.9028. T_Loss: 3.2370. Mask: 0.9851. :  74%|███████▍  | 37/50 [00:26<00:09,  1.34it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9018. T_Loss: 3.2277. Mask: 0.9853. :  74%|███████▍  | 37/50 [00:26<00:09,  1.34it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9018. T_Loss: 3.2277. Mask: 0.9853. :  76%|███████▌  | 38/50 [00:26<00:07,  1.61it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9026. T_Loss: 3.2432. Mask: 0.9856. :  76%|███████▌  | 38/50 [00:26<00:07,  1.61it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9026. T_Loss: 3.2432. Mask: 0.9856. :  78%|███████▊  | 39/50 [00:26<00:05,  1.87it/s]total : 1000  current step :  787
total : 1000  current step :  788
total : 1000  current step :  789
Train Iter: 790/1000. LR: 0.0494. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9022. T_Loss: 3.2507. Mask: 0.9856. :  78%|███████▊  | 39/50 [00:28<00:05,  1.87it/s]Train Iter: 790/1000. LR: 0.0494. Data: 0.48s. Batch: 0.70s. S_Loss: 0.9022. T_Loss: 3.2507. Mask: 0.9856. :  80%|████████  | 40/50 [00:28<00:07,  1.40it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9012. T_Loss: 3.2511. Mask: 0.9857. :  80%|████████  | 40/50 [00:28<00:07,  1.40it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9012. T_Loss: 3.2511. Mask: 0.9857. :  82%|████████▏ | 41/50 [00:28<00:05,  1.65it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9014. T_Loss: 3.2569. Mask: 0.9858. :  82%|████████▏ | 41/50 [00:28<00:05,  1.65it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9014. T_Loss: 3.2569. Mask: 0.9858. :  84%|████████▍ | 42/50 [00:28<00:04,  1.95it/s]total : 1000  current step :  790
total : 1000  current step :  791
total : 1000  current step :  792
Train Iter: 793/1000. LR: 0.0496. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9007. T_Loss: 3.2502. Mask: 0.9858. :  84%|████████▍ | 42/50 [00:29<00:04,  1.95it/s]Train Iter: 793/1000. LR: 0.0496. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9007. T_Loss: 3.2502. Mask: 0.9858. :  86%|████████▌ | 43/50 [00:29<00:04,  1.56it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9002. T_Loss: 3.2470. Mask: 0.9858. :  86%|████████▌ | 43/50 [00:29<00:04,  1.56it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9002. T_Loss: 3.2470. Mask: 0.9858. :  88%|████████▊ | 44/50 [00:29<00:03,  1.81it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9000. T_Loss: 3.2445. Mask: 0.9859. :  88%|████████▊ | 44/50 [00:30<00:03,  1.81it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9000. T_Loss: 3.2445. Mask: 0.9859. :  90%|█████████ | 45/50 [00:30<00:02,  1.89it/s]total : 1000  current step :  793
total : 1000  current step :  794
total : 1000  current step :  795
Train Iter: 796/1000. LR: 0.0498. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9009. T_Loss: 3.2443. Mask: 0.9857. :  90%|█████████ | 45/50 [00:31<00:02,  1.89it/s]Train Iter: 796/1000. LR: 0.0498. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9009. T_Loss: 3.2443. Mask: 0.9857. :  92%|█████████▏| 46/50 [00:31<00:03,  1.33it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9010. T_Loss: 3.2416. Mask: 0.9860. :  92%|█████████▏| 46/50 [00:32<00:03,  1.33it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9010. T_Loss: 3.2416. Mask: 0.9860. :  94%|█████████▍| 47/50 [00:32<00:01,  1.61it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9009. T_Loss: 3.2397. Mask: 0.9859. :  94%|█████████▍| 47/50 [00:32<00:01,  1.61it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9009. T_Loss: 3.2397. Mask: 0.9859. :  96%|█████████▌| 48/50 [00:32<00:01,  1.94it/s]total : 1000  current step :  796
total : 1000  current step :  797
total : 1000  current step :  798
Train Iter: 799/1000. LR: 0.0499. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9004. T_Loss: 3.2320. Mask: 0.9857. :  96%|█████████▌| 48/50 [00:33<00:01,  1.94it/s]Train Iter: 799/1000. LR: 0.0499. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9004. T_Loss: 3.2320. Mask: 0.9857. :  98%|█████████▊| 49/50 [00:33<00:00,  1.40it/s]total : 1000  current step :  799
Train Iter: 800/1000. LR: 0.0500. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8992. T_Loss: 3.2245. Mask: 0.9859. :  98%|█████████▊| 49/50 [00:34<00:00,  1.40it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8992. T_Loss: 3.2245. Mask: 0.9859. : 100%|██████████| 50/50 [00:34<00:00,  1.45it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8992. T_Loss: 3.2245. Mask: 0.9859. : 100%|██████████| 50/50 [00:34<00:00,  1.47it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8671. top1: 94.92. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8671. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.62it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8820. top1: 94.53. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.62it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8820. top1: 94.53. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8849. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8849. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8854. top1: 94.34. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8854. top1: 94.34. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9067. top1: 93.36. top5: 99.92. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9067. top1: 93.36. top5: 99.92. :  62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9153. top1: 93.16. top5: 99.93. :  62%|██████▎   | 5/8 [00:02<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9153. top1: 93.16. top5: 99.93. :  75%|███████▌  | 6/8 [00:02<00:00,  2.91it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9207. top1: 93.25. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  2.91it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9207. top1: 93.25. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9275. top1: 93.05. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9275. top1: 93.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9275. top1: 93.05. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.62it/s]
total : 1000  current step :  800
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.01s. Batch: 0.21s. S_Loss: 0.8919. T_Loss: 3.4663. Mask: 0.9883. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.01s. Batch: 0.21s. S_Loss: 0.8919. T_Loss: 3.4663. Mask: 0.9883. :   2%|▏         | 1/50 [00:00<00:10,  4.63it/s]total : 1000  current step :  801
Train Iter: 802/1000. LR: 0.0500. Data: 0.45s. Batch: 0.65s. S_Loss: 0.8885. T_Loss: 3.2997. Mask: 0.9805. :   2%|▏         | 1/50 [00:01<00:10,  4.63it/s]Train Iter: 802/1000. LR: 0.0500. Data: 0.45s. Batch: 0.65s. S_Loss: 0.8885. T_Loss: 3.2997. Mask: 0.9805. :   4%|▍         | 2/50 [00:01<00:35,  1.37it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9040. T_Loss: 3.2832. Mask: 0.9857. :   4%|▍         | 2/50 [00:01<00:35,  1.37it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9040. T_Loss: 3.2832. Mask: 0.9857. :   6%|▌         | 3/50 [00:01<00:27,  1.69it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9125. T_Loss: 3.3270. Mask: 0.9844. :   6%|▌         | 3/50 [00:02<00:27,  1.69it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9125. T_Loss: 3.3270. Mask: 0.9844. :   8%|▊         | 4/50 [00:02<00:24,  1.88it/s]total : 1000  current step :  802
total : 1000  current step :  803
total : 1000  current step :  804
Train Iter: 805/1000. LR: 0.0499. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9060. T_Loss: 3.2793. Mask: 0.9852. :   8%|▊         | 4/50 [00:03<00:24,  1.88it/s]Train Iter: 805/1000. LR: 0.0499. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9060. T_Loss: 3.2793. Mask: 0.9852. :  10%|█         | 5/50 [00:03<00:33,  1.33it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9032. T_Loss: 3.2901. Mask: 0.9850. :  10%|█         | 5/50 [00:03<00:33,  1.33it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9032. T_Loss: 3.2901. Mask: 0.9850. :  12%|█▏        | 6/50 [00:03<00:26,  1.63it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9069. T_Loss: 3.3019. Mask: 0.9855. :  12%|█▏        | 6/50 [00:04<00:26,  1.63it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9069. T_Loss: 3.3019. Mask: 0.9855. :  14%|█▍        | 7/50 [00:04<00:23,  1.81it/s]total : 1000  current step :  805
total : 1000  current step :  806
total : 1000  current step :  807
Train Iter: 808/1000. LR: 0.0498. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9043. T_Loss: 3.2918. Mask: 0.9863. :  14%|█▍        | 7/50 [00:05<00:23,  1.81it/s]Train Iter: 808/1000. LR: 0.0498. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9043. T_Loss: 3.2918. Mask: 0.9863. :  16%|█▌        | 8/50 [00:05<00:29,  1.44it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9020. T_Loss: 3.2679. Mask: 0.9852. :  16%|█▌        | 8/50 [00:05<00:29,  1.44it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9020. T_Loss: 3.2679. Mask: 0.9852. :  18%|█▊        | 9/50 [00:05<00:26,  1.56it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8972. T_Loss: 3.2361. Mask: 0.9840. :  18%|█▊        | 9/50 [00:05<00:26,  1.56it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8972. T_Loss: 3.2361. Mask: 0.9840. :  20%|██        | 10/50 [00:05<00:20,  1.94it/s]total : 1000  current step :  808
total : 1000  current step :  809
total : 1000  current step :  810
Train Iter: 811/1000. LR: 0.0496. Data: 0.44s. Batch: 0.64s. S_Loss: 0.8933. T_Loss: 3.2015. Mask: 0.9833. :  20%|██        | 10/50 [00:07<00:20,  1.94it/s]Train Iter: 811/1000. LR: 0.0496. Data: 0.44s. Batch: 0.64s. S_Loss: 0.8933. T_Loss: 3.2015. Mask: 0.9833. :  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8919. T_Loss: 3.1978. Mask: 0.9837. :  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8919. T_Loss: 3.1978. Mask: 0.9837. :  24%|██▍       | 12/50 [00:07<00:22,  1.66it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8933. T_Loss: 3.1886. Mask: 0.9838. :  24%|██▍       | 12/50 [00:07<00:22,  1.66it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8933. T_Loss: 3.1886. Mask: 0.9838. :  26%|██▌       | 13/50 [00:07<00:18,  1.97it/s]total : 1000  current step :  811
total : 1000  current step :  812
total : 1000  current step :  813
Train Iter: 814/1000. LR: 0.0494. Data: 0.42s. Batch: 0.62s. S_Loss: 0.8929. T_Loss: 3.1894. Mask: 0.9847. :  26%|██▌       | 13/50 [00:08<00:18,  1.97it/s]Train Iter: 814/1000. LR: 0.0494. Data: 0.42s. Batch: 0.62s. S_Loss: 0.8929. T_Loss: 3.1894. Mask: 0.9847. :  28%|██▊       | 14/50 [00:08<00:24,  1.47it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8925. T_Loss: 3.1768. Mask: 0.9836. :  28%|██▊       | 14/50 [00:09<00:24,  1.47it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8925. T_Loss: 3.1768. Mask: 0.9836. :  30%|███       | 15/50 [00:09<00:20,  1.68it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8915. T_Loss: 3.1921. Mask: 0.9839. :  30%|███       | 15/50 [00:09<00:20,  1.68it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8915. T_Loss: 3.1921. Mask: 0.9839. :  32%|███▏      | 16/50 [00:09<00:17,  1.92it/s]total : 1000  current step :  814
total : 1000  current step :  815
total : 1000  current step :  816
Train Iter: 817/1000. LR: 0.0491. Data: 0.42s. Batch: 0.62s. S_Loss: 0.8905. T_Loss: 3.2016. Mask: 0.9846. :  32%|███▏      | 16/50 [00:10<00:17,  1.92it/s]Train Iter: 817/1000. LR: 0.0491. Data: 0.42s. Batch: 0.62s. S_Loss: 0.8905. T_Loss: 3.2016. Mask: 0.9846. :  34%|███▍      | 17/50 [00:10<00:23,  1.43it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8886. T_Loss: 3.2010. Mask: 0.9842. :  34%|███▍      | 17/50 [00:10<00:23,  1.43it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8886. T_Loss: 3.2010. Mask: 0.9842. :  36%|███▌      | 18/50 [00:10<00:19,  1.67it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8869. T_Loss: 3.1822. Mask: 0.9836. :  36%|███▌      | 18/50 [00:11<00:19,  1.67it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8869. T_Loss: 3.1822. Mask: 0.9836. :  38%|███▊      | 19/50 [00:11<00:17,  1.79it/s]total : 1000  current step :  817
total : 1000  current step :  818
total : 1000  current step :  819
Train Iter: 820/1000. LR: 0.0488. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8870. T_Loss: 3.1823. Mask: 0.9832. :  38%|███▊      | 19/50 [00:12<00:17,  1.79it/s]Train Iter: 820/1000. LR: 0.0488. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8870. T_Loss: 3.1823. Mask: 0.9832. :  40%|████      | 20/50 [00:12<00:22,  1.32it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8868. T_Loss: 3.1772. Mask: 0.9834. :  40%|████      | 20/50 [00:12<00:22,  1.32it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8868. T_Loss: 3.1772. Mask: 0.9834. :  42%|████▏     | 21/50 [00:12<00:17,  1.61it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8870. T_Loss: 3.1747. Mask: 0.9837. :  42%|████▏     | 21/50 [00:13<00:17,  1.61it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8870. T_Loss: 3.1747. Mask: 0.9837. :  44%|████▍     | 22/50 [00:13<00:16,  1.73it/s]total : 1000  current step :  820
total : 1000  current step :  821
total : 1000  current step :  822
Train Iter: 823/1000. LR: 0.0484. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8873. T_Loss: 3.1925. Mask: 0.9842. :  44%|████▍     | 22/50 [00:14<00:16,  1.73it/s]Train Iter: 823/1000. LR: 0.0484. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8873. T_Loss: 3.1925. Mask: 0.9842. :  46%|████▌     | 23/50 [00:14<00:20,  1.29it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8868. T_Loss: 3.1965. Mask: 0.9842. :  46%|████▌     | 23/50 [00:14<00:20,  1.29it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8868. T_Loss: 3.1965. Mask: 0.9842. :  48%|████▊     | 24/50 [00:14<00:16,  1.59it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8864. T_Loss: 3.2020. Mask: 0.9842. :  48%|████▊     | 24/50 [00:15<00:16,  1.59it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8864. T_Loss: 3.2020. Mask: 0.9842. :  50%|█████     | 25/50 [00:15<00:14,  1.78it/s]total : 1000  current step :  823
total : 1000  current step :  824
total : 1000  current step :  825
Train Iter: 826/1000. LR: 0.0479. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8856. T_Loss: 3.2031. Mask: 0.9841. :  50%|█████     | 25/50 [00:16<00:14,  1.78it/s]Train Iter: 826/1000. LR: 0.0479. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8856. T_Loss: 3.2031. Mask: 0.9841. :  52%|█████▏    | 26/50 [00:16<00:18,  1.30it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8859. T_Loss: 3.2011. Mask: 0.9838. :  52%|█████▏    | 26/50 [00:17<00:18,  1.30it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8859. T_Loss: 3.2011. Mask: 0.9838. :  54%|█████▍    | 27/50 [00:17<00:15,  1.52it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8858. T_Loss: 3.2038. Mask: 0.9835. :  54%|█████▍    | 27/50 [00:17<00:15,  1.52it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8858. T_Loss: 3.2038. Mask: 0.9835. :  56%|█████▌    | 28/50 [00:17<00:13,  1.67it/s]total : 1000  current step :  826
total : 1000  current step :  827
total : 1000  current step :  828
Train Iter: 829/1000. LR: 0.0475. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8852. T_Loss: 3.2056. Mask: 0.9840. :  56%|█████▌    | 28/50 [00:18<00:13,  1.67it/s]Train Iter: 829/1000. LR: 0.0475. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8852. T_Loss: 3.2056. Mask: 0.9840. :  58%|█████▊    | 29/50 [00:18<00:16,  1.29it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8852. T_Loss: 3.2061. Mask: 0.9844. :  58%|█████▊    | 29/50 [00:19<00:16,  1.29it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8852. T_Loss: 3.2061. Mask: 0.9844. :  60%|██████    | 30/50 [00:19<00:12,  1.54it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8855. T_Loss: 3.1962. Mask: 0.9844. :  60%|██████    | 30/50 [00:19<00:12,  1.54it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8855. T_Loss: 3.1962. Mask: 0.9844. :  62%|██████▏   | 31/50 [00:19<00:10,  1.76it/s]total : 1000  current step :  829
total : 1000  current step :  830
total : 1000  current step :  831
Train Iter: 832/1000. LR: 0.0469. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8848. T_Loss: 3.1937. Mask: 0.9844. :  62%|██████▏   | 31/50 [00:20<00:10,  1.76it/s]Train Iter: 832/1000. LR: 0.0469. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8848. T_Loss: 3.1937. Mask: 0.9844. :  64%|██████▍   | 32/50 [00:20<00:13,  1.29it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8848. T_Loss: 3.1882. Mask: 0.9845. :  64%|██████▍   | 32/50 [00:21<00:13,  1.29it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8848. T_Loss: 3.1882. Mask: 0.9845. :  66%|██████▌   | 33/50 [00:21<00:11,  1.48it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8847. T_Loss: 3.1847. Mask: 0.9845. :  66%|██████▌   | 33/50 [00:21<00:11,  1.48it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8847. T_Loss: 3.1847. Mask: 0.9845. :  68%|██████▊   | 34/50 [00:21<00:09,  1.69it/s]total : 1000  current step :  832
total : 1000  current step :  833
total : 1000  current step :  834
Train Iter: 835/1000. LR: 0.0463. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8849. T_Loss: 3.1762. Mask: 0.9844. :  68%|██████▊   | 34/50 [00:22<00:09,  1.69it/s]Train Iter: 835/1000. LR: 0.0463. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8849. T_Loss: 3.1762. Mask: 0.9844. :  70%|███████   | 35/50 [00:22<00:11,  1.29it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8851. T_Loss: 3.1758. Mask: 0.9845. :  70%|███████   | 35/50 [00:22<00:11,  1.29it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8851. T_Loss: 3.1758. Mask: 0.9845. :  72%|███████▏  | 36/50 [00:22<00:08,  1.62it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8854. T_Loss: 3.1731. Mask: 0.9846. :  72%|███████▏  | 36/50 [00:23<00:08,  1.62it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8854. T_Loss: 3.1731. Mask: 0.9846. :  74%|███████▍  | 37/50 [00:23<00:06,  1.87it/s]total : 1000  current step :  835
total : 1000  current step :  836
total : 1000  current step :  837
Train Iter: 838/1000. LR: 0.0457. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8854. T_Loss: 3.1623. Mask: 0.9845. :  74%|███████▍  | 37/50 [00:24<00:06,  1.87it/s]Train Iter: 838/1000. LR: 0.0457. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8854. T_Loss: 3.1623. Mask: 0.9845. :  76%|███████▌  | 38/50 [00:24<00:09,  1.30it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8842. T_Loss: 3.1500. Mask: 0.9845. :  76%|███████▌  | 38/50 [00:24<00:09,  1.30it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8842. T_Loss: 3.1500. Mask: 0.9845. :  78%|███████▊  | 39/50 [00:24<00:07,  1.55it/s]total : 1000  current step :  838
total : 1000  current step :  839
Train Iter: 840/1000. LR: 0.0452. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8828. T_Loss: 3.1355. Mask: 0.9843. :  78%|███████▊  | 39/50 [00:25<00:07,  1.55it/s]Train Iter: 840/1000. LR: 0.0452. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8828. T_Loss: 3.1355. Mask: 0.9843. :  80%|████████  | 40/50 [00:25<00:06,  1.51it/s]total : 1000  current step :  840
Train Iter: 841/1000. LR: 0.0450. Data: 0.45s. Batch: 0.66s. S_Loss: 0.8821. T_Loss: 3.1267. Mask: 0.9842. :  80%|████████  | 40/50 [00:27<00:06,  1.51it/s]Train Iter: 841/1000. LR: 0.0450. Data: 0.45s. Batch: 0.66s. S_Loss: 0.8821. T_Loss: 3.1267. Mask: 0.9842. :  82%|████████▏ | 41/50 [00:27<00:07,  1.16it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8823. T_Loss: 3.1261. Mask: 0.9844. :  82%|████████▏ | 41/50 [00:27<00:07,  1.16it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8823. T_Loss: 3.1261. Mask: 0.9844. :  84%|████████▍ | 42/50 [00:27<00:05,  1.47it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8820. T_Loss: 3.1252. Mask: 0.9845. :  84%|████████▍ | 42/50 [00:27<00:05,  1.47it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8820. T_Loss: 3.1252. Mask: 0.9845. :  86%|████████▌ | 43/50 [00:27<00:03,  1.76it/s]total : 1000  current step :  841
total : 1000  current step :  842
total : 1000  current step :  843
Train Iter: 844/1000. LR: 0.0443. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8817. T_Loss: 3.1158. Mask: 0.9847. :  86%|████████▌ | 43/50 [00:28<00:03,  1.76it/s]Train Iter: 844/1000. LR: 0.0443. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8817. T_Loss: 3.1158. Mask: 0.9847. :  88%|████████▊ | 44/50 [00:28<00:04,  1.26it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8814. T_Loss: 3.1089. Mask: 0.9849. :  88%|████████▊ | 44/50 [00:29<00:04,  1.26it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8814. T_Loss: 3.1089. Mask: 0.9849. :  90%|█████████ | 45/50 [00:29<00:03,  1.49it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8821. T_Loss: 3.1150. Mask: 0.9851. :  90%|█████████ | 45/50 [00:29<00:03,  1.49it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8821. T_Loss: 3.1150. Mask: 0.9851. :  92%|█████████▏| 46/50 [00:29<00:02,  1.76it/s]total : 1000  current step :  844
total : 1000  current step :  845
total : 1000  current step :  846
Train Iter: 847/1000. LR: 0.0435. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8820. T_Loss: 3.1090. Mask: 0.9852. :  92%|█████████▏| 46/50 [00:30<00:02,  1.76it/s]Train Iter: 847/1000. LR: 0.0435. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8820. T_Loss: 3.1090. Mask: 0.9852. :  94%|█████████▍| 47/50 [00:30<00:02,  1.31it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8823. T_Loss: 3.1121. Mask: 0.9852. :  94%|█████████▍| 47/50 [00:31<00:02,  1.31it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8823. T_Loss: 3.1121. Mask: 0.9852. :  96%|█████████▌| 48/50 [00:31<00:01,  1.58it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8816. T_Loss: 3.1082. Mask: 0.9850. :  96%|█████████▌| 48/50 [00:31<00:01,  1.58it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8816. T_Loss: 3.1082. Mask: 0.9850. :  98%|█████████▊| 49/50 [00:31<00:00,  1.75it/s]total : 1000  current step :  847
total : 1000  current step :  848
total : 1000  current step :  849
Train Iter: 850/1000. LR: 0.0427. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8816. T_Loss: 3.1111. Mask: 0.9851. :  98%|█████████▊| 49/50 [00:32<00:00,  1.75it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8816. T_Loss: 3.1111. Mask: 0.9851. : 100%|██████████| 50/50 [00:32<00:00,  1.34it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.44s. Batch: 0.65s. S_Loss: 0.8816. T_Loss: 3.1111. Mask: 0.9851. : 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 0.8332. top1: 97.27. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 0.8332. top1: 97.27. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8458. top1: 96.29. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8458. top1: 96.29. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8484. top1: 96.35. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8484. top1: 96.35. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8501. top1: 96.29. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8501. top1: 96.29. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8828. top1: 94.92. top5: 99.92. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8828. top1: 94.92. top5: 99.92. :  62%|██████▎   | 5/8 [00:01<00:00,  3.03it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8999. top1: 94.27. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.03it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8999. top1: 94.27. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9105. top1: 93.92. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9105. top1: 93.92. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9204. top1: 93.40. top5: 99.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9204. top1: 93.40. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9204. top1: 93.40. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
total : 1000  current step :  850
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.21s. S_Loss: 0.9489. T_Loss: 3.3809. Mask: 0.9766. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.21s. S_Loss: 0.9489. T_Loss: 3.3809. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:10,  4.69it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.20s. S_Loss: 0.9148. T_Loss: 3.2561. Mask: 0.9844. :   2%|▏         | 1/50 [00:00<00:10,  4.69it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.20s. S_Loss: 0.9148. T_Loss: 3.2561. Mask: 0.9844. :   4%|▍         | 2/50 [00:00<00:09,  4.91it/s]total : 1000  current step :  851
total : 1000  current step :  852
Train Iter: 853/1000. LR: 0.0418. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9130. T_Loss: 3.2160. Mask: 0.9818. :   4%|▍         | 2/50 [00:01<00:09,  4.91it/s]Train Iter: 853/1000. LR: 0.0418. Data: 0.32s. Batch: 0.50s. S_Loss: 0.9130. T_Loss: 3.2160. Mask: 0.9818. :   6%|▌         | 3/50 [00:01<00:28,  1.65it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.27s. Batch: 0.45s. S_Loss: 0.9086. T_Loss: 3.2788. Mask: 0.9834. :   6%|▌         | 3/50 [00:01<00:28,  1.65it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.27s. Batch: 0.45s. S_Loss: 0.9086. T_Loss: 3.2788. Mask: 0.9834. :   8%|▊         | 4/50 [00:01<00:22,  2.01it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.26s. Batch: 0.44s. S_Loss: 0.9128. T_Loss: 3.2973. Mask: 0.9820. :   8%|▊         | 4/50 [00:02<00:22,  2.01it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.26s. Batch: 0.44s. S_Loss: 0.9128. T_Loss: 3.2973. Mask: 0.9820. :  10%|█         | 5/50 [00:02<00:20,  2.19it/s]total : 1000  current step :  853
total : 1000  current step :  854
total : 1000  current step :  855
Train Iter: 856/1000. LR: 0.0409. Data: 0.35s. Batch: 0.54s. S_Loss: 0.9085. T_Loss: 3.2780. Mask: 0.9805. :  10%|█         | 5/50 [00:03<00:20,  2.19it/s]Train Iter: 856/1000. LR: 0.0409. Data: 0.35s. Batch: 0.54s. S_Loss: 0.9085. T_Loss: 3.2780. Mask: 0.9805. :  12%|█▏        | 6/50 [00:03<00:28,  1.53it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.32s. Batch: 0.51s. S_Loss: 0.8956. T_Loss: 3.1881. Mask: 0.9821. :  12%|█▏        | 6/50 [00:03<00:28,  1.53it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.32s. Batch: 0.51s. S_Loss: 0.8956. T_Loss: 3.1881. Mask: 0.9821. :  14%|█▍        | 7/50 [00:03<00:23,  1.83it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.30s. Batch: 0.49s. S_Loss: 0.8918. T_Loss: 3.1906. Mask: 0.9819. :  14%|█▍        | 7/50 [00:03<00:23,  1.83it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.30s. Batch: 0.49s. S_Loss: 0.8918. T_Loss: 3.1906. Mask: 0.9819. :  16%|█▌        | 8/50 [00:03<00:20,  2.09it/s]total : 1000  current step :  856
total : 1000  current step :  857
total : 1000  current step :  858
Train Iter: 859/1000. LR: 0.0400. Data: 0.37s. Batch: 0.56s. S_Loss: 0.8852. T_Loss: 3.1642. Mask: 0.9839. :  16%|█▌        | 8/50 [00:05<00:20,  2.09it/s]Train Iter: 859/1000. LR: 0.0400. Data: 0.37s. Batch: 0.56s. S_Loss: 0.8852. T_Loss: 3.1642. Mask: 0.9839. :  18%|█▊        | 9/50 [00:05<00:27,  1.48it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.35s. Batch: 0.53s. S_Loss: 0.8829. T_Loss: 3.1480. Mask: 0.9852. :  18%|█▊        | 9/50 [00:05<00:27,  1.48it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.35s. Batch: 0.53s. S_Loss: 0.8829. T_Loss: 3.1480. Mask: 0.9852. :  20%|██        | 10/50 [00:05<00:22,  1.75it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.34s. Batch: 0.53s. S_Loss: 0.8822. T_Loss: 3.1524. Mask: 0.9862. :  20%|██        | 10/50 [00:05<00:22,  1.75it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.34s. Batch: 0.53s. S_Loss: 0.8822. T_Loss: 3.1524. Mask: 0.9862. :  22%|██▏       | 11/50 [00:05<00:21,  1.81it/s]total : 1000  current step :  859
total : 1000  current step :  860
total : 1000  current step :  861
Train Iter: 862/1000. LR: 0.0391. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8827. T_Loss: 3.1789. Mask: 0.9854. :  22%|██▏       | 11/50 [00:06<00:21,  1.81it/s]Train Iter: 862/1000. LR: 0.0391. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8827. T_Loss: 3.1789. Mask: 0.9854. :  24%|██▍       | 12/50 [00:06<00:27,  1.37it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.37s. Batch: 0.56s. S_Loss: 0.8833. T_Loss: 3.2046. Mask: 0.9853. :  24%|██▍       | 12/50 [00:07<00:27,  1.37it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.37s. Batch: 0.56s. S_Loss: 0.8833. T_Loss: 3.2046. Mask: 0.9853. :  26%|██▌       | 13/50 [00:07<00:22,  1.65it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.36s. Batch: 0.55s. S_Loss: 0.8819. T_Loss: 3.2073. Mask: 0.9858. :  26%|██▌       | 13/50 [00:07<00:22,  1.65it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.36s. Batch: 0.55s. S_Loss: 0.8819. T_Loss: 3.2073. Mask: 0.9858. :  28%|██▊       | 14/50 [00:07<00:19,  1.81it/s]total : 1000  current step :  862
total : 1000  current step :  863
total : 1000  current step :  864
Train Iter: 865/1000. LR: 0.0381. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8795. T_Loss: 3.2054. Mask: 0.9862. :  28%|██▊       | 14/50 [00:08<00:19,  1.81it/s]Train Iter: 865/1000. LR: 0.0381. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8795. T_Loss: 3.2054. Mask: 0.9862. :  30%|███       | 15/50 [00:08<00:26,  1.35it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.37s. Batch: 0.57s. S_Loss: 0.8773. T_Loss: 3.1891. Mask: 0.9861. :  30%|███       | 15/50 [00:09<00:26,  1.35it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.37s. Batch: 0.57s. S_Loss: 0.8773. T_Loss: 3.1891. Mask: 0.9861. :  32%|███▏      | 16/50 [00:09<00:20,  1.70it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.36s. Batch: 0.56s. S_Loss: 0.8774. T_Loss: 3.1972. Mask: 0.9860. :  32%|███▏      | 16/50 [00:09<00:20,  1.70it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.36s. Batch: 0.56s. S_Loss: 0.8774. T_Loss: 3.1972. Mask: 0.9860. :  34%|███▍      | 17/50 [00:09<00:17,  1.89it/s]total : 1000  current step :  865
total : 1000  current step :  866
total : 1000  current step :  867
Train Iter: 868/1000. LR: 0.0370. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8762. T_Loss: 3.1869. Mask: 0.9857. :  34%|███▍      | 17/50 [00:10<00:17,  1.89it/s]Train Iter: 868/1000. LR: 0.0370. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8762. T_Loss: 3.1869. Mask: 0.9857. :  36%|███▌      | 18/50 [00:10<00:21,  1.46it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.37s. Batch: 0.57s. S_Loss: 0.8753. T_Loss: 3.1770. Mask: 0.9854. :  36%|███▌      | 18/50 [00:10<00:21,  1.46it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.37s. Batch: 0.57s. S_Loss: 0.8753. T_Loss: 3.1770. Mask: 0.9854. :  38%|███▊      | 19/50 [00:10<00:17,  1.73it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.36s. Batch: 0.56s. S_Loss: 0.8755. T_Loss: 3.1746. Mask: 0.9854. :  38%|███▊      | 19/50 [00:11<00:17,  1.73it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.36s. Batch: 0.56s. S_Loss: 0.8755. T_Loss: 3.1746. Mask: 0.9854. :  40%|████      | 20/50 [00:11<00:15,  1.99it/s]total : 1000  current step :  868
total : 1000  current step :  869
total : 1000  current step :  870
Train Iter: 871/1000. LR: 0.0360. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8761. T_Loss: 3.1784. Mask: 0.9851. :  40%|████      | 20/50 [00:12<00:15,  1.99it/s]Train Iter: 871/1000. LR: 0.0360. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8761. T_Loss: 3.1784. Mask: 0.9851. :  42%|████▏     | 21/50 [00:12<00:20,  1.41it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8756. T_Loss: 3.1849. Mask: 0.9849. :  42%|████▏     | 21/50 [00:12<00:20,  1.41it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8756. T_Loss: 3.1849. Mask: 0.9849. :  44%|████▍     | 22/50 [00:12<00:17,  1.61it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.36s. Batch: 0.57s. S_Loss: 0.8741. T_Loss: 3.1877. Mask: 0.9849. :  44%|████▍     | 22/50 [00:13<00:17,  1.61it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.36s. Batch: 0.57s. S_Loss: 0.8741. T_Loss: 3.1877. Mask: 0.9849. :  46%|████▌     | 23/50 [00:13<00:14,  1.92it/s]total : 1000  current step :  871
total : 1000  current step :  872
total : 1000  current step :  873
Train Iter: 874/1000. LR: 0.0349. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8751. T_Loss: 3.1840. Mask: 0.9847. :  46%|████▌     | 23/50 [00:14<00:14,  1.92it/s]Train Iter: 874/1000. LR: 0.0349. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8751. T_Loss: 3.1840. Mask: 0.9847. :  48%|████▊     | 24/50 [00:14<00:18,  1.40it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.37s. Batch: 0.58s. S_Loss: 0.8741. T_Loss: 3.1717. Mask: 0.9845. :  48%|████▊     | 24/50 [00:14<00:18,  1.40it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.37s. Batch: 0.58s. S_Loss: 0.8741. T_Loss: 3.1717. Mask: 0.9845. :  50%|█████     | 25/50 [00:14<00:14,  1.73it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.36s. Batch: 0.57s. S_Loss: 0.8746. T_Loss: 3.1594. Mask: 0.9848. :  50%|█████     | 25/50 [00:14<00:14,  1.73it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.36s. Batch: 0.57s. S_Loss: 0.8746. T_Loss: 3.1594. Mask: 0.9848. :  52%|█████▏    | 26/50 [00:14<00:12,  1.97it/s]total : 1000  current step :  874
total : 1000  current step :  875
total : 1000  current step :  876
Train Iter: 877/1000. LR: 0.0338. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8747. T_Loss: 3.1598. Mask: 0.9850. :  52%|█████▏    | 26/50 [00:15<00:12,  1.97it/s]Train Iter: 877/1000. LR: 0.0338. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8747. T_Loss: 3.1598. Mask: 0.9850. :  54%|█████▍    | 27/50 [00:15<00:15,  1.52it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.37s. Batch: 0.58s. S_Loss: 0.8745. T_Loss: 3.1545. Mask: 0.9854. :  54%|█████▍    | 27/50 [00:16<00:15,  1.52it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.37s. Batch: 0.58s. S_Loss: 0.8745. T_Loss: 3.1545. Mask: 0.9854. :  56%|█████▌    | 28/50 [00:16<00:11,  1.89it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.36s. Batch: 0.57s. S_Loss: 0.8730. T_Loss: 3.1484. Mask: 0.9856. :  56%|█████▌    | 28/50 [00:16<00:11,  1.89it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.36s. Batch: 0.57s. S_Loss: 0.8730. T_Loss: 3.1484. Mask: 0.9856. :  58%|█████▊    | 29/50 [00:16<00:09,  2.19it/s]total : 1000  current step :  877
total : 1000  current step :  878
total : 1000  current step :  879
Train Iter: 880/1000. LR: 0.0327. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8712. T_Loss: 3.1360. Mask: 0.9855. :  58%|█████▊    | 29/50 [00:17<00:09,  2.19it/s]Train Iter: 880/1000. LR: 0.0327. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8712. T_Loss: 3.1360. Mask: 0.9855. :  60%|██████    | 30/50 [00:17<00:14,  1.37it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8719. T_Loss: 3.1370. Mask: 0.9855. :  60%|██████    | 30/50 [00:18<00:14,  1.37it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8719. T_Loss: 3.1370. Mask: 0.9855. :  62%|██████▏   | 31/50 [00:18<00:14,  1.31it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8706. T_Loss: 3.1340. Mask: 0.9857. :  62%|██████▏   | 31/50 [00:19<00:14,  1.31it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8706. T_Loss: 3.1340. Mask: 0.9857. :  64%|██████▍   | 32/50 [00:19<00:12,  1.40it/s]total : 1000  current step :  880
total : 1000  current step :  881
total : 1000  current step :  882
Train Iter: 883/1000. LR: 0.0316. Data: 0.41s. Batch: 0.61s. S_Loss: 0.8703. T_Loss: 3.1338. Mask: 0.9857. :  64%|██████▍   | 32/50 [00:20<00:12,  1.40it/s]Train Iter: 883/1000. LR: 0.0316. Data: 0.41s. Batch: 0.61s. S_Loss: 0.8703. T_Loss: 3.1338. Mask: 0.9857. :  66%|██████▌   | 33/50 [00:20<00:13,  1.22it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8707. T_Loss: 3.1388. Mask: 0.9858. :  66%|██████▌   | 33/50 [00:20<00:13,  1.22it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8707. T_Loss: 3.1388. Mask: 0.9858. :  68%|██████▊   | 34/50 [00:20<00:10,  1.45it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8705. T_Loss: 3.1413. Mask: 0.9853. :  68%|██████▊   | 34/50 [00:21<00:10,  1.45it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8705. T_Loss: 3.1413. Mask: 0.9853. :  70%|███████   | 35/50 [00:21<00:08,  1.67it/s]total : 1000  current step :  883
total : 1000  current step :  884
total : 1000  current step :  885
Train Iter: 886/1000. LR: 0.0305. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8704. T_Loss: 3.1385. Mask: 0.9852. :  70%|███████   | 35/50 [00:22<00:08,  1.67it/s]Train Iter: 886/1000. LR: 0.0305. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8704. T_Loss: 3.1385. Mask: 0.9852. :  72%|███████▏  | 36/50 [00:22<00:09,  1.43it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8704. T_Loss: 3.1399. Mask: 0.9854. :  72%|███████▏  | 36/50 [00:22<00:09,  1.43it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8704. T_Loss: 3.1399. Mask: 0.9854. :  74%|███████▍  | 37/50 [00:22<00:07,  1.64it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8704. T_Loss: 3.1343. Mask: 0.9857. :  74%|███████▍  | 37/50 [00:22<00:07,  1.64it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8704. T_Loss: 3.1343. Mask: 0.9857. :  76%|███████▌  | 38/50 [00:22<00:06,  1.84it/s]total : 1000  current step :  886
total : 1000  current step :  887
total : 1000  current step :  888
Train Iter: 889/1000. LR: 0.0293. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8709. T_Loss: 3.1374. Mask: 0.9858. :  76%|███████▌  | 38/50 [00:23<00:06,  1.84it/s]Train Iter: 889/1000. LR: 0.0293. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8709. T_Loss: 3.1374. Mask: 0.9858. :  78%|███████▊  | 39/50 [00:23<00:07,  1.50it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8706. T_Loss: 3.1351. Mask: 0.9858. :  78%|███████▊  | 39/50 [00:24<00:07,  1.50it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8706. T_Loss: 3.1351. Mask: 0.9858. :  80%|████████  | 40/50 [00:24<00:05,  1.84it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8705. T_Loss: 3.1290. Mask: 0.9860. :  80%|████████  | 40/50 [00:24<00:05,  1.84it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8705. T_Loss: 3.1290. Mask: 0.9860. :  82%|████████▏ | 41/50 [00:24<00:04,  2.00it/s]total : 1000  current step :  889
total : 1000  current step :  890
total : 1000  current step :  891
Train Iter: 892/1000. LR: 0.0281. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8703. T_Loss: 3.1267. Mask: 0.9863. :  82%|████████▏ | 41/50 [00:25<00:04,  2.00it/s]Train Iter: 892/1000. LR: 0.0281. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8703. T_Loss: 3.1267. Mask: 0.9863. :  84%|████████▍ | 42/50 [00:25<00:05,  1.45it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8701. T_Loss: 3.1212. Mask: 0.9863. :  84%|████████▍ | 42/50 [00:25<00:05,  1.45it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8701. T_Loss: 3.1212. Mask: 0.9863. :  86%|████████▌ | 43/50 [00:25<00:03,  1.78it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8706. T_Loss: 3.1183. Mask: 0.9865. :  86%|████████▌ | 43/50 [00:26<00:03,  1.78it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8706. T_Loss: 3.1183. Mask: 0.9865. :  88%|████████▊ | 44/50 [00:26<00:03,  1.81it/s]total : 1000  current step :  892
total : 1000  current step :  893
total : 1000  current step :  894
Train Iter: 895/1000. LR: 0.0270. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8711. T_Loss: 3.1193. Mask: 0.9867. :  88%|████████▊ | 44/50 [00:27<00:03,  1.81it/s]Train Iter: 895/1000. LR: 0.0270. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8711. T_Loss: 3.1193. Mask: 0.9867. :  90%|█████████ | 45/50 [00:27<00:03,  1.44it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8705. T_Loss: 3.1158. Mask: 0.9867. :  90%|█████████ | 45/50 [00:27<00:03,  1.44it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8705. T_Loss: 3.1158. Mask: 0.9867. :  92%|█████████▏| 46/50 [00:27<00:02,  1.63it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8713. T_Loss: 3.1124. Mask: 0.9866. :  92%|█████████▏| 46/50 [00:28<00:02,  1.63it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8713. T_Loss: 3.1124. Mask: 0.9866. :  94%|█████████▍| 47/50 [00:28<00:01,  1.88it/s]total : 1000  current step :  895
total : 1000  current step :  896
total : 1000  current step :  897
Train Iter: 898/1000. LR: 0.0258. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8708. T_Loss: 3.1093. Mask: 0.9866. :  94%|█████████▍| 47/50 [00:29<00:01,  1.88it/s]Train Iter: 898/1000. LR: 0.0258. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8708. T_Loss: 3.1093. Mask: 0.9866. :  96%|█████████▌| 48/50 [00:29<00:01,  1.42it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8703. T_Loss: 3.1025. Mask: 0.9864. :  96%|█████████▌| 48/50 [00:29<00:01,  1.42it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8703. T_Loss: 3.1025. Mask: 0.9864. :  98%|█████████▊| 49/50 [00:29<00:00,  1.72it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8701. T_Loss: 3.0981. Mask: 0.9864. :  98%|█████████▊| 49/50 [00:29<00:00,  1.72it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8701. T_Loss: 3.0981. Mask: 0.9864. : 100%|██████████| 50/50 [00:29<00:00,  1.99it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8701. T_Loss: 3.0981. Mask: 0.9864. : 100%|██████████| 50/50 [00:29<00:00,  1.67it/s]
total : 1000  current step :  898
total : 1000  current step :  899
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8040. top1: 98.83. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8040. top1: 98.83. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8142. top1: 98.24. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8142. top1: 98.24. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8171. top1: 98.31. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8171. top1: 98.31. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.04it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8206. top1: 97.85. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.04it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8206. top1: 97.85. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.36it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8639. top1: 95.78. top5: 99.92. :  50%|█████     | 4/8 [00:01<00:01,  3.36it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8639. top1: 95.78. top5: 99.92. :  62%|██████▎   | 5/8 [00:01<00:00,  3.40it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8887. top1: 94.53. top5: 99.80. :  62%|██████▎   | 5/8 [00:01<00:00,  3.40it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8887. top1: 94.53. top5: 99.80. :  75%|███████▌  | 6/8 [00:01<00:00,  3.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9042. top1: 94.08. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9042. top1: 94.08. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9168. top1: 93.35. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9168. top1: 93.35. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9168. top1: 93.35. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.95it/s]
total : 1000  current step :  900
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 0.81s. Batch: 1.05s. S_Loss: 0.8850. T_Loss: 3.3008. Mask: 0.9883. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 0.81s. Batch: 1.05s. S_Loss: 0.8850. T_Loss: 3.3008. Mask: 0.9883. :   2%|▏         | 1/50 [00:01<00:51,  1.05s/it]Train Iter: 902/1000. LR: 0.0242. Data: 0.46s. Batch: 0.67s. S_Loss: 0.8599. T_Loss: 3.2355. Mask: 0.9883. :   2%|▏         | 1/50 [00:01<00:51,  1.05s/it]Train Iter: 902/1000. LR: 0.0242. Data: 0.46s. Batch: 0.67s. S_Loss: 0.8599. T_Loss: 3.2355. Mask: 0.9883. :   4%|▍         | 2/50 [00:01<00:29,  1.65it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8460. T_Loss: 3.0137. Mask: 0.9857. :   4%|▍         | 2/50 [00:01<00:29,  1.65it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.35s. Batch: 0.57s. S_Loss: 0.8460. T_Loss: 3.0137. Mask: 0.9857. :   6%|▌         | 3/50 [00:01<00:23,  2.00it/s]total : 1000  current step :  901
total : 1000  current step :  902
total : 1000  current step :  903
Train Iter: 904/1000. LR: 0.0234. Data: 0.51s. Batch: 0.74s. S_Loss: 0.8515. T_Loss: 2.9799. Mask: 0.9873. :   6%|▌         | 3/50 [00:02<00:23,  2.00it/s]Train Iter: 904/1000. LR: 0.0234. Data: 0.51s. Batch: 0.74s. S_Loss: 0.8515. T_Loss: 2.9799. Mask: 0.9873. :   8%|▊         | 4/50 [00:02<00:36,  1.25it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.42s. Batch: 0.66s. S_Loss: 0.8528. T_Loss: 2.9735. Mask: 0.9891. :   8%|▊         | 4/50 [00:03<00:36,  1.25it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.42s. Batch: 0.66s. S_Loss: 0.8528. T_Loss: 2.9735. Mask: 0.9891. :  10%|█         | 5/50 [00:03<00:28,  1.57it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8510. T_Loss: 2.9394. Mask: 0.9896. :  10%|█         | 5/50 [00:03<00:28,  1.57it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8510. T_Loss: 2.9394. Mask: 0.9896. :  12%|█▏        | 6/50 [00:03<00:22,  1.93it/s]total : 1000  current step :  904
total : 1000  current step :  905
total : 1000  current step :  906
Train Iter: 907/1000. LR: 0.0223. Data: 0.46s. Batch: 0.70s. S_Loss: 0.8504. T_Loss: 2.9633. Mask: 0.9894. :  12%|█▏        | 6/50 [00:04<00:22,  1.93it/s]Train Iter: 907/1000. LR: 0.0223. Data: 0.46s. Batch: 0.70s. S_Loss: 0.8504. T_Loss: 2.9633. Mask: 0.9894. :  14%|█▍        | 7/50 [00:04<00:33,  1.28it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.41s. Batch: 0.66s. S_Loss: 0.8542. T_Loss: 2.9742. Mask: 0.9873. :  14%|█▍        | 7/50 [00:05<00:33,  1.28it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.41s. Batch: 0.66s. S_Loss: 0.8542. T_Loss: 2.9742. Mask: 0.9873. :  16%|█▌        | 8/50 [00:05<00:26,  1.57it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.37s. Batch: 0.61s. S_Loss: 0.8546. T_Loss: 2.9647. Mask: 0.9870. :  16%|█▌        | 8/50 [00:05<00:26,  1.57it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.37s. Batch: 0.61s. S_Loss: 0.8546. T_Loss: 2.9647. Mask: 0.9870. :  18%|█▊        | 9/50 [00:05<00:21,  1.92it/s]total : 1000  current step :  907
total : 1000  current step :  908
total : 1000  current step :  909
Train Iter: 910/1000. LR: 0.0211. Data: 0.43s. Batch: 0.67s. S_Loss: 0.8577. T_Loss: 2.9734. Mask: 0.9867. :  18%|█▊        | 9/50 [00:06<00:21,  1.92it/s]Train Iter: 910/1000. LR: 0.0211. Data: 0.43s. Batch: 0.67s. S_Loss: 0.8577. T_Loss: 2.9734. Mask: 0.9867. :  20%|██        | 10/50 [00:06<00:28,  1.40it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8583. T_Loss: 2.9843. Mask: 0.9872. :  20%|██        | 10/50 [00:07<00:28,  1.40it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8583. T_Loss: 2.9843. Mask: 0.9872. :  22%|██▏       | 11/50 [00:07<00:23,  1.64it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.37s. Batch: 0.61s. S_Loss: 0.8613. T_Loss: 3.0096. Mask: 0.9873. :  22%|██▏       | 11/50 [00:07<00:23,  1.64it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.37s. Batch: 0.61s. S_Loss: 0.8613. T_Loss: 3.0096. Mask: 0.9873. :  24%|██▍       | 12/50 [00:07<00:19,  1.91it/s]total : 1000  current step :  910
total : 1000  current step :  911
total : 1000  current step :  912
Train Iter: 913/1000. LR: 0.0199. Data: 0.41s. Batch: 0.66s. S_Loss: 0.8609. T_Loss: 3.0007. Mask: 0.9871. :  24%|██▍       | 12/50 [00:08<00:19,  1.91it/s]Train Iter: 913/1000. LR: 0.0199. Data: 0.41s. Batch: 0.66s. S_Loss: 0.8609. T_Loss: 3.0007. Mask: 0.9871. :  26%|██▌       | 13/50 [00:08<00:26,  1.39it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.39s. Batch: 0.63s. S_Loss: 0.8584. T_Loss: 2.9927. Mask: 0.9863. :  26%|██▌       | 13/50 [00:08<00:26,  1.39it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.39s. Batch: 0.63s. S_Loss: 0.8584. T_Loss: 2.9927. Mask: 0.9863. :  28%|██▊       | 14/50 [00:08<00:20,  1.72it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.38s. Batch: 0.62s. S_Loss: 0.8604. T_Loss: 2.9977. Mask: 0.9862. :  28%|██▊       | 14/50 [00:09<00:20,  1.72it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.38s. Batch: 0.62s. S_Loss: 0.8604. T_Loss: 2.9977. Mask: 0.9862. :  30%|███       | 15/50 [00:09<00:19,  1.80it/s]total : 1000  current step :  913
total : 1000  current step :  914
total : 1000  current step :  915
Train Iter: 916/1000. LR: 0.0188. Data: 0.42s. Batch: 0.66s. S_Loss: 0.8599. T_Loss: 3.0097. Mask: 0.9871. :  30%|███       | 15/50 [00:10<00:19,  1.80it/s]Train Iter: 916/1000. LR: 0.0188. Data: 0.42s. Batch: 0.66s. S_Loss: 0.8599. T_Loss: 3.0097. Mask: 0.9871. :  32%|███▏      | 16/50 [00:10<00:25,  1.32it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8584. T_Loss: 3.0160. Mask: 0.9874. :  32%|███▏      | 16/50 [00:11<00:25,  1.32it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8584. T_Loss: 3.0160. Mask: 0.9874. :  34%|███▍      | 17/50 [00:11<00:22,  1.49it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8581. T_Loss: 3.0211. Mask: 0.9876. :  34%|███▍      | 17/50 [00:11<00:22,  1.49it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8581. T_Loss: 3.0211. Mask: 0.9876. :  36%|███▌      | 18/50 [00:11<00:18,  1.74it/s]total : 1000  current step :  916
total : 1000  current step :  917
total : 1000  current step :  918
Train Iter: 919/1000. LR: 0.0176. Data: 0.43s. Batch: 0.66s. S_Loss: 0.8596. T_Loss: 3.0328. Mask: 0.9877. :  36%|███▌      | 18/50 [00:12<00:18,  1.74it/s]Train Iter: 919/1000. LR: 0.0176. Data: 0.43s. Batch: 0.66s. S_Loss: 0.8596. T_Loss: 3.0328. Mask: 0.9877. :  38%|███▊      | 19/50 [00:12<00:23,  1.31it/s]total : 1000  current step :  919
Train Iter: 920/1000. LR: 0.0173. Data: 0.43s. Batch: 0.66s. S_Loss: 0.8598. T_Loss: 3.0382. Mask: 0.9877. :  38%|███▊      | 19/50 [00:13<00:23,  1.31it/s]Train Iter: 920/1000. LR: 0.0173. Data: 0.43s. Batch: 0.66s. S_Loss: 0.8598. T_Loss: 3.0382. Mask: 0.9877. :  40%|████      | 20/50 [00:13<00:22,  1.31it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8626. T_Loss: 3.0502. Mask: 0.9877. :  40%|████      | 20/50 [00:14<00:22,  1.31it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8626. T_Loss: 3.0502. Mask: 0.9877. :  42%|████▏     | 21/50 [00:14<00:23,  1.25it/s]total : 1000  current step :  920
total : 1000  current step :  921
Train Iter: 922/1000. LR: 0.0165. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8630. T_Loss: 3.0593. Mask: 0.9881. :  42%|████▏     | 21/50 [00:15<00:23,  1.25it/s]Train Iter: 922/1000. LR: 0.0165. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8630. T_Loss: 3.0593. Mask: 0.9881. :  44%|████▍     | 22/50 [00:15<00:25,  1.08it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8614. T_Loss: 3.0600. Mask: 0.9886. :  44%|████▍     | 22/50 [00:15<00:25,  1.08it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8614. T_Loss: 3.0600. Mask: 0.9886. :  46%|████▌     | 23/50 [00:15<00:20,  1.34it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8616. T_Loss: 3.0686. Mask: 0.9886. :  46%|████▌     | 23/50 [00:16<00:20,  1.34it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8616. T_Loss: 3.0686. Mask: 0.9886. :  48%|████▊     | 24/50 [00:16<00:16,  1.55it/s]total : 1000  current step :  922
total : 1000  current step :  923
total : 1000  current step :  924
Train Iter: 925/1000. LR: 0.0154. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8611. T_Loss: 3.0730. Mask: 0.9888. :  48%|████▊     | 24/50 [00:17<00:16,  1.55it/s]Train Iter: 925/1000. LR: 0.0154. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8611. T_Loss: 3.0730. Mask: 0.9888. :  50%|█████     | 25/50 [00:17<00:20,  1.20it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8610. T_Loss: 3.0750. Mask: 0.9884. :  50%|█████     | 25/50 [00:17<00:20,  1.20it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8610. T_Loss: 3.0750. Mask: 0.9884. :  52%|█████▏    | 26/50 [00:17<00:16,  1.46it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8608. T_Loss: 3.0831. Mask: 0.9887. :  52%|█████▏    | 26/50 [00:18<00:16,  1.46it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8608. T_Loss: 3.0831. Mask: 0.9887. :  54%|█████▍    | 27/50 [00:18<00:13,  1.76it/s]total : 1000  current step :  925
total : 1000  current step :  926
total : 1000  current step :  927
Train Iter: 928/1000. LR: 0.0144. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8595. T_Loss: 3.0676. Mask: 0.9883. :  54%|█████▍    | 27/50 [00:19<00:13,  1.76it/s]Train Iter: 928/1000. LR: 0.0144. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8595. T_Loss: 3.0676. Mask: 0.9883. :  56%|█████▌    | 28/50 [00:19<00:16,  1.33it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8598. T_Loss: 3.0663. Mask: 0.9884. :  56%|█████▌    | 28/50 [00:19<00:16,  1.33it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8598. T_Loss: 3.0663. Mask: 0.9884. :  58%|█████▊    | 29/50 [00:19<00:13,  1.53it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8591. T_Loss: 3.0587. Mask: 0.9888. :  58%|█████▊    | 29/50 [00:20<00:13,  1.53it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8591. T_Loss: 3.0587. Mask: 0.9888. :  60%|██████    | 30/50 [00:20<00:11,  1.68it/s]total : 1000  current step :  928
total : 1000  current step :  929
total : 1000  current step :  930
Train Iter: 931/1000. LR: 0.0133. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8589. T_Loss: 3.0576. Mask: 0.9890. :  60%|██████    | 30/50 [00:21<00:11,  1.68it/s]Train Iter: 931/1000. LR: 0.0133. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8589. T_Loss: 3.0576. Mask: 0.9890. :  62%|██████▏   | 31/50 [00:21<00:14,  1.29it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8584. T_Loss: 3.0537. Mask: 0.9886. :  62%|██████▏   | 31/50 [00:21<00:14,  1.29it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8584. T_Loss: 3.0537. Mask: 0.9886. :  64%|██████▍   | 32/50 [00:21<00:11,  1.54it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8585. T_Loss: 3.0567. Mask: 0.9883. :  64%|██████▍   | 32/50 [00:22<00:11,  1.54it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8585. T_Loss: 3.0567. Mask: 0.9883. :  66%|██████▌   | 33/50 [00:22<00:09,  1.78it/s]total : 1000  current step :  931
total : 1000  current step :  932
total : 1000  current step :  933
Train Iter: 934/1000. LR: 0.0123. Data: 0.47s. Batch: 0.69s. S_Loss: 0.8584. T_Loss: 3.0555. Mask: 0.9881. :  66%|██████▌   | 33/50 [00:23<00:09,  1.78it/s]Train Iter: 934/1000. LR: 0.0123. Data: 0.47s. Batch: 0.69s. S_Loss: 0.8584. T_Loss: 3.0555. Mask: 0.9881. :  68%|██████▊   | 34/50 [00:23<00:13,  1.21it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8580. T_Loss: 3.0515. Mask: 0.9879. :  68%|██████▊   | 34/50 [00:24<00:13,  1.21it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8580. T_Loss: 3.0515. Mask: 0.9879. :  70%|███████   | 35/50 [00:24<00:11,  1.36it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8589. T_Loss: 3.0630. Mask: 0.9880. :  70%|███████   | 35/50 [00:24<00:11,  1.36it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8589. T_Loss: 3.0630. Mask: 0.9880. :  72%|███████▏  | 36/50 [00:24<00:08,  1.62it/s]total : 1000  current step :  934
total : 1000  current step :  935
total : 1000  current step :  936
Train Iter: 937/1000. LR: 0.0113. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8584. T_Loss: 3.0653. Mask: 0.9880. :  72%|███████▏  | 36/50 [00:25<00:08,  1.62it/s]Train Iter: 937/1000. LR: 0.0113. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8584. T_Loss: 3.0653. Mask: 0.9880. :  74%|███████▍  | 37/50 [00:25<00:10,  1.29it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8585. T_Loss: 3.0723. Mask: 0.9880. :  74%|███████▍  | 37/50 [00:25<00:10,  1.29it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8585. T_Loss: 3.0723. Mask: 0.9880. :  76%|███████▌  | 38/50 [00:25<00:07,  1.53it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8596. T_Loss: 3.0814. Mask: 0.9877. :  76%|███████▌  | 38/50 [00:26<00:07,  1.53it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8596. T_Loss: 3.0814. Mask: 0.9877. :  78%|███████▊  | 39/50 [00:26<00:06,  1.73it/s]total : 1000  current step :  937
total : 1000  current step :  938
total : 1000  current step :  939
Train Iter: 940/1000. LR: 0.0103. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8587. T_Loss: 3.0709. Mask: 0.9876. :  78%|███████▊  | 39/50 [00:27<00:06,  1.73it/s]Train Iter: 940/1000. LR: 0.0103. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8587. T_Loss: 3.0709. Mask: 0.9876. :  80%|████████  | 40/50 [00:27<00:08,  1.25it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8580. T_Loss: 3.0655. Mask: 0.9873. :  80%|████████  | 40/50 [00:27<00:08,  1.25it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8580. T_Loss: 3.0655. Mask: 0.9873. :  82%|████████▏ | 41/50 [00:27<00:05,  1.51it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8577. T_Loss: 3.0638. Mask: 0.9873. :  82%|████████▏ | 41/50 [00:28<00:05,  1.51it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8577. T_Loss: 3.0638. Mask: 0.9873. :  84%|████████▍ | 42/50 [00:28<00:04,  1.72it/s]total : 1000  current step :  940
total : 1000  current step :  941
total : 1000  current step :  942
Train Iter: 943/1000. LR: 0.0094. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8578. T_Loss: 3.0641. Mask: 0.9874. :  84%|████████▍ | 42/50 [00:29<00:04,  1.72it/s]Train Iter: 943/1000. LR: 0.0094. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8578. T_Loss: 3.0641. Mask: 0.9874. :  86%|████████▌ | 43/50 [00:29<00:05,  1.35it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8585. T_Loss: 3.0667. Mask: 0.9874. :  86%|████████▌ | 43/50 [00:29<00:05,  1.35it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8585. T_Loss: 3.0667. Mask: 0.9874. :  88%|████████▊ | 44/50 [00:29<00:03,  1.59it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8582. T_Loss: 3.0666. Mask: 0.9872. :  88%|████████▊ | 44/50 [00:30<00:03,  1.59it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8582. T_Loss: 3.0666. Mask: 0.9872. :  90%|█████████ | 45/50 [00:30<00:02,  1.87it/s]total : 1000  current step :  943
total : 1000  current step :  944
total : 1000  current step :  945
Train Iter: 946/1000. LR: 0.0085. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8582. T_Loss: 3.0665. Mask: 0.9873. :  90%|█████████ | 45/50 [00:31<00:02,  1.87it/s]Train Iter: 946/1000. LR: 0.0085. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8582. T_Loss: 3.0665. Mask: 0.9873. :  92%|█████████▏| 46/50 [00:31<00:02,  1.39it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8579. T_Loss: 3.0618. Mask: 0.9872. :  92%|█████████▏| 46/50 [00:31<00:02,  1.39it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8579. T_Loss: 3.0618. Mask: 0.9872. :  94%|█████████▍| 47/50 [00:31<00:01,  1.56it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8576. T_Loss: 3.0580. Mask: 0.9871. :  94%|█████████▍| 47/50 [00:32<00:01,  1.56it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8576. T_Loss: 3.0580. Mask: 0.9871. :  96%|█████████▌| 48/50 [00:32<00:01,  1.82it/s]total : 1000  current step :  946
total : 1000  current step :  947
total : 1000  current step :  948
Train Iter: 949/1000. LR: 0.0076. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8566. T_Loss: 3.0525. Mask: 0.9870. :  96%|█████████▌| 48/50 [00:33<00:01,  1.82it/s]Train Iter: 949/1000. LR: 0.0076. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8566. T_Loss: 3.0525. Mask: 0.9870. :  98%|█████████▊| 49/50 [00:33<00:00,  1.36it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8560. T_Loss: 3.0480. Mask: 0.9871. :  98%|█████████▊| 49/50 [00:33<00:00,  1.36it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8560. T_Loss: 3.0480. Mask: 0.9871. : 100%|██████████| 50/50 [00:33<00:00,  1.59it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8560. T_Loss: 3.0480. Mask: 0.9871. : 100%|██████████| 50/50 [00:33<00:00,  1.49it/s]
total : 1000  current step :  949
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.7784. top1: 100.00. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.7784. top1: 100.00. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.42it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.7855. top1: 99.41. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.42it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.7855. top1: 99.41. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.7887. top1: 99.09. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.7887. top1: 99.09. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7945. top1: 98.73. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7945. top1: 98.73. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.19it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8516. top1: 95.94. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.19it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8516. top1: 95.94. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8857. top1: 94.21. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8857. top1: 94.21. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9068. top1: 93.53. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9068. top1: 93.53. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9226. top1: 92.80. top5: 99.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9226. top1: 92.80. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9226. top1: 92.80. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  2.90it/s]
total : 1000  current step :  950
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.17s. S_Loss: 0.8508. T_Loss: 3.0778. Mask: 0.9883. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.17s. S_Loss: 0.8508. T_Loss: 3.0778. Mask: 0.9883. :   2%|▏         | 1/50 [00:00<00:08,  5.97it/s]total : 1000  current step :  951
Train Iter: 952/1000. LR: 0.0068. Data: 0.47s. Batch: 0.68s. S_Loss: 0.8443. T_Loss: 2.9063. Mask: 0.9805. :   2%|▏         | 1/50 [00:01<00:08,  5.97it/s]Train Iter: 952/1000. LR: 0.0068. Data: 0.47s. Batch: 0.68s. S_Loss: 0.8443. T_Loss: 2.9063. Mask: 0.9805. :   4%|▍         | 2/50 [00:01<00:36,  1.30it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.34s. Batch: 0.55s. S_Loss: 0.8320. T_Loss: 2.8586. Mask: 0.9831. :   4%|▍         | 2/50 [00:01<00:36,  1.30it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.34s. Batch: 0.55s. S_Loss: 0.8320. T_Loss: 2.8586. Mask: 0.9831. :   6%|▌         | 3/50 [00:01<00:26,  1.80it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.29s. Batch: 0.49s. S_Loss: 0.8395. T_Loss: 2.9482. Mask: 0.9844. :   6%|▌         | 3/50 [00:01<00:26,  1.80it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.29s. Batch: 0.49s. S_Loss: 0.8395. T_Loss: 2.9482. Mask: 0.9844. :   8%|▊         | 4/50 [00:01<00:21,  2.18it/s]total : 1000  current step :  952
total : 1000  current step :  953
total : 1000  current step :  954
Train Iter: 955/1000. LR: 0.0060. Data: 0.41s. Batch: 0.61s. S_Loss: 0.8482. T_Loss: 2.9561. Mask: 0.9859. :   8%|▊         | 4/50 [00:03<00:21,  2.18it/s]Train Iter: 955/1000. LR: 0.0060. Data: 0.41s. Batch: 0.61s. S_Loss: 0.8482. T_Loss: 2.9561. Mask: 0.9859. :  10%|█         | 5/50 [00:03<00:30,  1.47it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8517. T_Loss: 2.9588. Mask: 0.9844. :  10%|█         | 5/50 [00:03<00:30,  1.47it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8517. T_Loss: 2.9588. Mask: 0.9844. :  12%|█▏        | 6/50 [00:03<00:27,  1.63it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8505. T_Loss: 2.9376. Mask: 0.9860. :  12%|█▏        | 6/50 [00:03<00:27,  1.63it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8505. T_Loss: 2.9376. Mask: 0.9860. :  14%|█▍        | 7/50 [00:03<00:23,  1.79it/s]total : 1000  current step :  955
total : 1000  current step :  956
total : 1000  current step :  957
Train Iter: 958/1000. LR: 0.0052. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8503. T_Loss: 2.9678. Mask: 0.9873. :  14%|█▍        | 7/50 [00:05<00:23,  1.79it/s]Train Iter: 958/1000. LR: 0.0052. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8503. T_Loss: 2.9678. Mask: 0.9873. :  16%|█▌        | 8/50 [00:05<00:31,  1.35it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8493. T_Loss: 2.9640. Mask: 0.9878. :  16%|█▌        | 8/50 [00:05<00:31,  1.35it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8493. T_Loss: 2.9640. Mask: 0.9878. :  18%|█▊        | 9/50 [00:05<00:25,  1.58it/s]total : 1000  current step :  958
total : 1000  current step :  959
Train Iter: 960/1000. LR: 0.0048. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8486. T_Loss: 2.9721. Mask: 0.9887. :  18%|█▊        | 9/50 [00:06<00:25,  1.58it/s]Train Iter: 960/1000. LR: 0.0048. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8486. T_Loss: 2.9721. Mask: 0.9887. :  20%|██        | 10/50 [00:06<00:27,  1.48it/s]total : 1000  current step :  960
Train Iter: 961/1000. LR: 0.0045. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8498. T_Loss: 2.9757. Mask: 0.9886. :  20%|██        | 10/50 [00:07<00:27,  1.48it/s]Train Iter: 961/1000. LR: 0.0045. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8498. T_Loss: 2.9757. Mask: 0.9886. :  22%|██▏       | 11/50 [00:07<00:32,  1.20it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8498. T_Loss: 2.9670. Mask: 0.9886. :  22%|██▏       | 11/50 [00:07<00:32,  1.20it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8498. T_Loss: 2.9670. Mask: 0.9886. :  24%|██▍       | 12/50 [00:07<00:25,  1.51it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8499. T_Loss: 2.9556. Mask: 0.9889. :  24%|██▍       | 12/50 [00:08<00:25,  1.51it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8499. T_Loss: 2.9556. Mask: 0.9889. :  26%|██▌       | 13/50 [00:08<00:21,  1.76it/s]total : 1000  current step :  961
total : 1000  current step :  962
total : 1000  current step :  963
Train Iter: 964/1000. LR: 0.0039. Data: 0.44s. Batch: 0.66s. S_Loss: 0.8504. T_Loss: 2.9492. Mask: 0.9880. :  26%|██▌       | 13/50 [00:09<00:21,  1.76it/s]Train Iter: 964/1000. LR: 0.0039. Data: 0.44s. Batch: 0.66s. S_Loss: 0.8504. T_Loss: 2.9492. Mask: 0.9880. :  28%|██▊       | 14/50 [00:09<00:26,  1.34it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8492. T_Loss: 2.9382. Mask: 0.9875. :  28%|██▊       | 14/50 [00:09<00:26,  1.34it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.43s. Batch: 0.64s. S_Loss: 0.8492. T_Loss: 2.9382. Mask: 0.9875. :  30%|███       | 15/50 [00:09<00:22,  1.52it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8497. T_Loss: 2.9369. Mask: 0.9871. :  30%|███       | 15/50 [00:09<00:22,  1.52it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8497. T_Loss: 2.9369. Mask: 0.9871. :  32%|███▏      | 16/50 [00:09<00:18,  1.83it/s]total : 1000  current step :  964
total : 1000  current step :  965
total : 1000  current step :  966
Train Iter: 967/1000. LR: 0.0033. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8493. T_Loss: 2.9410. Mask: 0.9864. :  32%|███▏      | 16/50 [00:11<00:18,  1.83it/s]Train Iter: 967/1000. LR: 0.0033. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8493. T_Loss: 2.9410. Mask: 0.9864. :  34%|███▍      | 17/50 [00:11<00:22,  1.44it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8465. T_Loss: 2.9265. Mask: 0.9870. :  34%|███▍      | 17/50 [00:11<00:22,  1.44it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8465. T_Loss: 2.9265. Mask: 0.9870. :  36%|███▌      | 18/50 [00:11<00:19,  1.68it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8459. T_Loss: 2.9299. Mask: 0.9873. :  36%|███▌      | 18/50 [00:11<00:19,  1.68it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8459. T_Loss: 2.9299. Mask: 0.9873. :  38%|███▊      | 19/50 [00:11<00:15,  2.01it/s]total : 1000  current step :  967
total : 1000  current step :  968
total : 1000  current step :  969
Train Iter: 970/1000. LR: 0.0027. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8467. T_Loss: 2.9325. Mask: 0.9871. :  38%|███▊      | 19/50 [00:13<00:15,  2.01it/s]Train Iter: 970/1000. LR: 0.0027. Data: 0.43s. Batch: 0.65s. S_Loss: 0.8467. T_Loss: 2.9325. Mask: 0.9871. :  40%|████      | 20/50 [00:13<00:22,  1.32it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8475. T_Loss: 2.9307. Mask: 0.9868. :  40%|████      | 20/50 [00:13<00:22,  1.32it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8475. T_Loss: 2.9307. Mask: 0.9868. :  42%|████▏     | 21/50 [00:13<00:17,  1.66it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8472. T_Loss: 2.9228. Mask: 0.9874. :  42%|████▏     | 21/50 [00:13<00:17,  1.66it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8472. T_Loss: 2.9228. Mask: 0.9874. :  44%|████▍     | 22/50 [00:13<00:13,  2.02it/s]total : 1000  current step :  970
total : 1000  current step :  971
total : 1000  current step :  972
Train Iter: 973/1000. LR: 0.0022. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8490. T_Loss: 2.9357. Mask: 0.9876. :  44%|████▍     | 22/50 [00:14<00:13,  2.02it/s]Train Iter: 973/1000. LR: 0.0022. Data: 0.42s. Batch: 0.64s. S_Loss: 0.8490. T_Loss: 2.9357. Mask: 0.9876. :  46%|████▌     | 23/50 [00:14<00:19,  1.37it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8479. T_Loss: 2.9280. Mask: 0.9880. :  46%|████▌     | 23/50 [00:15<00:19,  1.37it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8479. T_Loss: 2.9280. Mask: 0.9880. :  48%|████▊     | 24/50 [00:15<00:16,  1.61it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8491. T_Loss: 2.9362. Mask: 0.9880. :  48%|████▊     | 24/50 [00:15<00:16,  1.61it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8491. T_Loss: 2.9362. Mask: 0.9880. :  50%|█████     | 25/50 [00:15<00:13,  1.87it/s]total : 1000  current step :  973
total : 1000  current step :  974
total : 1000  current step :  975
Train Iter: 976/1000. LR: 0.0018. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8494. T_Loss: 2.9333. Mask: 0.9881. :  50%|█████     | 25/50 [00:16<00:13,  1.87it/s]Train Iter: 976/1000. LR: 0.0018. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8494. T_Loss: 2.9333. Mask: 0.9881. :  52%|█████▏    | 26/50 [00:16<00:16,  1.48it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8491. T_Loss: 2.9233. Mask: 0.9878. :  52%|█████▏    | 26/50 [00:16<00:16,  1.48it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8491. T_Loss: 2.9233. Mask: 0.9878. :  54%|█████▍    | 27/50 [00:16<00:13,  1.75it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8494. T_Loss: 2.9328. Mask: 0.9881. :  54%|█████▍    | 27/50 [00:17<00:13,  1.75it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8494. T_Loss: 2.9328. Mask: 0.9881. :  56%|█████▌    | 28/50 [00:17<00:11,  1.89it/s]total : 1000  current step :  976
total : 1000  current step :  977
total : 1000  current step :  978
Train Iter: 979/1000. LR: 0.0013. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8495. T_Loss: 2.9380. Mask: 0.9884. :  56%|█████▌    | 28/50 [00:18<00:11,  1.89it/s]Train Iter: 979/1000. LR: 0.0013. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8495. T_Loss: 2.9380. Mask: 0.9884. :  58%|█████▊    | 29/50 [00:18<00:14,  1.46it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8497. T_Loss: 2.9380. Mask: 0.9883. :  58%|█████▊    | 29/50 [00:18<00:14,  1.46it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8497. T_Loss: 2.9380. Mask: 0.9883. :  60%|██████    | 30/50 [00:18<00:12,  1.65it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8505. T_Loss: 2.9407. Mask: 0.9880. :  60%|██████    | 30/50 [00:19<00:12,  1.65it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8505. T_Loss: 2.9407. Mask: 0.9880. :  62%|██████▏   | 31/50 [00:19<00:10,  1.88it/s]total : 1000  current step :  979
total : 1000  current step :  980
total : 1000  current step :  981
Train Iter: 982/1000. LR: 0.0010. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8501. T_Loss: 2.9344. Mask: 0.9878. :  62%|██████▏   | 31/50 [00:20<00:10,  1.88it/s]Train Iter: 982/1000. LR: 0.0010. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8501. T_Loss: 2.9344. Mask: 0.9878. :  64%|██████▍   | 32/50 [00:20<00:12,  1.45it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8494. T_Loss: 2.9340. Mask: 0.9878. :  64%|██████▍   | 32/50 [00:20<00:12,  1.45it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8494. T_Loss: 2.9340. Mask: 0.9878. :  66%|██████▌   | 33/50 [00:20<00:09,  1.73it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8486. T_Loss: 2.9296. Mask: 0.9881. :  66%|██████▌   | 33/50 [00:20<00:09,  1.73it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8486. T_Loss: 2.9296. Mask: 0.9881. :  68%|██████▊   | 34/50 [00:20<00:08,  1.94it/s]total : 1000  current step :  982
total : 1000  current step :  983
total : 1000  current step :  984
Train Iter: 985/1000. LR: 0.0007. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8483. T_Loss: 2.9291. Mask: 0.9882. :  68%|██████▊   | 34/50 [00:22<00:08,  1.94it/s]Train Iter: 985/1000. LR: 0.0007. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8483. T_Loss: 2.9291. Mask: 0.9882. :  70%|███████   | 35/50 [00:22<00:10,  1.37it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8489. T_Loss: 2.9321. Mask: 0.9881. :  70%|███████   | 35/50 [00:22<00:10,  1.37it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8489. T_Loss: 2.9321. Mask: 0.9881. :  72%|███████▏  | 36/50 [00:22<00:08,  1.61it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8496. T_Loss: 2.9370. Mask: 0.9881. :  72%|███████▏  | 36/50 [00:22<00:08,  1.61it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8496. T_Loss: 2.9370. Mask: 0.9881. :  74%|███████▍  | 37/50 [00:22<00:06,  1.86it/s]total : 1000  current step :  985
total : 1000  current step :  986
total : 1000  current step :  987
Train Iter: 988/1000. LR: 0.0004. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8493. T_Loss: 2.9313. Mask: 0.9883. :  74%|███████▍  | 37/50 [00:23<00:06,  1.86it/s]Train Iter: 988/1000. LR: 0.0004. Data: 0.41s. Batch: 0.63s. S_Loss: 0.8493. T_Loss: 2.9313. Mask: 0.9883. :  76%|███████▌  | 38/50 [00:23<00:08,  1.38it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8494. T_Loss: 2.9289. Mask: 0.9882. :  76%|███████▌  | 38/50 [00:24<00:08,  1.38it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8494. T_Loss: 2.9289. Mask: 0.9882. :  78%|███████▊  | 39/50 [00:24<00:06,  1.69it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8493. T_Loss: 2.9285. Mask: 0.9884. :  78%|███████▊  | 39/50 [00:24<00:06,  1.69it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.39s. Batch: 0.61s. S_Loss: 0.8493. T_Loss: 2.9285. Mask: 0.9884. :  80%|████████  | 40/50 [00:24<00:05,  1.98it/s]total : 1000  current step :  988
total : 1000  current step :  989
total : 1000  current step :  990
Train Iter: 991/1000. LR: 0.0002. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8488. T_Loss: 2.9198. Mask: 0.9884. :  80%|████████  | 40/50 [00:25<00:05,  1.98it/s]Train Iter: 991/1000. LR: 0.0002. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8488. T_Loss: 2.9198. Mask: 0.9884. :  82%|████████▏ | 41/50 [00:25<00:06,  1.45it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8485. T_Loss: 2.9165. Mask: 0.9885. :  82%|████████▏ | 41/50 [00:26<00:06,  1.45it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8485. T_Loss: 2.9165. Mask: 0.9885. :  84%|████████▍ | 42/50 [00:26<00:04,  1.63it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8492. T_Loss: 2.9194. Mask: 0.9883. :  84%|████████▍ | 42/50 [00:26<00:04,  1.63it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8492. T_Loss: 2.9194. Mask: 0.9883. :  86%|████████▌ | 43/50 [00:26<00:03,  1.90it/s]total : 1000  current step :  991
total : 1000  current step :  992
total : 1000  current step :  993
Train Iter: 994/1000. LR: 0.0001. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8486. T_Loss: 2.9145. Mask: 0.9883. :  86%|████████▌ | 43/50 [00:27<00:03,  1.90it/s]Train Iter: 994/1000. LR: 0.0001. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8486. T_Loss: 2.9145. Mask: 0.9883. :  88%|████████▊ | 44/50 [00:27<00:04,  1.43it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8482. T_Loss: 2.9128. Mask: 0.9885. :  88%|████████▊ | 44/50 [00:27<00:04,  1.43it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8482. T_Loss: 2.9128. Mask: 0.9885. :  90%|█████████ | 45/50 [00:27<00:02,  1.67it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8483. T_Loss: 2.9161. Mask: 0.9885. :  90%|█████████ | 45/50 [00:28<00:02,  1.67it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8483. T_Loss: 2.9161. Mask: 0.9885. :  92%|█████████▏| 46/50 [00:28<00:02,  1.80it/s]total : 1000  current step :  994
total : 1000  current step :  995
total : 1000  current step :  996
Train Iter: 997/1000. LR: 0.0000. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8485. T_Loss: 2.9146. Mask: 0.9884. :  92%|█████████▏| 46/50 [00:29<00:02,  1.80it/s]Train Iter: 997/1000. LR: 0.0000. Data: 0.41s. Batch: 0.62s. S_Loss: 0.8485. T_Loss: 2.9146. Mask: 0.9884. :  94%|█████████▍| 47/50 [00:29<00:02,  1.37it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8485. T_Loss: 2.9110. Mask: 0.9884. :  94%|█████████▍| 47/50 [00:29<00:02,  1.37it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 0.8485. T_Loss: 2.9110. Mask: 0.9884. :  96%|█████████▌| 48/50 [00:29<00:01,  1.64it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8486. T_Loss: 2.9155. Mask: 0.9885. :  96%|█████████▌| 48/50 [00:30<00:01,  1.64it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8486. T_Loss: 2.9155. Mask: 0.9885. :  98%|█████████▊| 49/50 [00:30<00:00,  1.86it/s]total : 1000  current step :  997
total : 1000  current step :  998
total : 1000  current step :  999
Train Iter: 1000/1000. LR: 0.0000. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8487. T_Loss: 2.9188. Mask: 0.9886. :  98%|█████████▊| 49/50 [00:31<00:00,  1.86it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8487. T_Loss: 2.9188. Mask: 0.9886. : 100%|██████████| 50/50 [00:31<00:00,  1.24it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.42s. Batch: 0.63s. S_Loss: 0.8487. T_Loss: 2.9188. Mask: 0.9886. : 100%|██████████| 50/50 [00:31<00:00,  1.58it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.7624. top1: 100.00. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.7624. top1: 100.00. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.7680. top1: 99.61. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.7680. top1: 99.61. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.58it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7708. top1: 99.35. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.58it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.7708. top1: 99.35. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7781. top1: 98.93. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.7781. top1: 98.93. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8445. top1: 95.39. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8445. top1: 95.39. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8846. top1: 93.49. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8846. top1: 93.49. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9091. top1: 92.52. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9091. top1: 92.52. top5: 99.61. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9268. top1: 91.55. top5: 99.50. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9268. top1: 91.55. top5: 99.50. : 100%|██████████| 8/8 [00:02<00:00,  3.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9268. top1: 91.55. top5: 99.50. : 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.66s. Batch: 0.72s. Loss: 0.8604. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.66s. Batch: 0.72s. Loss: 0.8604. :  33%|███▎      | 1/3 [00:00<00:01,  1.39it/s]Finetune Epoch:  1/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8578. :  33%|███▎      | 1/3 [00:01<00:01,  1.39it/s]Finetune Epoch:  1/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8578. :  67%|██████▋   | 2/3 [00:01<00:00,  2.17it/s]Finetune Epoch:  1/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8432. :  67%|██████▋   | 2/3 [00:01<00:00,  2.17it/s]Finetune Epoch:  1/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8432. : 100%|██████████| 3/3 [00:01<00:00,  2.51it/s]Finetune Epoch:  1/70. Data: 0.96s. Batch: 1.01s. Loss: 0.8432. : 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8869. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8869. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.9007. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.9007. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9020. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9020. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.99it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9023. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.99it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9023. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.26it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8923. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.26it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8923. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8815. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8815. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8732. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8732. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.29it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8704. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.29it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8704. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8704. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.93it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.64s. Batch: 0.71s. Loss: 0.8167. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.64s. Batch: 0.71s. Loss: 0.8167. :  33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]Finetune Epoch:  2/70. Data: 0.81s. Batch: 0.87s. Loss: 0.8417. :  33%|███▎      | 1/3 [00:01<00:01,  1.42it/s]Finetune Epoch:  2/70. Data: 0.81s. Batch: 0.87s. Loss: 0.8417. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch:  2/70. Data: 0.97s. Batch: 1.02s. Loss: 0.8451. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch:  2/70. Data: 0.97s. Batch: 1.02s. Loss: 0.8451. : 100%|██████████| 3/3 [00:01<00:00,  2.55it/s]Finetune Epoch:  2/70. Data: 0.97s. Batch: 1.02s. Loss: 0.8451. : 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8864. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8864. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9001. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9001. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9014. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9014. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9018. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9018. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.17it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8918. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.17it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8918. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8810. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8810. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8728. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8728. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8701. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8701. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8701. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.97it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.69s. Batch: 0.75s. Loss: 0.8438. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.69s. Batch: 0.75s. Loss: 0.8438. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch:  3/70. Data: 0.88s. Batch: 0.94s. Loss: 0.8305. :  33%|███▎      | 1/3 [00:01<00:01,  1.33it/s]Finetune Epoch:  3/70. Data: 0.88s. Batch: 0.94s. Loss: 0.8305. :  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s]Finetune Epoch:  3/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8464. :  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s]Finetune Epoch:  3/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8464. : 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]Finetune Epoch:  3/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8464. : 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8859. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8859. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8996. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8996. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.9008. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.9008. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9012. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9012. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8913. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8913. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.60it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8807. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.60it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8807. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.79it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8725. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.79it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8725. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8698. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8698. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8698. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.53it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8569. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8569. :  33%|███▎      | 1/3 [00:00<00:01,  1.16it/s]Finetune Epoch:  4/70. Data: 0.99s. Batch: 1.06s. Loss: 0.8488. :  33%|███▎      | 1/3 [00:01<00:01,  1.16it/s]Finetune Epoch:  4/70. Data: 0.99s. Batch: 1.06s. Loss: 0.8488. :  67%|██████▋   | 2/3 [00:01<00:00,  1.69it/s]Finetune Epoch:  4/70. Data: 1.19s. Batch: 1.25s. Loss: 0.8454. :  67%|██████▋   | 2/3 [00:01<00:00,  1.69it/s]Finetune Epoch:  4/70. Data: 1.19s. Batch: 1.25s. Loss: 0.8454. : 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]Finetune Epoch:  4/70. Data: 1.19s. Batch: 1.25s. Loss: 0.8454. : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8854. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8854. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8990. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8990. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9003. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9003. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9007. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9007. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8909. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8909. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8803. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8803. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8721. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8721. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8694. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8694. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.84it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8694. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.85it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8224. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8224. :  33%|███▎      | 1/3 [00:00<00:01,  1.14it/s]Finetune Epoch:  5/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8299. :  33%|███▎      | 1/3 [00:01<00:01,  1.14it/s]Finetune Epoch:  5/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8299. :  67%|██████▋   | 2/3 [00:01<00:00,  1.70it/s]Finetune Epoch:  5/70. Data: 1.19s. Batch: 1.25s. Loss: 0.8476. :  67%|██████▋   | 2/3 [00:01<00:00,  1.70it/s]Finetune Epoch:  5/70. Data: 1.19s. Batch: 1.25s. Loss: 0.8476. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]Finetune Epoch:  5/70. Data: 1.19s. Batch: 1.25s. Loss: 0.8476. : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.79s. Loss: 0.8849. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.79s. Loss: 0.8849. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.26it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8985. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.26it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8985. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8997. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8997. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9002. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9002. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8904. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8904. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8799. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8799. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8718. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8718. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.28it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8691. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.28it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8691. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8691. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8646. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8646. :  33%|███▎      | 1/3 [00:00<00:01,  1.20it/s]Finetune Epoch:  6/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8483. :  33%|███▎      | 1/3 [00:01<00:01,  1.20it/s]Finetune Epoch:  6/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8483. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch:  6/70. Data: 1.06s. Batch: 1.11s. Loss: 0.8422. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch:  6/70. Data: 1.06s. Batch: 1.11s. Loss: 0.8422. : 100%|██████████| 3/3 [00:01<00:00,  2.41it/s]Finetune Epoch:  6/70. Data: 1.06s. Batch: 1.11s. Loss: 0.8422. : 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8844. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8844. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.63it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8980. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.63it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8980. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8992. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8992. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.99it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8996. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.99it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8996. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.48it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8899. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.48it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8899. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8795. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8795. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.61it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8714. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.61it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8714. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8687. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8687. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8687. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.99it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.72s. Batch: 0.80s. Loss: 0.8418. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.72s. Batch: 0.80s. Loss: 0.8418. :  33%|███▎      | 1/3 [00:00<00:01,  1.26it/s]Finetune Epoch:  7/70. Data: 0.89s. Batch: 0.95s. Loss: 0.8390. :  33%|███▎      | 1/3 [00:01<00:01,  1.26it/s]Finetune Epoch:  7/70. Data: 0.89s. Batch: 0.95s. Loss: 0.8390. :  67%|██████▋   | 2/3 [00:01<00:00,  1.98it/s]Finetune Epoch:  7/70. Data: 1.06s. Batch: 1.12s. Loss: 0.8390. :  67%|██████▋   | 2/3 [00:01<00:00,  1.98it/s]Finetune Epoch:  7/70. Data: 1.06s. Batch: 1.12s. Loss: 0.8390. : 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]Finetune Epoch:  7/70. Data: 1.06s. Batch: 1.12s. Loss: 0.8390. : 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8840. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8840. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.26it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8975. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.26it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8975. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8987. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8987. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.42it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8991. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.42it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8991. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8895. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8895. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.62it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8791. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.62it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8791. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8711. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8711. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8684. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:03<00:00,  2.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8684. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8684. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.42it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8302. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.77s. Batch: 0.83s. Loss: 0.8302. :  33%|███▎      | 1/3 [00:00<00:01,  1.20it/s]Finetune Epoch:  8/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8297. :  33%|███▎      | 1/3 [00:01<00:01,  1.20it/s]Finetune Epoch:  8/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8297. :  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s]Finetune Epoch:  8/70. Data: 1.10s. Batch: 1.17s. Loss: 0.8478. :  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s]Finetune Epoch:  8/70. Data: 1.10s. Batch: 1.17s. Loss: 0.8478. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]Finetune Epoch:  8/70. Data: 1.10s. Batch: 1.17s. Loss: 0.8478. : 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8835. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8835. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8969. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8969. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.28it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8981. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.28it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8981. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8985. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8985. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8890. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8890. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8787. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8787. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8707. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8707. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8681. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8681. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8681. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.66s. Batch: 0.72s. Loss: 0.8348. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.66s. Batch: 0.72s. Loss: 0.8348. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch:  9/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8452. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch:  9/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8452. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch:  9/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8441. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch:  9/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8441. : 100%|██████████| 3/3 [00:01<00:00,  2.37it/s]Finetune Epoch:  9/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8441. : 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8828. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8828. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8962. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8962. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.71it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8974. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.71it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8974. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.03it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8978. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.03it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8978. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.33it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8884. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.33it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8884. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.23it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8782. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.23it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8782. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8703. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8703. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8677. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8677. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.76it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8677. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.95it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.71s. Batch: 0.79s. Loss: 0.8350. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.71s. Batch: 0.79s. Loss: 0.8350. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 10/70. Data: 0.85s. Batch: 0.93s. Loss: 0.8435. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 10/70. Data: 0.85s. Batch: 0.93s. Loss: 0.8435. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 10/70. Data: 1.00s. Batch: 1.07s. Loss: 0.8394. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 10/70. Data: 1.00s. Batch: 1.07s. Loss: 0.8394. : 100%|██████████| 3/3 [00:01<00:00,  2.49it/s]Finetune Epoch: 10/70. Data: 1.00s. Batch: 1.07s. Loss: 0.8394. : 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8823. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8823. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8956. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8956. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8968. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8968. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8973. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8973. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8880. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8880. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.26it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8778. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.26it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8778. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8699. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8699. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8674. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8674. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8674. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.91it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.61s. Batch: 0.68s. Loss: 0.8508. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.61s. Batch: 0.68s. Loss: 0.8508. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 11/70. Data: 0.80s. Batch: 0.85s. Loss: 0.8428. :  33%|███▎      | 1/3 [00:01<00:01,  1.47it/s]Finetune Epoch: 11/70. Data: 0.80s. Batch: 0.85s. Loss: 0.8428. :  67%|██████▋   | 2/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 11/70. Data: 0.98s. Batch: 1.03s. Loss: 0.8423. :  67%|██████▋   | 2/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 11/70. Data: 0.98s. Batch: 1.03s. Loss: 0.8423. : 100%|██████████| 3/3 [00:01<00:00,  2.38it/s]Finetune Epoch: 11/70. Data: 0.98s. Batch: 1.03s. Loss: 0.8423. : 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8818. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8818. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8951. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8951. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8962. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8962. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8967. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8967. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.27it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8875. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.27it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8875. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8774. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8774. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.46it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8696. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.46it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8696. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8670. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8670. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8670. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8324. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.73s. Batch: 0.78s. Loss: 0.8324. :  33%|███▎      | 1/3 [00:00<00:01,  1.28it/s]Finetune Epoch: 12/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8364. :  33%|███▎      | 1/3 [00:01<00:01,  1.28it/s]Finetune Epoch: 12/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8364. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 12/70. Data: 1.10s. Batch: 1.15s. Loss: 0.8395. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 12/70. Data: 1.10s. Batch: 1.15s. Loss: 0.8395. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]Finetune Epoch: 12/70. Data: 1.10s. Batch: 1.15s. Loss: 0.8395. : 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8814. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8814. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8946. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.79it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8946. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8958. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8958. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8963. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8963. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8871. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8871. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8771. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8771. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8693. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8693. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8667. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8667. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8667. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.66s. Batch: 0.73s. Loss: 0.8445. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.66s. Batch: 0.73s. Loss: 0.8445. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 13/70. Data: 0.83s. Batch: 0.90s. Loss: 0.8451. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 13/70. Data: 0.83s. Batch: 0.90s. Loss: 0.8451. :  67%|██████▋   | 2/3 [00:01<00:00,  1.98it/s]Finetune Epoch: 13/70. Data: 0.99s. Batch: 1.06s. Loss: 0.8439. :  67%|██████▋   | 2/3 [00:01<00:00,  1.98it/s]Finetune Epoch: 13/70. Data: 0.99s. Batch: 1.06s. Loss: 0.8439. : 100%|██████████| 3/3 [00:01<00:00,  2.42it/s]Finetune Epoch: 13/70. Data: 0.99s. Batch: 1.06s. Loss: 0.8439. : 100%|██████████| 3/3 [00:01<00:00,  1.93it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8809. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8809. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8941. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8941. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8953. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8953. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.86it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8957. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.86it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8957. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8867. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8867. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8767. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8767. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8689. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8689. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8664. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8664. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8664. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.66it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8195. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8195. :  33%|███▎      | 1/3 [00:00<00:01,  1.17it/s]Finetune Epoch: 14/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8359. :  33%|███▎      | 1/3 [00:01<00:01,  1.17it/s]Finetune Epoch: 14/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8359. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 14/70. Data: 1.16s. Batch: 1.22s. Loss: 0.8419. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 14/70. Data: 1.16s. Batch: 1.22s. Loss: 0.8419. : 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 14/70. Data: 1.16s. Batch: 1.22s. Loss: 0.8419. : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8805. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8805. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8936. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8936. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8948. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8948. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8952. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8952. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.60it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8862. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.60it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8862. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8763. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8763. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.91it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8685. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.91it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8685. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8660. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:03<00:00,  2.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8660. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  3.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8660. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.47it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.66s. Batch: 0.74s. Loss: 0.8448. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.66s. Batch: 0.74s. Loss: 0.8448. :  33%|███▎      | 1/3 [00:00<00:01,  1.34it/s]Finetune Epoch: 15/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8470. :  33%|███▎      | 1/3 [00:00<00:01,  1.34it/s]Finetune Epoch: 15/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8470. :  67%|██████▋   | 2/3 [00:00<00:00,  2.25it/s]Finetune Epoch: 15/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8436. :  67%|██████▋   | 2/3 [00:01<00:00,  2.25it/s]Finetune Epoch: 15/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8436. : 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]Finetune Epoch: 15/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8436. : 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8800. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8800. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8931. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8931. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8943. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8943. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8947. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8947. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.87it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8858. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.87it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8858. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.88it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8759. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.88it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8759. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8682. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8682. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8657. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8657. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8657. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.65it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.70s. Batch: 0.78s. Loss: 0.8442. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.70s. Batch: 0.78s. Loss: 0.8442. :  33%|███▎      | 1/3 [00:00<00:01,  1.29it/s]Finetune Epoch: 16/70. Data: 0.89s. Batch: 0.96s. Loss: 0.8584. :  33%|███▎      | 1/3 [00:01<00:01,  1.29it/s]Finetune Epoch: 16/70. Data: 0.89s. Batch: 0.96s. Loss: 0.8584. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 16/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8443. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 16/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8443. : 100%|██████████| 3/3 [00:01<00:00,  2.23it/s]Finetune Epoch: 16/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8443. : 100%|██████████| 3/3 [00:01<00:00,  1.72it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8796. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8796. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.25it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8926. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.25it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8926. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8938. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8938. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.25it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8942. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.25it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8942. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8853. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8853. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8755. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8755. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8679. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8679. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8654. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8654. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8654. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.51it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.84s. Batch: 0.93s. Loss: 0.8311. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.84s. Batch: 0.93s. Loss: 0.8311. :  33%|███▎      | 1/3 [00:00<00:01,  1.08it/s]Finetune Epoch: 17/70. Data: 1.02s. Batch: 1.09s. Loss: 0.8410. :  33%|███▎      | 1/3 [00:01<00:01,  1.08it/s]Finetune Epoch: 17/70. Data: 1.02s. Batch: 1.09s. Loss: 0.8410. :  67%|██████▋   | 2/3 [00:01<00:00,  1.74it/s]Finetune Epoch: 17/70. Data: 1.19s. Batch: 1.26s. Loss: 0.8466. :  67%|██████▋   | 2/3 [00:01<00:00,  1.74it/s]Finetune Epoch: 17/70. Data: 1.19s. Batch: 1.26s. Loss: 0.8466. : 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]Finetune Epoch: 17/70. Data: 1.19s. Batch: 1.26s. Loss: 0.8466. : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8790. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8790. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.88it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8920. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.88it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8920. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8932. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8932. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8936. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8936. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8849. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8849. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.35it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8751. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.35it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8751. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8675. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8675. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8650. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8650. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8650. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.73s. Batch: 0.80s. Loss: 0.8737. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.73s. Batch: 0.80s. Loss: 0.8737. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch: 18/70. Data: 0.88s. Batch: 0.95s. Loss: 0.8505. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch: 18/70. Data: 0.88s. Batch: 0.95s. Loss: 0.8505. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch: 18/70. Data: 1.06s. Batch: 1.13s. Loss: 0.8542. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch: 18/70. Data: 1.06s. Batch: 1.13s. Loss: 0.8542. : 100%|██████████| 3/3 [00:01<00:00,  2.26it/s]Finetune Epoch: 18/70. Data: 1.06s. Batch: 1.13s. Loss: 0.8542. : 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8785. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8785. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8914. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8914. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8926. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8926. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8931. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8931. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.29it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8844. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.29it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8844. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.49it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8747. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.49it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8747. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.63it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8671. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.63it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8671. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8647. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8647. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8647. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.73s. Batch: 0.79s. Loss: 0.8563. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.73s. Batch: 0.79s. Loss: 0.8563. :  33%|███▎      | 1/3 [00:00<00:01,  1.26it/s]Finetune Epoch: 19/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8505. :  33%|███▎      | 1/3 [00:01<00:01,  1.26it/s]Finetune Epoch: 19/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8505. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 19/70. Data: 1.06s. Batch: 1.12s. Loss: 0.8425. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 19/70. Data: 1.06s. Batch: 1.12s. Loss: 0.8425. : 100%|██████████| 3/3 [00:01<00:00,  2.38it/s]Finetune Epoch: 19/70. Data: 1.06s. Batch: 1.12s. Loss: 0.8425. : 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8779. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8779. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8907. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8907. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.28it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8919. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.28it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8919. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8924. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8924. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.82it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8838. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.82it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8838. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8743. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8743. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8667. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8667. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8643. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8643. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.51it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8643. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.62s. Batch: 0.68s. Loss: 0.8557. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.62s. Batch: 0.68s. Loss: 0.8557. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 20/70. Data: 0.79s. Batch: 0.85s. Loss: 0.8430. :  33%|███▎      | 1/3 [00:01<00:01,  1.47it/s]Finetune Epoch: 20/70. Data: 0.79s. Batch: 0.85s. Loss: 0.8430. :  67%|██████▋   | 2/3 [00:01<00:00,  2.11it/s]Finetune Epoch: 20/70. Data: 0.95s. Batch: 1.00s. Loss: 0.8371. :  67%|██████▋   | 2/3 [00:01<00:00,  2.11it/s]Finetune Epoch: 20/70. Data: 0.95s. Batch: 1.00s. Loss: 0.8371. : 100%|██████████| 3/3 [00:01<00:00,  2.54it/s]Finetune Epoch: 20/70. Data: 0.95s. Batch: 1.00s. Loss: 0.8371. : 100%|██████████| 3/3 [00:01<00:00,  1.98it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8773. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8773. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8901. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8901. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8912. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8912. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8917. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8917. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8833. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8833. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8738. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8738. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.90it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8664. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.90it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8664. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8640. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8640. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8640. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.84s. Batch: 0.90s. Loss: 0.8503. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.84s. Batch: 0.90s. Loss: 0.8503. :  33%|███▎      | 1/3 [00:00<00:01,  1.11it/s]Finetune Epoch: 21/70. Data: 1.06s. Batch: 1.12s. Loss: 0.8429. :  33%|███▎      | 1/3 [00:01<00:01,  1.11it/s]Finetune Epoch: 21/70. Data: 1.06s. Batch: 1.12s. Loss: 0.8429. :  67%|██████▋   | 2/3 [00:01<00:00,  1.58it/s]Finetune Epoch: 21/70. Data: 1.27s. Batch: 1.32s. Loss: 0.8399. :  67%|██████▋   | 2/3 [00:01<00:00,  1.58it/s]Finetune Epoch: 21/70. Data: 1.27s. Batch: 1.32s. Loss: 0.8399. : 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]Finetune Epoch: 21/70. Data: 1.27s. Batch: 1.32s. Loss: 0.8399. : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8768. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8768. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8896. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8896. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8907. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8907. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8912. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8912. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.73it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8829. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.73it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8829. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8735. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8735. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8660. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8660. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8637. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8637. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8637. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.67s. Batch: 0.73s. Loss: 0.8166. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.67s. Batch: 0.73s. Loss: 0.8166. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 22/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8423. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 22/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8423. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 22/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8482. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 22/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8482. : 100%|██████████| 3/3 [00:01<00:00,  2.64it/s]Finetune Epoch: 22/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8482. : 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8764. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8764. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8891. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8891. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8903. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8903. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8908. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8908. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8825. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8825. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8731. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8731. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8657. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8657. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8634. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8634. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8634. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.72s. Batch: 0.79s. Loss: 0.8270. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.72s. Batch: 0.79s. Loss: 0.8270. :  33%|███▎      | 1/3 [00:00<00:01,  1.26it/s]Finetune Epoch: 23/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8322. :  33%|███▎      | 1/3 [00:01<00:01,  1.26it/s]Finetune Epoch: 23/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8322. :  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s]Finetune Epoch: 23/70. Data: 1.10s. Batch: 1.15s. Loss: 0.8417. :  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s]Finetune Epoch: 23/70. Data: 1.10s. Batch: 1.15s. Loss: 0.8417. : 100%|██████████| 3/3 [00:01<00:00,  2.24it/s]Finetune Epoch: 23/70. Data: 1.10s. Batch: 1.15s. Loss: 0.8417. : 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8760. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8760. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.75it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8887. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.75it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8887. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.53it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8898. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.53it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8898. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8903. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8903. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8821. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8821. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8728. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8728. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.70it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8654. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.70it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8654. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8631. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8631. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8631. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.59s. Batch: 0.65s. Loss: 0.8189. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.59s. Batch: 0.65s. Loss: 0.8189. :  33%|███▎      | 1/3 [00:00<00:01,  1.55it/s]Finetune Epoch: 24/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8283. :  33%|███▎      | 1/3 [00:01<00:01,  1.55it/s]Finetune Epoch: 24/70. Data: 0.79s. Batch: 0.84s. Loss: 0.8283. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 24/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8385. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 24/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8385. : 100%|██████████| 3/3 [00:01<00:00,  2.13it/s]Finetune Epoch: 24/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8385. : 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8756. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8756. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.24it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8882. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.24it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8882. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8893. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8893. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8898. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8898. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8817. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8817. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8724. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8724. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8651. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8651. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8628. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8628. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8628. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8673. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8673. :  33%|███▎      | 1/3 [00:00<00:01,  1.14it/s]Finetune Epoch: 25/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8436. :  33%|███▎      | 1/3 [00:01<00:01,  1.14it/s]Finetune Epoch: 25/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8436. :  67%|██████▋   | 2/3 [00:01<00:00,  1.71it/s]Finetune Epoch: 25/70. Data: 1.20s. Batch: 1.25s. Loss: 0.8477. :  67%|██████▋   | 2/3 [00:01<00:00,  1.71it/s]Finetune Epoch: 25/70. Data: 1.20s. Batch: 1.25s. Loss: 0.8477. : 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 25/70. Data: 1.20s. Batch: 1.25s. Loss: 0.8477. : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8751. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8751. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8876. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.29it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8876. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8888. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8888. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.47it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8893. top1: 90.14. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.47it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8893. top1: 90.14. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8812. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8812. top1: 90.62. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8720. top1: 91.34. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8720. top1: 91.34. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8647. top1: 91.96. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8647. top1: 91.96. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8625. top1: 92.15. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8625. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.29it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8625. top1: 92.15. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.58it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.76s. Batch: 0.81s. Loss: 0.8454. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.76s. Batch: 0.81s. Loss: 0.8454. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch: 26/70. Data: 0.95s. Batch: 1.00s. Loss: 0.8487. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch: 26/70. Data: 0.95s. Batch: 1.00s. Loss: 0.8487. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 26/70. Data: 1.15s. Batch: 1.20s. Loss: 0.8454. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 26/70. Data: 1.15s. Batch: 1.20s. Loss: 0.8454. : 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 26/70. Data: 1.15s. Batch: 1.20s. Loss: 0.8454. : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8746. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8746. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8871. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8871. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8882. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8882. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8888. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8888. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8807. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8807. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8717. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.16it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8717. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8644. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8644. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8621. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8621. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8621. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.64s. Batch: 0.71s. Loss: 0.8290. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.64s. Batch: 0.71s. Loss: 0.8290. :  33%|███▎      | 1/3 [00:00<00:01,  1.40it/s]Finetune Epoch: 27/70. Data: 0.78s. Batch: 0.85s. Loss: 0.8529. :  33%|███▎      | 1/3 [00:00<00:01,  1.40it/s]Finetune Epoch: 27/70. Data: 0.78s. Batch: 0.85s. Loss: 0.8529. :  67%|██████▋   | 2/3 [00:00<00:00,  2.21it/s]Finetune Epoch: 27/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8475. :  67%|██████▋   | 2/3 [00:01<00:00,  2.21it/s]Finetune Epoch: 27/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8475. : 100%|██████████| 3/3 [00:01<00:00,  2.57it/s]Finetune Epoch: 27/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8475. : 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8743. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8743. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.59it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8867. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.59it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8867. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8878. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8878. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8884. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8884. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8804. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8804. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8713. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.21it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8713. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8641. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8641. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.51it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8618. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.51it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8618. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8618. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.95it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.89s. Batch: 0.98s. Loss: 0.8734. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.89s. Batch: 0.98s. Loss: 0.8734. :  33%|███▎      | 1/3 [00:00<00:01,  1.02it/s]Finetune Epoch: 28/70. Data: 1.09s. Batch: 1.16s. Loss: 0.8515. :  33%|███▎      | 1/3 [00:01<00:01,  1.02it/s]Finetune Epoch: 28/70. Data: 1.09s. Batch: 1.16s. Loss: 0.8515. :  67%|██████▋   | 2/3 [00:01<00:00,  1.63it/s]Finetune Epoch: 28/70. Data: 1.27s. Batch: 1.34s. Loss: 0.8482. :  67%|██████▋   | 2/3 [00:01<00:00,  1.63it/s]Finetune Epoch: 28/70. Data: 1.27s. Batch: 1.34s. Loss: 0.8482. : 100%|██████████| 3/3 [00:01<00:00,  1.98it/s]Finetune Epoch: 28/70. Data: 1.27s. Batch: 1.34s. Loss: 0.8482. : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8739. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8739. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8863. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8863. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8874. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8874. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8880. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8880. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8800. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8800. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8710. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8710. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8638. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8638. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8615. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8615. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8615. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8118. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8118. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch: 29/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8366. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch: 29/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8366. :  67%|██████▋   | 2/3 [00:00<00:00,  2.18it/s]Finetune Epoch: 29/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8496. :  67%|██████▋   | 2/3 [00:01<00:00,  2.18it/s]Finetune Epoch: 29/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8496. : 100%|██████████| 3/3 [00:01<00:00,  2.72it/s]Finetune Epoch: 29/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8496. : 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8735. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8735. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8858. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8858. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8870. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8870. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8875. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8875. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8796. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8796. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8707. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8707. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.03it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8635. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.03it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8635. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8613. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8613. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8613. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.69s. Batch: 0.74s. Loss: 0.8510. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.69s. Batch: 0.74s. Loss: 0.8510. :  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Finetune Epoch: 30/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8512. :  33%|███▎      | 1/3 [00:01<00:01,  1.35it/s]Finetune Epoch: 30/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8512. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 30/70. Data: 1.09s. Batch: 1.15s. Loss: 0.8476. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 30/70. Data: 1.09s. Batch: 1.15s. Loss: 0.8476. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]Finetune Epoch: 30/70. Data: 1.09s. Batch: 1.15s. Loss: 0.8476. : 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8729. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8729. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.75it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8852. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.75it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8852. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.49it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8863. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.49it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8863. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.83it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8869. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.83it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8869. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.22it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8791. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.22it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8791. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8703. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8703. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.50it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8632. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.50it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8632. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8609. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8609. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8609. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.71s. Batch: 0.76s. Loss: 0.8428. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.71s. Batch: 0.76s. Loss: 0.8428. :  33%|███▎      | 1/3 [00:00<00:01,  1.32it/s]Finetune Epoch: 31/70. Data: 0.86s. Batch: 0.91s. Loss: 0.8503. :  33%|███▎      | 1/3 [00:01<00:01,  1.32it/s]Finetune Epoch: 31/70. Data: 0.86s. Batch: 0.91s. Loss: 0.8503. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 31/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8518. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 31/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8518. : 100%|██████████| 3/3 [00:01<00:00,  2.44it/s]Finetune Epoch: 31/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8518. : 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8726. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8726. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.00it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8848. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.00it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8848. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.61it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8859. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.61it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8859. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.10it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8865. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.10it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8865. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8788. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8788. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8700. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8700. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.58it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8629. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.58it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8629. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8607. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8607. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.59it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8607. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.65s. Batch: 0.71s. Loss: 0.8527. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.65s. Batch: 0.71s. Loss: 0.8527. :  33%|███▎      | 1/3 [00:00<00:01,  1.40it/s]Finetune Epoch: 32/70. Data: 0.83s. Batch: 0.88s. Loss: 0.8322. :  33%|███▎      | 1/3 [00:01<00:01,  1.40it/s]Finetune Epoch: 32/70. Data: 0.83s. Batch: 0.88s. Loss: 0.8322. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 32/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8455. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 32/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8455. : 100%|██████████| 3/3 [00:01<00:00,  2.35it/s]Finetune Epoch: 32/70. Data: 1.00s. Batch: 1.05s. Loss: 0.8455. : 100%|██████████| 3/3 [00:01<00:00,  1.89it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8721. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8721. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8843. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8843. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8854. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8854. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8859. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8859. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8783. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8783. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8696. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8696. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8625. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8625. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8604. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8604. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8604. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8441. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8441. :  33%|███▎      | 1/3 [00:00<00:01,  1.18it/s]Finetune Epoch: 33/70. Data: 0.96s. Batch: 1.03s. Loss: 0.8304. :  33%|███▎      | 1/3 [00:01<00:01,  1.18it/s]Finetune Epoch: 33/70. Data: 0.96s. Batch: 1.03s. Loss: 0.8304. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 33/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8410. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 33/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8410. : 100%|██████████| 3/3 [00:01<00:00,  2.17it/s]Finetune Epoch: 33/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8410. : 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.79s. Loss: 0.8717. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.79s. Loss: 0.8717. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.27it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8838. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.27it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8838. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8849. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8849. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8855. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8855. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8779. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8779. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8693. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8693. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.38it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8622. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.38it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8622. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8601. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8601. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8601. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.74s. Batch: 0.80s. Loss: 0.8775. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.74s. Batch: 0.80s. Loss: 0.8775. :  33%|███▎      | 1/3 [00:00<00:01,  1.26it/s]Finetune Epoch: 34/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8524. :  33%|███▎      | 1/3 [00:01<00:01,  1.26it/s]Finetune Epoch: 34/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8524. :  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s]Finetune Epoch: 34/70. Data: 1.12s. Batch: 1.17s. Loss: 0.8561. :  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s]Finetune Epoch: 34/70. Data: 1.12s. Batch: 1.17s. Loss: 0.8561. : 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 34/70. Data: 1.12s. Batch: 1.17s. Loss: 0.8561. : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8712. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8712. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8833. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8833. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8844. top1: 90.36. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8844. top1: 90.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8850. top1: 90.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8850. top1: 90.23. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8775. top1: 90.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8775. top1: 90.70. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.35it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8689. top1: 91.41. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.35it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8689. top1: 91.41. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.29it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8619. top1: 92.02. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.29it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8619. top1: 92.02. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8598. top1: 92.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8598. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8598. top1: 92.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.71s. Batch: 0.78s. Loss: 0.8796. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.71s. Batch: 0.78s. Loss: 0.8796. :  33%|███▎      | 1/3 [00:00<00:01,  1.29it/s]Finetune Epoch: 35/70. Data: 0.88s. Batch: 0.94s. Loss: 0.8610. :  33%|███▎      | 1/3 [00:01<00:01,  1.29it/s]Finetune Epoch: 35/70. Data: 0.88s. Batch: 0.94s. Loss: 0.8610. :  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s]Finetune Epoch: 35/70. Data: 1.05s. Batch: 1.11s. Loss: 0.8468. :  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s]Finetune Epoch: 35/70. Data: 1.05s. Batch: 1.11s. Loss: 0.8468. : 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]Finetune Epoch: 35/70. Data: 1.05s. Batch: 1.11s. Loss: 0.8468. : 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8707. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8707. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.91it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8827. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.91it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8827. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.76it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8839. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.76it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8839. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8844. top1: 90.33. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8844. top1: 90.33. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.44it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8770. top1: 90.78. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.44it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8770. top1: 90.78. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.41it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8685. top1: 91.47. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.41it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8685. top1: 91.47. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8616. top1: 92.08. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.62it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8616. top1: 92.08. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8595. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8595. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8595. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.29it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8516. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.63s. Batch: 0.69s. Loss: 0.8516. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 36/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8557. :  33%|███▎      | 1/3 [00:01<00:01,  1.45it/s]Finetune Epoch: 36/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8557. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 36/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8417. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 36/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8417. : 100%|██████████| 3/3 [00:01<00:00,  2.48it/s]Finetune Epoch: 36/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8417. : 100%|██████████| 3/3 [00:01<00:00,  1.94it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8703. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8703. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.60it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8824. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.60it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8824. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8835. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8835. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.89it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8840. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.89it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8840. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.37it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8767. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.37it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8767. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8682. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8682. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8613. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8613. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8592. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8592. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.72it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8592. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.97it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.63s. Batch: 0.70s. Loss: 0.8277. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.63s. Batch: 0.70s. Loss: 0.8277. :  33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]Finetune Epoch: 37/70. Data: 0.80s. Batch: 0.87s. Loss: 0.8521. :  33%|███▎      | 1/3 [00:01<00:01,  1.42it/s]Finetune Epoch: 37/70. Data: 0.80s. Batch: 0.87s. Loss: 0.8521. :  67%|██████▋   | 2/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 37/70. Data: 0.98s. Batch: 1.05s. Loss: 0.8407. :  67%|██████▋   | 2/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 37/70. Data: 0.98s. Batch: 1.05s. Loss: 0.8407. : 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]Finetune Epoch: 37/70. Data: 0.98s. Batch: 1.05s. Loss: 0.8407. : 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8700. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8700. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.60it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8819. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.60it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8819. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8830. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8830. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8836. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8836. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.24it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8763. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.24it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8763. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8679. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8679. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8610. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8610. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8589. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8589. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8589. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.52s. Batch: 0.58s. Loss: 0.8315. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.52s. Batch: 0.58s. Loss: 0.8315. :  33%|███▎      | 1/3 [00:00<00:01,  1.73it/s]Finetune Epoch: 38/70. Data: 0.66s. Batch: 0.71s. Loss: 0.8432. :  33%|███▎      | 1/3 [00:00<00:01,  1.73it/s]Finetune Epoch: 38/70. Data: 0.66s. Batch: 0.71s. Loss: 0.8432. :  67%|██████▋   | 2/3 [00:00<00:00,  2.52it/s]Finetune Epoch: 38/70. Data: 0.83s. Batch: 0.87s. Loss: 0.8493. :  67%|██████▋   | 2/3 [00:01<00:00,  2.52it/s]Finetune Epoch: 38/70. Data: 0.83s. Batch: 0.87s. Loss: 0.8493. : 100%|██████████| 3/3 [00:01<00:00,  2.68it/s]Finetune Epoch: 38/70. Data: 0.83s. Batch: 0.87s. Loss: 0.8493. : 100%|██████████| 3/3 [00:01<00:00,  2.18it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8695. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8695. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8814. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8814. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8825. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8825. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8831. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8831. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8759. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8759. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.18it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8675. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.18it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8675. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8607. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.35it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8607. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.41it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8586. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.41it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8586. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8586. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.66s. Batch: 0.73s. Loss: 0.8472. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.66s. Batch: 0.73s. Loss: 0.8472. :  33%|███▎      | 1/3 [00:00<00:01,  1.37it/s]Finetune Epoch: 39/70. Data: 0.88s. Batch: 0.96s. Loss: 0.8561. :  33%|███▎      | 1/3 [00:01<00:01,  1.37it/s]Finetune Epoch: 39/70. Data: 0.88s. Batch: 0.96s. Loss: 0.8561. :  67%|██████▋   | 2/3 [00:01<00:00,  1.74it/s]Finetune Epoch: 39/70. Data: 1.07s. Batch: 1.15s. Loss: 0.8441. :  67%|██████▋   | 2/3 [00:01<00:00,  1.74it/s]Finetune Epoch: 39/70. Data: 1.07s. Batch: 1.15s. Loss: 0.8441. : 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]Finetune Epoch: 39/70. Data: 1.07s. Batch: 1.15s. Loss: 0.8441. : 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8690. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8690. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.70it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8808. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.70it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8808. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8819. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8819. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8825. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8825. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8754. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8754. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.18it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8672. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.18it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8672. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.29it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8604. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.29it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8604. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8583. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8583. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8583. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 40/70. Data: 0.72s. Batch: 0.79s. Loss: 0.8554. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 40/70. Data: 0.72s. Batch: 0.79s. Loss: 0.8554. :  33%|███▎      | 1/3 [00:00<00:01,  1.26it/s]Finetune Epoch: 40/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8456. :  33%|███▎      | 1/3 [00:01<00:01,  1.26it/s]Finetune Epoch: 40/70. Data: 0.87s. Batch: 0.93s. Loss: 0.8456. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 40/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8394. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 40/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8394. : 100%|██████████| 3/3 [00:01<00:00,  2.46it/s]Finetune Epoch: 40/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8394. : 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 0.8684. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 0.8684. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.22it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8802. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.22it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8802. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8813. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8813. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8819. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8819. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.64it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8749. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.64it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8749. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8668. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8668. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8600. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8600. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8580. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8580. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8580. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.51it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8550. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8550. :  33%|███▎      | 1/3 [00:00<00:01,  1.01it/s]Finetune Epoch: 41/70. Data: 1.15s. Batch: 1.21s. Loss: 0.8550. :  33%|███▎      | 1/3 [00:01<00:01,  1.01it/s]Finetune Epoch: 41/70. Data: 1.15s. Batch: 1.21s. Loss: 0.8550. :  67%|██████▋   | 2/3 [00:01<00:00,  1.51it/s]Finetune Epoch: 41/70. Data: 1.35s. Batch: 1.40s. Loss: 0.8454. :  67%|██████▋   | 2/3 [00:01<00:00,  1.51it/s]Finetune Epoch: 41/70. Data: 1.35s. Batch: 1.40s. Loss: 0.8454. : 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]Finetune Epoch: 41/70. Data: 1.35s. Batch: 1.40s. Loss: 0.8454. : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8680. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8680. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8797. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8797. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8808. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8808. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8814. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8814. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8745. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8745. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8664. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8664. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8597. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8597. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.85it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8577. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.85it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8577. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8577. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.57it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.72s. Batch: 0.79s. Loss: 0.8567. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.72s. Batch: 0.79s. Loss: 0.8567. :  33%|███▎      | 1/3 [00:00<00:01,  1.26it/s]Finetune Epoch: 42/70. Data: 0.92s. Batch: 1.00s. Loss: 0.8420. :  33%|███▎      | 1/3 [00:01<00:01,  1.26it/s]Finetune Epoch: 42/70. Data: 0.92s. Batch: 1.00s. Loss: 0.8420. :  67%|██████▋   | 2/3 [00:01<00:00,  1.74it/s]Finetune Epoch: 42/70. Data: 1.10s. Batch: 1.17s. Loss: 0.8434. :  67%|██████▋   | 2/3 [00:01<00:00,  1.74it/s]Finetune Epoch: 42/70. Data: 1.10s. Batch: 1.17s. Loss: 0.8434. : 100%|██████████| 3/3 [00:01<00:00,  2.22it/s]Finetune Epoch: 42/70. Data: 1.10s. Batch: 1.17s. Loss: 0.8434. : 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8675. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8675. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.26it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8793. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.26it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8793. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8803. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8803. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8809. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8809. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.68it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8741. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.68it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8741. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8661. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8661. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8594. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8594. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8574. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8574. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8574. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.62it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8369. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8369. :  33%|███▎      | 1/3 [00:00<00:01,  1.02it/s]Finetune Epoch: 43/70. Data: 1.10s. Batch: 1.15s. Loss: 0.8500. :  33%|███▎      | 1/3 [00:01<00:01,  1.02it/s]Finetune Epoch: 43/70. Data: 1.10s. Batch: 1.15s. Loss: 0.8500. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 43/70. Data: 1.28s. Batch: 1.35s. Loss: 0.8483. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 43/70. Data: 1.28s. Batch: 1.35s. Loss: 0.8483. : 100%|██████████| 3/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 43/70. Data: 1.28s. Batch: 1.35s. Loss: 0.8483. : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8670. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8670. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8787. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8787. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8797. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8797. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8803. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8803. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.82it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8736. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.82it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8736. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8657. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8657. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8591. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8591. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8571. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8571. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8571. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.51it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.76s. Batch: 0.81s. Loss: 0.8576. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.76s. Batch: 0.81s. Loss: 0.8576. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch: 44/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8395. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch: 44/70. Data: 0.94s. Batch: 0.99s. Loss: 0.8395. :  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s]Finetune Epoch: 44/70. Data: 1.11s. Batch: 1.17s. Loss: 0.8425. :  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s]Finetune Epoch: 44/70. Data: 1.11s. Batch: 1.17s. Loss: 0.8425. : 100%|██████████| 3/3 [00:01<00:00,  2.18it/s]Finetune Epoch: 44/70. Data: 1.11s. Batch: 1.17s. Loss: 0.8425. : 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8665. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8665. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8781. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8781. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8791. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8791. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8797. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8797. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.24it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8732. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.24it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8732. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8653. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8653. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8587. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8587. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8568. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8568. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8568. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.71s. Batch: 0.77s. Loss: 0.8493. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.71s. Batch: 0.77s. Loss: 0.8493. :  33%|███▎      | 1/3 [00:00<00:01,  1.30it/s]Finetune Epoch: 45/70. Data: 0.91s. Batch: 0.99s. Loss: 0.8573. :  33%|███▎      | 1/3 [00:01<00:01,  1.30it/s]Finetune Epoch: 45/70. Data: 0.91s. Batch: 0.99s. Loss: 0.8573. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 45/70. Data: 1.13s. Batch: 1.20s. Loss: 0.8436. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 45/70. Data: 1.13s. Batch: 1.20s. Loss: 0.8436. : 100%|██████████| 3/3 [00:01<00:00,  1.98it/s]Finetune Epoch: 45/70. Data: 1.13s. Batch: 1.20s. Loss: 0.8436. : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8661. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8661. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8776. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8776. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.11it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8787. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.11it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8787. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8793. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8793. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8728. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8728. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8650. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8650. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8585. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8585. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8566. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8566. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.51it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8566. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.81s. Batch: 0.86s. Loss: 0.8564. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.81s. Batch: 0.86s. Loss: 0.8564. :  33%|███▎      | 1/3 [00:00<00:01,  1.16it/s]Finetune Epoch: 46/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8468. :  33%|███▎      | 1/3 [00:01<00:01,  1.16it/s]Finetune Epoch: 46/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8468. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 46/70. Data: 1.15s. Batch: 1.21s. Loss: 0.8359. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 46/70. Data: 1.15s. Batch: 1.21s. Loss: 0.8359. : 100%|██████████| 3/3 [00:01<00:00,  2.19it/s]Finetune Epoch: 46/70. Data: 1.15s. Batch: 1.21s. Loss: 0.8359. : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8657. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8657. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8772. top1: 91.21. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8772. top1: 91.21. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8783. top1: 90.62. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8783. top1: 90.62. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8789. top1: 90.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8789. top1: 90.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8724. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8724. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8647. top1: 91.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8647. top1: 91.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8582. top1: 92.19. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8582. top1: 92.19. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8563. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8563. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8563. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.77s. Batch: 0.82s. Loss: 0.8233. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.77s. Batch: 0.82s. Loss: 0.8233. :  33%|███▎      | 1/3 [00:00<00:01,  1.21it/s]Finetune Epoch: 47/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8479. :  33%|███▎      | 1/3 [00:01<00:01,  1.21it/s]Finetune Epoch: 47/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8479. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 47/70. Data: 1.13s. Batch: 1.18s. Loss: 0.8408. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 47/70. Data: 1.13s. Batch: 1.18s. Loss: 0.8408. : 100%|██████████| 3/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 47/70. Data: 1.13s. Batch: 1.18s. Loss: 0.8408. : 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8652. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8652. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8767. top1: 91.21. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8767. top1: 91.21. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8778. top1: 90.62. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8778. top1: 90.62. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8784. top1: 90.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8784. top1: 90.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8720. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8720. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8644. top1: 91.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8644. top1: 91.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8579. top1: 92.19. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8579. top1: 92.19. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.00it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8560. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.00it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8560. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.23it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8560. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.77s. Batch: 0.86s. Loss: 0.8282. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.77s. Batch: 0.86s. Loss: 0.8282. :  33%|███▎      | 1/3 [00:00<00:01,  1.17it/s]Finetune Epoch: 48/70. Data: 0.96s. Batch: 1.04s. Loss: 0.8338. :  33%|███▎      | 1/3 [00:01<00:01,  1.17it/s]Finetune Epoch: 48/70. Data: 0.96s. Batch: 1.04s. Loss: 0.8338. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 48/70. Data: 1.16s. Batch: 1.23s. Loss: 0.8439. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 48/70. Data: 1.16s. Batch: 1.23s. Loss: 0.8439. : 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 48/70. Data: 1.16s. Batch: 1.23s. Loss: 0.8439. : 100%|██████████| 3/3 [00:01<00:00,  1.68it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8649. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8649. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8763. top1: 91.21. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8763. top1: 91.21. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8774. top1: 90.62. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8774. top1: 90.62. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8780. top1: 90.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8780. top1: 90.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8717. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8717. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8641. top1: 91.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8641. top1: 91.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8576. top1: 92.19. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8576. top1: 92.19. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8558. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8558. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.85it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8558. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.93it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.59s. Batch: 0.65s. Loss: 0.8425. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.59s. Batch: 0.65s. Loss: 0.8425. :  33%|███▎      | 1/3 [00:00<00:01,  1.55it/s]Finetune Epoch: 49/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8312. :  33%|███▎      | 1/3 [00:00<00:01,  1.55it/s]Finetune Epoch: 49/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8312. :  67%|██████▋   | 2/3 [00:00<00:00,  2.20it/s]Finetune Epoch: 49/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8367. :  67%|██████▋   | 2/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 49/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8367. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]Finetune Epoch: 49/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8367. : 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8645. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8645. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.93it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8758. top1: 91.21. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.93it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8758. top1: 91.21. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8769. top1: 90.62. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.52it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8769. top1: 90.62. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8775. top1: 90.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8775. top1: 90.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8713. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.06it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8713. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8637. top1: 91.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8637. top1: 91.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8573. top1: 92.19. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8573. top1: 92.19. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8555. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8555. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8555. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.95it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.68s. Batch: 0.75s. Loss: 0.8156. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.68s. Batch: 0.75s. Loss: 0.8156. :  33%|███▎      | 1/3 [00:00<00:01,  1.32it/s]Finetune Epoch: 50/70. Data: 0.84s. Batch: 0.91s. Loss: 0.8288. :  33%|███▎      | 1/3 [00:01<00:01,  1.32it/s]Finetune Epoch: 50/70. Data: 0.84s. Batch: 0.91s. Loss: 0.8288. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 50/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8417. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 50/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8417. : 100%|██████████| 3/3 [00:01<00:00,  2.41it/s]Finetune Epoch: 50/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8417. : 100%|██████████| 3/3 [00:01<00:00,  1.95it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8641. top1: 92.58. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8641. top1: 92.58. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8754. top1: 91.21. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8754. top1: 91.21. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8765. top1: 90.62. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8765. top1: 90.62. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.07it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8771. top1: 90.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.07it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8771. top1: 90.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8709. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8709. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8634. top1: 91.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8634. top1: 91.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8571. top1: 92.19. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8571. top1: 92.19. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8552. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8552. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.23it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8552. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.72s. Batch: 0.77s. Loss: 0.8644. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.72s. Batch: 0.77s. Loss: 0.8644. :  33%|███▎      | 1/3 [00:00<00:01,  1.29it/s]Finetune Epoch: 51/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8433. :  33%|███▎      | 1/3 [00:01<00:01,  1.29it/s]Finetune Epoch: 51/70. Data: 0.91s. Batch: 0.96s. Loss: 0.8433. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 51/70. Data: 1.09s. Batch: 1.14s. Loss: 0.8487. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 51/70. Data: 1.09s. Batch: 1.14s. Loss: 0.8487. : 100%|██████████| 3/3 [00:01<00:00,  2.21it/s]Finetune Epoch: 51/70. Data: 1.09s. Batch: 1.14s. Loss: 0.8487. : 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8637. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8637. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8750. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8750. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8761. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8761. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8767. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8767. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8706. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8706. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8631. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8631. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8568. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8568. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8550. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8550. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.76it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8550. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.71s. Batch: 0.79s. Loss: 0.8269. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.71s. Batch: 0.79s. Loss: 0.8269. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 52/70. Data: 0.88s. Batch: 0.95s. Loss: 0.8283. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 52/70. Data: 0.88s. Batch: 0.95s. Loss: 0.8283. :  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s]Finetune Epoch: 52/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8320. :  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s]Finetune Epoch: 52/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8320. : 100%|██████████| 3/3 [00:01<00:00,  2.54it/s]Finetune Epoch: 52/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8320. : 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8633. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8633. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8745. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8745. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8756. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8756. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.45it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8762. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.45it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8762. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.54it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8702. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.54it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8702. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8628. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.86it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8628. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8565. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8565. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8547. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8547. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8547. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.64it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.72s. Batch: 0.79s. Loss: 0.8646. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.72s. Batch: 0.79s. Loss: 0.8646. :  33%|███▎      | 1/3 [00:00<00:01,  1.26it/s]Finetune Epoch: 53/70. Data: 0.90s. Batch: 0.97s. Loss: 0.8532. :  33%|███▎      | 1/3 [00:01<00:01,  1.26it/s]Finetune Epoch: 53/70. Data: 0.90s. Batch: 0.97s. Loss: 0.8532. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 53/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8456. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 53/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8456. : 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 53/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8456. : 100%|██████████| 3/3 [00:01<00:00,  1.69it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8630. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8630. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8742. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8742. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8753. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8753. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8759. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8759. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.74it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8699. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.74it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8699. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.71it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8625. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.71it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8625. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.83it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8563. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.83it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8563. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8545. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8545. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8545. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.49it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.84s. Batch: 0.91s. Loss: 0.8549. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.84s. Batch: 0.91s. Loss: 0.8549. :  33%|███▎      | 1/3 [00:00<00:01,  1.10it/s]Finetune Epoch: 54/70. Data: 1.07s. Batch: 1.13s. Loss: 0.8458. :  33%|███▎      | 1/3 [00:01<00:01,  1.10it/s]Finetune Epoch: 54/70. Data: 1.07s. Batch: 1.13s. Loss: 0.8458. :  67%|██████▋   | 2/3 [00:01<00:00,  1.57it/s]Finetune Epoch: 54/70. Data: 1.26s. Batch: 1.32s. Loss: 0.8435. :  67%|██████▋   | 2/3 [00:01<00:00,  1.57it/s]Finetune Epoch: 54/70. Data: 1.26s. Batch: 1.32s. Loss: 0.8435. : 100%|██████████| 3/3 [00:01<00:00,  2.00it/s]Finetune Epoch: 54/70. Data: 1.26s. Batch: 1.32s. Loss: 0.8435. : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.8626. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.8626. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8738. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8738. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8748. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8748. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.29it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8754. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.29it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8754. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8695. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8695. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.71it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8622. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.71it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8622. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8560. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8560. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8542. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:03<00:00,  2.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8542. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.76it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8542. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.37it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.83s. Batch: 0.88s. Loss: 0.8328. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.83s. Batch: 0.88s. Loss: 0.8328. :  33%|███▎      | 1/3 [00:00<00:01,  1.14it/s]Finetune Epoch: 55/70. Data: 1.04s. Batch: 1.09s. Loss: 0.8446. :  33%|███▎      | 1/3 [00:01<00:01,  1.14it/s]Finetune Epoch: 55/70. Data: 1.04s. Batch: 1.09s. Loss: 0.8446. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 55/70. Data: 1.23s. Batch: 1.28s. Loss: 0.8443. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 55/70. Data: 1.23s. Batch: 1.28s. Loss: 0.8443. : 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 55/70. Data: 1.23s. Batch: 1.28s. Loss: 0.8443. : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8622. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8622. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8733. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8733. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8744. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8744. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8750. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8750. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.45it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8692. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.45it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8692. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.66it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8619. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.66it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8619. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8557. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8557. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8540. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8540. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8540. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.53it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.86s. Batch: 0.92s. Loss: 0.8517. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.86s. Batch: 0.92s. Loss: 0.8517. :  33%|███▎      | 1/3 [00:00<00:01,  1.09it/s]Finetune Epoch: 56/70. Data: 1.04s. Batch: 1.10s. Loss: 0.8369. :  33%|███▎      | 1/3 [00:01<00:01,  1.09it/s]Finetune Epoch: 56/70. Data: 1.04s. Batch: 1.10s. Loss: 0.8369. :  67%|██████▋   | 2/3 [00:01<00:00,  1.68it/s]Finetune Epoch: 56/70. Data: 1.22s. Batch: 1.27s. Loss: 0.8475. :  67%|██████▋   | 2/3 [00:01<00:00,  1.68it/s]Finetune Epoch: 56/70. Data: 1.22s. Batch: 1.27s. Loss: 0.8475. : 100%|██████████| 3/3 [00:01<00:00,  2.12it/s]Finetune Epoch: 56/70. Data: 1.22s. Batch: 1.27s. Loss: 0.8475. : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.79s. Loss: 0.8618. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.79s. Loss: 0.8618. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.27it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8729. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.27it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8729. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8739. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8739. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8746. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8746. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.69it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8688. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.69it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8688. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.62it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8616. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.62it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8616. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.80it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8555. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.80it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8555. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8537. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:03<00:00,  2.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8537. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8537. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.42it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8555. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8555. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch: 57/70. Data: 0.93s. Batch: 1.00s. Loss: 0.8531. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch: 57/70. Data: 0.93s. Batch: 1.00s. Loss: 0.8531. :  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s]Finetune Epoch: 57/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8381. :  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s]Finetune Epoch: 57/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8381. : 100%|██████████| 3/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 57/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8381. : 100%|██████████| 3/3 [00:01<00:00,  1.68it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.86s. Loss: 0.8613. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.86s. Loss: 0.8613. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:06,  1.16it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8724. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:06,  1.16it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8724. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.89it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8734. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.89it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8734. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.36it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8740. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.36it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8740. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8684. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8684. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8613. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8613. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8552. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8552. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8534. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8534. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8534. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.52it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.75s. Batch: 0.83s. Loss: 0.8579. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.75s. Batch: 0.83s. Loss: 0.8579. :  33%|███▎      | 1/3 [00:00<00:01,  1.21it/s]Finetune Epoch: 58/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8423. :  33%|███▎      | 1/3 [00:01<00:01,  1.21it/s]Finetune Epoch: 58/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8423. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 58/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8453. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 58/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8453. : 100%|██████████| 3/3 [00:01<00:00,  2.08it/s]Finetune Epoch: 58/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8453. : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 0.8611. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 0.8611. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.31it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8721. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.31it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8721. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8731. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8731. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.29it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8737. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.29it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8737. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8681. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8681. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8610. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8610. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.91it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8549. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.91it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8549. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8532. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:03<00:00,  2.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8532. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  3.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8532. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.41it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.83s. Batch: 0.93s. Loss: 0.8737. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.83s. Batch: 0.93s. Loss: 0.8737. :  33%|███▎      | 1/3 [00:00<00:01,  1.07it/s]Finetune Epoch: 59/70. Data: 1.04s. Batch: 1.12s. Loss: 0.8532. :  33%|███▎      | 1/3 [00:01<00:01,  1.07it/s]Finetune Epoch: 59/70. Data: 1.04s. Batch: 1.12s. Loss: 0.8532. :  67%|██████▋   | 2/3 [00:01<00:00,  1.65it/s]Finetune Epoch: 59/70. Data: 1.23s. Batch: 1.30s. Loss: 0.8406. :  67%|██████▋   | 2/3 [00:01<00:00,  1.65it/s]Finetune Epoch: 59/70. Data: 1.23s. Batch: 1.30s. Loss: 0.8406. : 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 59/70. Data: 1.23s. Batch: 1.30s. Loss: 0.8406. : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8607. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8607. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8716. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8716. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8727. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8727. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.29it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8733. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.29it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8733. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.30it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8678. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.30it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8678. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8607. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8607. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.65it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8547. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.65it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8547. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8530. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:03<00:00,  2.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8530. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  3.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8530. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.34it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8606. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8606. :  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Finetune Epoch: 60/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8427. :  33%|███▎      | 1/3 [00:01<00:01,  1.13it/s]Finetune Epoch: 60/70. Data: 0.98s. Batch: 1.04s. Loss: 0.8427. :  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s]Finetune Epoch: 60/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8396. :  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s]Finetune Epoch: 60/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8396. : 100%|██████████| 3/3 [00:01<00:00,  2.23it/s]Finetune Epoch: 60/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8396. : 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8604. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8604. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8713. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8713. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8723. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8723. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8730. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8730. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.86it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8675. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.86it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8675. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.81it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8605. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.81it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8605. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8544. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8544. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8528. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8528. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8528. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.59it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8405. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.78s. Batch: 0.84s. Loss: 0.8405. :  33%|███▎      | 1/3 [00:00<00:01,  1.19it/s]Finetune Epoch: 61/70. Data: 0.97s. Batch: 1.02s. Loss: 0.8415. :  33%|███▎      | 1/3 [00:01<00:01,  1.19it/s]Finetune Epoch: 61/70. Data: 0.97s. Batch: 1.02s. Loss: 0.8415. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 61/70. Data: 1.16s. Batch: 1.21s. Loss: 0.8472. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 61/70. Data: 1.16s. Batch: 1.21s. Loss: 0.8472. : 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 61/70. Data: 1.16s. Batch: 1.21s. Loss: 0.8472. : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8600. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8600. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8708. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8708. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8719. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8719. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8725. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8725. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8671. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8671. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8602. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8602. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8542. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8542. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8525. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8525. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8525. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.68s. Batch: 0.75s. Loss: 0.8815. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.68s. Batch: 0.75s. Loss: 0.8815. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch: 62/70. Data: 0.89s. Batch: 0.95s. Loss: 0.8667. :  33%|███▎      | 1/3 [00:01<00:01,  1.33it/s]Finetune Epoch: 62/70. Data: 0.89s. Batch: 0.95s. Loss: 0.8667. :  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s]Finetune Epoch: 62/70. Data: 1.09s. Batch: 1.15s. Loss: 0.8482. :  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s]Finetune Epoch: 62/70. Data: 1.09s. Batch: 1.15s. Loss: 0.8482. : 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 62/70. Data: 1.09s. Batch: 1.15s. Loss: 0.8482. : 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8596. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8596. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8704. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8704. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8714. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8714. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.52it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8721. top1: 90.43. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.52it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8721. top1: 90.43. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8667. top1: 90.86. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8667. top1: 90.86. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.89it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8599. top1: 91.54. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.89it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8599. top1: 91.54. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8539. top1: 92.13. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8539. top1: 92.13. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8522. top1: 92.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8522. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8522. top1: 92.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.62s. Batch: 0.72s. Loss: 0.8429. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.62s. Batch: 0.72s. Loss: 0.8429. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 63/70. Data: 0.84s. Batch: 0.93s. Loss: 0.8507. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 63/70. Data: 0.84s. Batch: 0.93s. Loss: 0.8507. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 63/70. Data: 1.04s. Batch: 1.12s. Loss: 0.8513. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 63/70. Data: 1.04s. Batch: 1.12s. Loss: 0.8513. : 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]Finetune Epoch: 63/70. Data: 1.04s. Batch: 1.12s. Loss: 0.8513. : 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8591. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8591. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8699. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8699. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8709. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8709. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8716. top1: 90.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8716. top1: 90.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.87it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8663. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.87it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8663. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.83it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8596. top1: 91.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.83it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8596. top1: 91.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8536. top1: 92.19. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8536. top1: 92.19. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8520. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8520. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8520. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.77s. Batch: 0.86s. Loss: 0.8649. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.77s. Batch: 0.86s. Loss: 0.8649. :  33%|███▎      | 1/3 [00:00<00:01,  1.17it/s]Finetune Epoch: 64/70. Data: 0.95s. Batch: 1.03s. Loss: 0.8428. :  33%|███▎      | 1/3 [00:01<00:01,  1.17it/s]Finetune Epoch: 64/70. Data: 0.95s. Batch: 1.03s. Loss: 0.8428. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 64/70. Data: 1.11s. Batch: 1.19s. Loss: 0.8423. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 64/70. Data: 1.11s. Batch: 1.19s. Loss: 0.8423. : 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]Finetune Epoch: 64/70. Data: 1.11s. Batch: 1.19s. Loss: 0.8423. : 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.8588. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.8588. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8695. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8695. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8705. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8705. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.43it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8712. top1: 90.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.43it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8712. top1: 90.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8660. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8660. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.92it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8593. top1: 91.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.92it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8593. top1: 91.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8534. top1: 92.19. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8534. top1: 92.19. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8518. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8518. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8518. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.61it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.71s. Batch: 0.78s. Loss: 0.8483. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.71s. Batch: 0.78s. Loss: 0.8483. :  33%|███▎      | 1/3 [00:00<00:01,  1.28it/s]Finetune Epoch: 65/70. Data: 0.90s. Batch: 0.95s. Loss: 0.8449. :  33%|███▎      | 1/3 [00:01<00:01,  1.28it/s]Finetune Epoch: 65/70. Data: 0.90s. Batch: 0.95s. Loss: 0.8449. :  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s]Finetune Epoch: 65/70. Data: 1.07s. Batch: 1.13s. Loss: 0.8420. :  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s]Finetune Epoch: 65/70. Data: 1.07s. Batch: 1.13s. Loss: 0.8420. : 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 65/70. Data: 1.07s. Batch: 1.13s. Loss: 0.8420. : 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8583. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8583. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.28it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8690. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.28it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8690. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8700. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.01it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8700. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8707. top1: 90.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8707. top1: 90.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.71it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8656. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.71it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8656. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.86it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8590. top1: 91.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.86it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8590. top1: 91.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8531. top1: 92.19. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8531. top1: 92.19. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8515. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8515. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.23it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8515. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.53it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.82s. Batch: 0.89s. Loss: 0.8142. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.82s. Batch: 0.89s. Loss: 0.8142. :  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Finetune Epoch: 66/70. Data: 1.04s. Batch: 1.10s. Loss: 0.8302. :  33%|███▎      | 1/3 [00:01<00:01,  1.13it/s]Finetune Epoch: 66/70. Data: 1.04s. Batch: 1.10s. Loss: 0.8302. :  67%|██████▋   | 2/3 [00:01<00:00,  1.62it/s]Finetune Epoch: 66/70. Data: 1.22s. Batch: 1.28s. Loss: 0.8409. :  67%|██████▋   | 2/3 [00:01<00:00,  1.62it/s]Finetune Epoch: 66/70. Data: 1.22s. Batch: 1.28s. Loss: 0.8409. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 66/70. Data: 1.22s. Batch: 1.28s. Loss: 0.8409. : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8579. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8579. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8686. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.29it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8686. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8696. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.02it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8696. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.47it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8703. top1: 90.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.47it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8703. top1: 90.53. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.64it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8653. top1: 90.94. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.64it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8653. top1: 90.94. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.80it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8587. top1: 91.60. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.80it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8587. top1: 91.60. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8529. top1: 92.19. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8529. top1: 92.19. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8513. top1: 92.30. top5: 100.00. :  88%|████████▊ | 7/8 [00:03<00:00,  2.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8513. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8513. top1: 92.30. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.48it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.89s. Batch: 0.97s. Loss: 0.8561. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.89s. Batch: 0.97s. Loss: 0.8561. :  33%|███▎      | 1/3 [00:00<00:01,  1.03it/s]Finetune Epoch: 67/70. Data: 1.05s. Batch: 1.12s. Loss: 0.8418. :  33%|███▎      | 1/3 [00:01<00:01,  1.03it/s]Finetune Epoch: 67/70. Data: 1.05s. Batch: 1.12s. Loss: 0.8418. :  67%|██████▋   | 2/3 [00:01<00:00,  1.73it/s]Finetune Epoch: 67/70. Data: 1.23s. Batch: 1.29s. Loss: 0.8465. :  67%|██████▋   | 2/3 [00:01<00:00,  1.73it/s]Finetune Epoch: 67/70. Data: 1.23s. Batch: 1.29s. Loss: 0.8465. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 67/70. Data: 1.23s. Batch: 1.29s. Loss: 0.8465. : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8575. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8575. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8681. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8681. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8692. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8692. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.42it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8698. top1: 90.62. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.42it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8698. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.53it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8649. top1: 91.02. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.53it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8649. top1: 91.02. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.67it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8584. top1: 91.67. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.67it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8584. top1: 91.67. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.65it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8526. top1: 92.24. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.65it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8526. top1: 92.24. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8511. top1: 92.35. top5: 100.00. :  88%|████████▊ | 7/8 [00:03<00:00,  2.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8511. top1: 92.35. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8511. top1: 92.35. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.47it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 68/70. Data: 0.78s. Batch: 0.86s. Loss: 0.8336. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 68/70. Data: 0.78s. Batch: 0.86s. Loss: 0.8336. :  33%|███▎      | 1/3 [00:00<00:01,  1.17it/s]Finetune Epoch: 68/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8384. :  33%|███▎      | 1/3 [00:01<00:01,  1.17it/s]Finetune Epoch: 68/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8384. :  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s]Finetune Epoch: 68/70. Data: 1.10s. Batch: 1.16s. Loss: 0.8395. :  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s]Finetune Epoch: 68/70. Data: 1.10s. Batch: 1.16s. Loss: 0.8395. : 100%|██████████| 3/3 [00:01<00:00,  2.35it/s]Finetune Epoch: 68/70. Data: 1.10s. Batch: 1.16s. Loss: 0.8395. : 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.8571. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.8571. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8677. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8677. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8687. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8687. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8694. top1: 90.62. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8694. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.92it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8646. top1: 91.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.92it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8646. top1: 91.02. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8581. top1: 91.67. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8581. top1: 91.67. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8524. top1: 92.24. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.85it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8524. top1: 92.24. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.85it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8508. top1: 92.35. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.85it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8508. top1: 92.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8508. top1: 92.35. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.50it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8428. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8428. :  33%|███▎      | 1/3 [00:00<00:01,  1.18it/s]Finetune Epoch: 69/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8515. :  33%|███▎      | 1/3 [00:01<00:01,  1.18it/s]Finetune Epoch: 69/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8515. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 69/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8501. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 69/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8501. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 69/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8501. : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8568. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8568. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8673. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8673. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8683. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8683. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8690. top1: 90.62. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8690. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8642. top1: 91.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8642. top1: 91.02. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.07it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8579. top1: 91.67. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.07it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8579. top1: 91.67. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8521. top1: 92.24. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8521. top1: 92.24. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8506. top1: 92.35. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8506. top1: 92.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.61it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8506. top1: 92.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.57s. Batch: 0.63s. Loss: 0.8644. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.57s. Batch: 0.63s. Loss: 0.8644. :  33%|███▎      | 1/3 [00:00<00:01,  1.60it/s]Finetune Epoch: 70/70. Data: 0.75s. Batch: 0.80s. Loss: 0.8557. :  33%|███▎      | 1/3 [00:00<00:01,  1.60it/s]Finetune Epoch: 70/70. Data: 0.75s. Batch: 0.80s. Loss: 0.8557. :  67%|██████▋   | 2/3 [00:00<00:00,  2.18it/s]Finetune Epoch: 70/70. Data: 0.93s. Batch: 0.98s. Loss: 0.8446. :  67%|██████▋   | 2/3 [00:01<00:00,  2.18it/s]Finetune Epoch: 70/70. Data: 0.93s. Batch: 0.98s. Loss: 0.8446. : 100%|██████████| 3/3 [00:01<00:00,  2.34it/s]Finetune Epoch: 70/70. Data: 0.93s. Batch: 0.98s. Loss: 0.8446. : 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8565. top1: 92.19. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8565. top1: 92.19. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8670. top1: 91.02. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8670. top1: 91.02. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8681. top1: 90.49. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8681. top1: 90.49. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8687. top1: 90.62. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8687. top1: 90.62. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8640. top1: 91.02. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8640. top1: 91.02. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8576. top1: 91.67. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8576. top1: 91.67. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8519. top1: 92.24. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8519. top1: 92.24. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8504. top1: 92.35. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8504. top1: 92.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8504. top1: 92.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.67it/s]
total 9984 correct 8252 accuracy 82.65224358974359
[INFO] main.py:349 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 10 -> 10
[INFO] main.py:357 > [2-3] Start to train under online
[INFO] main.py:372 > Train over streamed data once
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42 pseudo_stream_size 42
num_stuff 237
[INFO] rainbow_memory.py:120 > Streamed samples: 800
[INFO] rainbow_memory.py:121 > In-memory samples: 500
[INFO] rainbow_memory.py:122 > Pseudo samples: 9984
[INFO] rainbow_memory.py:128 > Train samples: 11284
[INFO] rainbow_memory.py:129 > Test samples: 6000
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([38, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
last_idx 17
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
final_idx 237
task2/train/loss 0.5715184364499164 0
task2/test/loss 6.9293250655007625 0
task2/test/acc 0.3055 0
task2/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 1/1 | train_loss 0.5715 | train_acc 0.7199 | test_loss 6.9293 | test_acc 0.3055 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[INFO] finetune.py:679 > Compute uncertainty by vr_randaug!
[WARNING] finetune.py:639 > Fill the unused slots by breaking the equilibrium.
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
frog          123
truck         115
bird           72
automobile     68
deer           63
dog            59
Name: klass, dtype: int64
[INFO] main.py:388 > Train over memory
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42 pseudo_stream_size 42
num_stuff 0
[INFO] rainbow_memory.py:120 > Streamed samples: 0
[INFO] rainbow_memory.py:121 > In-memory samples: 500
[INFO] rainbow_memory.py:122 > Pseudo samples: 0
[INFO] rainbow_memory.py:128 > Train samples: 500
[INFO] rainbow_memory.py:129 > Test samples: 6000
last_idx 11
final_idx 0
task2/train/loss 2.72242663304011 0
task2/test/loss 2.439412012805034 0
task2/test/acc 0.2976666666666667 0
task2/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 1/256 | train_loss 2.7224 | train_acc 0.3800 | test_loss 2.4394 | test_acc 0.2977 | lr 0.0050
last_idx 11
final_idx 0
task2/train/loss 2.277563363313675 1
task2/test/loss 2.959324671404205 1
task2/test/acc 0.2796666666666667 1
task2/train/lr 0.05 1
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 2/256 | train_loss 2.2776 | train_acc 0.2320 | test_loss 2.9593 | test_acc 0.2797 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 1.8797190686066945 2
task2/test/loss 1.7332224767573559 2
task2/test/acc 0.3273333333333333 2
task2/train/lr 0.05 2
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 3/256 | train_loss 1.8797 | train_acc 0.3200 | test_loss 1.7332 | test_acc 0.3273 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 1.5829793612162273 3
task2/test/loss 1.4343296620097474 3
task2/test/acc 0.3755 3
task2/train/lr 0.02525 3
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 4/256 | train_loss 1.5830 | train_acc 0.3620 | test_loss 1.4343 | test_acc 0.3755 | lr 0.0253
last_idx 11
final_idx 0
task2/train/loss 1.511177529891332 4
task2/test/loss 1.387672657949211 4
task2/test/acc 0.3968333333333333 4
task2/train/lr 0.05 4
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 5/256 | train_loss 1.5112 | train_acc 0.4000 | test_loss 1.3877 | test_acc 0.3968 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 1.4549910724163055 5
task2/test/loss 1.3783384966154169 5
task2/test/acc 0.3895 5
task2/train/lr 0.04275089283436705 5
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 6/256 | train_loss 1.4550 | train_acc 0.4160 | test_loss 1.3783 | test_acc 0.3895 | lr 0.0428
last_idx 11
final_idx 0
task2/train/loss 1.4154715935389202 6
task2/test/loss 1.234034826720718 6
task2/test/acc 0.45916666666666667 6
task2/train/lr 0.02525 6
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 7/256 | train_loss 1.4155 | train_acc 0.4340 | test_loss 1.2340 | test_acc 0.4592 | lr 0.0253
last_idx 11
final_idx 0
task2/train/loss 1.3675505816936493 7
task2/test/loss 1.2674623414547774 7
task2/test/acc 0.44233333333333336 7
task2/train/lr 0.00774910716563295 7
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 8/256 | train_loss 1.3676 | train_acc 0.4600 | test_loss 1.2675 | test_acc 0.4423 | lr 0.0077
last_idx 11
final_idx 0
task2/train/loss 1.3459689716498058 8
task2/test/loss 1.4159425627576174 8
task2/test/acc 0.38616666666666666 8
task2/train/lr 0.05 8
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 9/256 | train_loss 1.3460 | train_acc 0.4940 | test_loss 1.4159 | test_acc 0.3862 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 1.2572734256585438 9
task2/test/loss 1.4678859293025777 9
task2/test/acc 0.44116666666666665 9
task2/train/lr 0.04811601842965435 9
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 10/256 | train_loss 1.2573 | train_acc 0.4460 | test_loss 1.4679 | test_acc 0.4412 | lr 0.0481
last_idx 11
final_idx 0
task2/train/loss 1.356046348810196 10
task2/test/loss 1.2937152916497558 10
task2/test/acc 0.44783333333333336 10
task2/train/lr 0.04275089283436705 10
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 11/256 | train_loss 1.3560 | train_acc 0.4640 | test_loss 1.2937 | test_acc 0.4478 | lr 0.0428
last_idx 11
final_idx 0
task2/train/loss 1.3155320783456166 11
task2/test/loss 1.3592727393129447 11
task2/test/acc 0.43433333333333335 11
task2/train/lr 0.03472141495103598 11
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 12/256 | train_loss 1.3155 | train_acc 0.4840 | test_loss 1.3593 | test_acc 0.4343 | lr 0.0347
last_idx 11
final_idx 0
task2/train/loss 1.1652978112300236 12
task2/test/loss 1.1790632261847058 12
task2/test/acc 0.49283333333333335 12
task2/train/lr 0.02525 12
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 13/256 | train_loss 1.1653 | train_acc 0.5400 | test_loss 1.1791 | test_acc 0.4928 | lr 0.0253
last_idx 11
final_idx 0
task2/train/loss 1.2613563189903896 13
task2/test/loss 1.1347475439092538 13
task2/test/acc 0.5338333333333334 13
task2/train/lr 0.01577858504896403 13
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 14/256 | train_loss 1.2614 | train_acc 0.4880 | test_loss 1.1347 | test_acc 0.5338 | lr 0.0158
last_idx 11
final_idx 0
task2/train/loss 1.218309258421262 14
task2/test/loss 1.1333039677056083 14
task2/test/acc 0.5326666666666666 14
task2/train/lr 0.00774910716563295 14
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 15/256 | train_loss 1.2183 | train_acc 0.5280 | test_loss 1.1333 | test_acc 0.5327 | lr 0.0077
last_idx 11
final_idx 0
task2/train/loss 1.155310293038686 15
task2/test/loss 1.1202718371892497 15
task2/test/acc 0.539 15
task2/train/lr 0.0023839815703456534 15
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 16/256 | train_loss 1.1553 | train_acc 0.5300 | test_loss 1.1203 | test_acc 0.5390 | lr 0.0024
last_idx 11
final_idx 0
task2/train/loss 1.2440485159556072 16
task2/test/loss 1.278661564318803 16
task2/test/acc 0.49616666666666664 16
task2/train/lr 0.05 16
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 17/256 | train_loss 1.2440 | train_acc 0.5180 | test_loss 1.2787 | test_acc 0.4962 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 1.1812402407328289 17
task2/test/loss 1.5542987136945237 17
task2/test/acc 0.4126666666666667 17
task2/train/lr 0.049524435689979954 17
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 18/256 | train_loss 1.1812 | train_acc 0.5720 | test_loss 1.5543 | test_acc 0.4127 | lr 0.0495
last_idx 11
final_idx 0
task2/train/loss 1.2762700219949086 18
task2/test/loss 1.3598234190122924 18
task2/test/acc 0.4665 18
task2/train/lr 0.04811601842965435 18
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 19/256 | train_loss 1.2763 | train_acc 0.5020 | test_loss 1.3598 | test_acc 0.4665 | lr 0.0481
last_idx 11
final_idx 0
task2/train/loss 1.2020713835954666 19
task2/test/loss 1.426446259021759 19
task2/test/acc 0.4615 19
task2/train/lr 0.045828872904488 19
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 20/256 | train_loss 1.2021 | train_acc 0.5540 | test_loss 1.4264 | test_acc 0.4615 | lr 0.0458
last_idx 11
final_idx 0
task2/train/loss 1.2408723086118698 20
task2/test/loss 1.2020654473861638 20
task2/test/acc 0.518 20
task2/train/lr 0.04275089283436705 20
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 21/256 | train_loss 1.2409 | train_acc 0.5420 | test_loss 1.2021 | test_acc 0.5180 | lr 0.0428
last_idx 11
final_idx 0
task2/train/loss 1.5472223659356434 21
task2/test/loss 1.3836071456435823 21
task2/test/acc 0.45216666666666666 21
task2/train/lr 0.039000363267235154 21
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 22/256 | train_loss 1.5472 | train_acc 0.3660 | test_loss 1.3836 | test_acc 0.4522 | lr 0.0390
last_idx 11
final_idx 0
task2/train/loss 1.205699195464452 22
task2/test/loss 1.1174616322030115 22
task2/test/acc 0.5536666666666666 22
task2/train/lr 0.03472141495103598 22
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 23/256 | train_loss 1.2057 | train_acc 0.5280 | test_loss 1.1175 | test_acc 0.5537 | lr 0.0347
last_idx 11
final_idx 0
task2/train/loss 1.2050069520870845 23
task2/test/loss 1.190625183773737 23
task2/test/acc 0.5356666666666666 23
task2/train/lr 0.03007848546989918 23
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 24/256 | train_loss 1.2050 | train_acc 0.5480 | test_loss 1.1906 | test_acc 0.5357 | lr 0.0301
last_idx 11
final_idx 0
task2/train/loss 1.0161740680535634 24
task2/test/loss 1.1056684721125303 24
task2/test/acc 0.571 24
task2/train/lr 0.02525 24
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 25/256 | train_loss 1.0162 | train_acc 0.6180 | test_loss 1.1057 | test_acc 0.5710 | lr 0.0253
last_idx 11
final_idx 0
task2/train/loss 1.0611641158660252 25
task2/test/loss 1.0848108884626932 25
task2/test/acc 0.5715 25
task2/train/lr 0.02042151453010083 25
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 26/256 | train_loss 1.0612 | train_acc 0.5760 | test_loss 1.0848 | test_acc 0.5715 | lr 0.0204
last_idx 11
final_idx 0
task2/train/loss 1.0086880375941594 26
task2/test/loss 1.0428253460104449 26
task2/test/acc 0.5875 26
task2/train/lr 0.01577858504896403 26
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 27/256 | train_loss 1.0087 | train_acc 0.6260 | test_loss 1.0428 | test_acc 0.5875 | lr 0.0158
last_idx 11
final_idx 0
task2/train/loss 1.1103897988796234 27
task2/test/loss 1.0754128668430079 27
task2/test/acc 0.5831666666666667 27
task2/train/lr 0.011499636732764853 27
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 28/256 | train_loss 1.1104 | train_acc 0.6340 | test_loss 1.0754 | test_acc 0.5832 | lr 0.0115
last_idx 11
final_idx 0
task2/train/loss 1.1917291978995006 28
task2/test/loss 1.2260941219155805 28
task2/test/acc 0.5586666666666666 28
task2/train/lr 0.00774910716563295 28
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 29/256 | train_loss 1.1917 | train_acc 0.5520 | test_loss 1.2261 | test_acc 0.5587 | lr 0.0077
last_idx 11
final_idx 0
task2/train/loss 1.1844338377316792 29
task2/test/loss 1.0886602634496063 29
task2/test/acc 0.5921666666666666 29
task2/train/lr 0.004671127095512003 29
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 30/256 | train_loss 1.1844 | train_acc 0.5420 | test_loss 1.0887 | test_acc 0.5922 | lr 0.0047
last_idx 11
final_idx 0
task2/train/loss 0.9635825405518214 30
task2/test/loss 1.0339263716753382 30
task2/test/acc 0.6023333333333334 30
task2/train/lr 0.0023839815703456534 30
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 31/256 | train_loss 0.9636 | train_acc 0.6620 | test_loss 1.0339 | test_acc 0.6023 | lr 0.0024
last_idx 11
final_idx 0
task2/train/loss 1.0719272643327713 31
task2/test/loss 1.0350184277461394 31
task2/test/acc 0.6018333333333333 31
task2/train/lr 0.0009755643100200469 31
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 32/256 | train_loss 1.0719 | train_acc 0.6260 | test_loss 1.0350 | test_acc 0.6018 | lr 0.0010
last_idx 11
final_idx 0
task2/train/loss 1.1699670900901158 32
task2/test/loss 1.2392034787331185 32
task2/test/acc 0.5408333333333334 32
task2/train/lr 0.05 32
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 33/256 | train_loss 1.1700 | train_acc 0.5300 | test_loss 1.2392 | test_acc 0.5408 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 1.3244150926669438 33
task2/test/loss 1.2805338567190796 33
task2/test/acc 0.5013333333333333 33
task2/train/lr 0.049880821985136874 33
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 34/256 | train_loss 1.3244 | train_acc 0.5080 | test_loss 1.2805 | test_acc 0.5013 | lr 0.0499
last_idx 11
final_idx 0
task2/train/loss 1.0666477332512538 34
task2/test/loss 1.3237327459084727 34
task2/test/acc 0.5038333333333334 34
task2/train/lr 0.049524435689979954 34
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 35/256 | train_loss 1.0666 | train_acc 0.6200 | test_loss 1.3237 | test_acc 0.5038 | lr 0.0495
last_idx 11
final_idx 0
task2/train/loss 1.0617456883192062 35
task2/test/loss 1.1568505065719576 35
task2/test/acc 0.569 35
task2/train/lr 0.048934273309372174 35
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 36/256 | train_loss 1.0617 | train_acc 0.6200 | test_loss 1.1569 | test_acc 0.5690 | lr 0.0489
last_idx 11
final_idx 0
task2/train/loss 1.000078722834587 36
task2/test/loss 1.3661703573961328 36
task2/test/acc 0.5151666666666667 36
task2/train/lr 0.04811601842965435 36
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 37/256 | train_loss 1.0001 | train_acc 0.6160 | test_loss 1.3662 | test_acc 0.5152 | lr 0.0481
last_idx 11
final_idx 0
task2/train/loss 1.177413448691368 37
task2/test/loss 1.1361052811580852 37
task2/test/acc 0.5655 37
task2/train/lr 0.04707755129262179 37
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 38/256 | train_loss 1.1774 | train_acc 0.5740 | test_loss 1.1361 | test_acc 0.5655 | lr 0.0471
last_idx 11
final_idx 0
task2/train/loss 1.168544441461563 38
task2/test/loss 1.1405316693504362 38
task2/test/acc 0.5643333333333334 38
task2/train/lr 0.045828872904488 38
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 39/256 | train_loss 1.1685 | train_acc 0.5640 | test_loss 1.1405 | test_acc 0.5643 | lr 0.0458
last_idx 11
final_idx 0
task2/train/loss 1.1247414747873943 39
task2/test/loss 1.1141392653875977 39
task2/test/acc 0.5738333333333333 39
task2/train/lr 0.04438200872072774 39
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 40/256 | train_loss 1.1247 | train_acc 0.5480 | test_loss 1.1141 | test_acc 0.5738 | lr 0.0444
last_idx 11
final_idx 0
task2/train/loss 0.9896177798509598 40
task2/test/loss 1.0225907916570232 40
task2/test/acc 0.6166666666666667 40
task2/train/lr 0.04275089283436705 40
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 41/256 | train_loss 0.9896 | train_acc 0.6340 | test_loss 1.0226 | test_acc 0.6167 | lr 0.0428
last_idx 11
final_idx 0
task2/train/loss 1.073924054702123 41
task2/test/loss 1.0222750695517464 41
task2/test/acc 0.6125 41
task2/train/lr 0.040951233783050225 41
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 42/256 | train_loss 1.0739 | train_acc 0.6360 | test_loss 1.0223 | test_acc 0.6125 | lr 0.0410
last_idx 11
final_idx 0
task2/train/loss 1.2020361920197804 42
task2/test/loss 1.047593914461832 42
task2/test/acc 0.5941666666666666 42
task2/train/lr 0.039000363267235154 42
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 43/256 | train_loss 1.2020 | train_acc 0.5240 | test_loss 1.0476 | test_acc 0.5942 | lr 0.0390
last_idx 11
final_idx 0
task2/train/loss 0.9698343773682913 43
task2/test/loss 1.0562380611026374 43
task2/test/acc 0.606 43
task2/train/lr 0.03691706923644345 43
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 44/256 | train_loss 0.9698 | train_acc 0.6600 | test_loss 1.0562 | test_acc 0.6060 | lr 0.0369
last_idx 11
final_idx 0
task2/train/loss 0.8447072903315226 44
task2/test/loss 0.9914898182788905 44
task2/test/acc 0.6471666666666667 44
task2/train/lr 0.03472141495103598 44
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 45/256 | train_loss 0.8447 | train_acc 0.6880 | test_loss 0.9915 | test_acc 0.6472 | lr 0.0347
last_idx 11
final_idx 0
task2/train/loss 1.1219240178664525 45
task2/test/loss 1.1132746182218956 45
task2/test/acc 0.5996666666666667 45
task2/train/lr 0.03243454576204794 45
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 46/256 | train_loss 1.1219 | train_acc 0.5620 | test_loss 1.1133 | test_acc 0.5997 | lr 0.0324
last_idx 11
final_idx 0
task2/train/loss 1.0334368795156479 46
task2/test/loss 1.3286531054190476 46
task2/test/acc 0.5525 46
task2/train/lr 0.03007848546989918 46
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 47/256 | train_loss 1.0334 | train_acc 0.6380 | test_loss 1.3287 | test_acc 0.5525 | lr 0.0301
last_idx 11
final_idx 0
task2/train/loss 0.9178494016329447 47
task2/test/loss 1.294628808587572 47
task2/test/acc 0.5648333333333333 47
task2/train/lr 0.027675924223156633 47
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 48/256 | train_loss 0.9178 | train_acc 0.6880 | test_loss 1.2946 | test_acc 0.5648 | lr 0.0277
last_idx 11
final_idx 0
task2/train/loss 1.0679991741975148 48
task2/test/loss 1.1213521850805213 48
task2/test/acc 0.5966666666666667 48
task2/train/lr 0.02525 48
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 49/256 | train_loss 1.0680 | train_acc 0.5940 | test_loss 1.1214 | test_acc 0.5967 | lr 0.0253
last_idx 11
final_idx 0
task2/train/loss 1.1089221636454265 49
task2/test/loss 0.9548258194088066 49
task2/test/acc 0.631 49
task2/train/lr 0.022824075776843374 49
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 50/256 | train_loss 1.1089 | train_acc 0.6280 | test_loss 0.9548 | test_acc 0.6310 | lr 0.0228
last_idx 11
final_idx 0
task2/train/loss 1.1585904111464818 50
task2/test/loss 0.9929449971574936 50
task2/test/acc 0.6231666666666666 50
task2/train/lr 0.02042151453010083 50
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 51/256 | train_loss 1.1586 | train_acc 0.5760 | test_loss 0.9929 | test_acc 0.6232 | lr 0.0204
last_idx 11
final_idx 0
task2/train/loss 0.9740155786275864 51
task2/test/loss 0.9796922452693438 51
task2/test/acc 0.6395 51
task2/train/lr 0.018065454237952062 51
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 52/256 | train_loss 0.9740 | train_acc 0.6840 | test_loss 0.9797 | test_acc 0.6395 | lr 0.0181
last_idx 11
final_idx 0
task2/train/loss 1.0048042088747025 52
task2/test/loss 0.9567592346320187 52
task2/test/acc 0.649 52
task2/train/lr 0.01577858504896403 52
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 53/256 | train_loss 1.0048 | train_acc 0.6940 | test_loss 0.9568 | test_acc 0.6490 | lr 0.0158
last_idx 11
final_idx 0
task2/train/loss 0.9724091961979866 53
task2/test/loss 1.0953263809537366 53
task2/test/acc 0.6066666666666667 53
task2/train/lr 0.013582930763556558 53
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 54/256 | train_loss 0.9724 | train_acc 0.6660 | test_loss 1.0953 | test_acc 0.6067 | lr 0.0136
last_idx 11
final_idx 0
task2/train/loss 0.8551256383458773 54
task2/test/loss 0.9835773412763638 54
task2/test/acc 0.642 54
task2/train/lr 0.011499636732764853 54
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 55/256 | train_loss 0.8551 | train_acc 0.7220 | test_loss 0.9836 | test_acc 0.6420 | lr 0.0115
last_idx 11
final_idx 0
task2/train/loss 0.8426047116518021 55
task2/test/loss 1.1136229318858932 55
task2/test/acc 0.6141666666666666 55
task2/train/lr 0.009548766216949778 55
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 56/256 | train_loss 0.8426 | train_acc 0.7220 | test_loss 1.1136 | test_acc 0.6142 | lr 0.0095
last_idx 11
final_idx 0
task2/train/loss 0.8424581711490949 56
task2/test/loss 0.9902843049842945 56
task2/test/acc 0.6458333333333334 56
task2/train/lr 0.00774910716563295 56
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 57/256 | train_loss 0.8425 | train_acc 0.6800 | test_loss 0.9903 | test_acc 0.6458 | lr 0.0077
last_idx 11
final_idx 0
task2/train/loss 0.7229856277505556 57
task2/test/loss 0.929636777317437 57
task2/test/acc 0.6643333333333333 57
task2/train/lr 0.0061179912792722595 57
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 58/256 | train_loss 0.7230 | train_acc 0.7620 | test_loss 0.9296 | test_acc 0.6643 | lr 0.0061
last_idx 11
final_idx 0
task2/train/loss 0.8853580032785734 58
task2/test/loss 0.9583030493155013 58
task2/test/acc 0.6613333333333333 58
task2/train/lr 0.004671127095512003 58
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 59/256 | train_loss 0.8854 | train_acc 0.6700 | test_loss 0.9583 | test_acc 0.6613 | lr 0.0047
last_idx 11
final_idx 0
task2/train/loss 0.9134979099035263 59
task2/test/loss 0.9454998313075434 59
task2/test/acc 0.6603333333333333 59
task2/train/lr 0.0034224487073782153 59
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 60/256 | train_loss 0.9135 | train_acc 0.6320 | test_loss 0.9455 | test_acc 0.6603 | lr 0.0034
last_idx 11
final_idx 0
task2/train/loss 1.0477351993322372 60
task2/test/loss 0.9153904394946829 60
task2/test/acc 0.6653333333333333 60
task2/train/lr 0.0023839815703456534 60
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 61/256 | train_loss 1.0477 | train_acc 0.6060 | test_loss 0.9154 | test_acc 0.6653 | lr 0.0024
last_idx 11
final_idx 0
task2/train/loss 0.8713120023409525 61
task2/test/loss 0.9079775412152283 61
task2/test/acc 0.669 61
task2/train/lr 0.0015657266906278318 61
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 62/256 | train_loss 0.8713 | train_acc 0.6820 | test_loss 0.9080 | test_acc 0.6690 | lr 0.0016
last_idx 11
final_idx 0
task2/train/loss 0.9383720308542252 62
task2/test/loss 0.9107570167440567 62
task2/test/acc 0.6695 62
task2/train/lr 0.0009755643100200469 62
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 63/256 | train_loss 0.9384 | train_acc 0.6440 | test_loss 0.9108 | test_acc 0.6695 | lr 0.0010
last_idx 11
final_idx 0
task2/train/loss 0.8196396132310232 63
task2/test/loss 0.9123516463450272 63
task2/test/acc 0.6711666666666667 63
task2/train/lr 0.0006191780148631288 63
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 64/256 | train_loss 0.8196 | train_acc 0.7460 | test_loss 0.9124 | test_acc 0.6712 | lr 0.0006
last_idx 11
final_idx 0
task2/train/loss 0.7331305369734764 64
task2/test/loss 1.4560099228890273 64
task2/test/acc 0.5581666666666667 64
task2/train/lr 0.05 64
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 65/256 | train_loss 0.7331 | train_acc 0.7400 | test_loss 1.4560 | test_acc 0.5582 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 0.8825144519408544 65
task2/test/loss 1.4294852221751735 65
task2/test/acc 0.5308333333333334 65
task2/train/lr 0.04997018754107802 65
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 66/256 | train_loss 0.8825 | train_acc 0.6520 | test_loss 1.4295 | test_acc 0.5308 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 1.0748294343551 66
task2/test/loss 1.2712171464940927 66
task2/test/acc 0.565 66
task2/train/lr 0.049880821985136874 66
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 67/256 | train_loss 1.0748 | train_acc 0.6400 | test_loss 1.2712 | test_acc 0.5650 | lr 0.0499
last_idx 11
final_idx 0
task2/train/loss 0.9830875595410665 67
task2/test/loss 1.0520860541911021 67
task2/test/acc 0.6156666666666667 67
task2/train/lr 0.04973211862162834 67
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 68/256 | train_loss 0.9831 | train_acc 0.6800 | test_loss 1.0521 | test_acc 0.6157 | lr 0.0497
last_idx 11
final_idx 0
task2/train/loss 1.0324448396762211 68
task2/test/loss 1.2505696958651509 68
task2/test/acc 0.5603333333333333 68
task2/train/lr 0.049524435689979954 68
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 69/256 | train_loss 1.0324 | train_acc 0.6400 | test_loss 1.2506 | test_acc 0.5603 | lr 0.0495
last_idx 11
final_idx 0
task2/train/loss 1.2127751211325328 69
task2/test/loss 1.5808704969656728 69
task2/test/acc 0.4726666666666667 69
task2/train/lr 0.04925827351656497 69
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 70/256 | train_loss 1.2128 | train_acc 0.5740 | test_loss 1.5809 | test_acc 0.4727 | lr 0.0493
last_idx 11
final_idx 0
task2/train/loss 1.1779162635405858 70
task2/test/loss 1.0958506406223687 70
task2/test/acc 0.5768333333333333 70
task2/train/lr 0.048934273309372174 70
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 71/256 | train_loss 1.1779 | train_acc 0.5960 | test_loss 1.0959 | test_acc 0.5768 | lr 0.0489
last_idx 11
final_idx 0
task2/train/loss 1.0386593043804169 71
task2/test/loss 1.0836967486534677 71
task2/test/acc 0.6071666666666666 71
task2/train/lr 0.048553215613279764 71
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 72/256 | train_loss 1.0387 | train_acc 0.6440 | test_loss 1.0837 | test_acc 0.6072 | lr 0.0486
last_idx 11
final_idx 0
task2/train/loss 1.0976138363281887 72
task2/test/loss 1.223998316230565 72
task2/test/acc 0.5795 72
task2/train/lr 0.04811601842965435 72
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 73/256 | train_loss 1.0976 | train_acc 0.5900 | test_loss 1.2240 | test_acc 0.5795 | lr 0.0481
last_idx 11
final_idx 0
task2/train/loss 0.9886128852764765 73
task2/test/loss 1.8977233245968819 73
task2/test/acc 0.4266666666666667 73
task2/train/lr 0.047623735004805226 73
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 74/256 | train_loss 0.9886 | train_acc 0.6500 | test_loss 1.8977 | test_acc 0.4267 | lr 0.0476
last_idx 11
final_idx 0
task2/train/loss 1.0887047698100407 74
task2/test/loss 1.093417165053152 74
task2/test/acc 0.5745 74
task2/train/lr 0.04707755129262179 74
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 75/256 | train_loss 1.0887 | train_acc 0.6240 | test_loss 1.0934 | test_acc 0.5745 | lr 0.0471
last_idx 11
final_idx 0
task2/train/loss 0.9126039147377014 75
task2/test/loss 1.1727510301301078 75
task2/test/acc 0.583 75
task2/train/lr 0.046478783097506735 75
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 76/256 | train_loss 0.9126 | train_acc 0.6820 | test_loss 1.1728 | test_acc 0.5830 | lr 0.0465
last_idx 11
final_idx 0
task2/train/loss 0.9993459681669871 76
task2/test/loss 1.5336514113158206 76
task2/test/acc 0.47483333333333333 76
task2/train/lr 0.045828872904488 76
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 77/256 | train_loss 0.9993 | train_acc 0.6600 | test_loss 1.5337 | test_acc 0.4748 | lr 0.0458
last_idx 11
final_idx 0
task2/train/loss 1.1058044383923213 77
task2/test/loss 1.2070408728871032 77
task2/test/acc 0.5705 77
task2/train/lr 0.04512938640414596 77
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 78/256 | train_loss 1.1058 | train_acc 0.5940 | test_loss 1.2070 | test_acc 0.5705 | lr 0.0451
last_idx 11
final_idx 0
task2/train/loss 0.9478040138880411 78
task2/test/loss 0.8941150757953198 78
task2/test/acc 0.668 78
task2/train/lr 0.04438200872072774 78
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 79/256 | train_loss 0.9478 | train_acc 0.6880 | test_loss 0.8941 | test_acc 0.6680 | lr 0.0444
last_idx 11
final_idx 0
task2/train/loss 0.8137213985125223 79
task2/test/loss 1.1984701038059526 79
task2/test/acc 0.5735 79
task2/train/lr 0.043588540352535246 79
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 80/256 | train_loss 0.8137 | train_acc 0.7220 | test_loss 1.1985 | test_acc 0.5735 | lr 0.0436
last_idx 11
final_idx 0
task2/train/loss 0.8994848926862081 80
task2/test/loss 1.1196323302540465 80
task2/test/acc 0.6095 80
task2/train/lr 0.04275089283436705 80
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 81/256 | train_loss 0.8995 | train_acc 0.6880 | test_loss 1.1196 | test_acc 0.6095 | lr 0.0428
last_idx 11
final_idx 0
task2/train/loss 1.0521785418192546 81
task2/test/loss 1.177565431942905 81
task2/test/acc 0.5701666666666667 81
task2/train/lr 0.04187108413246371 81
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 82/256 | train_loss 1.0522 | train_acc 0.6360 | test_loss 1.1776 | test_acc 0.5702 | lr 0.0419
last_idx 11
final_idx 0
task2/train/loss 1.0221953243017197 82
task2/test/loss 1.118927001953125 82
task2/test/acc 0.6008333333333333 82
task2/train/lr 0.040951233783050225 82
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 83/256 | train_loss 1.0222 | train_acc 0.6440 | test_loss 1.1189 | test_acc 0.6008 | lr 0.0410
last_idx 11
final_idx 0
task2/train/loss 1.0654270400603612 83
task2/test/loss 1.1874731191753471 83
task2/test/acc 0.5926666666666667 83
task2/train/lr 0.03999355778618773 83
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 84/256 | train_loss 1.0654 | train_acc 0.6160 | test_loss 1.1875 | test_acc 0.5927 | lr 0.0400
last_idx 11
final_idx 0
task2/train/loss 0.767471432685852 84
task2/test/loss 1.0321559292556595 84
task2/test/acc 0.6356666666666667 84
task2/train/lr 0.039000363267235154 84
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 85/256 | train_loss 0.7675 | train_acc 0.7780 | test_loss 1.0322 | test_acc 0.6357 | lr 0.0390
last_idx 11
final_idx 0
task2/train/loss 0.8840644136071205 85
task2/test/loss 1.119304451411658 85
task2/test/acc 0.6173333333333333 85
task2/train/lr 0.03797404291878224 85
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 86/256 | train_loss 0.8841 | train_acc 0.6660 | test_loss 1.1193 | test_acc 0.6173 | lr 0.0380
last_idx 11
final_idx 0
task2/train/loss 1.0035769542058308 86
task2/test/loss 1.0765011823960464 86
task2/test/acc 0.6106666666666667 86
task2/train/lr 0.03691706923644345 86
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 87/256 | train_loss 1.0036 | train_acc 0.6260 | test_loss 1.0765 | test_acc 0.6107 | lr 0.0369
last_idx 11
final_idx 0
task2/train/loss 1.032392715414365 87
task2/test/loss 1.0765640230944558 87
task2/test/acc 0.6145 87
task2/train/lr 0.03583198856239948 87
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 88/256 | train_loss 1.0324 | train_acc 0.6560 | test_loss 1.0766 | test_acc 0.6145 | lr 0.0358
last_idx 11
final_idx 0
task2/train/loss 0.8504790018002192 88
task2/test/loss 1.2184480168741114 88
task2/test/acc 0.6035 88
task2/train/lr 0.03472141495103598 88
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 89/256 | train_loss 0.8505 | train_acc 0.6960 | test_loss 1.2184 | test_acc 0.6035 | lr 0.0347
last_idx 11
final_idx 0
task2/train/loss 0.8919923404852549 89
task2/test/loss 0.9819119757544386 89
task2/test/acc 0.657 89
task2/train/lr 0.03358802387145745 89
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 90/256 | train_loss 0.8920 | train_acc 0.7560 | test_loss 0.9819 | test_acc 0.6570 | lr 0.0336
last_idx 11
final_idx 0
task2/train/loss 0.8636335929234823 90
task2/test/loss 1.1634065639363589 90
task2/test/acc 0.6128333333333333 90
task2/train/lr 0.03243454576204794 90
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 91/256 | train_loss 0.8636 | train_acc 0.7100 | test_loss 1.1634 | test_acc 0.6128 | lr 0.0324
last_idx 11
final_idx 0
task2/train/loss 0.9908631419142088 91
task2/test/loss 1.140647506409318 91
task2/test/acc 0.6015 91
task2/train/lr 0.03126375945260579 91
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 92/256 | train_loss 0.9909 | train_acc 0.6540 | test_loss 1.1406 | test_acc 0.6015 | lr 0.0313
last_idx 11
final_idx 0
task2/train/loss 0.8814496273795763 92
task2/test/loss 1.1132184499806732 92
task2/test/acc 0.6145 92
task2/train/lr 0.03007848546989918 92
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 93/256 | train_loss 0.8814 | train_acc 0.7640 | test_loss 1.1132 | test_acc 0.6145 | lr 0.0301
last_idx 11
final_idx 0
task2/train/loss 1.049645113448302 93
task2/test/loss 0.9731711366750898 93
task2/test/acc 0.6456666666666667 93
task2/train/lr 0.028881579242770204 93
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 94/256 | train_loss 1.0496 | train_acc 0.7000 | test_loss 0.9732 | test_acc 0.6457 | lr 0.0289
last_idx 11
final_idx 0
task2/train/loss 0.8918873444199562 94
task2/test/loss 1.1076200325993726 94
task2/test/acc 0.6101666666666666 94
task2/train/lr 0.027675924223156633 94
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 95/256 | train_loss 0.8919 | train_acc 0.7180 | test_loss 1.1076 | test_acc 0.6102 | lr 0.0277
last_idx 11
final_idx 0
task2/train/loss 0.7599692444006602 95
task2/test/loss 1.0070744219922672 95
task2/test/acc 0.6468333333333334 95
task2/train/lr 0.0264644249396036 95
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 96/256 | train_loss 0.7600 | train_acc 0.7560 | test_loss 1.0071 | test_acc 0.6468 | lr 0.0265
last_idx 11
final_idx 0
task2/train/loss 0.7578966394066811 96
task2/test/loss 0.8762319127138514 96
task2/test/acc 0.691 96
task2/train/lr 0.02525 96
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 97/256 | train_loss 0.7579 | train_acc 0.7700 | test_loss 0.8762 | test_acc 0.6910 | lr 0.0253
last_idx 11
final_idx 0
task2/train/loss 0.8119411170482635 97
task2/test/loss 1.0402209633675805 97
task2/test/acc 0.6368333333333334 97
task2/train/lr 0.024035575060396407 97
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 98/256 | train_loss 0.8119 | train_acc 0.7260 | test_loss 1.0402 | test_acc 0.6368 | lr 0.0240
last_idx 11
final_idx 0
task2/train/loss 0.8093760485450426 98
task2/test/loss 1.0064656767096833 98
task2/test/acc 0.655 98
task2/train/lr 0.022824075776843374 98
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 99/256 | train_loss 0.8094 | train_acc 0.7220 | test_loss 1.0065 | test_acc 0.6550 | lr 0.0228
last_idx 11
final_idx 0
task2/train/loss 0.8087489033738772 99
task2/test/loss 1.0974805513872719 99
task2/test/acc 0.6343333333333333 99
task2/train/lr 0.0216184207572298 99
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 100/256 | train_loss 0.8087 | train_acc 0.7120 | test_loss 1.0975 | test_acc 0.6343 | lr 0.0216
last_idx 11
final_idx 0
task2/train/loss 0.756847562889258 100
task2/test/loss 1.0493027026200816 100
task2/test/acc 0.6503333333333333 100
task2/train/lr 0.02042151453010083 100
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 101/256 | train_loss 0.7568 | train_acc 0.7560 | test_loss 1.0493 | test_acc 0.6503 | lr 0.0204
last_idx 11
final_idx 0
task2/train/loss 0.6579190169771513 101
task2/test/loss 1.0938008312326277 101
task2/test/acc 0.643 101
task2/train/lr 0.019236240547394222 101
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 102/256 | train_loss 0.6579 | train_acc 0.8200 | test_loss 1.0938 | test_acc 0.6430 | lr 0.0192
last_idx 11
final_idx 0
task2/train/loss 0.6146354004740715 102
task2/test/loss 1.1171675700775898 102
task2/test/acc 0.635 102
task2/train/lr 0.018065454237952062 102
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 103/256 | train_loss 0.6146 | train_acc 0.7420 | test_loss 1.1172 | test_acc 0.6350 | lr 0.0181
last_idx 11
final_idx 0
task2/train/loss 0.8273690144220988 103
task2/test/loss 0.9213764550041978 103
task2/test/acc 0.6825 103
task2/train/lr 0.016911976128542557 103
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 104/256 | train_loss 0.8274 | train_acc 0.7560 | test_loss 0.9214 | test_acc 0.6825 | lr 0.0169
last_idx 11
final_idx 0
task2/train/loss 0.7878022988637289 104
task2/test/loss 0.9347851360148757 104
task2/test/acc 0.673 104
task2/train/lr 0.01577858504896403 104
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 105/256 | train_loss 0.7878 | train_acc 0.7260 | test_loss 0.9348 | test_acc 0.6730 | lr 0.0158
last_idx 11
final_idx 0
task2/train/loss 0.735000361998876 105
task2/test/loss 0.9040004452214624 105
task2/test/acc 0.691 105
task2/train/lr 0.014668011437600525 105
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 106/256 | train_loss 0.7350 | train_acc 0.7800 | test_loss 0.9040 | test_acc 0.6910 | lr 0.0147
last_idx 11
final_idx 0
task2/train/loss 0.6808978468179703 106
task2/test/loss 0.9219603751697679 106
task2/test/acc 0.6805 106
task2/train/lr 0.013582930763556558 106
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 107/256 | train_loss 0.6809 | train_acc 0.8200 | test_loss 0.9220 | test_acc 0.6805 | lr 0.0136
last_idx 11
final_idx 0
task2/train/loss 0.8100013695657253 107
task2/test/loss 0.9321471594110893 107
task2/test/acc 0.6821666666666667 107
task2/train/lr 0.012525957081217764 107
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 108/256 | train_loss 0.8100 | train_acc 0.7560 | test_loss 0.9321 | test_acc 0.6822 | lr 0.0125
last_idx 11
final_idx 0
task2/train/loss 0.7328764746586481 108
task2/test/loss 0.9992068405569035 108
task2/test/acc 0.6606666666666666 108
task2/train/lr 0.011499636732764853 108
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 109/256 | train_loss 0.7329 | train_acc 0.7220 | test_loss 0.9992 | test_acc 0.6607 | lr 0.0115
last_idx 11
final_idx 0
task2/train/loss 0.965921531120936 109
task2/test/loss 0.928850017977457 109
task2/test/acc 0.6713333333333333 109
task2/train/lr 0.010506442213812275 109
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 110/256 | train_loss 0.9659 | train_acc 0.7100 | test_loss 0.9289 | test_acc 0.6713 | lr 0.0105
last_idx 11
final_idx 0
task2/train/loss 1.0728607028722763 110
task2/test/loss 0.9022235924745128 110
task2/test/acc 0.6806666666666666 110
task2/train/lr 0.009548766216949778 110
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 111/256 | train_loss 1.0729 | train_acc 0.6540 | test_loss 0.9022 | test_acc 0.6807 | lr 0.0095
last_idx 11
final_idx 0
task2/train/loss 0.8637534106771151 111
task2/test/loss 0.9141558589291399 111
task2/test/acc 0.678 111
task2/train/lr 0.008628915867536294 111
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 112/256 | train_loss 0.8638 | train_acc 0.6580 | test_loss 0.9142 | test_acc 0.6780 | lr 0.0086
last_idx 11
final_idx 0
task2/train/loss 0.7528768504659334 112
task2/test/loss 0.8923089782686999 112
task2/test/acc 0.6885 112
task2/train/lr 0.00774910716563295 112
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 113/256 | train_loss 0.7529 | train_acc 0.7880 | test_loss 0.8923 | test_acc 0.6885 | lr 0.0077
last_idx 11
final_idx 0
task2/train/loss 0.8229882332185904 113
task2/test/loss 0.8858758841987944 113
task2/test/acc 0.6978333333333333 113
task2/train/lr 0.006911459647464768 113
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 114/256 | train_loss 0.8230 | train_acc 0.7200 | test_loss 0.8859 | test_acc 0.6978 | lr 0.0069
last_idx 11
final_idx 0
task2/train/loss 0.8682723964254061 114
task2/test/loss 0.8975823758727443 114
task2/test/acc 0.691 114
task2/train/lr 0.0061179912792722595 114
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 115/256 | train_loss 0.8683 | train_acc 0.7060 | test_loss 0.8976 | test_acc 0.6910 | lr 0.0061
last_idx 11
final_idx 0
task2/train/loss 0.6969800665974617 115
task2/test/loss 0.8944731176334576 115
task2/test/acc 0.6903333333333334 115
task2/train/lr 0.005370613595854041 115
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 116/256 | train_loss 0.6970 | train_acc 0.7880 | test_loss 0.8945 | test_acc 0.6903 | lr 0.0054
last_idx 11
final_idx 0
task2/train/loss 0.6034835999210676 116
task2/test/loss 0.8961517777103577 116
task2/test/acc 0.6903333333333334 116
task2/train/lr 0.004671127095512003 116
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 117/256 | train_loss 0.6035 | train_acc 0.8040 | test_loss 0.8962 | test_acc 0.6903 | lr 0.0047
last_idx 11
final_idx 0
task2/train/loss 0.7447682321071625 117
task2/test/loss 0.8977243552242753 117
task2/test/acc 0.6933333333333334 117
task2/train/lr 0.004021216902493268 117
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 118/256 | train_loss 0.7448 | train_acc 0.7780 | test_loss 0.8977 | test_acc 0.6933 | lr 0.0040
last_idx 11
final_idx 0
task2/train/loss 0.853973581145207 118
task2/test/loss 0.8712654084402279 118
task2/test/acc 0.6953333333333334 118
task2/train/lr 0.0034224487073782153 118
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 119/256 | train_loss 0.8540 | train_acc 0.7660 | test_loss 0.8713 | test_acc 0.6953 | lr 0.0034
last_idx 11
final_idx 0
task2/train/loss 0.7138342882196108 119
task2/test/loss 0.8850936121749182 119
task2/test/acc 0.6953333333333334 119
task2/train/lr 0.0028762649951947776 119
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 120/256 | train_loss 0.7138 | train_acc 0.7780 | test_loss 0.8851 | test_acc 0.6953 | lr 0.0029
last_idx 11
final_idx 0
task2/train/loss 0.545545065154632 120
task2/test/loss 0.8896166830167284 120
task2/test/acc 0.6955 120
task2/train/lr 0.0023839815703456534 120
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 121/256 | train_loss 0.5455 | train_acc 0.8260 | test_loss 0.8896 | test_acc 0.6955 | lr 0.0024
last_idx 11
final_idx 0
task2/train/loss 0.8789395242929459 121
task2/test/loss 0.8814726576317836 121
task2/test/acc 0.6945 121
task2/train/lr 0.0019467843867202379 121
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 122/256 | train_loss 0.8789 | train_acc 0.7180 | test_loss 0.8815 | test_acc 0.6945 | lr 0.0019
last_idx 11
final_idx 0
task2/train/loss 0.7389563135802746 122
task2/test/loss 0.8761311643315057 122
task2/test/acc 0.6948333333333333 122
task2/train/lr 0.0015657266906278318 122
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 123/256 | train_loss 0.7390 | train_acc 0.7380 | test_loss 0.8761 | test_acc 0.6948 | lr 0.0016
last_idx 11
final_idx 0
task2/train/loss 0.9159217663109303 123
task2/test/loss 0.8816091022352233 123
task2/test/acc 0.695 123
task2/train/lr 0.0012417264834350366 123
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 124/256 | train_loss 0.9159 | train_acc 0.7140 | test_loss 0.8816 | test_acc 0.6950 | lr 0.0012
last_idx 11
final_idx 0
task2/train/loss 0.5865730481843153 124
task2/test/loss 0.8710210749702733 124
task2/test/acc 0.697 124
task2/train/lr 0.0009755643100200469 124
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 125/256 | train_loss 0.5866 | train_acc 0.8160 | test_loss 0.8710 | test_acc 0.6970 | lr 0.0010
last_idx 11
final_idx 0
task2/train/loss 0.5350244132181009 125
task2/test/loss 0.8720632663173397 125
task2/test/acc 0.6978333333333333 125
task2/train/lr 0.0007678813783716699 125
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 126/256 | train_loss 0.5350 | train_acc 0.8100 | test_loss 0.8721 | test_acc 0.6978 | lr 0.0008
last_idx 11
final_idx 0
task2/train/loss 0.6521545300881068 126
task2/test/loss 0.8780643298243084 126
task2/test/acc 0.6948333333333333 126
task2/train/lr 0.0006191780148631288 126
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 127/256 | train_loss 0.6522 | train_acc 0.7280 | test_loss 0.8781 | test_acc 0.6948 | lr 0.0006
last_idx 11
final_idx 0
task2/train/loss 0.7704089557131132 127
task2/test/loss 0.8840046709906446 127
task2/test/acc 0.6965 127
task2/train/lr 0.0005298124589219829 127
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 128/256 | train_loss 0.7704 | train_acc 0.7460 | test_loss 0.8840 | test_acc 0.6965 | lr 0.0005
last_idx 11
final_idx 0
task2/train/loss 0.760759025812149 128
task2/test/loss 1.5503548575415664 128
task2/test/acc 0.5338333333333334 128
task2/train/lr 0.05 128
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 129/256 | train_loss 0.7608 | train_acc 0.7600 | test_loss 1.5504 | test_acc 0.5338 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 0.9029748017589251 129
task2/test/loss 1.0615280464182806 129
task2/test/acc 0.6261666666666666 129
task2/train/lr 0.04999254576273106 129
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 130/256 | train_loss 0.9030 | train_acc 0.6520 | test_loss 1.0615 | test_acc 0.6262 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 1.1966525043050449 130
task2/test/loss 1.4389301677689934 130
task2/test/acc 0.5078333333333334 130
task2/train/lr 0.04997018754107802 130
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 131/256 | train_loss 1.1967 | train_acc 0.5720 | test_loss 1.4389 | test_acc 0.5078 | lr 0.0500
last_idx 11
final_idx 0
task2/train/loss 1.0500392268101375 131
task2/test/loss 1.2539417149811765 131
task2/test/acc 0.571 131
task2/train/lr 0.04993293880279759 131
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 132/256 | train_loss 1.0500 | train_acc 0.6100 | test_loss 1.2539 | test_acc 0.5710 | lr 0.0499
last_idx 11
final_idx 0
task2/train/loss 0.8367736985286077 132
task2/test/loss 1.1719250396220353 132
task2/test/acc 0.6051666666666666 132
task2/train/lr 0.049880821985136874 132
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 133/256 | train_loss 0.8368 | train_acc 0.7540 | test_loss 1.1719 | test_acc 0.6052 | lr 0.0499
last_idx 11
final_idx 0
task2/train/loss 0.951821987827619 133
task2/test/loss 0.9773151949591881 133
task2/test/acc 0.6406666666666667 133
task2/train/lr 0.04981386848131808 133
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 134/256 | train_loss 0.9518 | train_acc 0.6560 | test_loss 0.9773 | test_acc 0.6407 | lr 0.0498
last_idx 11
final_idx 0
task2/train/loss 0.8105378473798434 134
task2/test/loss 1.4254840861489302 134
task2/test/acc 0.5705 134
task2/train/lr 0.04973211862162834 134
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 135/256 | train_loss 0.8105 | train_acc 0.7420 | test_loss 1.4255 | test_acc 0.5705 | lr 0.0497
last_idx 11
final_idx 0
task2/train/loss 0.7024167478084564 135
task2/test/loss 1.431385472362494 135
task2/test/acc 0.5863333333333334 135
task2/train/lr 0.04963562164912629 135
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 136/256 | train_loss 0.7024 | train_acc 0.7080 | test_loss 1.4314 | test_acc 0.5863 | lr 0.0496
last_idx 11
final_idx 0
task2/train/loss 1.0139512022336323 136
task2/test/loss 1.1944234079688134 136
task2/test/acc 0.5943333333333334 136
task2/train/lr 0.049524435689979954 136
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 137/256 | train_loss 1.0140 | train_acc 0.6480 | test_loss 1.1944 | test_acc 0.5943 | lr 0.0495
last_idx 11
final_idx 0
task2/train/loss 0.9485370541612307 137
task2/test/loss 1.0895233227715004 137
task2/test/acc 0.6133333333333333 137
task2/train/lr 0.04939862771845358 137
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 138/256 | train_loss 0.9485 | train_acc 0.6140 | test_loss 1.0895 | test_acc 0.6133 | lr 0.0494
last_idx 11
final_idx 0
task2/train/loss 0.7383949135740598 138
task2/test/loss 1.2590350343363128 138
task2/test/acc 0.6005 138
task2/train/lr 0.04925827351656497 138
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 139/256 | train_loss 0.7384 | train_acc 0.6980 | test_loss 1.2590 | test_acc 0.6005 | lr 0.0493
last_idx 11
final_idx 0
task2/train/loss 0.9985799317558607 139
task2/test/loss 1.1007676483505833 139
task2/test/acc 0.6276666666666667 139
task2/train/lr 0.04910345762843714 139
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 140/256 | train_loss 0.9986 | train_acc 0.7000 | test_loss 1.1008 | test_acc 0.6277 | lr 0.0491
last_idx 11
final_idx 0
task2/train/loss 0.7418138881524404 140
task2/test/loss 1.3508139128667596 140
task2/test/acc 0.5636666666666666 140
task2/train/lr 0.048934273309372174 140
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 141/256 | train_loss 0.7418 | train_acc 0.7440 | test_loss 1.3508 | test_acc 0.5637 | lr 0.0489
last_idx 11
final_idx 0
task2/train/loss 0.9627326875925064 141
task2/test/loss 1.17756105208919 141
task2/test/acc 0.6163333333333333 141
task2/train/lr 0.04875082246967766 141
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 142/256 | train_loss 0.9627 | train_acc 0.6760 | test_loss 1.1776 | test_acc 0.6163 | lr 0.0488
last_idx 11
final_idx 0
task2/train/loss 0.9802487020691236 142
task2/test/loss 1.0860859150216526 142
task2/test/acc 0.6155 142
task2/train/lr 0.048553215613279764 142
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 143/256 | train_loss 0.9802 | train_acc 0.6340 | test_loss 1.0861 | test_acc 0.6155 | lr 0.0486
last_idx 11
final_idx 0
task2/train/loss 0.8456578354040781 143
task2/test/loss 1.0919584423086068 143
task2/test/acc 0.6376666666666667 143
task2/train/lr 0.04834157177115979 143
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 144/256 | train_loss 0.8457 | train_acc 0.7220 | test_loss 1.0920 | test_acc 0.6377 | lr 0.0483
last_idx 11
final_idx 0
task2/train/loss 0.9451277802387873 144
task2/test/loss 1.145020718557121 144
task2/test/acc 0.6106666666666667 144
task2/train/lr 0.04811601842965435 144
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 145/256 | train_loss 0.9451 | train_acc 0.7040 | test_loss 1.1450 | test_acc 0.6107 | lr 0.0481
last_idx 11
final_idx 0
task2/train/loss 0.7675368810693423 145
task2/test/loss 1.022086243124774 145
task2/test/acc 0.651 145
task2/train/lr 0.047876691453662384 145
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 146/256 | train_loss 0.7675 | train_acc 0.7380 | test_loss 1.0221 | test_acc 0.6510 | lr 0.0479
last_idx 11
final_idx 0
task2/train/loss 0.6806452944874763 146
task2/test/loss 1.204666977925022 146
task2/test/acc 0.6128333333333333 146
task2/train/lr 0.047623735004805226 146
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 147/256 | train_loss 0.6806 | train_acc 0.8060 | test_loss 1.2047 | test_acc 0.6128 | lr 0.0476
last_idx 11
final_idx 0
task2/train/loss 0.9011356458067894 147
task2/test/loss 1.092282946636207 147
task2/test/acc 0.6128333333333333 147
task2/train/lr 0.047357301454589 147
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 148/256 | train_loss 0.9011 | train_acc 0.6980 | test_loss 1.0923 | test_acc 0.6128 | lr 0.0474
last_idx 11
final_idx 0
task2/train/loss 0.868499144911766 148
task2/test/loss 1.0566560088718024 148
task2/test/acc 0.6238333333333334 148
task2/train/lr 0.04707755129262179 148
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 149/256 | train_loss 0.8685 | train_acc 0.7000 | test_loss 1.0567 | test_acc 0.6238 | lr 0.0471
last_idx 11
final_idx 0
task2/train/loss 0.9073365951577822 149
task2/test/loss 0.9431281381279882 149
task2/test/acc 0.6686666666666666 149
task2/train/lr 0.04678465302994061 149
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 150/256 | train_loss 0.9073 | train_acc 0.6680 | test_loss 0.9431 | test_acc 0.6687 | lr 0.0468
last_idx 11
final_idx 0
task2/train/loss 0.9444039116303126 150
task2/test/loss 0.9403050709379851 150
task2/test/acc 0.6696666666666666 150
task2/train/lr 0.046478783097506735 150
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 151/256 | train_loss 0.9444 | train_acc 0.6260 | test_loss 0.9403 | test_acc 0.6697 | lr 0.0465
last_idx 11
final_idx 0
task2/train/loss 0.8410660748680433 151
task2/test/loss 1.0681961384350367 151
task2/test/acc 0.6235 151
task2/train/lr 0.04616012573993025 151
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 152/256 | train_loss 0.8411 | train_acc 0.7000 | test_loss 1.0682 | test_acc 0.6235 | lr 0.0462
last_idx 11
final_idx 0
task2/train/loss 0.8535141348838806 152
task2/test/loss 0.9647741996458847 152
task2/test/acc 0.6593333333333333 152
task2/train/lr 0.045828872904488 152
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 153/256 | train_loss 0.8535 | train_acc 0.7060 | test_loss 0.9648 | test_acc 0.6593 | lr 0.0458
last_idx 11
final_idx 0
task2/train/loss 0.9880871474742889 153
task2/test/loss 1.093533943821914 153
task2/test/acc 0.6136666666666667 153
task2/train/lr 0.0454852241255017 153
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 154/256 | train_loss 0.9881 | train_acc 0.6760 | test_loss 1.0935 | test_acc 0.6137 | lr 0.0455
last_idx 11
final_idx 0
task2/train/loss 0.826454870402813 154
task2/test/loss 1.2006996839585966 154
task2/test/acc 0.603 154
task2/train/lr 0.04512938640414596 154
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 155/256 | train_loss 0.8265 | train_acc 0.7180 | test_loss 1.2007 | test_acc 0.6030 | lr 0.0451
last_idx 11
final_idx 0
task2/train/loss 0.841683586438497 155
task2/test/loss 1.0794374779628142 155
task2/test/acc 0.6376666666666667 155
task2/train/lr 0.04476157408375851 155
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 156/256 | train_loss 0.8417 | train_acc 0.7500 | test_loss 1.0794 | test_acc 0.6377 | lr 0.0448
last_idx 11
final_idx 0
task2/train/loss 1.0776845042904217 156
task2/test/loss 1.0945608942178044 156
task2/test/acc 0.6171666666666666 156
task2/train/lr 0.04438200872072774 156
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 157/256 | train_loss 1.0777 | train_acc 0.6120 | test_loss 1.0946 | test_acc 0.6172 | lr 0.0444
last_idx 11
final_idx 0
task2/train/loss 0.6492387726902962 157
task2/test/loss 0.9604969059463835 157
task2/test/acc 0.6693333333333333 157
task2/train/lr 0.0439909189510355 157
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 158/256 | train_loss 0.6492 | train_acc 0.7920 | test_loss 0.9605 | test_acc 0.6693 | lr 0.0440
last_idx 11
final_idx 0
task2/train/loss 0.8252873991926511 158
task2/test/loss 1.2577028831426245 158
task2/test/acc 0.6061666666666666 158
task2/train/lr 0.043588540352535246 158
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 159/256 | train_loss 0.8253 | train_acc 0.7400 | test_loss 1.2577 | test_acc 0.6062 | lr 0.0436
last_idx 11
final_idx 0
task2/train/loss 0.6414745797713598 159
task2/test/loss 1.1339096783721534 159
task2/test/acc 0.6336666666666667 159
task2/train/lr 0.043175115303048815 159
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 160/256 | train_loss 0.6415 | train_acc 0.7720 | test_loss 1.1339 | test_acc 0.6337 | lr 0.0432
last_idx 11
final_idx 0
task2/train/loss 1.0494613324602444 160
task2/test/loss 1.077785031203806 160
task2/test/acc 0.6266666666666667 160
task2/train/lr 0.04275089283436705 160
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 161/256 | train_loss 1.0495 | train_acc 0.6280 | test_loss 1.0778 | test_acc 0.6267 | lr 0.0428
last_idx 11
final_idx 0
task2/train/loss 0.7126559540629387 161
task2/test/loss 0.9197478628289091 161
task2/test/acc 0.6851666666666667 161
task2/train/lr 0.042316128482242414 161
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 162/256 | train_loss 0.7127 | train_acc 0.7820 | test_loss 0.9197 | test_acc 0.6852 | lr 0.0423
last_idx 11
final_idx 0
task2/train/loss 0.7860950157046318 162
task2/test/loss 1.0031877301470207 162
task2/test/acc 0.6476666666666666 162
task2/train/lr 0.04187108413246371 162
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 163/256 | train_loss 0.7861 | train_acc 0.7040 | test_loss 1.0032 | test_acc 0.6477 | lr 0.0419
last_idx 11
final_idx 0
task2/train/loss 0.8481964195768038 163
task2/test/loss 1.0632873451622733 163
task2/test/acc 0.6381666666666667 163
task2/train/lr 0.04141602786310598 163
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 164/256 | train_loss 0.8482 | train_acc 0.6940 | test_loss 1.0633 | test_acc 0.6382 | lr 0.0414
last_idx 11
final_idx 0
task2/train/loss 0.6907695805033048 164
task2/test/loss 0.9713750830749526 164
task2/test/acc 0.6576666666666666 164
task2/train/lr 0.040951233783050225 164
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 165/256 | train_loss 0.6908 | train_acc 0.7660 | test_loss 0.9714 | test_acc 0.6577 | lr 0.0410
last_idx 11
final_idx 0
task2/train/loss 0.8838643158475558 165
task2/test/loss 1.1579056793864626 165
task2/test/acc 0.609 165
task2/train/lr 0.040476981866870515 165
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 166/256 | train_loss 0.8839 | train_acc 0.6720 | test_loss 1.1579 | test_acc 0.6090 | lr 0.0405
last_idx 11
final_idx 0
task2/train/loss 0.6930974945425987 166
task2/test/loss 1.0372794625097819 166
task2/test/acc 0.6375 166
task2/train/lr 0.03999355778618773 166
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 167/256 | train_loss 0.6931 | train_acc 0.7780 | test_loss 1.0373 | test_acc 0.6375 | lr 0.0400
last_idx 11
final_idx 0
task2/train/loss 0.6247018575668335 167
task2/test/loss 1.0148773566432243 167
task2/test/acc 0.6535 167
task2/train/lr 0.039501252737591676 167
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 168/256 | train_loss 0.6247 | train_acc 0.7980 | test_loss 1.0149 | test_acc 0.6535 | lr 0.0395
last_idx 11
final_idx 0
task2/train/loss 1.2045659460127354 168
task2/test/loss 1.0080375081866326 168
task2/test/acc 0.6323333333333333 168
task2/train/lr 0.039000363267235154 168
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 169/256 | train_loss 1.2046 | train_acc 0.5520 | test_loss 1.0080 | test_acc 0.6323 | lr 0.0390
last_idx 11
final_idx 0
task2/train/loss 0.8015918110807737 169
task2/test/loss 0.8672988086286253 169
task2/test/acc 0.684 169
task2/train/lr 0.03849119109220566 169
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 170/256 | train_loss 0.8016 | train_acc 0.7640 | test_loss 0.8673 | test_acc 0.6840 | lr 0.0385
last_idx 11
final_idx 0
task2/train/loss 0.9147032325466474 170
task2/test/loss 1.028124406498714 170
task2/test/acc 0.6486666666666666 170
task2/train/lr 0.03797404291878224 170
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 171/256 | train_loss 0.9147 | train_acc 0.6660 | test_loss 1.0281 | test_acc 0.6487 | lr 0.0380
last_idx 11
final_idx 0
task2/train/loss 0.7283199081818262 171
task2/test/loss 0.875533832359488 171
task2/test/acc 0.698 171
task2/train/lr 0.03744923025768716 171
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 172/256 | train_loss 0.7283 | train_acc 0.7060 | test_loss 0.8755 | test_acc 0.6980 | lr 0.0374
last_idx 11
final_idx 0
task2/train/loss 0.9237985635797182 172
task2/test/loss 1.0538011630521202 172
task2/test/acc 0.634 172
task2/train/lr 0.03691706923644345 172
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 173/256 | train_loss 0.9238 | train_acc 0.6480 | test_loss 1.0538 | test_acc 0.6340 | lr 0.0369
last_idx 11
final_idx 0
task2/train/loss 0.7986441651980082 173
task2/test/loss 1.2804434272277094 173
task2/test/acc 0.5886666666666667 173
task2/train/lr 0.03637788040895152 173
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 174/256 | train_loss 0.7986 | train_acc 0.7880 | test_loss 1.2804 | test_acc 0.5887 | lr 0.0364
last_idx 11
final_idx 0
task2/train/loss 0.5773933852712313 174
task2/test/loss 1.0931138409315235 174
task2/test/acc 0.6396666666666667 174
task2/train/lr 0.03583198856239948 174
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 175/256 | train_loss 0.5774 | train_acc 0.8460 | test_loss 1.0931 | test_acc 0.6397 | lr 0.0358
last_idx 11
final_idx 0
task2/train/loss 0.7571276053786278 175
task2/test/loss 0.9946338913954087 175
task2/test/acc 0.6741666666666667 175
task2/train/lr 0.0352797225216235 175
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 176/256 | train_loss 0.7571 | train_acc 0.7520 | test_loss 0.9946 | test_acc 0.6742 | lr 0.0353
last_idx 11
final_idx 0
task2/train/loss 0.7561260561148325 176
task2/test/loss 0.9351185738605304 176
task2/test/acc 0.6751666666666667 176
task2/train/lr 0.03472141495103598 176
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 177/256 | train_loss 0.7561 | train_acc 0.7580 | test_loss 0.9351 | test_acc 0.6752 | lr 0.0347
last_idx 11
final_idx 0
task2/train/loss 0.8398261591792107 177
task2/test/loss 0.9261516717663647 177
task2/test/acc 0.6811666666666667 177
task2/train/lr 0.034157402154240964 177
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 178/256 | train_loss 0.8398 | train_acc 0.6640 | test_loss 0.9262 | test_acc 0.6812 | lr 0.0342
last_idx 11
final_idx 0
task2/train/loss 0.6442266081770261 178
task2/test/loss 1.002870568883245 178
task2/test/acc 0.6591666666666667 178
task2/train/lr 0.03358802387145745 178
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 179/256 | train_loss 0.6442 | train_acc 0.8440 | test_loss 1.0029 | test_acc 0.6592 | lr 0.0336
last_idx 11
final_idx 0
task2/train/loss 0.7214374989271164 179
task2/test/loss 0.9358239050981773 179
task2/test/acc 0.6755 179
task2/train/lr 0.03301362307487257 179
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 180/256 | train_loss 0.7214 | train_acc 0.7180 | test_loss 0.9358 | test_acc 0.6755 | lr 0.0330
last_idx 11
final_idx 0
task2/train/loss 0.8870481960475445 180
task2/test/loss 0.877554236972419 180
task2/test/acc 0.6886666666666666 180
task2/train/lr 0.03243454576204794 180
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 181/256 | train_loss 0.8870 | train_acc 0.7080 | test_loss 0.8776 | test_acc 0.6887 | lr 0.0324
last_idx 11
final_idx 0
task2/train/loss 0.6917314355572065 181
task2/test/loss 1.2362782328668303 181
task2/test/acc 0.597 181
task2/train/lr 0.03185114074750374 181
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 182/256 | train_loss 0.6917 | train_acc 0.7620 | test_loss 1.2363 | test_acc 0.5970 | lr 0.0319
last_idx 11
final_idx 0
task2/train/loss 0.9142532646656036 182
task2/test/loss 1.115532861791388 182
task2/test/acc 0.6265 182
task2/train/lr 0.03126375945260579 182
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 183/256 | train_loss 0.9143 | train_acc 0.6920 | test_loss 1.1155 | test_acc 0.6265 | lr 0.0313
last_idx 11
final_idx 0
task2/train/loss 0.6632266143957773 183
task2/test/loss 1.030544311569555 183
task2/test/acc 0.6445 183
task2/train/lr 0.030672755693882527 183
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 184/256 | train_loss 0.6632 | train_acc 0.7880 | test_loss 1.0305 | test_acc 0.6445 | lr 0.0307
last_idx 11
final_idx 0
task2/train/loss 0.8666050160924593 184
task2/test/loss 0.9178008293583445 184
task2/test/acc 0.6876666666666666 184
task2/train/lr 0.03007848546989918 184
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 185/256 | train_loss 0.8666 | train_acc 0.6980 | test_loss 0.9178 | test_acc 0.6877 | lr 0.0301
last_idx 11
final_idx 0
task2/train/loss 0.6480003794034322 185
task2/test/loss 0.909110299644679 185
task2/test/acc 0.6908333333333333 185
task2/train/lr 0.029481306746817457 185
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 186/256 | train_loss 0.6480 | train_acc 0.8320 | test_loss 0.9091 | test_acc 0.6908 | lr 0.0295
last_idx 11
final_idx 0
task2/train/loss 0.7202465683221817 186
task2/test/loss 0.9612258228942425 186
task2/test/acc 0.67 186
task2/train/lr 0.028881579242770204 186
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 187/256 | train_loss 0.7202 | train_acc 0.7880 | test_loss 0.9612 | test_acc 0.6700 | lr 0.0289
last_idx 11
final_idx 0
task2/train/loss 0.6900537324448427 187
task2/test/loss 0.9367741342008549 187
task2/test/acc 0.6816666666666666 187
task2/train/lr 0.028279664211180604 187
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 188/256 | train_loss 0.6901 | train_acc 0.8160 | test_loss 0.9368 | test_acc 0.6817 | lr 0.0283
last_idx 11
final_idx 0
task2/train/loss 0.738001479456822 188
task2/test/loss 0.9229135180476808 188
task2/test/acc 0.6833333333333333 188
task2/train/lr 0.027675924223156633 188
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 189/256 | train_loss 0.7380 | train_acc 0.7840 | test_loss 0.9229 | test_acc 0.6833 | lr 0.0277
last_idx 11
final_idx 0
task2/train/loss 0.8332864890495936 189
task2/test/loss 1.0354118767010905 189
task2/test/acc 0.6423333333333333 189
task2/train/lr 0.027070722949091772 189
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 190/256 | train_loss 0.8333 | train_acc 0.7180 | test_loss 1.0354 | test_acc 0.6423 | lr 0.0271
last_idx 11
final_idx 0
task2/train/loss 0.575784157961607 190
task2/test/loss 0.988768193299753 190
task2/test/acc 0.6696666666666666 190
task2/train/lr 0.0264644249396036 190
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 191/256 | train_loss 0.5758 | train_acc 0.8480 | test_loss 0.9888 | test_acc 0.6697 | lr 0.0265
last_idx 11
final_idx 0
task2/train/loss 0.8402620901664098 191
task2/test/loss 0.9818418328888225 191
task2/test/acc 0.6638333333333334 191
task2/train/lr 0.02585739540594208 191
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 192/256 | train_loss 0.8403 | train_acc 0.7200 | test_loss 0.9818 | test_acc 0.6638 | lr 0.0259
last_idx 11
final_idx 0
task2/train/loss 0.5001349362234274 192
task2/test/loss 1.0250471147307514 192
task2/test/acc 0.6635 192
task2/train/lr 0.02525 192
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 193/256 | train_loss 0.5001 | train_acc 0.8400 | test_loss 1.0250 | test_acc 0.6635 | lr 0.0253
last_idx 11
final_idx 0
task2/train/loss 0.8078971467912197 193
task2/test/loss 0.9220182815172376 193
task2/test/acc 0.6808333333333333 193
task2/train/lr 0.024642604594057926 193
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 194/256 | train_loss 0.8079 | train_acc 0.7280 | test_loss 0.9220 | test_acc 0.6808 | lr 0.0246
last_idx 11
final_idx 0
task2/train/loss 0.784855047861735 194
task2/test/loss 0.92875792309098 194
task2/test/acc 0.6781666666666667 194
task2/train/lr 0.024035575060396407 194
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 195/256 | train_loss 0.7849 | train_acc 0.7480 | test_loss 0.9288 | test_acc 0.6782 | lr 0.0240
last_idx 11
final_idx 0
task2/train/loss 0.7184673100709915 195
task2/test/loss 0.9915094366908943 195
task2/test/acc 0.6733333333333333 195
task2/train/lr 0.023429277050908234 195
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 196/256 | train_loss 0.7185 | train_acc 0.7940 | test_loss 0.9915 | test_acc 0.6733 | lr 0.0234
last_idx 11
final_idx 0
task2/train/loss 0.6231352339188257 196
task2/test/loss 0.8962951589674846 196
task2/test/acc 0.6888333333333333 196
task2/train/lr 0.022824075776843374 196
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 197/256 | train_loss 0.6231 | train_acc 0.7640 | test_loss 0.8963 | test_acc 0.6888 | lr 0.0228
last_idx 11
final_idx 0
task2/train/loss 0.6115375024576982 197
task2/test/loss 1.026869234159915 197
task2/test/acc 0.649 197
task2/train/lr 0.022220335788819403 197
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 198/256 | train_loss 0.6115 | train_acc 0.8180 | test_loss 1.0269 | test_acc 0.6490 | lr 0.0222
last_idx 11
final_idx 0
task2/train/loss 0.7948933566610018 198
task2/test/loss 0.9425197241515139 198
task2/test/acc 0.6735 198
task2/train/lr 0.0216184207572298 198
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 199/256 | train_loss 0.7949 | train_acc 0.7540 | test_loss 0.9425 | test_acc 0.6735 | lr 0.0216
last_idx 11
final_idx 0
task2/train/loss 0.4400490050514539 199
task2/test/loss 0.9320695789626045 199
task2/test/acc 0.6863333333333334 199
task2/train/lr 0.021018693253182546 199
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 200/256 | train_loss 0.4400 | train_acc 0.8520 | test_loss 0.9321 | test_acc 0.6863 | lr 0.0210
last_idx 11
final_idx 0
task2/train/loss 0.692512609064579 200
task2/test/loss 1.0344333263644336 200
task2/test/acc 0.647 200
task2/train/lr 0.02042151453010083 200
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 201/256 | train_loss 0.6925 | train_acc 0.7460 | test_loss 1.0344 | test_acc 0.6470 | lr 0.0204
last_idx 11
final_idx 0
task2/train/loss 0.7774381302297115 201
task2/test/loss 1.0238531845329453 201
task2/test/acc 0.6515 201
task2/train/lr 0.019827244306117476 201
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 202/256 | train_loss 0.7774 | train_acc 0.7400 | test_loss 1.0239 | test_acc 0.6515 | lr 0.0198
last_idx 11
final_idx 0
task2/train/loss 0.8072749810914198 202
task2/test/loss 1.0170501177980953 202
task2/test/acc 0.6646666666666666 202
task2/train/lr 0.019236240547394222 202
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 203/256 | train_loss 0.8073 | train_acc 0.7300 | test_loss 1.0171 | test_acc 0.6647 | lr 0.0192
last_idx 11
final_idx 0
task2/train/loss 0.8461092760165533 203
task2/test/loss 0.8781439751169108 203
task2/test/acc 0.6955 203
task2/train/lr 0.018648859252496267 203
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 204/256 | train_loss 0.8461 | train_acc 0.7000 | test_loss 0.8781 | test_acc 0.6955 | lr 0.0186
last_idx 11
final_idx 0
task2/train/loss 0.6444961428642273 204
task2/test/loss 0.8722911966543128 204
task2/test/acc 0.7035 204
task2/train/lr 0.018065454237952062 204
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 205/256 | train_loss 0.6445 | train_acc 0.8080 | test_loss 0.8723 | test_acc 0.7035 | lr 0.0181
last_idx 11
final_idx 0
task2/train/loss 0.5484107273320357 205
task2/test/loss 0.9207658463150915 205
task2/test/acc 0.691 205
task2/train/lr 0.017486376925127438 205
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 206/256 | train_loss 0.5484 | train_acc 0.7980 | test_loss 0.9208 | test_acc 0.6910 | lr 0.0175
last_idx 11
final_idx 0
task2/train/loss 0.6225944248338541 206
task2/test/loss 0.879822747985812 206
task2/test/acc 0.707 206
task2/train/lr 0.016911976128542557 206
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 207/256 | train_loss 0.6226 | train_acc 0.7800 | test_loss 0.8798 | test_acc 0.7070 | lr 0.0169
last_idx 11
final_idx 0
task2/train/loss 0.8589857965707779 207
task2/test/loss 0.9274155930446012 207
task2/test/acc 0.6788333333333333 207
task2/train/lr 0.016342597845759043 207
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 208/256 | train_loss 0.8590 | train_acc 0.7220 | test_loss 0.9274 | test_acc 0.6788 | lr 0.0163
last_idx 11
final_idx 0
task2/train/loss 0.78154631331563 208
task2/test/loss 0.8839539100218864 208
task2/test/acc 0.6941666666666667 208
task2/train/lr 0.01577858504896403 208
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 209/256 | train_loss 0.7815 | train_acc 0.7160 | test_loss 0.8840 | test_acc 0.6942 | lr 0.0158
last_idx 11
final_idx 0
task2/train/loss 0.6026002864042918 209
task2/test/loss 0.968233739535739 209
task2/test/acc 0.6668333333333333 209
task2/train/lr 0.015220277478376504 209
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 210/256 | train_loss 0.6026 | train_acc 0.7380 | test_loss 0.9682 | test_acc 0.6668 | lr 0.0152
last_idx 11
final_idx 0
task2/train/loss 0.5296269605557123 210
task2/test/loss 0.9057187494570321 210
task2/test/acc 0.6826666666666666 210
task2/train/lr 0.014668011437600525 210
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 211/256 | train_loss 0.5296 | train_acc 0.8300 | test_loss 0.9057 | test_acc 0.6827 | lr 0.0147
last_idx 11
final_idx 0
task2/train/loss 0.6740745169421037 211
task2/test/loss 0.9580299430084924 211
task2/test/acc 0.6708333333333333 211
task2/train/lr 0.014122119591048485 211
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 212/256 | train_loss 0.6741 | train_acc 0.7000 | test_loss 0.9580 | test_acc 0.6708 | lr 0.0141
last_idx 11
final_idx 0
task2/train/loss 0.5796049286921819 212
task2/test/loss 0.998295705140072 212
task2/test/acc 0.6553333333333333 212
task2/train/lr 0.013582930763556558 212
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 213/256 | train_loss 0.5796 | train_acc 0.7940 | test_loss 0.9983 | test_acc 0.6553 | lr 0.0136
last_idx 11
final_idx 0
task2/train/loss 0.45318084085981053 213
task2/test/loss 0.9303508415056841 213
task2/test/acc 0.6876666666666666 213
task2/train/lr 0.013050769742312849 213
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 214/256 | train_loss 0.4532 | train_acc 0.8920 | test_loss 0.9304 | test_acc 0.6877 | lr 0.0131
last_idx 11
final_idx 0
task2/train/loss 0.7492766032616297 214
task2/test/loss 0.8686645004871117 214
task2/test/acc 0.7055 214
task2/train/lr 0.012525957081217764 214
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 215/256 | train_loss 0.7493 | train_acc 0.7800 | test_loss 0.8687 | test_acc 0.7055 | lr 0.0125
last_idx 11
final_idx 0
task2/train/loss 0.8921559800704321 215
task2/test/loss 0.8504647505979468 215
task2/test/acc 0.7028333333333333 215
task2/train/lr 0.01200880890779435 215
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 216/256 | train_loss 0.8922 | train_acc 0.7400 | test_loss 0.8505 | test_acc 0.7028 | lr 0.0120
last_idx 11
final_idx 0
task2/train/loss 0.7384735271334648 216
task2/test/loss 0.8564137436830215 216
task2/test/acc 0.699 216
task2/train/lr 0.011499636732764853 216
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 217/256 | train_loss 0.7385 | train_acc 0.7820 | test_loss 0.8564 | test_acc 0.6990 | lr 0.0115
last_idx 11
final_idx 0
task2/train/loss 0.43645519639054936 217
task2/test/loss 0.8696543381814539 217
task2/test/acc 0.6991666666666667 217
task2/train/lr 0.010998747262408329 217
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 218/256 | train_loss 0.4365 | train_acc 0.8500 | test_loss 0.8697 | test_acc 0.6992 | lr 0.0110
last_idx 11
final_idx 0
task2/train/loss 0.5954676357408365 218
task2/test/loss 0.886273752598867 218
task2/test/acc 0.6935 218
task2/train/lr 0.010506442213812275 218
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 219/256 | train_loss 0.5955 | train_acc 0.8400 | test_loss 0.8863 | test_acc 0.6935 | lr 0.0105
last_idx 11
final_idx 0
task2/train/loss 0.6056170904388031 219
task2/test/loss 0.9627456094009162 219
task2/test/acc 0.6795 219
task2/train/lr 0.01002301813312949 219
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 220/256 | train_loss 0.6056 | train_acc 0.8340 | test_loss 0.9627 | test_acc 0.6795 | lr 0.0100
last_idx 11
final_idx 0
task2/train/loss 0.529755155245463 220
task2/test/loss 0.8781870333817754 220
task2/test/acc 0.699 220
task2/train/lr 0.009548766216949778 220
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 221/256 | train_loss 0.5298 | train_acc 0.8740 | test_loss 0.8782 | test_acc 0.6990 | lr 0.0095
last_idx 11
final_idx 0
task2/train/loss 0.6742707242568334 221
task2/test/loss 0.852129238474108 221
task2/test/acc 0.7023333333333334 221
task2/train/lr 0.009083972136894032 221
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 222/256 | train_loss 0.6743 | train_acc 0.8400 | test_loss 0.8521 | test_acc 0.7023 | lr 0.0091
last_idx 11
final_idx 0
task2/train/loss 0.7036623830596606 222
task2/test/loss 0.8540991475112247 222
task2/test/acc 0.703 222
task2/train/lr 0.008628915867536294 222
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 223/256 | train_loss 0.7037 | train_acc 0.7640 | test_loss 0.8541 | test_acc 0.7030 | lr 0.0086
last_idx 11
final_idx 0
task2/train/loss 0.6069244357446829 223
task2/test/loss 0.8365629985384697 223
task2/test/acc 0.7106666666666667 223
task2/train/lr 0.008183871517757594 223
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 224/256 | train_loss 0.6069 | train_acc 0.8060 | test_loss 0.8366 | test_acc 0.7107 | lr 0.0082
last_idx 11
final_idx 0
task2/train/loss 0.6108234028021494 224
task2/test/loss 0.8518983483532049 224
task2/test/acc 0.7023333333333334 224
task2/train/lr 0.00774910716563295 224
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 225/256 | train_loss 0.6108 | train_acc 0.8200 | test_loss 0.8519 | test_acc 0.7023 | lr 0.0077
last_idx 11
final_idx 0
task2/train/loss 0.458530609185497 225
task2/test/loss 0.8616109264157984 225
task2/test/acc 0.7055 225
task2/train/lr 0.007324884696951197 225
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 226/256 | train_loss 0.4585 | train_acc 0.8900 | test_loss 0.8616 | test_acc 0.7055 | lr 0.0073
last_idx 11
final_idx 0
task2/train/loss 0.6816999626656374 226
task2/test/loss 0.8495994749730521 226
task2/test/acc 0.7053333333333334 226
task2/train/lr 0.006911459647464768 226
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 227/256 | train_loss 0.6817 | train_acc 0.8500 | test_loss 0.8496 | test_acc 0.7053 | lr 0.0069
last_idx 11
final_idx 0
task2/train/loss 0.6993809007108212 227
task2/test/loss 0.8568076830710808 227
task2/test/acc 0.7056666666666667 227
task2/train/lr 0.006509081048964508 227
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 228/256 | train_loss 0.6994 | train_acc 0.7840 | test_loss 0.8568 | test_acc 0.7057 | lr 0.0065
last_idx 11
final_idx 0
task2/train/loss 0.8063818849623203 228
task2/test/loss 0.8356127986942765 228
task2/test/acc 0.7123333333333334 228
task2/train/lr 0.0061179912792722595 228
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 229/256 | train_loss 0.8064 | train_acc 0.7080 | test_loss 0.8356 | test_acc 0.7123 | lr 0.0061
last_idx 11
final_idx 0
task2/train/loss 0.5091578538219134 229
task2/test/loss 0.8465661237927249 229
task2/test/acc 0.7125 229
task2/train/lr 0.005738425916241496 229
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 230/256 | train_loss 0.5092 | train_acc 0.8340 | test_loss 0.8466 | test_acc 0.7125 | lr 0.0057
last_idx 11
final_idx 0
task2/train/loss 0.5667449167619149 230
task2/test/loss 0.8621442250526734 230
task2/test/acc 0.705 230
task2/train/lr 0.005370613595854041 230
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 231/256 | train_loss 0.5667 | train_acc 0.7440 | test_loss 0.8621 | test_acc 0.7050 | lr 0.0054
last_idx 11
final_idx 0
task2/train/loss 0.6339232673247656 231
task2/test/loss 0.8447966005680335 231
task2/test/acc 0.7096666666666667 231
task2/train/lr 0.005014775874498306 231
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 232/256 | train_loss 0.6339 | train_acc 0.8740 | test_loss 0.8448 | test_acc 0.7097 | lr 0.0050
last_idx 11
final_idx 0
task2/train/loss 0.6190766859799623 232
task2/test/loss 0.8742434280197116 232
task2/test/acc 0.7011666666666667 232
task2/train/lr 0.004671127095512003 232
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 233/256 | train_loss 0.6191 | train_acc 0.8280 | test_loss 0.8742 | test_acc 0.7012 | lr 0.0047
last_idx 11
final_idx 0
task2/train/loss 0.491959214831392 233
task2/test/loss 0.8625614977013456 233
task2/test/acc 0.6976666666666667 233
task2/train/lr 0.004339874260069749 233
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 234/256 | train_loss 0.4920 | train_acc 0.8560 | test_loss 0.8626 | test_acc 0.6977 | lr 0.0043
last_idx 11
final_idx 0
task2/train/loss 0.6398955980936686 234
task2/test/loss 0.8423354799730064 234
task2/test/acc 0.7085 234
task2/train/lr 0.004021216902493268 234
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 235/256 | train_loss 0.6399 | train_acc 0.8660 | test_loss 0.8423 | test_acc 0.7085 | lr 0.0040
last_idx 11
final_idx 0
task2/train/loss 0.8202923697729906 235
task2/test/loss 0.8332303986497169 235
task2/test/acc 0.7091666666666666 235
task2/train/lr 0.0037153469700593944 235
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 236/256 | train_loss 0.8203 | train_acc 0.7640 | test_loss 0.8332 | test_acc 0.7092 | lr 0.0037
last_idx 11
final_idx 0
task2/train/loss 0.42752276981870335 236
task2/test/loss 0.8273412779952488 236
task2/test/acc 0.7093333333333334 236
task2/train/lr 0.0034224487073782153 236
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 237/256 | train_loss 0.4275 | train_acc 0.8760 | test_loss 0.8273 | test_acc 0.7093 | lr 0.0034
last_idx 11
final_idx 0
task2/train/loss 0.4989793933928013 237
task2/test/loss 0.8491738323312606 237
task2/test/acc 0.7051666666666667 237
task2/train/lr 0.0031426985454109987 237
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 238/256 | train_loss 0.4990 | train_acc 0.7980 | test_loss 0.8492 | test_acc 0.7052 | lr 0.0031
last_idx 11
final_idx 0
task2/train/loss 0.532727587968111 238
task2/test/loss 0.8594462872639189 238
task2/test/acc 0.701 238
task2/train/lr 0.0028762649951947776 238
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 239/256 | train_loss 0.5327 | train_acc 0.8420 | test_loss 0.8594 | test_acc 0.7010 | lr 0.0029
last_idx 11
final_idx 0
task2/train/loss 0.4490989086528619 239
task2/test/loss 0.8553025386411778 239
task2/test/acc 0.704 239
task2/train/lr 0.0026233085463376153 239
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 240/256 | train_loss 0.4491 | train_acc 0.8440 | test_loss 0.8553 | test_acc 0.7040 | lr 0.0026
last_idx 11
final_idx 0
task2/train/loss 0.8113100379705429 240
task2/test/loss 0.857681079067453 240
task2/test/acc 0.7046666666666667 240
task2/train/lr 0.0023839815703456534 240
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 241/256 | train_loss 0.8113 | train_acc 0.6820 | test_loss 0.8577 | test_acc 0.7047 | lr 0.0024
last_idx 11
final_idx 0
task2/train/loss 0.621867286041379 241
task2/test/loss 0.8485533529824584 241
task2/test/acc 0.7073333333333334 241
task2/train/lr 0.0021584282288402137 241
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 242/256 | train_loss 0.6219 | train_acc 0.8440 | test_loss 0.8486 | test_acc 0.7073 | lr 0.0022
last_idx 11
final_idx 0
task2/train/loss 0.37236358287433785 242
task2/test/loss 0.8528449638699093 242
task2/test/acc 0.7055 242
task2/train/lr 0.0019467843867202379 242
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 243/256 | train_loss 0.3724 | train_acc 0.9240 | test_loss 0.8528 | test_acc 0.7055 | lr 0.0019
last_idx 11
final_idx 0
task2/train/loss 0.68908063073953 243
task2/test/loss 0.8399910517852672 243
task2/test/acc 0.7076666666666667 243
task2/train/lr 0.0017491775303223424 243
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 244/256 | train_loss 0.6891 | train_acc 0.8580 | test_loss 0.8400 | test_acc 0.7077 | lr 0.0017
last_idx 11
final_idx 0
task2/train/loss 0.4867998951425155 244
task2/test/loss 0.8385451742767418 244
task2/test/acc 0.7083333333333334 244
task2/train/lr 0.0015657266906278318 244
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 245/256 | train_loss 0.4868 | train_acc 0.8600 | test_loss 0.8385 | test_acc 0.7083 | lr 0.0016
last_idx 11
final_idx 0
task2/train/loss 0.20167763717472553 245
task2/test/loss 0.829846107611691 245
task2/test/acc 0.7111666666666666 245
task2/train/lr 0.001396542371562864 245
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 246/256 | train_loss 0.2017 | train_acc 0.9600 | test_loss 0.8298 | test_acc 0.7112 | lr 0.0014
last_idx 11
final_idx 0
task2/train/loss 0.5354564276834329 246
task2/test/loss 0.8417096888496928 246
task2/test/acc 0.709 246
task2/train/lr 0.0012417264834350366 246
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 247/256 | train_loss 0.5355 | train_acc 0.8900 | test_loss 0.8417 | test_acc 0.7090 | lr 0.0012
last_idx 11
final_idx 0
task2/train/loss 0.7342123985290527 247
task2/test/loss 0.8597510830111748 247
task2/test/acc 0.7041666666666667 247
task2/train/lr 0.0011013722815464207 247
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 248/256 | train_loss 0.7342 | train_acc 0.7600 | test_loss 0.8598 | test_acc 0.7042 | lr 0.0011
last_idx 11
final_idx 0
task2/train/loss 0.6634719663610061 248
task2/test/loss 0.8631275594234467 248
task2/test/acc 0.7066666666666667 248
task2/train/lr 0.0009755643100200469 248
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 249/256 | train_loss 0.6635 | train_acc 0.7640 | test_loss 0.8631 | test_acc 0.7067 | lr 0.0010
last_idx 11
final_idx 0
task2/train/loss 0.5036095591882864 249
task2/test/loss 0.8462594931360579 249
task2/test/acc 0.7108333333333333 249
task2/train/lr 0.0008643783508737047 249
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 250/256 | train_loss 0.5036 | train_acc 0.8840 | test_loss 0.8463 | test_acc 0.7108 | lr 0.0009
last_idx 11
final_idx 0
task2/train/loss 0.7234844056268533 250
task2/test/loss 0.83881097564297 250
task2/test/acc 0.7088333333333333 250
task2/train/lr 0.0007678813783716699 250
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 251/256 | train_loss 0.7235 | train_acc 0.7540 | test_loss 0.8388 | test_acc 0.7088 | lr 0.0008
last_idx 11
final_idx 0
task2/train/loss 0.6198304556310177 251
task2/test/loss 0.8423391419605617 251
task2/test/acc 0.711 251
task2/train/lr 0.0006861315186819283 251
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 252/256 | train_loss 0.6198 | train_acc 0.7640 | test_loss 0.8423 | test_acc 0.7110 | lr 0.0007
last_idx 11
final_idx 0
task2/train/loss 0.5394221879541874 252
task2/test/loss 0.8471639373876753 252
task2/test/acc 0.707 252
task2/train/lr 0.0006191780148631288 252
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 253/256 | train_loss 0.5394 | train_acc 0.8240 | test_loss 0.8472 | test_acc 0.7070 | lr 0.0006
last_idx 11
final_idx 0
task2/train/loss 0.4406250199923913 253
task2/test/loss 0.840841060028459 253
task2/test/acc 0.7093333333333334 253
task2/train/lr 0.0005670611972024174 253
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 254/256 | train_loss 0.4406 | train_acc 0.8700 | test_loss 0.8408 | test_acc 0.7093 | lr 0.0006
last_idx 11
final_idx 0
task2/train/loss 0.7647278849035501 254
task2/test/loss 0.8398222241305957 254
task2/test/acc 0.7091666666666666 254
task2/train/lr 0.0005298124589219829 254
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 255/256 | train_loss 0.7647 | train_acc 0.7700 | test_loss 0.8398 | test_acc 0.7092 | lr 0.0005
last_idx 11
final_idx 0
task2/train/loss 0.5284135490655899 255
task2/test/loss 0.8369369534027837 255
task2/test/acc 0.709 255
task2/train/lr 0.0005074542372689448 255
[INFO] rainbow_memory.py:184 > Task 2 | Epoch 256/256 | train_loss 0.5284 | train_acc 0.8660 | test_loss 0.8369 | test_acc 0.7090 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (2)
[INFO] main.py:398 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (2)
[INFO] main.py:405 > [2-5] Report task result
Metrics/TaskAcc 0.7125 2

##################################################
# Task 3 iteration
##################################################

[INFO] main.py:316 > [2-1] Prepare a datalist for the current task
total : 1000  current step :  0
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.59s. Batch: 0.86s. S_Loss: 2.4189. T_Loss: 2.6668. Mask: 0.0117. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.59s. Batch: 0.86s. S_Loss: 2.4189. T_Loss: 2.6668. Mask: 0.0117. :   2%|▏         | 1/50 [00:00<00:42,  1.16it/s]Train Iter:   2/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4277. T_Loss: 2.6988. Mask: 0.0059. :   2%|▏         | 1/50 [00:01<00:42,  1.16it/s]Train Iter:   2/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4277. T_Loss: 2.6988. Mask: 0.0059. :   4%|▍         | 2/50 [00:01<00:27,  1.77it/s]Train Iter:   3/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.4436. T_Loss: 2.6628. Mask: 0.0052. :   4%|▍         | 2/50 [00:01<00:27,  1.77it/s]Train Iter:   3/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.4436. T_Loss: 2.6628. Mask: 0.0052. :   6%|▌         | 3/50 [00:01<00:23,  2.03it/s]total : 1000  current step :  1
total : 1000  current step :  2
total : 1000  current step :  3
Train Iter:   4/1000. LR: 0.0000. Data: 0.56s. Batch: 0.77s. S_Loss: 2.4241. T_Loss: 2.6778. Mask: 0.0049. :   6%|▌         | 3/50 [00:03<00:23,  2.03it/s]Train Iter:   4/1000. LR: 0.0000. Data: 0.56s. Batch: 0.77s. S_Loss: 2.4241. T_Loss: 2.6778. Mask: 0.0049. :   8%|▊         | 4/50 [00:03<00:40,  1.13it/s]Train Iter:   5/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.4362. T_Loss: 2.6845. Mask: 0.0055. :   8%|▊         | 4/50 [00:03<00:40,  1.13it/s]Train Iter:   5/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.4362. T_Loss: 2.6845. Mask: 0.0055. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4290. T_Loss: 2.6746. Mask: 0.0052. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.4290. T_Loss: 2.6746. Mask: 0.0052. :  12%|█▏        | 6/50 [00:03<00:27,  1.60it/s]total : 1000  current step :  4
total : 1000  current step :  5
total : 1000  current step :  6
Train Iter:   7/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.4271. T_Loss: 2.6635. Mask: 0.0045. :  12%|█▏        | 6/50 [00:04<00:27,  1.60it/s]Train Iter:   7/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.4271. T_Loss: 2.6635. Mask: 0.0045. :  14%|█▍        | 7/50 [00:04<00:32,  1.33it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.4196. T_Loss: 2.6535. Mask: 0.0054. :  14%|█▍        | 7/50 [00:05<00:32,  1.33it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.4196. T_Loss: 2.6535. Mask: 0.0054. :  16%|█▌        | 8/50 [00:05<00:25,  1.64it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.4177. T_Loss: 2.6492. Mask: 0.0056. :  16%|█▌        | 8/50 [00:05<00:25,  1.64it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.4177. T_Loss: 2.6492. Mask: 0.0056. :  18%|█▊        | 9/50 [00:05<00:20,  1.98it/s]total : 1000  current step :  7
total : 1000  current step :  8
total : 1000  current step :  9
Train Iter:  10/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.4162. T_Loss: 2.6422. Mask: 0.0055. :  18%|█▊        | 9/50 [00:06<00:20,  1.98it/s]Train Iter:  10/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.4162. T_Loss: 2.6422. Mask: 0.0055. :  20%|██        | 10/50 [00:06<00:25,  1.55it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.4125. T_Loss: 2.6308. Mask: 0.0060. :  20%|██        | 10/50 [00:06<00:25,  1.55it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.4125. T_Loss: 2.6308. Mask: 0.0060. :  22%|██▏       | 11/50 [00:06<00:22,  1.74it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4100. T_Loss: 2.6277. Mask: 0.0059. :  22%|██▏       | 11/50 [00:07<00:22,  1.74it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4100. T_Loss: 2.6277. Mask: 0.0059. :  24%|██▍       | 12/50 [00:07<00:18,  2.02it/s]total : 1000  current step :  10
total : 1000  current step :  11
total : 1000  current step :  12
Train Iter:  13/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.4097. T_Loss: 2.6149. Mask: 0.0054. :  24%|██▍       | 12/50 [00:08<00:18,  2.02it/s]Train Iter:  13/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.4097. T_Loss: 2.6149. Mask: 0.0054. :  26%|██▌       | 13/50 [00:08<00:25,  1.47it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4095. T_Loss: 2.5967. Mask: 0.0050. :  26%|██▌       | 13/50 [00:08<00:25,  1.47it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4095. T_Loss: 2.5967. Mask: 0.0050. :  28%|██▊       | 14/50 [00:08<00:20,  1.79it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.4138. T_Loss: 2.5808. Mask: 0.0049. :  28%|██▊       | 14/50 [00:08<00:20,  1.79it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.4138. T_Loss: 2.5808. Mask: 0.0049. :  30%|███       | 15/50 [00:08<00:16,  2.12it/s]total : 1000  current step :  13
total : 1000  current step :  14
total : 1000  current step :  15
Train Iter:  16/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4122. T_Loss: 2.5593. Mask: 0.0049. :  30%|███       | 15/50 [00:09<00:16,  2.12it/s]Train Iter:  16/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4122. T_Loss: 2.5593. Mask: 0.0049. :  32%|███▏      | 16/50 [00:09<00:20,  1.63it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4140. T_Loss: 2.5352. Mask: 0.0046. :  32%|███▏      | 16/50 [00:10<00:20,  1.63it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4140. T_Loss: 2.5352. Mask: 0.0046. :  34%|███▍      | 17/50 [00:10<00:18,  1.83it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.4139. T_Loss: 2.5148. Mask: 0.0054. :  34%|███▍      | 17/50 [00:10<00:18,  1.83it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.4139. T_Loss: 2.5148. Mask: 0.0054. :  36%|███▌      | 18/50 [00:10<00:15,  2.10it/s]total : 1000  current step :  16
total : 1000  current step :  17
total : 1000  current step :  18
Train Iter:  19/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4147. T_Loss: 2.4872. Mask: 0.0068. :  36%|███▌      | 18/50 [00:11<00:15,  2.10it/s]Train Iter:  19/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4147. T_Loss: 2.4872. Mask: 0.0068. :  38%|███▊      | 19/50 [00:11<00:20,  1.50it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4160. T_Loss: 2.4596. Mask: 0.0084. :  38%|███▊      | 19/50 [00:12<00:20,  1.50it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4160. T_Loss: 2.4596. Mask: 0.0084. :  40%|████      | 20/50 [00:12<00:17,  1.68it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.4187. T_Loss: 2.4344. Mask: 0.0108. :  40%|████      | 20/50 [00:12<00:17,  1.68it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.4187. T_Loss: 2.4344. Mask: 0.0108. :  42%|████▏     | 21/50 [00:12<00:14,  1.98it/s]total : 1000  current step :  19
total : 1000  current step :  20
total : 1000  current step :  21
Train Iter:  22/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4233. T_Loss: 2.4095. Mask: 0.0151. :  42%|████▏     | 21/50 [00:13<00:14,  1.98it/s]Train Iter:  22/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4233. T_Loss: 2.4095. Mask: 0.0151. :  44%|████▍     | 22/50 [00:13<00:18,  1.48it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4276. T_Loss: 2.3843. Mask: 0.0194. :  44%|████▍     | 22/50 [00:13<00:18,  1.48it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4276. T_Loss: 2.3843. Mask: 0.0194. :  46%|████▌     | 23/50 [00:13<00:16,  1.64it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4318. T_Loss: 2.3550. Mask: 0.0247. :  46%|████▌     | 23/50 [00:14<00:16,  1.64it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4318. T_Loss: 2.3550. Mask: 0.0247. :  48%|████▊     | 24/50 [00:14<00:14,  1.78it/s]total : 1000  current step :  22
total : 1000  current step :  23
total : 1000  current step :  24
Train Iter:  25/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4357. T_Loss: 2.3333. Mask: 0.0334. :  48%|████▊     | 24/50 [00:15<00:14,  1.78it/s]Train Iter:  25/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4357. T_Loss: 2.3333. Mask: 0.0334. :  50%|█████     | 25/50 [00:15<00:17,  1.43it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4398. T_Loss: 2.3096. Mask: 0.0422. :  50%|█████     | 25/50 [00:15<00:17,  1.43it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4398. T_Loss: 2.3096. Mask: 0.0422. :  52%|█████▏    | 26/50 [00:15<00:15,  1.57it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4452. T_Loss: 2.2866. Mask: 0.0557. :  52%|█████▏    | 26/50 [00:16<00:15,  1.57it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4452. T_Loss: 2.2866. Mask: 0.0557. :  54%|█████▍    | 27/50 [00:16<00:12,  1.83it/s]total : 1000  current step :  25
total : 1000  current step :  26
total : 1000  current step :  27
Train Iter:  28/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.4502. T_Loss: 2.2680. Mask: 0.0724. :  54%|█████▍    | 27/50 [00:17<00:12,  1.83it/s]Train Iter:  28/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.4502. T_Loss: 2.2680. Mask: 0.0724. :  56%|█████▌    | 28/50 [00:17<00:15,  1.40it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4541. T_Loss: 2.2489. Mask: 0.0885. :  56%|█████▌    | 28/50 [00:17<00:15,  1.40it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4541. T_Loss: 2.2489. Mask: 0.0885. :  58%|█████▊    | 29/50 [00:17<00:13,  1.58it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4576. T_Loss: 2.2310. Mask: 0.1059. :  58%|█████▊    | 29/50 [00:18<00:13,  1.58it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4576. T_Loss: 2.2310. Mask: 0.1059. :  60%|██████    | 30/50 [00:18<00:10,  1.86it/s]total : 1000  current step :  28
total : 1000  current step :  29
total : 1000  current step :  30
Train Iter:  31/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.4614. T_Loss: 2.2123. Mask: 0.1237. :  60%|██████    | 30/50 [00:19<00:10,  1.86it/s]Train Iter:  31/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.4614. T_Loss: 2.2123. Mask: 0.1237. :  62%|██████▏   | 31/50 [00:19<00:13,  1.43it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4651. T_Loss: 2.1955. Mask: 0.1423. :  62%|██████▏   | 31/50 [00:19<00:13,  1.43it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4651. T_Loss: 2.1955. Mask: 0.1423. :  64%|██████▍   | 32/50 [00:19<00:10,  1.66it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4691. T_Loss: 2.1814. Mask: 0.1600. :  64%|██████▍   | 32/50 [00:19<00:10,  1.66it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4691. T_Loss: 2.1814. Mask: 0.1600. :  66%|██████▌   | 33/50 [00:19<00:08,  2.03it/s]total : 1000  current step :  31
total : 1000  current step :  32
total : 1000  current step :  33
Train Iter:  34/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4723. T_Loss: 2.1661. Mask: 0.1785. :  66%|██████▌   | 33/50 [00:20<00:08,  2.03it/s]Train Iter:  34/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.4723. T_Loss: 2.1661. Mask: 0.1785. :  68%|██████▊   | 34/50 [00:20<00:11,  1.44it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4764. T_Loss: 2.1509. Mask: 0.1969. :  68%|██████▊   | 34/50 [00:21<00:11,  1.44it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4764. T_Loss: 2.1509. Mask: 0.1969. :  70%|███████   | 35/50 [00:21<00:09,  1.63it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4792. T_Loss: 2.1356. Mask: 0.2134. :  70%|███████   | 35/50 [00:21<00:09,  1.63it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4792. T_Loss: 2.1356. Mask: 0.2134. :  72%|███████▏  | 36/50 [00:21<00:07,  1.93it/s]total : 1000  current step :  34
total : 1000  current step :  35
total : 1000  current step :  36
Train Iter:  37/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4828. T_Loss: 2.1197. Mask: 0.2316. :  72%|███████▏  | 36/50 [00:22<00:07,  1.93it/s]Train Iter:  37/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.4828. T_Loss: 2.1197. Mask: 0.2316. :  74%|███████▍  | 37/50 [00:22<00:08,  1.46it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4854. T_Loss: 2.1066. Mask: 0.2463. :  74%|███████▍  | 37/50 [00:23<00:08,  1.46it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.4854. T_Loss: 2.1066. Mask: 0.2463. :  76%|███████▌  | 38/50 [00:23<00:06,  1.74it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4890. T_Loss: 2.0942. Mask: 0.2621. :  76%|███████▌  | 38/50 [00:23<00:06,  1.74it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.4890. T_Loss: 2.0942. Mask: 0.2621. :  78%|███████▊  | 39/50 [00:23<00:05,  2.08it/s]total : 1000  current step :  37
total : 1000  current step :  38
total : 1000  current step :  39
Train Iter:  40/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.4919. T_Loss: 2.0824. Mask: 0.2772. :  78%|███████▊  | 39/50 [00:24<00:05,  2.08it/s]Train Iter:  40/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.4919. T_Loss: 2.0824. Mask: 0.2772. :  80%|████████  | 40/50 [00:24<00:07,  1.26it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.4945. T_Loss: 2.0683. Mask: 0.2901. :  80%|████████  | 40/50 [00:25<00:07,  1.26it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.4945. T_Loss: 2.0683. Mask: 0.2901. :  82%|████████▏ | 41/50 [00:25<00:07,  1.25it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.4961. T_Loss: 2.0560. Mask: 0.3039. :  82%|████████▏ | 41/50 [00:27<00:07,  1.25it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.4961. T_Loss: 2.0560. Mask: 0.3039. :  84%|████████▍ | 42/50 [00:27<00:08,  1.05s/it]total : 1000  current step :  40
total : 1000  current step :  41
total : 1000  current step :  42
Train Iter:  43/1000. LR: 0.0000. Data: 0.42s. Batch: 0.66s. S_Loss: 2.4988. T_Loss: 2.0462. Mask: 0.3182. :  84%|████████▍ | 42/50 [00:28<00:08,  1.05s/it]Train Iter:  43/1000. LR: 0.0000. Data: 0.42s. Batch: 0.66s. S_Loss: 2.4988. T_Loss: 2.0462. Mask: 0.3182. :  86%|████████▌ | 43/50 [00:28<00:07,  1.06s/it]Train Iter:  44/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5006. T_Loss: 2.0350. Mask: 0.3305. :  86%|████████▌ | 43/50 [00:28<00:07,  1.06s/it]Train Iter:  44/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5006. T_Loss: 2.0350. Mask: 0.3305. :  88%|████████▊ | 44/50 [00:28<00:05,  1.19it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.5027. T_Loss: 2.0229. Mask: 0.3417. :  88%|████████▊ | 44/50 [00:29<00:05,  1.19it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.5027. T_Loss: 2.0229. Mask: 0.3417. :  90%|█████████ | 45/50 [00:29<00:03,  1.47it/s]total : 1000  current step :  43
total : 1000  current step :  44
total : 1000  current step :  45
Train Iter:  46/1000. LR: 0.0000. Data: 0.42s. Batch: 0.66s. S_Loss: 2.5051. T_Loss: 2.0118. Mask: 0.3521. :  90%|█████████ | 45/50 [00:30<00:03,  1.47it/s]Train Iter:  46/1000. LR: 0.0000. Data: 0.42s. Batch: 0.66s. S_Loss: 2.5051. T_Loss: 2.0118. Mask: 0.3521. :  92%|█████████▏| 46/50 [00:30<00:03,  1.18it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5070. T_Loss: 2.0016. Mask: 0.3629. :  92%|█████████▏| 46/50 [00:30<00:03,  1.18it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5070. T_Loss: 2.0016. Mask: 0.3629. :  94%|█████████▍| 47/50 [00:30<00:02,  1.36it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5091. T_Loss: 1.9923. Mask: 0.3728. :  94%|█████████▍| 47/50 [00:31<00:02,  1.36it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5091. T_Loss: 1.9923. Mask: 0.3728. :  96%|█████████▌| 48/50 [00:31<00:01,  1.59it/s]total : 1000  current step :  46
total : 1000  current step :  47
total : 1000  current step :  48
Train Iter:  49/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.5111. T_Loss: 1.9826. Mask: 0.3825. :  96%|█████████▌| 48/50 [00:32<00:01,  1.59it/s]Train Iter:  49/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.5111. T_Loss: 1.9826. Mask: 0.3825. :  98%|█████████▊| 49/50 [00:32<00:00,  1.17it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5130. T_Loss: 1.9746. Mask: 0.3922. :  98%|█████████▊| 49/50 [00:32<00:00,  1.17it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5130. T_Loss: 1.9746. Mask: 0.3922. : 100%|██████████| 50/50 [00:32<00:00,  1.44it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5130. T_Loss: 1.9746. Mask: 0.3922. : 100%|██████████| 50/50 [00:32<00:00,  1.52it/s]
total : 1000  current step :  49
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 8.1799. top1: 0.00. top5: 20.70. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 8.1799. top1: 0.00. top5: 20.70. :  12%|█▎        | 1/8 [00:00<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 8.2855. top1: 0.00. top5: 22.46. :  12%|█▎        | 1/8 [00:01<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 8.2855. top1: 0.00. top5: 22.46. :  25%|██▌       | 2/8 [00:01<00:03,  1.96it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 8.2606. top1: 0.00. top5: 21.88. :  25%|██▌       | 2/8 [00:01<00:03,  1.96it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 8.2606. top1: 0.00. top5: 21.88. :  38%|███▊      | 3/8 [00:01<00:02,  2.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 8.3151. top1: 0.00. top5: 20.41. :  38%|███▊      | 3/8 [00:01<00:02,  2.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 8.3151. top1: 0.00. top5: 20.41. :  50%|█████     | 4/8 [00:01<00:01,  2.67it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 8.5027. top1: 0.00. top5: 16.33. :  50%|█████     | 4/8 [00:02<00:01,  2.67it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 8.5027. top1: 0.00. top5: 16.33. :  62%|██████▎   | 5/8 [00:02<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 8.5810. top1: 0.00. top5: 13.61. :  62%|██████▎   | 5/8 [00:02<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 8.5810. top1: 0.00. top5: 13.61. :  75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 8.6449. top1: 0.00. top5: 11.66. :  75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 8.6449. top1: 0.00. top5: 11.66. :  88%|████████▊ | 7/8 [00:02<00:00,  2.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 8.6852. top1: 0.00. top5: 10.45. :  88%|████████▊ | 7/8 [00:03<00:00,  2.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 8.6852. top1: 0.00. top5: 10.45. : 100%|██████████| 8/8 [00:03<00:00,  2.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 8.6852. top1: 0.00. top5: 10.45. : 100%|██████████| 8/8 [00:03<00:00,  2.43it/s]
total : 1000  current step :  50
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.01s. Batch: 0.20s. S_Loss: 2.6142. T_Loss: 1.5726. Mask: 0.8711. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.01s. Batch: 0.20s. S_Loss: 2.6142. T_Loss: 1.5726. Mask: 0.8711. :   2%|▏         | 1/50 [00:00<00:09,  5.04it/s]total : 1000  current step :  51
Train Iter:  52/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.6029. T_Loss: 1.5587. Mask: 0.8750. :   2%|▏         | 1/50 [00:01<00:09,  5.04it/s]Train Iter:  52/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.6029. T_Loss: 1.5587. Mask: 0.8750. :   4%|▍         | 2/50 [00:01<00:35,  1.37it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.5967. T_Loss: 1.5738. Mask: 0.8828. :   4%|▍         | 2/50 [00:01<00:35,  1.37it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.5967. T_Loss: 1.5738. Mask: 0.8828. :   6%|▌         | 3/50 [00:01<00:25,  1.81it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.29s. Batch: 0.49s. S_Loss: 2.6002. T_Loss: 1.5731. Mask: 0.8838. :   6%|▌         | 3/50 [00:01<00:25,  1.81it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.29s. Batch: 0.49s. S_Loss: 2.6002. T_Loss: 1.5731. Mask: 0.8838. :   8%|▊         | 4/50 [00:01<00:21,  2.19it/s]total : 1000  current step :  52
total : 1000  current step :  53
total : 1000  current step :  54
Train Iter:  55/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.5945. T_Loss: 1.5787. Mask: 0.8891. :   8%|▊         | 4/50 [00:03<00:21,  2.19it/s]Train Iter:  55/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.5945. T_Loss: 1.5787. Mask: 0.8891. :  10%|█         | 5/50 [00:03<00:34,  1.30it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.5984. T_Loss: 1.5880. Mask: 0.8887. :  10%|█         | 5/50 [00:03<00:34,  1.30it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.5984. T_Loss: 1.5880. Mask: 0.8887. :  12%|█▏        | 6/50 [00:03<00:25,  1.73it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.6006. T_Loss: 1.5855. Mask: 0.8884. :  12%|█▏        | 6/50 [00:03<00:25,  1.73it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.6006. T_Loss: 1.5855. Mask: 0.8884. :  14%|█▍        | 7/50 [00:03<00:21,  1.97it/s]total : 1000  current step :  55
total : 1000  current step :  56
total : 1000  current step :  57
Train Iter:  58/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.6020. T_Loss: 1.5853. Mask: 0.8906. :  14%|█▍        | 7/50 [00:04<00:21,  1.97it/s]Train Iter:  58/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.6020. T_Loss: 1.5853. Mask: 0.8906. :  16%|█▌        | 8/50 [00:04<00:28,  1.46it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.6040. T_Loss: 1.5892. Mask: 0.8937. :  16%|█▌        | 8/50 [00:05<00:28,  1.46it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.38s. Batch: 0.58s. S_Loss: 2.6040. T_Loss: 1.5892. Mask: 0.8937. :  18%|█▊        | 9/50 [00:05<00:24,  1.71it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.6018. T_Loss: 1.5957. Mask: 0.8938. :  18%|█▊        | 9/50 [00:05<00:24,  1.71it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.6018. T_Loss: 1.5957. Mask: 0.8938. :  20%|██        | 10/50 [00:05<00:22,  1.80it/s]total : 1000  current step :  58
total : 1000  current step :  59
total : 1000  current step :  60
Train Iter:  61/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.6033. T_Loss: 1.5958. Mask: 0.8931. :  20%|██        | 10/50 [00:07<00:22,  1.80it/s]Train Iter:  61/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.6033. T_Loss: 1.5958. Mask: 0.8931. :  22%|██▏       | 11/50 [00:07<00:30,  1.29it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.6027. T_Loss: 1.5997. Mask: 0.8949. :  22%|██▏       | 11/50 [00:07<00:30,  1.29it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.6027. T_Loss: 1.5997. Mask: 0.8949. :  24%|██▍       | 12/50 [00:07<00:23,  1.60it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.6026. T_Loss: 1.6016. Mask: 0.8954. :  24%|██▍       | 12/50 [00:07<00:23,  1.60it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 2.6026. T_Loss: 1.6016. Mask: 0.8954. :  26%|██▌       | 13/50 [00:07<00:19,  1.85it/s]total : 1000  current step :  61
total : 1000  current step :  62
total : 1000  current step :  63
Train Iter:  64/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.6001. T_Loss: 1.6057. Mask: 0.8959. :  26%|██▌       | 13/50 [00:08<00:19,  1.85it/s]Train Iter:  64/1000. LR: 0.0000. Data: 0.44s. Batch: 0.63s. S_Loss: 2.6001. T_Loss: 1.6057. Mask: 0.8959. :  28%|██▊       | 14/50 [00:08<00:26,  1.38it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.5970. T_Loss: 1.6086. Mask: 0.8987. :  28%|██▊       | 14/50 [00:09<00:26,  1.38it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.5970. T_Loss: 1.6086. Mask: 0.8987. :  30%|███       | 15/50 [00:09<00:20,  1.67it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.5975. T_Loss: 1.6091. Mask: 0.8987. :  30%|███       | 15/50 [00:09<00:20,  1.67it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.41s. Batch: 0.61s. S_Loss: 2.5975. T_Loss: 1.6091. Mask: 0.8987. :  32%|███▏      | 16/50 [00:09<00:20,  1.67it/s]total : 1000  current step :  64
total : 1000  current step :  65
total : 1000  current step :  66
Train Iter:  67/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.5976. T_Loss: 1.6136. Mask: 0.8991. :  32%|███▏      | 16/50 [00:10<00:20,  1.67it/s]Train Iter:  67/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.5976. T_Loss: 1.6136. Mask: 0.8991. :  34%|███▍      | 17/50 [00:10<00:25,  1.30it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.5966. T_Loss: 1.6172. Mask: 0.9019. :  34%|███▍      | 17/50 [00:11<00:25,  1.30it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.5966. T_Loss: 1.6172. Mask: 0.9019. :  36%|███▌      | 18/50 [00:11<00:21,  1.52it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.5955. T_Loss: 1.6187. Mask: 0.8976. :  36%|███▌      | 18/50 [00:11<00:21,  1.52it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.5955. T_Loss: 1.6187. Mask: 0.8976. :  38%|███▊      | 19/50 [00:11<00:19,  1.59it/s]total : 1000  current step :  67
total : 1000  current step :  68
total : 1000  current step :  69
Train Iter:  70/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.5964. T_Loss: 1.6210. Mask: 0.8979. :  38%|███▊      | 19/50 [00:13<00:19,  1.59it/s]Train Iter:  70/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.5964. T_Loss: 1.6210. Mask: 0.8979. :  40%|████      | 20/50 [00:13<00:23,  1.26it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.5960. T_Loss: 1.6190. Mask: 0.8977. :  40%|████      | 20/50 [00:13<00:23,  1.26it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.5960. T_Loss: 1.6190. Mask: 0.8977. :  42%|████▏     | 21/50 [00:13<00:18,  1.58it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.5965. T_Loss: 1.6239. Mask: 0.8993. :  42%|████▏     | 21/50 [00:13<00:18,  1.58it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.5965. T_Loss: 1.6239. Mask: 0.8993. :  44%|████▍     | 22/50 [00:13<00:15,  1.75it/s]total : 1000  current step :  70
total : 1000  current step :  71
total : 1000  current step :  72
Train Iter:  73/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.5957. T_Loss: 1.6267. Mask: 0.9006. :  44%|████▍     | 22/50 [00:15<00:15,  1.75it/s]Train Iter:  73/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.5957. T_Loss: 1.6267. Mask: 0.9006. :  46%|████▌     | 23/50 [00:15<00:21,  1.25it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.5971. T_Loss: 1.6278. Mask: 0.9010. :  46%|████▌     | 23/50 [00:15<00:21,  1.25it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.5971. T_Loss: 1.6278. Mask: 0.9010. :  48%|████▊     | 24/50 [00:15<00:16,  1.60it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.5972. T_Loss: 1.6328. Mask: 0.9012. :  48%|████▊     | 24/50 [00:15<00:16,  1.60it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.42s. Batch: 0.62s. S_Loss: 2.5972. T_Loss: 1.6328. Mask: 0.9012. :  50%|█████     | 25/50 [00:15<00:13,  1.79it/s]total : 1000  current step :  73
total : 1000  current step :  74
total : 1000  current step :  75
Train Iter:  76/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.5972. T_Loss: 1.6347. Mask: 0.9014. :  50%|█████     | 25/50 [00:17<00:13,  1.79it/s]Train Iter:  76/1000. LR: 0.0000. Data: 0.45s. Batch: 0.65s. S_Loss: 2.5972. T_Loss: 1.6347. Mask: 0.9014. :  52%|█████▏    | 26/50 [00:17<00:19,  1.26it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.5972. T_Loss: 1.6385. Mask: 0.9022. :  52%|█████▏    | 26/50 [00:17<00:19,  1.26it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.44s. Batch: 0.64s. S_Loss: 2.5972. T_Loss: 1.6385. Mask: 0.9022. :  54%|█████▍    | 27/50 [00:17<00:14,  1.54it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.5973. T_Loss: 1.6415. Mask: 0.9030. :  54%|█████▍    | 27/50 [00:17<00:14,  1.54it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.43s. Batch: 0.63s. S_Loss: 2.5973. T_Loss: 1.6415. Mask: 0.9030. :  56%|█████▌    | 28/50 [00:17<00:12,  1.80it/s]total : 1000  current step :  76
total : 1000  current step :  77
total : 1000  current step :  78
Train Iter:  79/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.5974. T_Loss: 1.6458. Mask: 0.9034. :  56%|█████▌    | 28/50 [00:19<00:12,  1.80it/s]Train Iter:  79/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.5974. T_Loss: 1.6458. Mask: 0.9034. :  58%|█████▊    | 29/50 [00:19<00:17,  1.20it/s]total : 1000  current step :  79
Train Iter:  80/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.5981. T_Loss: 1.6508. Mask: 0.9047. :  58%|█████▊    | 29/50 [00:19<00:17,  1.20it/s]Train Iter:  80/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.5981. T_Loss: 1.6508. Mask: 0.9047. :  60%|██████    | 30/50 [00:19<00:15,  1.25it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5983. T_Loss: 1.6516. Mask: 0.9051. :  60%|██████    | 30/50 [00:20<00:15,  1.25it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5983. T_Loss: 1.6516. Mask: 0.9051. :  62%|██████▏   | 31/50 [00:20<00:15,  1.22it/s]total : 1000  current step :  80
total : 1000  current step :  81
Train Iter:  82/1000. LR: 0.0000. Data: 0.48s. Batch: 0.68s. S_Loss: 2.5987. T_Loss: 1.6549. Mask: 0.9048. :  62%|██████▏   | 31/50 [00:21<00:15,  1.22it/s]Train Iter:  82/1000. LR: 0.0000. Data: 0.48s. Batch: 0.68s. S_Loss: 2.5987. T_Loss: 1.6549. Mask: 0.9048. :  64%|██████▍   | 32/50 [00:21<00:16,  1.06it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.5982. T_Loss: 1.6591. Mask: 0.9041. :  64%|██████▍   | 32/50 [00:22<00:16,  1.06it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.5982. T_Loss: 1.6591. Mask: 0.9041. :  66%|██████▌   | 33/50 [00:22<00:13,  1.30it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5980. T_Loss: 1.6607. Mask: 0.9043. :  66%|██████▌   | 33/50 [00:22<00:13,  1.30it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5980. T_Loss: 1.6607. Mask: 0.9043. :  68%|██████▊   | 34/50 [00:22<00:10,  1.54it/s]total : 1000  current step :  82
total : 1000  current step :  83
total : 1000  current step :  84
Train Iter:  85/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.5975. T_Loss: 1.6630. Mask: 0.9041. :  68%|██████▊   | 34/50 [00:23<00:10,  1.54it/s]Train Iter:  85/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.5975. T_Loss: 1.6630. Mask: 0.9041. :  70%|███████   | 35/50 [00:23<00:11,  1.28it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.5973. T_Loss: 1.6659. Mask: 0.9039. :  70%|███████   | 35/50 [00:24<00:11,  1.28it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.47s. Batch: 0.67s. S_Loss: 2.5973. T_Loss: 1.6659. Mask: 0.9039. :  72%|███████▏  | 36/50 [00:24<00:09,  1.48it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5969. T_Loss: 1.6696. Mask: 0.9041. :  72%|███████▏  | 36/50 [00:24<00:09,  1.48it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5969. T_Loss: 1.6696. Mask: 0.9041. :  74%|███████▍  | 37/50 [00:24<00:08,  1.59it/s]total : 1000  current step :  85
total : 1000  current step :  86
total : 1000  current step :  87
Train Iter:  88/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.5967. T_Loss: 1.6726. Mask: 0.9041. :  74%|███████▍  | 37/50 [00:25<00:08,  1.59it/s]Train Iter:  88/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.5967. T_Loss: 1.6726. Mask: 0.9041. :  76%|███████▌  | 38/50 [00:25<00:09,  1.27it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5970. T_Loss: 1.6756. Mask: 0.9037. :  76%|███████▌  | 38/50 [00:26<00:09,  1.27it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5970. T_Loss: 1.6756. Mask: 0.9037. :  78%|███████▊  | 39/50 [00:26<00:07,  1.52it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.5968. T_Loss: 1.6784. Mask: 0.9041. :  78%|███████▊  | 39/50 [00:26<00:07,  1.52it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.46s. Batch: 0.66s. S_Loss: 2.5968. T_Loss: 1.6784. Mask: 0.9041. :  80%|████████  | 40/50 [00:26<00:05,  1.76it/s]total : 1000  current step :  88
total : 1000  current step :  89
total : 1000  current step :  90
Train Iter:  91/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.5964. T_Loss: 1.6812. Mask: 0.9042. :  80%|████████  | 40/50 [00:27<00:05,  1.76it/s]Train Iter:  91/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.5964. T_Loss: 1.6812. Mask: 0.9042. :  82%|████████▏ | 41/50 [00:27<00:06,  1.30it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5968. T_Loss: 1.6843. Mask: 0.9048. :  82%|████████▏ | 41/50 [00:28<00:06,  1.30it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5968. T_Loss: 1.6843. Mask: 0.9048. :  84%|████████▍ | 42/50 [00:28<00:05,  1.59it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.5963. T_Loss: 1.6865. Mask: 0.9047. :  84%|████████▍ | 42/50 [00:28<00:05,  1.59it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.5963. T_Loss: 1.6865. Mask: 0.9047. :  86%|████████▌ | 43/50 [00:28<00:03,  1.78it/s]total : 1000  current step :  91
total : 1000  current step :  92
total : 1000  current step :  93
Train Iter:  94/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5969. T_Loss: 1.6887. Mask: 0.9058. :  86%|████████▌ | 43/50 [00:29<00:03,  1.78it/s]Train Iter:  94/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5969. T_Loss: 1.6887. Mask: 0.9058. :  88%|████████▊ | 44/50 [00:29<00:04,  1.35it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5969. T_Loss: 1.6909. Mask: 0.9061. :  88%|████████▊ | 44/50 [00:30<00:04,  1.35it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5969. T_Loss: 1.6909. Mask: 0.9061. :  90%|█████████ | 45/50 [00:30<00:03,  1.55it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.5972. T_Loss: 1.6916. Mask: 0.9062. :  90%|█████████ | 45/50 [00:30<00:03,  1.55it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.5972. T_Loss: 1.6916. Mask: 0.9062. :  92%|█████████▏| 46/50 [00:30<00:02,  1.90it/s]total : 1000  current step :  94
total : 1000  current step :  95
total : 1000  current step :  96
Train Iter:  97/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5968. T_Loss: 1.6935. Mask: 0.9063. :  92%|█████████▏| 46/50 [00:31<00:02,  1.90it/s]Train Iter:  97/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.5968. T_Loss: 1.6935. Mask: 0.9063. :  94%|█████████▍| 47/50 [00:31<00:02,  1.35it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.5970. T_Loss: 1.6948. Mask: 0.9071. :  94%|█████████▍| 47/50 [00:31<00:02,  1.35it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.5970. T_Loss: 1.6948. Mask: 0.9071. :  96%|█████████▌| 48/50 [00:31<00:01,  1.59it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5974. T_Loss: 1.6966. Mask: 0.9074. :  96%|█████████▌| 48/50 [00:32<00:01,  1.59it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5974. T_Loss: 1.6966. Mask: 0.9074. :  98%|█████████▊| 49/50 [00:32<00:00,  1.83it/s]total : 1000  current step :  97
total : 1000  current step :  98
total : 1000  current step :  99
Train Iter: 100/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5973. T_Loss: 1.6995. Mask: 0.9079. :  98%|█████████▊| 49/50 [00:33<00:00,  1.83it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5973. T_Loss: 1.6995. Mask: 0.9079. : 100%|██████████| 50/50 [00:33<00:00,  1.40it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5973. T_Loss: 1.6995. Mask: 0.9079. : 100%|██████████| 50/50 [00:33<00:00,  1.50it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 4.4184. top1: 0.00. top5: 3.12. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 4.4184. top1: 0.00. top5: 3.12. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 4.4631. top1: 0.00. top5: 3.32. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 4.4631. top1: 0.00. top5: 3.32. :  25%|██▌       | 2/8 [00:00<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 4.4520. top1: 0.00. top5: 3.39. :  25%|██▌       | 2/8 [00:01<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 4.4520. top1: 0.00. top5: 3.39. :  38%|███▊      | 3/8 [00:01<00:01,  2.70it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 4.4727. top1: 0.00. top5: 3.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.70it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 4.4727. top1: 0.00. top5: 3.61. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 4.5241. top1: 0.00. top5: 2.89. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 4.5241. top1: 0.00. top5: 2.89. :  62%|██████▎   | 5/8 [00:01<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 4.5370. top1: 0.00. top5: 2.47. :  62%|██████▎   | 5/8 [00:02<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 4.5370. top1: 0.00. top5: 2.47. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 4.5555. top1: 0.00. top5: 2.12. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 4.5555. top1: 0.00. top5: 2.12. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 4.5661. top1: 0.00. top5: 1.90. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 4.5661. top1: 0.00. top5: 1.90. : 100%|██████████| 8/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 4.5661. top1: 0.00. top5: 1.90. : 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
total : 1000  current step :  100
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.23s. S_Loss: 2.6121. T_Loss: 1.8908. Mask: 0.9414. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.23s. S_Loss: 2.6121. T_Loss: 1.8908. Mask: 0.9414. :   2%|▏         | 1/50 [00:00<00:11,  4.30it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.27s. S_Loss: 2.5987. T_Loss: 1.8743. Mask: 0.9277. :   2%|▏         | 1/50 [00:00<00:11,  4.30it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.27s. S_Loss: 2.5987. T_Loss: 1.8743. Mask: 0.9277. :   4%|▍         | 2/50 [00:00<00:13,  3.65it/s]total : 1000  current step :  101
total : 1000  current step :  102
Train Iter: 103/1000. LR: 0.0000. Data: 0.31s. Batch: 0.57s. S_Loss: 2.6068. T_Loss: 1.8495. Mask: 0.9323. :   4%|▍         | 2/50 [00:01<00:13,  3.65it/s]Train Iter: 103/1000. LR: 0.0000. Data: 0.31s. Batch: 0.57s. S_Loss: 2.6068. T_Loss: 1.8495. Mask: 0.9323. :   6%|▌         | 3/50 [00:01<00:32,  1.47it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.25s. Batch: 0.50s. S_Loss: 2.6091. T_Loss: 1.8337. Mask: 0.9355. :   6%|▌         | 3/50 [00:01<00:32,  1.47it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.25s. Batch: 0.50s. S_Loss: 2.6091. T_Loss: 1.8337. Mask: 0.9355. :   8%|▊         | 4/50 [00:01<00:24,  1.89it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.22s. Batch: 0.47s. S_Loss: 2.6059. T_Loss: 1.8576. Mask: 0.9375. :   8%|▊         | 4/50 [00:02<00:24,  1.89it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.22s. Batch: 0.47s. S_Loss: 2.6059. T_Loss: 1.8576. Mask: 0.9375. :  10%|█         | 5/50 [00:02<00:21,  2.13it/s]total : 1000  current step :  103
total : 1000  current step :  104
total : 1000  current step :  105
Train Iter: 106/1000. LR: 0.0000. Data: 0.32s. Batch: 0.56s. S_Loss: 2.6092. T_Loss: 1.8574. Mask: 0.9382. :  10%|█         | 5/50 [00:03<00:21,  2.13it/s]Train Iter: 106/1000. LR: 0.0000. Data: 0.32s. Batch: 0.56s. S_Loss: 2.6092. T_Loss: 1.8574. Mask: 0.9382. :  12%|█▏        | 6/50 [00:03<00:28,  1.54it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.30s. Batch: 0.54s. S_Loss: 2.6120. T_Loss: 1.8529. Mask: 0.9325. :  12%|█▏        | 6/50 [00:03<00:28,  1.54it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.30s. Batch: 0.54s. S_Loss: 2.6120. T_Loss: 1.8529. Mask: 0.9325. :  14%|█▍        | 7/50 [00:03<00:24,  1.73it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.27s. Batch: 0.52s. S_Loss: 2.6141. T_Loss: 1.8569. Mask: 0.9326. :  14%|█▍        | 7/50 [00:04<00:24,  1.73it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.27s. Batch: 0.52s. S_Loss: 2.6141. T_Loss: 1.8569. Mask: 0.9326. :  16%|█▌        | 8/50 [00:04<00:21,  1.93it/s]total : 1000  current step :  106
total : 1000  current step :  107
total : 1000  current step :  108
Train Iter: 109/1000. LR: 0.0000. Data: 0.33s. Batch: 0.58s. S_Loss: 2.6158. T_Loss: 1.8583. Mask: 0.9323. :  16%|█▌        | 8/50 [00:05<00:21,  1.93it/s]Train Iter: 109/1000. LR: 0.0000. Data: 0.33s. Batch: 0.58s. S_Loss: 2.6158. T_Loss: 1.8583. Mask: 0.9323. :  18%|█▊        | 9/50 [00:05<00:28,  1.45it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.31s. Batch: 0.55s. S_Loss: 2.6139. T_Loss: 1.8528. Mask: 0.9309. :  18%|█▊        | 9/50 [00:05<00:28,  1.45it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.31s. Batch: 0.55s. S_Loss: 2.6139. T_Loss: 1.8528. Mask: 0.9309. :  20%|██        | 10/50 [00:05<00:22,  1.74it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.29s. Batch: 0.53s. S_Loss: 2.6103. T_Loss: 1.8573. Mask: 0.9339. :  20%|██        | 10/50 [00:05<00:22,  1.74it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.29s. Batch: 0.53s. S_Loss: 2.6103. T_Loss: 1.8573. Mask: 0.9339. :  22%|██▏       | 11/50 [00:05<00:19,  1.99it/s]total : 1000  current step :  109
total : 1000  current step :  110
total : 1000  current step :  111
Train Iter: 112/1000. LR: 0.0000. Data: 0.33s. Batch: 0.58s. S_Loss: 2.6099. T_Loss: 1.8667. Mask: 0.9342. :  22%|██▏       | 11/50 [00:06<00:19,  1.99it/s]Train Iter: 112/1000. LR: 0.0000. Data: 0.33s. Batch: 0.58s. S_Loss: 2.6099. T_Loss: 1.8667. Mask: 0.9342. :  24%|██▍       | 12/50 [00:06<00:25,  1.50it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.31s. Batch: 0.56s. S_Loss: 2.6095. T_Loss: 1.8684. Mask: 0.9360. :  24%|██▍       | 12/50 [00:07<00:25,  1.50it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.31s. Batch: 0.56s. S_Loss: 2.6095. T_Loss: 1.8684. Mask: 0.9360. :  26%|██▌       | 13/50 [00:07<00:20,  1.78it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.29s. Batch: 0.54s. S_Loss: 2.6099. T_Loss: 1.8667. Mask: 0.9355. :  26%|██▌       | 13/50 [00:07<00:20,  1.78it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.29s. Batch: 0.54s. S_Loss: 2.6099. T_Loss: 1.8667. Mask: 0.9355. :  28%|██▊       | 14/50 [00:07<00:17,  2.02it/s]total : 1000  current step :  112
total : 1000  current step :  113
total : 1000  current step :  114
Train Iter: 115/1000. LR: 0.0000. Data: 0.33s. Batch: 0.58s. S_Loss: 2.6104. T_Loss: 1.8747. Mask: 0.9362. :  28%|██▊       | 14/50 [00:08<00:17,  2.02it/s]Train Iter: 115/1000. LR: 0.0000. Data: 0.33s. Batch: 0.58s. S_Loss: 2.6104. T_Loss: 1.8747. Mask: 0.9362. :  30%|███       | 15/50 [00:08<00:23,  1.48it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.31s. Batch: 0.56s. S_Loss: 2.6080. T_Loss: 1.8726. Mask: 0.9358. :  30%|███       | 15/50 [00:08<00:23,  1.48it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.31s. Batch: 0.56s. S_Loss: 2.6080. T_Loss: 1.8726. Mask: 0.9358. :  32%|███▏      | 16/50 [00:08<00:19,  1.78it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.30s. Batch: 0.55s. S_Loss: 2.6075. T_Loss: 1.8772. Mask: 0.9368. :  32%|███▏      | 16/50 [00:09<00:19,  1.78it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.30s. Batch: 0.55s. S_Loss: 2.6075. T_Loss: 1.8772. Mask: 0.9368. :  34%|███▍      | 17/50 [00:09<00:16,  2.04it/s]total : 1000  current step :  115
total : 1000  current step :  116
total : 1000  current step :  117
Train Iter: 118/1000. LR: 0.0000. Data: 0.33s. Batch: 0.58s. S_Loss: 2.6079. T_Loss: 1.8751. Mask: 0.9366. :  34%|███▍      | 17/50 [00:10<00:16,  2.04it/s]Train Iter: 118/1000. LR: 0.0000. Data: 0.33s. Batch: 0.58s. S_Loss: 2.6079. T_Loss: 1.8751. Mask: 0.9366. :  36%|███▌      | 18/50 [00:10<00:21,  1.50it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.32s. Batch: 0.56s. S_Loss: 2.6070. T_Loss: 1.8824. Mask: 0.9379. :  36%|███▌      | 18/50 [00:10<00:21,  1.50it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.32s. Batch: 0.56s. S_Loss: 2.6070. T_Loss: 1.8824. Mask: 0.9379. :  38%|███▊      | 19/50 [00:10<00:17,  1.77it/s]total : 1000  current step :  118
total : 1000  current step :  119
Train Iter: 120/1000. LR: 0.0000. Data: 0.33s. Batch: 0.57s. S_Loss: 2.6065. T_Loss: 1.8856. Mask: 0.9379. :  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]Train Iter: 120/1000. LR: 0.0000. Data: 0.33s. Batch: 0.57s. S_Loss: 2.6065. T_Loss: 1.8856. Mask: 0.9379. :  40%|████      | 20/50 [00:11<00:17,  1.68it/s]total : 1000  current step :  120
Train Iter: 121/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.6057. T_Loss: 1.8890. Mask: 0.9384. :  40%|████      | 20/50 [00:12<00:17,  1.68it/s]Train Iter: 121/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.6057. T_Loss: 1.8890. Mask: 0.9384. :  42%|████▏     | 21/50 [00:12<00:22,  1.29it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.6051. T_Loss: 1.8897. Mask: 0.9368. :  42%|████▏     | 21/50 [00:12<00:22,  1.29it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.6051. T_Loss: 1.8897. Mask: 0.9368. :  44%|████▍     | 22/50 [00:12<00:18,  1.51it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.6053. T_Loss: 1.8883. Mask: 0.9373. :  44%|████▍     | 22/50 [00:13<00:18,  1.51it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.6053. T_Loss: 1.8883. Mask: 0.9373. :  46%|████▌     | 23/50 [00:13<00:16,  1.65it/s]total : 1000  current step :  121
total : 1000  current step :  122
total : 1000  current step :  123
Train Iter: 124/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.6052. T_Loss: 1.8901. Mask: 0.9367. :  46%|████▌     | 23/50 [00:14<00:16,  1.65it/s]Train Iter: 124/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.6052. T_Loss: 1.8901. Mask: 0.9367. :  48%|████▊     | 24/50 [00:14<00:20,  1.28it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.6049. T_Loss: 1.8968. Mask: 0.9367. :  48%|████▊     | 24/50 [00:15<00:20,  1.28it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.6049. T_Loss: 1.8968. Mask: 0.9367. :  50%|█████     | 25/50 [00:15<00:18,  1.37it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.6044. T_Loss: 1.8998. Mask: 0.9375. :  50%|█████     | 25/50 [00:15<00:18,  1.37it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.6044. T_Loss: 1.8998. Mask: 0.9375. :  52%|█████▏    | 26/50 [00:15<00:14,  1.70it/s]total : 1000  current step :  124
total : 1000  current step :  125
total : 1000  current step :  126
Train Iter: 127/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.6044. T_Loss: 1.9039. Mask: 0.9376. :  52%|█████▏    | 26/50 [00:16<00:14,  1.70it/s]Train Iter: 127/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.6044. T_Loss: 1.9039. Mask: 0.9376. :  54%|█████▍    | 27/50 [00:16<00:17,  1.31it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.6051. T_Loss: 1.9077. Mask: 0.9375. :  54%|█████▍    | 27/50 [00:17<00:17,  1.31it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.6051. T_Loss: 1.9077. Mask: 0.9375. :  56%|█████▌    | 28/50 [00:17<00:14,  1.53it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.6053. T_Loss: 1.9109. Mask: 0.9375. :  56%|█████▌    | 28/50 [00:17<00:14,  1.53it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.6053. T_Loss: 1.9109. Mask: 0.9375. :  58%|█████▊    | 29/50 [00:17<00:11,  1.82it/s]total : 1000  current step :  127
total : 1000  current step :  128
total : 1000  current step :  129
Train Iter: 130/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.6048. T_Loss: 1.9155. Mask: 0.9367. :  58%|█████▊    | 29/50 [00:18<00:11,  1.82it/s]Train Iter: 130/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.6048. T_Loss: 1.9155. Mask: 0.9367. :  60%|██████    | 30/50 [00:18<00:14,  1.35it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.6053. T_Loss: 1.9165. Mask: 0.9354. :  60%|██████    | 30/50 [00:19<00:14,  1.35it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.6053. T_Loss: 1.9165. Mask: 0.9354. :  62%|██████▏   | 31/50 [00:19<00:12,  1.55it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.6063. T_Loss: 1.9148. Mask: 0.9358. :  62%|██████▏   | 31/50 [00:19<00:12,  1.55it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.6063. T_Loss: 1.9148. Mask: 0.9358. :  64%|██████▍   | 32/50 [00:19<00:10,  1.80it/s]total : 1000  current step :  130
total : 1000  current step :  131
total : 1000  current step :  132
Train Iter: 133/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.6057. T_Loss: 1.9185. Mask: 0.9366. :  64%|██████▍   | 32/50 [00:20<00:10,  1.80it/s]Train Iter: 133/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.6057. T_Loss: 1.9185. Mask: 0.9366. :  66%|██████▌   | 33/50 [00:20<00:13,  1.26it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.6059. T_Loss: 1.9218. Mask: 0.9365. :  66%|██████▌   | 33/50 [00:21<00:13,  1.26it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.6059. T_Loss: 1.9218. Mask: 0.9365. :  68%|██████▊   | 34/50 [00:21<00:10,  1.54it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.6051. T_Loss: 1.9220. Mask: 0.9360. :  68%|██████▊   | 34/50 [00:21<00:10,  1.54it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.6051. T_Loss: 1.9220. Mask: 0.9360. :  70%|███████   | 35/50 [00:21<00:08,  1.75it/s]total : 1000  current step :  133
total : 1000  current step :  134
total : 1000  current step :  135
Train Iter: 136/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.6052. T_Loss: 1.9255. Mask: 0.9367. :  70%|███████   | 35/50 [00:22<00:08,  1.75it/s]Train Iter: 136/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.6052. T_Loss: 1.9255. Mask: 0.9367. :  72%|███████▏  | 36/50 [00:22<00:10,  1.34it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.6052. T_Loss: 1.9249. Mask: 0.9368. :  72%|███████▏  | 36/50 [00:23<00:10,  1.34it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.6052. T_Loss: 1.9249. Mask: 0.9368. :  74%|███████▍  | 37/50 [00:23<00:08,  1.51it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.6056. T_Loss: 1.9279. Mask: 0.9372. :  74%|███████▍  | 37/50 [00:23<00:08,  1.51it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.6056. T_Loss: 1.9279. Mask: 0.9372. :  76%|███████▌  | 38/50 [00:23<00:06,  1.76it/s]total : 1000  current step :  136
total : 1000  current step :  137
total : 1000  current step :  138
Train Iter: 139/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.6052. T_Loss: 1.9298. Mask: 0.9373. :  76%|███████▌  | 38/50 [00:24<00:06,  1.76it/s]Train Iter: 139/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.6052. T_Loss: 1.9298. Mask: 0.9373. :  78%|███████▊  | 39/50 [00:24<00:08,  1.30it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.6054. T_Loss: 1.9313. Mask: 0.9376. :  78%|███████▊  | 39/50 [00:25<00:08,  1.30it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.6054. T_Loss: 1.9313. Mask: 0.9376. :  80%|████████  | 40/50 [00:25<00:06,  1.52it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.6060. T_Loss: 1.9359. Mask: 0.9377. :  80%|████████  | 40/50 [00:25<00:06,  1.52it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.6060. T_Loss: 1.9359. Mask: 0.9377. :  82%|████████▏ | 41/50 [00:25<00:05,  1.77it/s]total : 1000  current step :  139
total : 1000  current step :  140
total : 1000  current step :  141
Train Iter: 142/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6063. T_Loss: 1.9373. Mask: 0.9379. :  82%|████████▏ | 41/50 [00:26<00:05,  1.77it/s]Train Iter: 142/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6063. T_Loss: 1.9373. Mask: 0.9379. :  84%|████████▍ | 42/50 [00:26<00:06,  1.29it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.6065. T_Loss: 1.9383. Mask: 0.9371. :  84%|████████▍ | 42/50 [00:26<00:06,  1.29it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.6065. T_Loss: 1.9383. Mask: 0.9371. :  86%|████████▌ | 43/50 [00:26<00:04,  1.55it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.6060. T_Loss: 1.9393. Mask: 0.9371. :  86%|████████▌ | 43/50 [00:27<00:04,  1.55it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.6060. T_Loss: 1.9393. Mask: 0.9371. :  88%|████████▊ | 44/50 [00:27<00:03,  1.60it/s]total : 1000  current step :  142
total : 1000  current step :  143
total : 1000  current step :  144
Train Iter: 145/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6051. T_Loss: 1.9402. Mask: 0.9368. :  88%|████████▊ | 44/50 [00:28<00:03,  1.60it/s]Train Iter: 145/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6051. T_Loss: 1.9402. Mask: 0.9368. :  90%|█████████ | 45/50 [00:28<00:03,  1.27it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6062. T_Loss: 1.9431. Mask: 0.9368. :  90%|█████████ | 45/50 [00:29<00:03,  1.27it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6062. T_Loss: 1.9431. Mask: 0.9368. :  92%|█████████▏| 46/50 [00:29<00:02,  1.48it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6062. T_Loss: 1.9433. Mask: 0.9370. :  92%|█████████▏| 46/50 [00:29<00:02,  1.48it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6062. T_Loss: 1.9433. Mask: 0.9370. :  94%|█████████▍| 47/50 [00:29<00:01,  1.69it/s]total : 1000  current step :  145
total : 1000  current step :  146
total : 1000  current step :  147
Train Iter: 148/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6054. T_Loss: 1.9446. Mask: 0.9371. :  94%|█████████▍| 47/50 [00:30<00:01,  1.69it/s]Train Iter: 148/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6054. T_Loss: 1.9446. Mask: 0.9371. :  96%|█████████▌| 48/50 [00:30<00:01,  1.20it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.40s. Batch: 0.64s. S_Loss: 2.6056. T_Loss: 1.9499. Mask: 0.9377. :  96%|█████████▌| 48/50 [00:31<00:01,  1.20it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.40s. Batch: 0.64s. S_Loss: 2.6056. T_Loss: 1.9499. Mask: 0.9377. :  98%|█████████▊| 49/50 [00:31<00:00,  1.40it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6052. T_Loss: 1.9550. Mask: 0.9380. :  98%|█████████▊| 49/50 [00:31<00:00,  1.40it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6052. T_Loss: 1.9550. Mask: 0.9380. : 100%|██████████| 50/50 [00:31<00:00,  1.66it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6052. T_Loss: 1.9550. Mask: 0.9380. : 100%|██████████| 50/50 [00:31<00:00,  1.58it/s]
total : 1000  current step :  148
total : 1000  current step :  149
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 3.2918. top1: 0.00. top5: 0.78. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 3.2918. top1: 0.00. top5: 0.78. :  12%|█▎        | 1/8 [00:00<00:05,  1.32it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 3.3126. top1: 0.00. top5: 0.78. :  12%|█▎        | 1/8 [00:01<00:05,  1.32it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 3.3126. top1: 0.00. top5: 0.78. :  25%|██▌       | 2/8 [00:01<00:02,  2.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 3.3065. top1: 0.00. top5: 0.91. :  25%|██▌       | 2/8 [00:01<00:02,  2.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 3.3065. top1: 0.00. top5: 0.91. :  38%|███▊      | 3/8 [00:01<00:01,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 3.3130. top1: 0.00. top5: 0.98. :  38%|███▊      | 3/8 [00:01<00:01,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 3.3130. top1: 0.00. top5: 0.98. :  50%|█████     | 4/8 [00:01<00:01,  2.82it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 3.3156. top1: 0.00. top5: 0.78. :  50%|█████     | 4/8 [00:01<00:01,  2.82it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 3.3156. top1: 0.00. top5: 0.78. :  62%|██████▎   | 5/8 [00:01<00:00,  3.09it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 3.3069. top1: 0.00. top5: 0.78. :  62%|██████▎   | 5/8 [00:02<00:00,  3.09it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 3.3069. top1: 0.00. top5: 0.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 3.3079. top1: 0.00. top5: 0.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 3.3079. top1: 0.00. top5: 0.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 3.3078. top1: 0.00. top5: 0.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 3.3078. top1: 0.00. top5: 0.60. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 3.3078. top1: 0.00. top5: 0.60. : 100%|██████████| 8/8 [00:02<00:00,  2.67it/s]
total : 1000  current step :  150
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 0.89s. Batch: 1.10s. S_Loss: 2.5761. T_Loss: 2.2200. Mask: 0.9492. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 0.89s. Batch: 1.10s. S_Loss: 2.5761. T_Loss: 2.2200. Mask: 0.9492. :   2%|▏         | 1/50 [00:01<00:54,  1.11s/it]Train Iter: 152/1000. LR: 0.0000. Data: 0.60s. Batch: 0.80s. S_Loss: 2.6023. T_Loss: 2.1107. Mask: 0.9316. :   2%|▏         | 1/50 [00:01<00:54,  1.11s/it]Train Iter: 152/1000. LR: 0.0000. Data: 0.60s. Batch: 0.80s. S_Loss: 2.6023. T_Loss: 2.1107. Mask: 0.9316. :   4%|▍         | 2/50 [00:01<00:36,  1.33it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.6011. T_Loss: 2.0912. Mask: 0.9219. :   4%|▍         | 2/50 [00:01<00:36,  1.33it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.6011. T_Loss: 2.0912. Mask: 0.9219. :   6%|▌         | 3/50 [00:01<00:25,  1.82it/s]total : 1000  current step :  151
total : 1000  current step :  152
total : 1000  current step :  153
Train Iter: 154/1000. LR: 0.0000. Data: 0.58s. Batch: 0.79s. S_Loss: 2.6005. T_Loss: 2.1407. Mask: 0.9258. :   6%|▌         | 3/50 [00:03<00:25,  1.82it/s]Train Iter: 154/1000. LR: 0.0000. Data: 0.58s. Batch: 0.79s. S_Loss: 2.6005. T_Loss: 2.1407. Mask: 0.9258. :   8%|▊         | 4/50 [00:03<00:37,  1.22it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.49s. Batch: 0.69s. S_Loss: 2.6047. T_Loss: 2.1504. Mask: 0.9227. :   8%|▊         | 4/50 [00:03<00:37,  1.22it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.49s. Batch: 0.69s. S_Loss: 2.6047. T_Loss: 2.1504. Mask: 0.9227. :  10%|█         | 5/50 [00:03<00:28,  1.58it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.6048. T_Loss: 2.1692. Mask: 0.9258. :  10%|█         | 5/50 [00:03<00:28,  1.58it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.6048. T_Loss: 2.1692. Mask: 0.9258. :  12%|█▏        | 6/50 [00:03<00:26,  1.69it/s]total : 1000  current step :  154
total : 1000  current step :  155
total : 1000  current step :  156
Train Iter: 157/1000. LR: 0.0000. Data: 0.51s. Batch: 0.72s. S_Loss: 2.6063. T_Loss: 2.1638. Mask: 0.9269. :  12%|█▏        | 6/50 [00:05<00:26,  1.69it/s]Train Iter: 157/1000. LR: 0.0000. Data: 0.51s. Batch: 0.72s. S_Loss: 2.6063. T_Loss: 2.1638. Mask: 0.9269. :  14%|█▍        | 7/50 [00:05<00:32,  1.34it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.6043. T_Loss: 2.1802. Mask: 0.9316. :  14%|█▍        | 7/50 [00:05<00:32,  1.34it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.6043. T_Loss: 2.1802. Mask: 0.9316. :  16%|█▌        | 8/50 [00:05<00:25,  1.62it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.6007. T_Loss: 2.1674. Mask: 0.9314. :  16%|█▌        | 8/50 [00:05<00:25,  1.62it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.6007. T_Loss: 2.1674. Mask: 0.9314. :  18%|█▊        | 9/50 [00:05<00:22,  1.84it/s]total : 1000  current step :  157
total : 1000  current step :  158
total : 1000  current step :  159
Train Iter: 160/1000. LR: 0.0000. Data: 0.50s. Batch: 0.71s. S_Loss: 2.6008. T_Loss: 2.1523. Mask: 0.9313. :  18%|█▊        | 9/50 [00:07<00:22,  1.84it/s]Train Iter: 160/1000. LR: 0.0000. Data: 0.50s. Batch: 0.71s. S_Loss: 2.6008. T_Loss: 2.1523. Mask: 0.9313. :  20%|██        | 10/50 [00:07<00:32,  1.24it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.51s. Batch: 0.72s. S_Loss: 2.6001. T_Loss: 2.1487. Mask: 0.9329. :  20%|██        | 10/50 [00:07<00:32,  1.24it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.51s. Batch: 0.72s. S_Loss: 2.6001. T_Loss: 2.1487. Mask: 0.9329. :  22%|██▏       | 11/50 [00:07<00:31,  1.23it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.50s. Batch: 0.71s. S_Loss: 2.5982. T_Loss: 2.1476. Mask: 0.9336. :  22%|██▏       | 11/50 [00:08<00:31,  1.23it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.50s. Batch: 0.71s. S_Loss: 2.5982. T_Loss: 2.1476. Mask: 0.9336. :  24%|██▍       | 12/50 [00:08<00:27,  1.37it/s]total : 1000  current step :  160
total : 1000  current step :  161
total : 1000  current step :  162
Train Iter: 163/1000. LR: 0.0000. Data: 0.53s. Batch: 0.74s. S_Loss: 2.5992. T_Loss: 2.1378. Mask: 0.9327. :  24%|██▍       | 12/50 [00:09<00:27,  1.37it/s]Train Iter: 163/1000. LR: 0.0000. Data: 0.53s. Batch: 0.74s. S_Loss: 2.5992. T_Loss: 2.1378. Mask: 0.9327. :  26%|██▌       | 13/50 [00:09<00:32,  1.15it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.6010. T_Loss: 2.1487. Mask: 0.9316. :  26%|██▌       | 13/50 [00:10<00:32,  1.15it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.6010. T_Loss: 2.1487. Mask: 0.9316. :  28%|██▊       | 14/50 [00:10<00:26,  1.37it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.5996. T_Loss: 2.1462. Mask: 0.9315. :  28%|██▊       | 14/50 [00:10<00:26,  1.37it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.5996. T_Loss: 2.1462. Mask: 0.9315. :  30%|███       | 15/50 [00:10<00:21,  1.62it/s]total : 1000  current step :  163
total : 1000  current step :  164
total : 1000  current step :  165
Train Iter: 166/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.5994. T_Loss: 2.1531. Mask: 0.9319. :  30%|███       | 15/50 [00:11<00:21,  1.62it/s]Train Iter: 166/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.5994. T_Loss: 2.1531. Mask: 0.9319. :  32%|███▏      | 16/50 [00:11<00:25,  1.35it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.6008. T_Loss: 2.1499. Mask: 0.9324. :  32%|███▏      | 16/50 [00:12<00:25,  1.35it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.6008. T_Loss: 2.1499. Mask: 0.9324. :  34%|███▍      | 17/50 [00:12<00:22,  1.49it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6015. T_Loss: 2.1495. Mask: 0.9327. :  34%|███▍      | 17/50 [00:12<00:22,  1.49it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6015. T_Loss: 2.1495. Mask: 0.9327. :  36%|███▌      | 18/50 [00:12<00:17,  1.85it/s]total : 1000  current step :  166
total : 1000  current step :  167
total : 1000  current step :  168
Train Iter: 169/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6012. T_Loss: 2.1466. Mask: 0.9344. :  36%|███▌      | 18/50 [00:13<00:17,  1.85it/s]Train Iter: 169/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6012. T_Loss: 2.1466. Mask: 0.9344. :  38%|███▊      | 19/50 [00:13<00:22,  1.41it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6017. T_Loss: 2.1494. Mask: 0.9346. :  38%|███▊      | 19/50 [00:13<00:22,  1.41it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6017. T_Loss: 2.1494. Mask: 0.9346. :  40%|████      | 20/50 [00:13<00:18,  1.66it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6021. T_Loss: 2.1533. Mask: 0.9343. :  40%|████      | 20/50 [00:14<00:18,  1.66it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6021. T_Loss: 2.1533. Mask: 0.9343. :  42%|████▏     | 21/50 [00:14<00:15,  1.91it/s]total : 1000  current step :  169
total : 1000  current step :  170
total : 1000  current step :  171
Train Iter: 172/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.6026. T_Loss: 2.1570. Mask: 0.9345. :  42%|████▏     | 21/50 [00:15<00:15,  1.91it/s]Train Iter: 172/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.6026. T_Loss: 2.1570. Mask: 0.9345. :  44%|████▍     | 22/50 [00:15<00:20,  1.40it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6021. T_Loss: 2.1531. Mask: 0.9353. :  44%|████▍     | 22/50 [00:15<00:20,  1.40it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6021. T_Loss: 2.1531. Mask: 0.9353. :  46%|████▌     | 23/50 [00:15<00:15,  1.70it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.6035. T_Loss: 2.1602. Mask: 0.9349. :  46%|████▌     | 23/50 [00:15<00:15,  1.70it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.6035. T_Loss: 2.1602. Mask: 0.9349. :  48%|████▊     | 24/50 [00:15<00:13,  1.99it/s]total : 1000  current step :  172
total : 1000  current step :  173
total : 1000  current step :  174
Train Iter: 175/1000. LR: 0.0000. Data: 0.44s. Batch: 0.68s. S_Loss: 2.6041. T_Loss: 2.1596. Mask: 0.9348. :  48%|████▊     | 24/50 [00:16<00:13,  1.99it/s]Train Iter: 175/1000. LR: 0.0000. Data: 0.44s. Batch: 0.68s. S_Loss: 2.6041. T_Loss: 2.1596. Mask: 0.9348. :  50%|█████     | 25/50 [00:16<00:17,  1.44it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.6038. T_Loss: 2.1650. Mask: 0.9351. :  50%|█████     | 25/50 [00:17<00:17,  1.44it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.6038. T_Loss: 2.1650. Mask: 0.9351. :  52%|█████▏    | 26/50 [00:17<00:13,  1.72it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.6036. T_Loss: 2.1659. Mask: 0.9363. :  52%|█████▏    | 26/50 [00:17<00:13,  1.72it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.6036. T_Loss: 2.1659. Mask: 0.9363. :  54%|█████▍    | 27/50 [00:17<00:11,  2.03it/s]total : 1000  current step :  175
total : 1000  current step :  176
total : 1000  current step :  177
Train Iter: 178/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.6028. T_Loss: 2.1686. Mask: 0.9371. :  54%|█████▍    | 27/50 [00:18<00:11,  2.03it/s]Train Iter: 178/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.6028. T_Loss: 2.1686. Mask: 0.9371. :  56%|█████▌    | 28/50 [00:18<00:14,  1.48it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6025. T_Loss: 2.1688. Mask: 0.9370. :  56%|█████▌    | 28/50 [00:19<00:14,  1.48it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6025. T_Loss: 2.1688. Mask: 0.9370. :  58%|█████▊    | 29/50 [00:19<00:12,  1.70it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6017. T_Loss: 2.1712. Mask: 0.9368. :  58%|█████▊    | 29/50 [00:19<00:12,  1.70it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6017. T_Loss: 2.1712. Mask: 0.9368. :  60%|██████    | 30/50 [00:19<00:09,  2.04it/s]total : 1000  current step :  178
total : 1000  current step :  179
total : 1000  current step :  180
Train Iter: 181/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6019. T_Loss: 2.1718. Mask: 0.9372. :  60%|██████    | 30/50 [00:20<00:09,  2.04it/s]Train Iter: 181/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6019. T_Loss: 2.1718. Mask: 0.9372. :  62%|██████▏   | 31/50 [00:20<00:12,  1.52it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6015. T_Loss: 2.1719. Mask: 0.9371. :  62%|██████▏   | 31/50 [00:20<00:12,  1.52it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6015. T_Loss: 2.1719. Mask: 0.9371. :  64%|██████▍   | 32/50 [00:20<00:10,  1.80it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6020. T_Loss: 2.1745. Mask: 0.9375. :  64%|██████▍   | 32/50 [00:21<00:10,  1.80it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6020. T_Loss: 2.1745. Mask: 0.9375. :  66%|██████▌   | 33/50 [00:21<00:08,  2.01it/s]total : 1000  current step :  181
total : 1000  current step :  182
total : 1000  current step :  183
Train Iter: 184/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6027. T_Loss: 2.1737. Mask: 0.9382. :  66%|██████▌   | 33/50 [00:22<00:08,  2.01it/s]Train Iter: 184/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6027. T_Loss: 2.1737. Mask: 0.9382. :  68%|██████▊   | 34/50 [00:22<00:10,  1.50it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6019. T_Loss: 2.1764. Mask: 0.9384. :  68%|██████▊   | 34/50 [00:22<00:10,  1.50it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6019. T_Loss: 2.1764. Mask: 0.9384. :  70%|███████   | 35/50 [00:22<00:08,  1.80it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6010. T_Loss: 2.1801. Mask: 0.9391. :  70%|███████   | 35/50 [00:22<00:08,  1.80it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.40s. Batch: 0.63s. S_Loss: 2.6010. T_Loss: 2.1801. Mask: 0.9391. :  72%|███████▏  | 36/50 [00:22<00:06,  2.11it/s]total : 1000  current step :  184
total : 1000  current step :  185
total : 1000  current step :  186
Train Iter: 187/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6014. T_Loss: 2.1901. Mask: 0.9398. :  72%|███████▏  | 36/50 [00:23<00:06,  2.11it/s]Train Iter: 187/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6014. T_Loss: 2.1901. Mask: 0.9398. :  74%|███████▍  | 37/50 [00:23<00:09,  1.37it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6020. T_Loss: 2.1919. Mask: 0.9406. :  74%|███████▍  | 37/50 [00:24<00:09,  1.37it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6020. T_Loss: 2.1919. Mask: 0.9406. :  76%|███████▌  | 38/50 [00:24<00:07,  1.69it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.6016. T_Loss: 2.1923. Mask: 0.9411. :  76%|███████▌  | 38/50 [00:24<00:07,  1.69it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.6016. T_Loss: 2.1923. Mask: 0.9411. :  78%|███████▊  | 39/50 [00:24<00:06,  1.80it/s]total : 1000  current step :  187
total : 1000  current step :  188
total : 1000  current step :  189
Train Iter: 190/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6019. T_Loss: 2.1962. Mask: 0.9413. :  78%|███████▊  | 39/50 [00:25<00:06,  1.80it/s]Train Iter: 190/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6019. T_Loss: 2.1962. Mask: 0.9413. :  80%|████████  | 40/50 [00:25<00:07,  1.31it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.6015. T_Loss: 2.2002. Mask: 0.9411. :  80%|████████  | 40/50 [00:26<00:07,  1.31it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.6015. T_Loss: 2.2002. Mask: 0.9411. :  82%|████████▏ | 41/50 [00:26<00:05,  1.57it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.6015. T_Loss: 2.2076. Mask: 0.9416. :  82%|████████▏ | 41/50 [00:26<00:05,  1.57it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.6015. T_Loss: 2.2076. Mask: 0.9416. :  84%|████████▍ | 42/50 [00:26<00:04,  1.85it/s]total : 1000  current step :  190
total : 1000  current step :  191
total : 1000  current step :  192
Train Iter: 193/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6013. T_Loss: 2.2091. Mask: 0.9412. :  84%|████████▍ | 42/50 [00:27<00:04,  1.85it/s]Train Iter: 193/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.6013. T_Loss: 2.2091. Mask: 0.9412. :  86%|████████▌ | 43/50 [00:27<00:05,  1.37it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.6013. T_Loss: 2.2111. Mask: 0.9415. :  86%|████████▌ | 43/50 [00:28<00:05,  1.37it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.6013. T_Loss: 2.2111. Mask: 0.9415. :  88%|████████▊ | 44/50 [00:28<00:03,  1.61it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.6000. T_Loss: 2.2129. Mask: 0.9420. :  88%|████████▊ | 44/50 [00:28<00:03,  1.61it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.6000. T_Loss: 2.2129. Mask: 0.9420. :  90%|█████████ | 45/50 [00:28<00:02,  1.79it/s]total : 1000  current step :  193
total : 1000  current step :  194
total : 1000  current step :  195
Train Iter: 196/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5996. T_Loss: 2.2145. Mask: 0.9416. :  90%|█████████ | 45/50 [00:29<00:02,  1.79it/s]Train Iter: 196/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5996. T_Loss: 2.2145. Mask: 0.9416. :  92%|█████████▏| 46/50 [00:29<00:03,  1.25it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.6000. T_Loss: 2.2183. Mask: 0.9422. :  92%|█████████▏| 46/50 [00:30<00:03,  1.25it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.6000. T_Loss: 2.2183. Mask: 0.9422. :  94%|█████████▍| 47/50 [00:30<00:01,  1.55it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6002. T_Loss: 2.2218. Mask: 0.9419. :  94%|█████████▍| 47/50 [00:30<00:01,  1.55it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.6002. T_Loss: 2.2218. Mask: 0.9419. :  96%|█████████▌| 48/50 [00:30<00:01,  1.81it/s]total : 1000  current step :  196
total : 1000  current step :  197
total : 1000  current step :  198
Train Iter: 199/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5999. T_Loss: 2.2208. Mask: 0.9421. :  96%|█████████▌| 48/50 [00:31<00:01,  1.81it/s]Train Iter: 199/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5999. T_Loss: 2.2208. Mask: 0.9421. :  98%|█████████▊| 49/50 [00:31<00:00,  1.31it/s]total : 1000  current step :  199
Train Iter: 200/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5998. T_Loss: 2.2189. Mask: 0.9427. :  98%|█████████▊| 49/50 [00:32<00:00,  1.31it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5998. T_Loss: 2.2189. Mask: 0.9427. : 100%|██████████| 50/50 [00:32<00:00,  1.29it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.5998. T_Loss: 2.2189. Mask: 0.9427. : 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 2.8834. top1: 0.00. top5: 0.78. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 2.8834. top1: 0.00. top5: 0.78. :  12%|█▎        | 1/8 [00:00<00:04,  1.43it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 2.8961. top1: 0.00. top5: 1.17. :  12%|█▎        | 1/8 [00:01<00:04,  1.43it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 2.8961. top1: 0.00. top5: 1.17. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 2.8930. top1: 0.00. top5: 1.56. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 2.8930. top1: 0.00. top5: 1.56. :  38%|███▊      | 3/8 [00:01<00:01,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 2.8941. top1: 0.00. top5: 1.56. :  38%|███▊      | 3/8 [00:01<00:01,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 2.8941. top1: 0.00. top5: 1.56. :  50%|█████     | 4/8 [00:01<00:01,  2.73it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.8768. top1: 0.00. top5: 1.72. :  50%|█████     | 4/8 [00:01<00:01,  2.73it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.8768. top1: 0.00. top5: 1.72. :  62%|██████▎   | 5/8 [00:01<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.8598. top1: 0.00. top5: 1.95. :  62%|██████▎   | 5/8 [00:02<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.8598. top1: 0.00. top5: 1.95. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.8533. top1: 0.00. top5: 2.23. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.8533. top1: 0.00. top5: 2.23. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.8484. top1: 0.00. top5: 2.40. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.8484. top1: 0.00. top5: 2.40. : 100%|██████████| 8/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.8484. top1: 0.00. top5: 2.40. : 100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
total : 1000  current step :  200
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.01s. Batch: 0.25s. S_Loss: 2.5865. T_Loss: 2.3810. Mask: 0.9648. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.01s. Batch: 0.25s. S_Loss: 2.5865. T_Loss: 2.3810. Mask: 0.9648. :   2%|▏         | 1/50 [00:00<00:12,  3.92it/s]total : 1000  current step :  201
Train Iter: 202/1000. LR: 0.0000. Data: 0.48s. Batch: 0.68s. S_Loss: 2.6026. T_Loss: 2.3800. Mask: 0.9688. :   2%|▏         | 1/50 [00:01<00:12,  3.92it/s]Train Iter: 202/1000. LR: 0.0000. Data: 0.48s. Batch: 0.68s. S_Loss: 2.6026. T_Loss: 2.3800. Mask: 0.9688. :   4%|▍         | 2/50 [00:01<00:36,  1.32it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.6027. T_Loss: 2.4031. Mask: 0.9570. :   4%|▍         | 2/50 [00:01<00:36,  1.32it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.39s. Batch: 0.58s. S_Loss: 2.6027. T_Loss: 2.4031. Mask: 0.9570. :   6%|▌         | 3/50 [00:01<00:27,  1.72it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.6091. T_Loss: 2.3745. Mask: 0.9502. :   6%|▌         | 3/50 [00:02<00:27,  1.72it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.6091. T_Loss: 2.3745. Mask: 0.9502. :   8%|▊         | 4/50 [00:02<00:24,  1.90it/s]total : 1000  current step :  202
total : 1000  current step :  203
total : 1000  current step :  204
Train Iter: 205/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.6075. T_Loss: 2.3997. Mask: 0.9531. :   8%|▊         | 4/50 [00:03<00:24,  1.90it/s]Train Iter: 205/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.6075. T_Loss: 2.3997. Mask: 0.9531. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.41s. Batch: 0.62s. S_Loss: 2.6057. T_Loss: 2.4522. Mask: 0.9525. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.41s. Batch: 0.62s. S_Loss: 2.6057. T_Loss: 2.4522. Mask: 0.9525. :  12%|█▏        | 6/50 [00:03<00:28,  1.55it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.6030. T_Loss: 2.4570. Mask: 0.9498. :  12%|█▏        | 6/50 [00:04<00:28,  1.55it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.6030. T_Loss: 2.4570. Mask: 0.9498. :  14%|█▍        | 7/50 [00:04<00:24,  1.76it/s]total : 1000  current step :  205
total : 1000  current step :  206
total : 1000  current step :  207
Train Iter: 208/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6051. T_Loss: 2.4565. Mask: 0.9507. :  14%|█▍        | 7/50 [00:05<00:24,  1.76it/s]Train Iter: 208/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6051. T_Loss: 2.4565. Mask: 0.9507. :  16%|█▌        | 8/50 [00:05<00:33,  1.25it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.6021. T_Loss: 2.4677. Mask: 0.9514. :  16%|█▌        | 8/50 [00:05<00:33,  1.25it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.44s. Batch: 0.65s. S_Loss: 2.6021. T_Loss: 2.4677. Mask: 0.9514. :  18%|█▊        | 9/50 [00:05<00:27,  1.48it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.6027. T_Loss: 2.4558. Mask: 0.9480. :  18%|█▊        | 9/50 [00:06<00:27,  1.48it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.6027. T_Loss: 2.4558. Mask: 0.9480. :  20%|██        | 10/50 [00:06<00:23,  1.72it/s]total : 1000  current step :  208
total : 1000  current step :  209
total : 1000  current step :  210
Train Iter: 211/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.5999. T_Loss: 2.4452. Mask: 0.9478. :  20%|██        | 10/50 [00:07<00:23,  1.72it/s]Train Iter: 211/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.5999. T_Loss: 2.4452. Mask: 0.9478. :  22%|██▏       | 11/50 [00:07<00:33,  1.17it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5966. T_Loss: 2.4367. Mask: 0.9456. :  22%|██▏       | 11/50 [00:07<00:33,  1.17it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5966. T_Loss: 2.4367. Mask: 0.9456. :  24%|██▍       | 12/50 [00:07<00:25,  1.49it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.5970. T_Loss: 2.4229. Mask: 0.9447. :  24%|██▍       | 12/50 [00:08<00:25,  1.49it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.43s. Batch: 0.64s. S_Loss: 2.5970. T_Loss: 2.4229. Mask: 0.9447. :  26%|██▌       | 13/50 [00:08<00:22,  1.65it/s]total : 1000  current step :  211
total : 1000  current step :  212
total : 1000  current step :  213
Train Iter: 214/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5953. T_Loss: 2.4101. Mask: 0.9448. :  26%|██▌       | 13/50 [00:09<00:22,  1.65it/s]Train Iter: 214/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5953. T_Loss: 2.4101. Mask: 0.9448. :  28%|██▊       | 14/50 [00:09<00:29,  1.22it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5962. T_Loss: 2.4122. Mask: 0.9458. :  28%|██▊       | 14/50 [00:10<00:29,  1.22it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5962. T_Loss: 2.4122. Mask: 0.9458. :  30%|███       | 15/50 [00:10<00:24,  1.43it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.5959. T_Loss: 2.4072. Mask: 0.9453. :  30%|███       | 15/50 [00:10<00:24,  1.43it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.5959. T_Loss: 2.4072. Mask: 0.9453. :  32%|███▏      | 16/50 [00:10<00:21,  1.60it/s]total : 1000  current step :  214
total : 1000  current step :  215
total : 1000  current step :  216
Train Iter: 217/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5975. T_Loss: 2.4266. Mask: 0.9446. :  32%|███▏      | 16/50 [00:11<00:21,  1.60it/s]Train Iter: 217/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5975. T_Loss: 2.4266. Mask: 0.9446. :  34%|███▍      | 17/50 [00:11<00:26,  1.24it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.5976. T_Loss: 2.4333. Mask: 0.9440. :  34%|███▍      | 17/50 [00:12<00:26,  1.24it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.5976. T_Loss: 2.4333. Mask: 0.9440. :  36%|███▌      | 18/50 [00:12<00:22,  1.44it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5961. T_Loss: 2.4399. Mask: 0.9457. :  36%|███▌      | 18/50 [00:12<00:22,  1.44it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5961. T_Loss: 2.4399. Mask: 0.9457. :  38%|███▊      | 19/50 [00:12<00:17,  1.75it/s]total : 1000  current step :  217
total : 1000  current step :  218
total : 1000  current step :  219
Train Iter: 220/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5973. T_Loss: 2.4579. Mask: 0.9463. :  38%|███▊      | 19/50 [00:13<00:17,  1.75it/s]Train Iter: 220/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5973. T_Loss: 2.4579. Mask: 0.9463. :  40%|████      | 20/50 [00:13<00:24,  1.22it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.5965. T_Loss: 2.4656. Mask: 0.9462. :  40%|████      | 20/50 [00:14<00:24,  1.22it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.5965. T_Loss: 2.4656. Mask: 0.9462. :  42%|████▏     | 21/50 [00:14<00:20,  1.44it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5969. T_Loss: 2.4680. Mask: 0.9451. :  42%|████▏     | 21/50 [00:14<00:20,  1.44it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5969. T_Loss: 2.4680. Mask: 0.9451. :  44%|████▍     | 22/50 [00:14<00:16,  1.72it/s]total : 1000  current step :  220
total : 1000  current step :  221
total : 1000  current step :  222
Train Iter: 223/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5968. T_Loss: 2.4686. Mask: 0.9450. :  44%|████▍     | 22/50 [00:15<00:16,  1.72it/s]Train Iter: 223/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5968. T_Loss: 2.4686. Mask: 0.9450. :  46%|████▌     | 23/50 [00:15<00:21,  1.24it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5975. T_Loss: 2.4671. Mask: 0.9453. :  46%|████▌     | 23/50 [00:16<00:21,  1.24it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5975. T_Loss: 2.4671. Mask: 0.9453. :  48%|████▊     | 24/50 [00:16<00:16,  1.57it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5975. T_Loss: 2.4686. Mask: 0.9464. :  48%|████▊     | 24/50 [00:16<00:16,  1.57it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5975. T_Loss: 2.4686. Mask: 0.9464. :  50%|█████     | 25/50 [00:16<00:13,  1.84it/s]total : 1000  current step :  223
total : 1000  current step :  224
total : 1000  current step :  225
Train Iter: 226/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.5986. T_Loss: 2.4673. Mask: 0.9456. :  50%|█████     | 25/50 [00:17<00:13,  1.84it/s]Train Iter: 226/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.5986. T_Loss: 2.4673. Mask: 0.9456. :  52%|█████▏    | 26/50 [00:17<00:18,  1.29it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5988. T_Loss: 2.4612. Mask: 0.9463. :  52%|█████▏    | 26/50 [00:18<00:18,  1.29it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5988. T_Loss: 2.4612. Mask: 0.9463. :  54%|█████▍    | 27/50 [00:18<00:14,  1.57it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5992. T_Loss: 2.4516. Mask: 0.9466. :  54%|█████▍    | 27/50 [00:18<00:14,  1.57it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5992. T_Loss: 2.4516. Mask: 0.9466. :  56%|█████▌    | 28/50 [00:18<00:12,  1.82it/s]total : 1000  current step :  226
total : 1000  current step :  227
total : 1000  current step :  228
Train Iter: 229/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.5975. T_Loss: 2.4521. Mask: 0.9473. :  56%|█████▌    | 28/50 [00:19<00:12,  1.82it/s]Train Iter: 229/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.5975. T_Loss: 2.4521. Mask: 0.9473. :  58%|█████▊    | 29/50 [00:19<00:15,  1.35it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.5988. T_Loss: 2.4508. Mask: 0.9480. :  58%|█████▊    | 29/50 [00:20<00:15,  1.35it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.5988. T_Loss: 2.4508. Mask: 0.9480. :  60%|██████    | 30/50 [00:20<00:12,  1.59it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.6001. T_Loss: 2.4614. Mask: 0.9482. :  60%|██████    | 30/50 [00:20<00:12,  1.59it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.6001. T_Loss: 2.4614. Mask: 0.9482. :  62%|██████▏   | 31/50 [00:20<00:09,  1.96it/s]total : 1000  current step :  229
total : 1000  current step :  230
total : 1000  current step :  231
Train Iter: 232/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5992. T_Loss: 2.4647. Mask: 0.9487. :  62%|██████▏   | 31/50 [00:21<00:09,  1.96it/s]Train Iter: 232/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5992. T_Loss: 2.4647. Mask: 0.9487. :  64%|██████▍   | 32/50 [00:21<00:12,  1.39it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5993. T_Loss: 2.4657. Mask: 0.9491. :  64%|██████▍   | 32/50 [00:21<00:12,  1.39it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5993. T_Loss: 2.4657. Mask: 0.9491. :  66%|██████▌   | 33/50 [00:21<00:10,  1.66it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.5990. T_Loss: 2.4679. Mask: 0.9496. :  66%|██████▌   | 33/50 [00:22<00:10,  1.66it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.5990. T_Loss: 2.4679. Mask: 0.9496. :  68%|██████▊   | 34/50 [00:22<00:08,  1.87it/s]total : 1000  current step :  232
total : 1000  current step :  233
total : 1000  current step :  234
Train Iter: 235/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5997. T_Loss: 2.4622. Mask: 0.9492. :  68%|██████▊   | 34/50 [00:23<00:08,  1.87it/s]Train Iter: 235/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.5997. T_Loss: 2.4622. Mask: 0.9492. :  70%|███████   | 35/50 [00:23<00:11,  1.35it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5997. T_Loss: 2.4586. Mask: 0.9492. :  70%|███████   | 35/50 [00:23<00:11,  1.35it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5997. T_Loss: 2.4586. Mask: 0.9492. :  72%|███████▏  | 36/50 [00:23<00:09,  1.54it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.6002. T_Loss: 2.4585. Mask: 0.9497. :  72%|███████▏  | 36/50 [00:24<00:09,  1.54it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.6002. T_Loss: 2.4585. Mask: 0.9497. :  74%|███████▍  | 37/50 [00:24<00:07,  1.83it/s]total : 1000  current step :  235
total : 1000  current step :  236
total : 1000  current step :  237
Train Iter: 238/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6001. T_Loss: 2.4669. Mask: 0.9496. :  74%|███████▍  | 37/50 [00:25<00:07,  1.83it/s]Train Iter: 238/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6001. T_Loss: 2.4669. Mask: 0.9496. :  76%|███████▌  | 38/50 [00:25<00:09,  1.26it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.6000. T_Loss: 2.4671. Mask: 0.9501. :  76%|███████▌  | 38/50 [00:25<00:09,  1.26it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.6000. T_Loss: 2.4671. Mask: 0.9501. :  78%|███████▊  | 39/50 [00:25<00:07,  1.49it/s]total : 1000  current step :  238
total : 1000  current step :  239
Train Iter: 240/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6007. T_Loss: 2.4706. Mask: 0.9508. :  78%|███████▊  | 39/50 [00:26<00:07,  1.49it/s]Train Iter: 240/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6007. T_Loss: 2.4706. Mask: 0.9508. :  80%|████████  | 40/50 [00:26<00:07,  1.42it/s]total : 1000  current step :  240
Train Iter: 241/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.6009. T_Loss: 2.4728. Mask: 0.9510. :  80%|████████  | 40/50 [00:27<00:07,  1.42it/s]Train Iter: 241/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.6009. T_Loss: 2.4728. Mask: 0.9510. :  82%|████████▏ | 41/50 [00:27<00:07,  1.16it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.6001. T_Loss: 2.4758. Mask: 0.9518. :  82%|████████▏ | 41/50 [00:28<00:07,  1.16it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.6001. T_Loss: 2.4758. Mask: 0.9518. :  84%|████████▍ | 42/50 [00:28<00:05,  1.35it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6009. T_Loss: 2.4791. Mask: 0.9517. :  84%|████████▍ | 42/50 [00:28<00:05,  1.35it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.6009. T_Loss: 2.4791. Mask: 0.9517. :  86%|████████▌ | 43/50 [00:28<00:04,  1.64it/s]total : 1000  current step :  241
total : 1000  current step :  242
total : 1000  current step :  243
Train Iter: 244/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6006. T_Loss: 2.4826. Mask: 0.9513. :  86%|████████▌ | 43/50 [00:29<00:04,  1.64it/s]Train Iter: 244/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6006. T_Loss: 2.4826. Mask: 0.9513. :  88%|████████▊ | 44/50 [00:29<00:04,  1.26it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.6005. T_Loss: 2.4857. Mask: 0.9515. :  88%|████████▊ | 44/50 [00:30<00:04,  1.26it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.6005. T_Loss: 2.4857. Mask: 0.9515. :  90%|█████████ | 45/50 [00:30<00:03,  1.60it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5999. T_Loss: 2.4883. Mask: 0.9519. :  90%|█████████ | 45/50 [00:30<00:03,  1.60it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.5999. T_Loss: 2.4883. Mask: 0.9519. :  92%|█████████▏| 46/50 [00:30<00:02,  1.83it/s]total : 1000  current step :  244
total : 1000  current step :  245
total : 1000  current step :  246
Train Iter: 247/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.5998. T_Loss: 2.4950. Mask: 0.9520. :  92%|█████████▏| 46/50 [00:31<00:02,  1.83it/s]Train Iter: 247/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.5998. T_Loss: 2.4950. Mask: 0.9520. :  94%|█████████▍| 47/50 [00:31<00:02,  1.29it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.6004. T_Loss: 2.4946. Mask: 0.9517. :  94%|█████████▍| 47/50 [00:32<00:02,  1.29it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.6004. T_Loss: 2.4946. Mask: 0.9517. :  96%|█████████▌| 48/50 [00:32<00:01,  1.52it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.6005. T_Loss: 2.4978. Mask: 0.9522. :  96%|█████████▌| 48/50 [00:32<00:01,  1.52it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.6005. T_Loss: 2.4978. Mask: 0.9522. :  98%|█████████▊| 49/50 [00:32<00:00,  1.69it/s]total : 1000  current step :  247
total : 1000  current step :  248
total : 1000  current step :  249
Train Iter: 250/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.6009. T_Loss: 2.4976. Mask: 0.9523. :  98%|█████████▊| 49/50 [00:33<00:00,  1.69it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.6009. T_Loss: 2.4976. Mask: 0.9523. : 100%|██████████| 50/50 [00:33<00:00,  1.27it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.6009. T_Loss: 2.4976. Mask: 0.9523. : 100%|██████████| 50/50 [00:33<00:00,  1.47it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.94s. Loss: 2.7125. top1: 0.00. top5: 1.17. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.94s. Loss: 2.7125. top1: 0.00. top5: 1.17. :  12%|█▎        | 1/8 [00:00<00:06,  1.07it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.61s. Loss: 2.7195. top1: 0.00. top5: 1.37. :  12%|█▎        | 1/8 [00:01<00:06,  1.07it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.61s. Loss: 2.7195. top1: 0.00. top5: 1.37. :  25%|██▌       | 2/8 [00:01<00:03,  1.81it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 2.7180. top1: 0.00. top5: 1.43. :  25%|██▌       | 2/8 [00:01<00:03,  1.81it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 2.7180. top1: 0.00. top5: 1.43. :  38%|███▊      | 3/8 [00:01<00:02,  2.33it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 2.7165. top1: 0.00. top5: 1.66. :  38%|███▊      | 3/8 [00:01<00:02,  2.33it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 2.7165. top1: 0.00. top5: 1.66. :  50%|█████     | 4/8 [00:01<00:01,  2.72it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 2.6895. top1: 0.00. top5: 3.91. :  50%|█████     | 4/8 [00:02<00:01,  2.72it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 2.6895. top1: 0.00. top5: 3.91. :  62%|██████▎   | 5/8 [00:02<00:00,  3.09it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.6681. top1: 0.00. top5: 5.14. :  62%|██████▎   | 5/8 [00:02<00:00,  3.09it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 2.6681. top1: 0.00. top5: 5.14. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.6579. top1: 0.00. top5: 5.86. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.6579. top1: 0.00. top5: 5.86. :  88%|████████▊ | 7/8 [00:02<00:00,  3.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 2.6504. top1: 0.00. top5: 6.05. :  88%|████████▊ | 7/8 [00:02<00:00,  3.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 2.6504. top1: 0.00. top5: 6.05. : 100%|██████████| 8/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 2.6504. top1: 0.00. top5: 6.05. : 100%|██████████| 8/8 [00:03<00:00,  2.58it/s]
total : 1000  current step :  250
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.27s. S_Loss: 2.6014. T_Loss: 2.5227. Mask: 0.9570. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.27s. S_Loss: 2.6014. T_Loss: 2.5227. Mask: 0.9570. :   2%|▏         | 1/50 [00:00<00:13,  3.67it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.24s. S_Loss: 2.6058. T_Loss: 2.6303. Mask: 0.9668. :   2%|▏         | 1/50 [00:00<00:13,  3.67it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.24s. S_Loss: 2.6058. T_Loss: 2.6303. Mask: 0.9668. :   4%|▍         | 2/50 [00:00<00:11,  4.17it/s]total : 1000  current step :  251
total : 1000  current step :  252
Train Iter: 253/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.6038. T_Loss: 2.6259. Mask: 0.9544. :   4%|▍         | 2/50 [00:01<00:11,  4.17it/s]Train Iter: 253/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.6038. T_Loss: 2.6259. Mask: 0.9544. :   6%|▌         | 3/50 [00:01<00:32,  1.43it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.28s. Batch: 0.50s. S_Loss: 2.6042. T_Loss: 2.6555. Mask: 0.9551. :   6%|▌         | 3/50 [00:02<00:32,  1.43it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.28s. Batch: 0.50s. S_Loss: 2.6042. T_Loss: 2.6555. Mask: 0.9551. :   8%|▊         | 4/50 [00:02<00:24,  1.86it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.24s. Batch: 0.46s. S_Loss: 2.5995. T_Loss: 2.6136. Mask: 0.9547. :   8%|▊         | 4/50 [00:02<00:24,  1.86it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.24s. Batch: 0.46s. S_Loss: 2.5995. T_Loss: 2.6136. Mask: 0.9547. :  10%|█         | 5/50 [00:02<00:19,  2.26it/s]total : 1000  current step :  253
total : 1000  current step :  254
total : 1000  current step :  255
Train Iter: 256/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.5976. T_Loss: 2.5929. Mask: 0.9557. :  10%|█         | 5/50 [00:03<00:19,  2.26it/s]Train Iter: 256/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.5976. T_Loss: 2.5929. Mask: 0.9557. :  12%|█▏        | 6/50 [00:03<00:31,  1.41it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.5949. T_Loss: 2.5750. Mask: 0.9554. :  12%|█▏        | 6/50 [00:03<00:31,  1.41it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.32s. Batch: 0.54s. S_Loss: 2.5949. T_Loss: 2.5750. Mask: 0.9554. :  14%|█▍        | 7/50 [00:03<00:24,  1.74it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.5958. T_Loss: 2.5561. Mask: 0.9570. :  14%|█▍        | 7/50 [00:04<00:24,  1.74it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.30s. Batch: 0.51s. S_Loss: 2.5958. T_Loss: 2.5561. Mask: 0.9570. :  16%|█▌        | 8/50 [00:04<00:20,  2.03it/s]total : 1000  current step :  256
total : 1000  current step :  257
total : 1000  current step :  258
Train Iter: 259/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.5956. T_Loss: 2.5814. Mask: 0.9575. :  16%|█▌        | 8/50 [00:05<00:20,  2.03it/s]Train Iter: 259/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.5956. T_Loss: 2.5814. Mask: 0.9575. :  18%|█▊        | 9/50 [00:05<00:30,  1.36it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.34s. Batch: 0.56s. S_Loss: 2.5908. T_Loss: 2.5956. Mask: 0.9574. :  18%|█▊        | 9/50 [00:05<00:30,  1.36it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.34s. Batch: 0.56s. S_Loss: 2.5908. T_Loss: 2.5956. Mask: 0.9574. :  20%|██        | 10/50 [00:05<00:22,  1.75it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.5889. T_Loss: 2.6358. Mask: 0.9556. :  20%|██        | 10/50 [00:06<00:22,  1.75it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.5889. T_Loss: 2.6358. Mask: 0.9556. :  22%|██▏       | 11/50 [00:06<00:20,  1.88it/s]total : 1000  current step :  259
total : 1000  current step :  260
total : 1000  current step :  261
Train Iter: 262/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.5889. T_Loss: 2.6659. Mask: 0.9557. :  22%|██▏       | 11/50 [00:07<00:20,  1.88it/s]Train Iter: 262/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.5889. T_Loss: 2.6659. Mask: 0.9557. :  24%|██▍       | 12/50 [00:07<00:28,  1.36it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5898. T_Loss: 2.6664. Mask: 0.9555. :  24%|██▍       | 12/50 [00:07<00:28,  1.36it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5898. T_Loss: 2.6664. Mask: 0.9555. :  26%|██▌       | 13/50 [00:07<00:21,  1.69it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.33s. Batch: 0.56s. S_Loss: 2.5932. T_Loss: 2.6567. Mask: 0.9554. :  26%|██▌       | 13/50 [00:07<00:21,  1.69it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.33s. Batch: 0.56s. S_Loss: 2.5932. T_Loss: 2.6567. Mask: 0.9554. :  28%|██▊       | 14/50 [00:07<00:18,  1.94it/s]total : 1000  current step :  262
total : 1000  current step :  263
total : 1000  current step :  264
Train Iter: 265/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5924. T_Loss: 2.6555. Mask: 0.9547. :  28%|██▊       | 14/50 [00:08<00:18,  1.94it/s]Train Iter: 265/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5924. T_Loss: 2.6555. Mask: 0.9547. :  30%|███       | 15/50 [00:08<00:24,  1.45it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5904. T_Loss: 2.6546. Mask: 0.9546. :  30%|███       | 15/50 [00:09<00:24,  1.45it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5904. T_Loss: 2.6546. Mask: 0.9546. :  32%|███▏      | 16/50 [00:09<00:19,  1.71it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.5889. T_Loss: 2.6570. Mask: 0.9538. :  32%|███▏      | 16/50 [00:09<00:19,  1.71it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.5889. T_Loss: 2.6570. Mask: 0.9538. :  34%|███▍      | 17/50 [00:09<00:17,  1.91it/s]total : 1000  current step :  265
total : 1000  current step :  266
total : 1000  current step :  267
Train Iter: 268/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.5864. T_Loss: 2.6594. Mask: 0.9531. :  34%|███▍      | 17/50 [00:10<00:17,  1.91it/s]Train Iter: 268/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.5864. T_Loss: 2.6594. Mask: 0.9531. :  36%|███▌      | 18/50 [00:10<00:23,  1.36it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5881. T_Loss: 2.6715. Mask: 0.9539. :  36%|███▌      | 18/50 [00:11<00:23,  1.36it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5881. T_Loss: 2.6715. Mask: 0.9539. :  38%|███▊      | 19/50 [00:11<00:18,  1.64it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.5880. T_Loss: 2.6691. Mask: 0.9533. :  38%|███▊      | 19/50 [00:11<00:18,  1.64it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.5880. T_Loss: 2.6691. Mask: 0.9533. :  40%|████      | 20/50 [00:11<00:15,  1.99it/s]total : 1000  current step :  268
total : 1000  current step :  269
total : 1000  current step :  270
Train Iter: 271/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.5901. T_Loss: 2.6668. Mask: 0.9533. :  40%|████      | 20/50 [00:12<00:15,  1.99it/s]Train Iter: 271/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.5901. T_Loss: 2.6668. Mask: 0.9533. :  42%|████▏     | 21/50 [00:12<00:19,  1.48it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5892. T_Loss: 2.6705. Mask: 0.9540. :  42%|████▏     | 21/50 [00:13<00:19,  1.48it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5892. T_Loss: 2.6705. Mask: 0.9540. :  44%|████▍     | 22/50 [00:13<00:17,  1.59it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.5891. T_Loss: 2.6842. Mask: 0.9545. :  44%|████▍     | 22/50 [00:13<00:17,  1.59it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.5891. T_Loss: 2.6842. Mask: 0.9545. :  46%|████▌     | 23/50 [00:13<00:13,  1.99it/s]total : 1000  current step :  271
total : 1000  current step :  272
total : 1000  current step :  273
Train Iter: 274/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.5900. T_Loss: 2.6880. Mask: 0.9546. :  46%|████▌     | 23/50 [00:14<00:13,  1.99it/s]Train Iter: 274/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.5900. T_Loss: 2.6880. Mask: 0.9546. :  48%|████▊     | 24/50 [00:14<00:18,  1.42it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5904. T_Loss: 2.6921. Mask: 0.9541. :  48%|████▊     | 24/50 [00:14<00:18,  1.42it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5904. T_Loss: 2.6921. Mask: 0.9541. :  50%|█████     | 25/50 [00:14<00:15,  1.65it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.5919. T_Loss: 2.6916. Mask: 0.9542. :  50%|█████     | 25/50 [00:15<00:15,  1.65it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.5919. T_Loss: 2.6916. Mask: 0.9542. :  52%|█████▏    | 26/50 [00:15<00:11,  2.03it/s]total : 1000  current step :  274
total : 1000  current step :  275
total : 1000  current step :  276
Train Iter: 277/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.5917. T_Loss: 2.6954. Mask: 0.9549. :  52%|█████▏    | 26/50 [00:16<00:11,  2.03it/s]Train Iter: 277/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.5917. T_Loss: 2.6954. Mask: 0.9549. :  54%|█████▍    | 27/50 [00:16<00:15,  1.46it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5909. T_Loss: 2.6923. Mask: 0.9558. :  54%|█████▍    | 27/50 [00:16<00:15,  1.46it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5909. T_Loss: 2.6923. Mask: 0.9558. :  56%|█████▌    | 28/50 [00:16<00:12,  1.72it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.5914. T_Loss: 2.6958. Mask: 0.9558. :  56%|█████▌    | 28/50 [00:16<00:12,  1.72it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.5914. T_Loss: 2.6958. Mask: 0.9558. :  58%|█████▊    | 29/50 [00:16<00:09,  2.12it/s]total : 1000  current step :  277
total : 1000  current step :  278
total : 1000  current step :  279
Train Iter: 280/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.5917. T_Loss: 2.6974. Mask: 0.9560. :  58%|█████▊    | 29/50 [00:18<00:09,  2.12it/s]Train Iter: 280/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.5917. T_Loss: 2.6974. Mask: 0.9560. :  60%|██████    | 30/50 [00:18<00:14,  1.41it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5920. T_Loss: 2.7008. Mask: 0.9558. :  60%|██████    | 30/50 [00:18<00:14,  1.41it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5920. T_Loss: 2.7008. Mask: 0.9558. :  62%|██████▏   | 31/50 [00:18<00:11,  1.59it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5916. T_Loss: 2.6969. Mask: 0.9553. :  62%|██████▏   | 31/50 [00:18<00:11,  1.59it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5916. T_Loss: 2.6969. Mask: 0.9553. :  64%|██████▍   | 32/50 [00:18<00:10,  1.74it/s]total : 1000  current step :  280
total : 1000  current step :  281
total : 1000  current step :  282
Train Iter: 283/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.5914. T_Loss: 2.7051. Mask: 0.9553. :  64%|██████▍   | 32/50 [00:19<00:10,  1.74it/s]Train Iter: 283/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.5914. T_Loss: 2.7051. Mask: 0.9553. :  66%|██████▌   | 33/50 [00:19<00:12,  1.40it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5914. T_Loss: 2.7037. Mask: 0.9551. :  66%|██████▌   | 33/50 [00:20<00:12,  1.40it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5914. T_Loss: 2.7037. Mask: 0.9551. :  68%|██████▊   | 34/50 [00:20<00:09,  1.70it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5913. T_Loss: 2.7085. Mask: 0.9555. :  68%|██████▊   | 34/50 [00:20<00:09,  1.70it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 2.5913. T_Loss: 2.7085. Mask: 0.9555. :  70%|███████   | 35/50 [00:20<00:07,  1.98it/s]total : 1000  current step :  283
total : 1000  current step :  284
total : 1000  current step :  285
Train Iter: 286/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.5901. T_Loss: 2.7178. Mask: 0.9561. :  70%|███████   | 35/50 [00:21<00:07,  1.98it/s]Train Iter: 286/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.5901. T_Loss: 2.7178. Mask: 0.9561. :  72%|███████▏  | 36/50 [00:21<00:09,  1.55it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5902. T_Loss: 2.7156. Mask: 0.9564. :  72%|███████▏  | 36/50 [00:21<00:09,  1.55it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5902. T_Loss: 2.7156. Mask: 0.9564. :  74%|███████▍  | 37/50 [00:21<00:07,  1.85it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5901. T_Loss: 2.7146. Mask: 0.9564. :  74%|███████▍  | 37/50 [00:22<00:07,  1.85it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5901. T_Loss: 2.7146. Mask: 0.9564. :  76%|███████▌  | 38/50 [00:22<00:06,  1.91it/s]total : 1000  current step :  286
total : 1000  current step :  287
total : 1000  current step :  288
Train Iter: 289/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5896. T_Loss: 2.7162. Mask: 0.9567. :  76%|███████▌  | 38/50 [00:23<00:06,  1.91it/s]Train Iter: 289/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5896. T_Loss: 2.7162. Mask: 0.9567. :  78%|███████▊  | 39/50 [00:23<00:06,  1.59it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5889. T_Loss: 2.7205. Mask: 0.9567. :  78%|███████▊  | 39/50 [00:23<00:06,  1.59it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5889. T_Loss: 2.7205. Mask: 0.9567. :  80%|████████  | 40/50 [00:23<00:05,  1.81it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5889. T_Loss: 2.7224. Mask: 0.9564. :  80%|████████  | 40/50 [00:23<00:05,  1.81it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5889. T_Loss: 2.7224. Mask: 0.9564. :  82%|████████▏ | 41/50 [00:23<00:04,  2.08it/s]total : 1000  current step :  289
total : 1000  current step :  290
total : 1000  current step :  291
Train Iter: 292/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.5894. T_Loss: 2.7201. Mask: 0.9562. :  82%|████████▏ | 41/50 [00:24<00:04,  2.08it/s]Train Iter: 292/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.5894. T_Loss: 2.7201. Mask: 0.9562. :  84%|████████▍ | 42/50 [00:24<00:05,  1.51it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5890. T_Loss: 2.7224. Mask: 0.9560. :  84%|████████▍ | 42/50 [00:25<00:05,  1.51it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5890. T_Loss: 2.7224. Mask: 0.9560. :  86%|████████▌ | 43/50 [00:25<00:03,  1.77it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.5886. T_Loss: 2.7259. Mask: 0.9564. :  86%|████████▌ | 43/50 [00:25<00:03,  1.77it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.5886. T_Loss: 2.7259. Mask: 0.9564. :  88%|████████▊ | 44/50 [00:25<00:02,  2.00it/s]total : 1000  current step :  292
total : 1000  current step :  293
total : 1000  current step :  294
Train Iter: 295/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.5889. T_Loss: 2.7307. Mask: 0.9566. :  88%|████████▊ | 44/50 [00:26<00:02,  2.00it/s]Train Iter: 295/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.5889. T_Loss: 2.7307. Mask: 0.9566. :  90%|█████████ | 45/50 [00:26<00:03,  1.49it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5890. T_Loss: 2.7299. Mask: 0.9567. :  90%|█████████ | 45/50 [00:27<00:03,  1.49it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5890. T_Loss: 2.7299. Mask: 0.9567. :  92%|█████████▏| 46/50 [00:27<00:02,  1.66it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5891. T_Loss: 2.7260. Mask: 0.9566. :  92%|█████████▏| 46/50 [00:27<00:02,  1.66it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5891. T_Loss: 2.7260. Mask: 0.9566. :  94%|█████████▍| 47/50 [00:27<00:01,  1.91it/s]total : 1000  current step :  295
total : 1000  current step :  296
total : 1000  current step :  297
Train Iter: 298/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5896. T_Loss: 2.7310. Mask: 0.9569. :  94%|█████████▍| 47/50 [00:28<00:01,  1.91it/s]Train Iter: 298/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5896. T_Loss: 2.7310. Mask: 0.9569. :  96%|█████████▌| 48/50 [00:28<00:01,  1.44it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5893. T_Loss: 2.7380. Mask: 0.9563. :  96%|█████████▌| 48/50 [00:28<00:01,  1.44it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.5893. T_Loss: 2.7380. Mask: 0.9563. :  98%|█████████▊| 49/50 [00:28<00:00,  1.71it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5892. T_Loss: 2.7340. Mask: 0.9565. :  98%|█████████▊| 49/50 [00:29<00:00,  1.71it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5892. T_Loss: 2.7340. Mask: 0.9565. : 100%|██████████| 50/50 [00:29<00:00,  1.92it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.35s. Batch: 0.58s. S_Loss: 2.5892. T_Loss: 2.7340. Mask: 0.9565. : 100%|██████████| 50/50 [00:29<00:00,  1.71it/s]
total : 1000  current step :  298
total : 1000  current step :  299
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 2.6321. top1: 0.00. top5: 1.17. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 2.6321. top1: 0.00. top5: 1.17. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 2.6355. top1: 0.00. top5: 1.56. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 2.6355. top1: 0.00. top5: 1.56. :  25%|██▌       | 2/8 [00:00<00:02,  2.55it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.6345. top1: 0.00. top5: 1.95. :  25%|██▌       | 2/8 [00:01<00:02,  2.55it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.6345. top1: 0.00. top5: 1.95. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.6318. top1: 0.00. top5: 2.05. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 2.6318. top1: 0.00. top5: 2.05. :  50%|█████     | 4/8 [00:01<00:01,  3.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.5993. top1: 0.00. top5: 6.09. :  50%|█████     | 4/8 [00:01<00:01,  3.21it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.5993. top1: 0.00. top5: 6.09. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.5752. top1: 0.00. top5: 8.40. :  62%|██████▎   | 5/8 [00:01<00:00,  3.58it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 2.5752. top1: 0.00. top5: 8.40. :  75%|███████▌  | 6/8 [00:01<00:00,  3.82it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.5625. top1: 0.00. top5: 9.99. :  75%|███████▌  | 6/8 [00:02<00:00,  3.82it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 2.5625. top1: 0.00. top5: 9.99. :  88%|████████▊ | 7/8 [00:02<00:00,  3.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.5535. top1: 0.00. top5: 10.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.5535. top1: 0.00. top5: 10.80. : 100%|██████████| 8/8 [00:02<00:00,  4.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 2.5535. top1: 0.00. top5: 10.80. : 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]
total : 1000  current step :  300
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.87s. Batch: 1.10s. S_Loss: 2.6002. T_Loss: 2.6704. Mask: 0.9844. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.87s. Batch: 1.10s. S_Loss: 2.6002. T_Loss: 2.6704. Mask: 0.9844. :   2%|▏         | 1/50 [00:01<00:53,  1.10s/it]Train Iter: 302/1000. LR: 0.0189. Data: 0.52s. Batch: 0.75s. S_Loss: 2.1036. T_Loss: 2.4799. Mask: 0.9707. :   2%|▏         | 1/50 [00:01<00:53,  1.10s/it]Train Iter: 302/1000. LR: 0.0189. Data: 0.52s. Batch: 0.75s. S_Loss: 2.1036. T_Loss: 2.4799. Mask: 0.9707. :   4%|▍         | 2/50 [00:01<00:32,  1.46it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.37s. Batch: 0.59s. S_Loss: 1.8444. T_Loss: 2.5043. Mask: 0.9635. :   4%|▍         | 2/50 [00:01<00:32,  1.46it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.37s. Batch: 0.59s. S_Loss: 1.8444. T_Loss: 2.5043. Mask: 0.9635. :   6%|▌         | 3/50 [00:01<00:23,  2.02it/s]total : 1000  current step :  301
total : 1000  current step :  302
total : 1000  current step :  303
Train Iter: 304/1000. LR: 0.0190. Data: 0.51s. Batch: 0.73s. S_Loss: 1.7075. T_Loss: 2.5572. Mask: 0.9600. :   6%|▌         | 3/50 [00:02<00:23,  2.02it/s]Train Iter: 304/1000. LR: 0.0190. Data: 0.51s. Batch: 0.73s. S_Loss: 1.7075. T_Loss: 2.5572. Mask: 0.9600. :   8%|▊         | 4/50 [00:02<00:34,  1.32it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.42s. Batch: 0.65s. S_Loss: 1.6371. T_Loss: 2.6130. Mask: 0.9594. :   8%|▊         | 4/50 [00:03<00:34,  1.32it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.42s. Batch: 0.65s. S_Loss: 1.6371. T_Loss: 2.6130. Mask: 0.9594. :  10%|█         | 5/50 [00:03<00:27,  1.64it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5909. T_Loss: 2.5906. Mask: 0.9583. :  10%|█         | 5/50 [00:03<00:27,  1.64it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5909. T_Loss: 2.5906. Mask: 0.9583. :  12%|█▏        | 6/50 [00:03<00:22,  1.97it/s]total : 1000  current step :  304
total : 1000  current step :  305
total : 1000  current step :  306
Train Iter: 307/1000. LR: 0.0192. Data: 0.45s. Batch: 0.69s. S_Loss: 1.5694. T_Loss: 2.6309. Mask: 0.9587. :  12%|█▏        | 6/50 [00:04<00:22,  1.97it/s]Train Iter: 307/1000. LR: 0.0192. Data: 0.45s. Batch: 0.69s. S_Loss: 1.5694. T_Loss: 2.6309. Mask: 0.9587. :  14%|█▍        | 7/50 [00:04<00:32,  1.33it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.41s. Batch: 0.66s. S_Loss: 1.5673. T_Loss: 2.6662. Mask: 0.9585. :  14%|█▍        | 7/50 [00:05<00:32,  1.33it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.41s. Batch: 0.66s. S_Loss: 1.5673. T_Loss: 2.6662. Mask: 0.9585. :  16%|█▌        | 8/50 [00:05<00:27,  1.54it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.36s. Batch: 0.60s. S_Loss: 1.5707. T_Loss: 2.6930. Mask: 0.9596. :  16%|█▌        | 8/50 [00:05<00:27,  1.54it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.36s. Batch: 0.60s. S_Loss: 1.5707. T_Loss: 2.6930. Mask: 0.9596. :  18%|█▊        | 9/50 [00:05<00:20,  1.97it/s]total : 1000  current step :  307
total : 1000  current step :  308
total : 1000  current step :  309
Train Iter: 310/1000. LR: 0.0194. Data: 0.41s. Batch: 0.65s. S_Loss: 1.5777. T_Loss: 2.7060. Mask: 0.9602. :  18%|█▊        | 9/50 [00:06<00:20,  1.97it/s]Train Iter: 310/1000. LR: 0.0194. Data: 0.41s. Batch: 0.65s. S_Loss: 1.5777. T_Loss: 2.7060. Mask: 0.9602. :  20%|██        | 10/50 [00:06<00:26,  1.48it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.37s. Batch: 0.62s. S_Loss: 1.5860. T_Loss: 2.7360. Mask: 0.9602. :  20%|██        | 10/50 [00:06<00:26,  1.48it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.37s. Batch: 0.62s. S_Loss: 1.5860. T_Loss: 2.7360. Mask: 0.9602. :  22%|██▏       | 11/50 [00:06<00:21,  1.79it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.34s. Batch: 0.58s. S_Loss: 1.5931. T_Loss: 2.7446. Mask: 0.9590. :  22%|██▏       | 11/50 [00:07<00:21,  1.79it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.34s. Batch: 0.58s. S_Loss: 1.5931. T_Loss: 2.7446. Mask: 0.9590. :  24%|██▍       | 12/50 [00:07<00:17,  2.23it/s]total : 1000  current step :  310
total : 1000  current step :  311
total : 1000  current step :  312
Train Iter: 313/1000. LR: 0.0196. Data: 0.38s. Batch: 0.62s. S_Loss: 1.6018. T_Loss: 2.7919. Mask: 0.9594. :  24%|██▍       | 12/50 [00:08<00:17,  2.23it/s]Train Iter: 313/1000. LR: 0.0196. Data: 0.38s. Batch: 0.62s. S_Loss: 1.6018. T_Loss: 2.7919. Mask: 0.9594. :  26%|██▌       | 13/50 [00:08<00:23,  1.55it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.36s. Batch: 0.60s. S_Loss: 1.6082. T_Loss: 2.8149. Mask: 0.9593. :  26%|██▌       | 13/50 [00:08<00:23,  1.55it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.36s. Batch: 0.60s. S_Loss: 1.6082. T_Loss: 2.8149. Mask: 0.9593. :  28%|██▊       | 14/50 [00:08<00:19,  1.82it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.34s. Batch: 0.58s. S_Loss: 1.6158. T_Loss: 2.8126. Mask: 0.9578. :  28%|██▊       | 14/50 [00:08<00:19,  1.82it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.34s. Batch: 0.58s. S_Loss: 1.6158. T_Loss: 2.8126. Mask: 0.9578. :  30%|███       | 15/50 [00:08<00:16,  2.15it/s]total : 1000  current step :  313
total : 1000  current step :  314
total : 1000  current step :  315
Train Iter: 316/1000. LR: 0.0198. Data: 0.37s. Batch: 0.61s. S_Loss: 1.6221. T_Loss: 2.8375. Mask: 0.9587. :  30%|███       | 15/50 [00:09<00:16,  2.15it/s]Train Iter: 316/1000. LR: 0.0198. Data: 0.37s. Batch: 0.61s. S_Loss: 1.6221. T_Loss: 2.8375. Mask: 0.9587. :  32%|███▏      | 16/50 [00:09<00:22,  1.48it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.36s. Batch: 0.61s. S_Loss: 1.6236. T_Loss: 2.8524. Mask: 0.9582. :  32%|███▏      | 16/50 [00:10<00:22,  1.48it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.36s. Batch: 0.61s. S_Loss: 1.6236. T_Loss: 2.8524. Mask: 0.9582. :  34%|███▍      | 17/50 [00:10<00:20,  1.59it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.34s. Batch: 0.59s. S_Loss: 1.6238. T_Loss: 2.8410. Mask: 0.9581. :  34%|███▍      | 17/50 [00:10<00:20,  1.59it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.34s. Batch: 0.59s. S_Loss: 1.6238. T_Loss: 2.8410. Mask: 0.9581. :  36%|███▌      | 18/50 [00:10<00:16,  1.91it/s]total : 1000  current step :  316
total : 1000  current step :  317
total : 1000  current step :  318
Train Iter: 319/1000. LR: 0.0199. Data: 0.37s. Batch: 0.62s. S_Loss: 1.6218. T_Loss: 2.8453. Mask: 0.9589. :  36%|███▌      | 18/50 [00:11<00:16,  1.91it/s]Train Iter: 319/1000. LR: 0.0199. Data: 0.37s. Batch: 0.62s. S_Loss: 1.6218. T_Loss: 2.8453. Mask: 0.9589. :  38%|███▊      | 19/50 [00:11<00:22,  1.41it/s]total : 1000  current step :  319
Train Iter: 320/1000. LR: 0.0200. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6177. T_Loss: 2.8370. Mask: 0.9590. :  38%|███▊      | 19/50 [00:12<00:22,  1.41it/s]Train Iter: 320/1000. LR: 0.0200. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6177. T_Loss: 2.8370. Mask: 0.9590. :  40%|████      | 20/50 [00:12<00:21,  1.39it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6153. T_Loss: 2.8273. Mask: 0.9589. :  40%|████      | 20/50 [00:13<00:21,  1.39it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6153. T_Loss: 2.8273. Mask: 0.9589. :  42%|████▏     | 21/50 [00:13<00:20,  1.41it/s]total : 1000  current step :  320
total : 1000  current step :  321
Train Iter: 322/1000. LR: 0.0201. Data: 0.40s. Batch: 0.65s. S_Loss: 1.6112. T_Loss: 2.8292. Mask: 0.9586. :  42%|████▏     | 21/50 [00:14<00:20,  1.41it/s]Train Iter: 322/1000. LR: 0.0201. Data: 0.40s. Batch: 0.65s. S_Loss: 1.6112. T_Loss: 2.8292. Mask: 0.9586. :  44%|████▍     | 22/50 [00:14<00:22,  1.25it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.38s. Batch: 0.62s. S_Loss: 1.6067. T_Loss: 2.8150. Mask: 0.9589. :  44%|████▍     | 22/50 [00:14<00:22,  1.25it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.38s. Batch: 0.62s. S_Loss: 1.6067. T_Loss: 2.8150. Mask: 0.9589. :  46%|████▌     | 23/50 [00:14<00:16,  1.64it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.38s. Batch: 0.62s. S_Loss: 1.6017. T_Loss: 2.8192. Mask: 0.9588. :  46%|████▌     | 23/50 [00:14<00:16,  1.64it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.38s. Batch: 0.62s. S_Loss: 1.6017. T_Loss: 2.8192. Mask: 0.9588. :  48%|████▊     | 24/50 [00:14<00:14,  1.80it/s]total : 1000  current step :  322
total : 1000  current step :  323
total : 1000  current step :  324
Train Iter: 325/1000. LR: 0.0203. Data: 0.40s. Batch: 0.63s. S_Loss: 1.5964. T_Loss: 2.8294. Mask: 0.9581. :  48%|████▊     | 24/50 [00:15<00:14,  1.80it/s]Train Iter: 325/1000. LR: 0.0203. Data: 0.40s. Batch: 0.63s. S_Loss: 1.5964. T_Loss: 2.8294. Mask: 0.9581. :  50%|█████     | 25/50 [00:15<00:17,  1.45it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.38s. Batch: 0.62s. S_Loss: 1.5892. T_Loss: 2.8340. Mask: 0.9585. :  50%|█████     | 25/50 [00:16<00:17,  1.45it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.38s. Batch: 0.62s. S_Loss: 1.5892. T_Loss: 2.8340. Mask: 0.9585. :  52%|█████▏    | 26/50 [00:16<00:13,  1.79it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.38s. Batch: 0.61s. S_Loss: 1.5818. T_Loss: 2.8377. Mask: 0.9585. :  52%|█████▏    | 26/50 [00:16<00:13,  1.79it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.38s. Batch: 0.61s. S_Loss: 1.5818. T_Loss: 2.8377. Mask: 0.9585. :  54%|█████▍    | 27/50 [00:16<00:11,  2.06it/s]total : 1000  current step :  325
total : 1000  current step :  326
total : 1000  current step :  327
Train Iter: 328/1000. LR: 0.0205. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5729. T_Loss: 2.8405. Mask: 0.9583. :  54%|█████▍    | 27/50 [00:17<00:11,  2.06it/s]Train Iter: 328/1000. LR: 0.0205. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5729. T_Loss: 2.8405. Mask: 0.9583. :  56%|█████▌    | 28/50 [00:17<00:15,  1.45it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.38s. Batch: 0.61s. S_Loss: 1.5638. T_Loss: 2.8498. Mask: 0.9582. :  56%|█████▌    | 28/50 [00:17<00:15,  1.45it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.38s. Batch: 0.61s. S_Loss: 1.5638. T_Loss: 2.8498. Mask: 0.9582. :  58%|█████▊    | 29/50 [00:17<00:11,  1.84it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5546. T_Loss: 2.8439. Mask: 0.9576. :  58%|█████▊    | 29/50 [00:18<00:11,  1.84it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5546. T_Loss: 2.8439. Mask: 0.9576. :  60%|██████    | 30/50 [00:18<00:09,  2.08it/s]total : 1000  current step :  328
total : 1000  current step :  329
total : 1000  current step :  330
Train Iter: 331/1000. LR: 0.0207. Data: 0.39s. Batch: 0.62s. S_Loss: 1.5445. T_Loss: 2.8389. Mask: 0.9578. :  60%|██████    | 30/50 [00:19<00:09,  2.08it/s]Train Iter: 331/1000. LR: 0.0207. Data: 0.39s. Batch: 0.62s. S_Loss: 1.5445. T_Loss: 2.8389. Mask: 0.9578. :  62%|██████▏   | 31/50 [00:19<00:12,  1.52it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.38s. Batch: 0.61s. S_Loss: 1.5347. T_Loss: 2.8357. Mask: 0.9583. :  62%|██████▏   | 31/50 [00:19<00:12,  1.52it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.38s. Batch: 0.61s. S_Loss: 1.5347. T_Loss: 2.8357. Mask: 0.9583. :  64%|██████▍   | 32/50 [00:19<00:09,  1.82it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5256. T_Loss: 2.8383. Mask: 0.9586. :  64%|██████▍   | 32/50 [00:19<00:09,  1.82it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.37s. Batch: 0.60s. S_Loss: 1.5256. T_Loss: 2.8383. Mask: 0.9586. :  66%|██████▌   | 33/50 [00:19<00:08,  2.02it/s]total : 1000  current step :  331
total : 1000  current step :  332
total : 1000  current step :  333
Train Iter: 334/1000. LR: 0.0209. Data: 0.39s. Batch: 0.61s. S_Loss: 1.5164. T_Loss: 2.8478. Mask: 0.9592. :  66%|██████▌   | 33/50 [00:20<00:08,  2.02it/s]Train Iter: 334/1000. LR: 0.0209. Data: 0.39s. Batch: 0.61s. S_Loss: 1.5164. T_Loss: 2.8478. Mask: 0.9592. :  68%|██████▊   | 34/50 [00:20<00:10,  1.56it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.38s. Batch: 0.60s. S_Loss: 1.5075. T_Loss: 2.8626. Mask: 0.9589. :  68%|██████▊   | 34/50 [00:21<00:10,  1.56it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.38s. Batch: 0.60s. S_Loss: 1.5075. T_Loss: 2.8626. Mask: 0.9589. :  70%|███████   | 35/50 [00:21<00:08,  1.79it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4985. T_Loss: 2.8684. Mask: 0.9591. :  70%|███████   | 35/50 [00:21<00:08,  1.79it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4985. T_Loss: 2.8684. Mask: 0.9591. :  72%|███████▏  | 36/50 [00:21<00:06,  2.11it/s]total : 1000  current step :  334
total : 1000  current step :  335
total : 1000  current step :  336
Train Iter: 337/1000. LR: 0.0211. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4895. T_Loss: 2.8685. Mask: 0.9590. :  72%|███████▏  | 36/50 [00:22<00:06,  2.11it/s]Train Iter: 337/1000. LR: 0.0211. Data: 0.39s. Batch: 0.61s. S_Loss: 1.4895. T_Loss: 2.8685. Mask: 0.9590. :  74%|███████▍  | 37/50 [00:22<00:08,  1.51it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4815. T_Loss: 2.8835. Mask: 0.9591. :  74%|███████▍  | 37/50 [00:22<00:08,  1.51it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4815. T_Loss: 2.8835. Mask: 0.9591. :  76%|███████▌  | 38/50 [00:22<00:06,  1.78it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4725. T_Loss: 2.8847. Mask: 0.9591. :  76%|███████▌  | 38/50 [00:23<00:06,  1.78it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4725. T_Loss: 2.8847. Mask: 0.9591. :  78%|███████▊  | 39/50 [00:23<00:05,  2.18it/s]total : 1000  current step :  337
total : 1000  current step :  338
total : 1000  current step :  339
Train Iter: 340/1000. LR: 0.0213. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4645. T_Loss: 2.8873. Mask: 0.9591. :  78%|███████▊  | 39/50 [00:24<00:05,  2.18it/s]Train Iter: 340/1000. LR: 0.0213. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4645. T_Loss: 2.8873. Mask: 0.9591. :  80%|████████  | 40/50 [00:24<00:06,  1.61it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.37s. Batch: 0.60s. S_Loss: 1.4575. T_Loss: 2.8924. Mask: 0.9585. :  80%|████████  | 40/50 [00:24<00:06,  1.61it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.37s. Batch: 0.60s. S_Loss: 1.4575. T_Loss: 2.8924. Mask: 0.9585. :  82%|████████▏ | 41/50 [00:24<00:05,  1.80it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4490. T_Loss: 2.8964. Mask: 0.9590. :  82%|████████▏ | 41/50 [00:24<00:05,  1.80it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4490. T_Loss: 2.8964. Mask: 0.9590. :  84%|████████▍ | 42/50 [00:24<00:04,  1.97it/s]total : 1000  current step :  340
total : 1000  current step :  341
total : 1000  current step :  342
Train Iter: 343/1000. LR: 0.0214. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4417. T_Loss: 2.8986. Mask: 0.9591. :  84%|████████▍ | 42/50 [00:25<00:04,  1.97it/s]Train Iter: 343/1000. LR: 0.0214. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4417. T_Loss: 2.8986. Mask: 0.9591. :  86%|████████▌ | 43/50 [00:25<00:04,  1.51it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4346. T_Loss: 2.8926. Mask: 0.9593. :  86%|████████▌ | 43/50 [00:26<00:04,  1.51it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4346. T_Loss: 2.8926. Mask: 0.9593. :  88%|████████▊ | 44/50 [00:26<00:03,  1.74it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4284. T_Loss: 2.8957. Mask: 0.9593. :  88%|████████▊ | 44/50 [00:26<00:03,  1.74it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4284. T_Loss: 2.8957. Mask: 0.9593. :  90%|█████████ | 45/50 [00:26<00:02,  2.04it/s]total : 1000  current step :  343
total : 1000  current step :  344
total : 1000  current step :  345
Train Iter: 346/1000. LR: 0.0216. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4215. T_Loss: 2.8952. Mask: 0.9598. :  90%|█████████ | 45/50 [00:27<00:02,  2.04it/s]Train Iter: 346/1000. LR: 0.0216. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4215. T_Loss: 2.8952. Mask: 0.9598. :  92%|█████████▏| 46/50 [00:27<00:02,  1.50it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4148. T_Loss: 2.8994. Mask: 0.9600. :  92%|█████████▏| 46/50 [00:27<00:02,  1.50it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4148. T_Loss: 2.8994. Mask: 0.9600. :  94%|█████████▍| 47/50 [00:27<00:01,  1.84it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4083. T_Loss: 2.8990. Mask: 0.9604. :  94%|█████████▍| 47/50 [00:28<00:01,  1.84it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.37s. Batch: 0.59s. S_Loss: 1.4083. T_Loss: 2.8990. Mask: 0.9604. :  96%|█████████▌| 48/50 [00:28<00:00,  2.14it/s]total : 1000  current step :  346
total : 1000  current step :  347
total : 1000  current step :  348
Train Iter: 349/1000. LR: 0.0218. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4024. T_Loss: 2.9026. Mask: 0.9602. :  96%|█████████▌| 48/50 [00:29<00:00,  2.14it/s]Train Iter: 349/1000. LR: 0.0218. Data: 0.38s. Batch: 0.60s. S_Loss: 1.4024. T_Loss: 2.9026. Mask: 0.9602. :  98%|█████████▊| 49/50 [00:29<00:00,  1.50it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.37s. Batch: 0.59s. S_Loss: 1.3963. T_Loss: 2.9098. Mask: 0.9604. :  98%|█████████▊| 49/50 [00:29<00:00,  1.50it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.37s. Batch: 0.59s. S_Loss: 1.3963. T_Loss: 2.9098. Mask: 0.9604. : 100%|██████████| 50/50 [00:29<00:00,  1.70it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.37s. Batch: 0.59s. S_Loss: 1.3963. T_Loss: 2.9098. Mask: 0.9604. : 100%|██████████| 50/50 [00:29<00:00,  1.68it/s]
total : 1000  current step :  349
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 1.6050. top1: 77.73. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 1.6050. top1: 77.73. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.5953. top1: 78.52. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.5953. top1: 78.52. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.5947. top1: 78.52. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.66it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.5947. top1: 78.52. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.20it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.5911. top1: 79.59. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.20it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.5911. top1: 79.59. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.44it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.5953. top1: 78.44. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.44it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.5953. top1: 78.44. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.67it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.5990. top1: 77.15. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.67it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.5990. top1: 77.15. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.6009. top1: 77.23. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.6009. top1: 77.23. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.6022. top1: 77.25. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.6022. top1: 77.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  4.00it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.6022. top1: 77.25. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.19it/s]
total : 1000  current step :  350
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.25s. S_Loss: 1.0967. T_Loss: 3.0864. Mask: 0.9727. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.25s. S_Loss: 1.0967. T_Loss: 3.0864. Mask: 0.9727. :   2%|▏         | 1/50 [00:00<00:12,  3.91it/s]total : 1000  current step :  351
Train Iter: 352/1000. LR: 0.0220. Data: 0.40s. Batch: 0.65s. S_Loss: 1.1362. T_Loss: 3.1987. Mask: 0.9590. :   2%|▏         | 1/50 [00:01<00:12,  3.91it/s]Train Iter: 352/1000. LR: 0.0220. Data: 0.40s. Batch: 0.65s. S_Loss: 1.1362. T_Loss: 3.1987. Mask: 0.9590. :   4%|▍         | 2/50 [00:01<00:34,  1.39it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.29s. Batch: 0.55s. S_Loss: 1.1401. T_Loss: 3.2663. Mask: 0.9622. :   4%|▍         | 2/50 [00:01<00:34,  1.39it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.29s. Batch: 0.55s. S_Loss: 1.1401. T_Loss: 3.2663. Mask: 0.9622. :   6%|▌         | 3/50 [00:01<00:25,  1.82it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.22s. Batch: 0.48s. S_Loss: 1.1265. T_Loss: 3.1257. Mask: 0.9590. :   6%|▌         | 3/50 [00:01<00:25,  1.82it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.22s. Batch: 0.48s. S_Loss: 1.1265. T_Loss: 3.1257. Mask: 0.9590. :   8%|▊         | 4/50 [00:01<00:20,  2.24it/s]total : 1000  current step :  352
total : 1000  current step :  353
total : 1000  current step :  354
Train Iter: 355/1000. LR: 0.0222. Data: 0.36s. Batch: 0.62s. S_Loss: 1.1282. T_Loss: 3.1709. Mask: 0.9641. :   8%|▊         | 4/50 [00:03<00:20,  2.24it/s]Train Iter: 355/1000. LR: 0.0222. Data: 0.36s. Batch: 0.62s. S_Loss: 1.1282. T_Loss: 3.1709. Mask: 0.9641. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.30s. Batch: 0.57s. S_Loss: 1.1231. T_Loss: 3.1976. Mask: 0.9655. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.30s. Batch: 0.57s. S_Loss: 1.1231. T_Loss: 3.1976. Mask: 0.9655. :  12%|█▏        | 6/50 [00:03<00:25,  1.72it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.27s. Batch: 0.54s. S_Loss: 1.1186. T_Loss: 3.1666. Mask: 0.9654. :  12%|█▏        | 6/50 [00:03<00:25,  1.72it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.27s. Batch: 0.54s. S_Loss: 1.1186. T_Loss: 3.1666. Mask: 0.9654. :  14%|█▍        | 7/50 [00:03<00:21,  2.02it/s]total : 1000  current step :  355
total : 1000  current step :  356
total : 1000  current step :  357
Train Iter: 358/1000. LR: 0.0224. Data: 0.35s. Batch: 0.61s. S_Loss: 1.1102. T_Loss: 3.1386. Mask: 0.9634. :  14%|█▍        | 7/50 [00:04<00:21,  2.02it/s]Train Iter: 358/1000. LR: 0.0224. Data: 0.35s. Batch: 0.61s. S_Loss: 1.1102. T_Loss: 3.1386. Mask: 0.9634. :  16%|█▌        | 8/50 [00:04<00:29,  1.42it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.33s. Batch: 0.58s. S_Loss: 1.1045. T_Loss: 3.1244. Mask: 0.9631. :  16%|█▌        | 8/50 [00:05<00:29,  1.42it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.33s. Batch: 0.58s. S_Loss: 1.1045. T_Loss: 3.1244. Mask: 0.9631. :  18%|█▊        | 9/50 [00:05<00:23,  1.71it/s]total : 1000  current step :  358
total : 1000  current step :  359
Train Iter: 360/1000. LR: 0.0225. Data: 0.35s. Batch: 0.59s. S_Loss: 1.0996. T_Loss: 3.1341. Mask: 0.9652. :  18%|█▊        | 9/50 [00:05<00:23,  1.71it/s]Train Iter: 360/1000. LR: 0.0225. Data: 0.35s. Batch: 0.59s. S_Loss: 1.0996. T_Loss: 3.1341. Mask: 0.9652. :  20%|██        | 10/50 [00:05<00:24,  1.60it/s]total : 1000  current step :  360
Train Iter: 361/1000. LR: 0.0226. Data: 0.42s. Batch: 0.67s. S_Loss: 1.0933. T_Loss: 3.1062. Mask: 0.9663. :  20%|██        | 10/50 [00:07<00:24,  1.60it/s]Train Iter: 361/1000. LR: 0.0226. Data: 0.42s. Batch: 0.67s. S_Loss: 1.0933. T_Loss: 3.1062. Mask: 0.9663. :  22%|██▏       | 11/50 [00:07<00:34,  1.14it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0903. T_Loss: 3.0981. Mask: 0.9668. :  22%|██▏       | 11/50 [00:07<00:34,  1.14it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0903. T_Loss: 3.0981. Mask: 0.9668. :  24%|██▍       | 12/50 [00:07<00:27,  1.41it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.38s. Batch: 0.62s. S_Loss: 1.0868. T_Loss: 3.0889. Mask: 0.9675. :  24%|██▍       | 12/50 [00:08<00:27,  1.41it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.38s. Batch: 0.62s. S_Loss: 1.0868. T_Loss: 3.0889. Mask: 0.9675. :  26%|██▌       | 13/50 [00:08<00:21,  1.71it/s]total : 1000  current step :  361
total : 1000  current step :  362
total : 1000  current step :  363
Train Iter: 364/1000. LR: 0.0228. Data: 0.42s. Batch: 0.67s. S_Loss: 1.0858. T_Loss: 3.1017. Mask: 0.9682. :  26%|██▌       | 13/50 [00:09<00:21,  1.71it/s]Train Iter: 364/1000. LR: 0.0228. Data: 0.42s. Batch: 0.67s. S_Loss: 1.0858. T_Loss: 3.1017. Mask: 0.9682. :  28%|██▊       | 14/50 [00:09<00:29,  1.23it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.40s. Batch: 0.65s. S_Loss: 1.0848. T_Loss: 3.1375. Mask: 0.9659. :  28%|██▊       | 14/50 [00:09<00:29,  1.23it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.40s. Batch: 0.65s. S_Loss: 1.0848. T_Loss: 3.1375. Mask: 0.9659. :  30%|███       | 15/50 [00:09<00:23,  1.49it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.38s. Batch: 0.63s. S_Loss: 1.0846. T_Loss: 3.1364. Mask: 0.9646. :  30%|███       | 15/50 [00:10<00:23,  1.49it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.38s. Batch: 0.63s. S_Loss: 1.0846. T_Loss: 3.1364. Mask: 0.9646. :  32%|███▏      | 16/50 [00:10<00:19,  1.77it/s]total : 1000  current step :  364
total : 1000  current step :  365
total : 1000  current step :  366
Train Iter: 367/1000. LR: 0.0229. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0847. T_Loss: 3.1546. Mask: 0.9642. :  32%|███▏      | 16/50 [00:11<00:19,  1.77it/s]Train Iter: 367/1000. LR: 0.0229. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0847. T_Loss: 3.1546. Mask: 0.9642. :  34%|███▍      | 17/50 [00:11<00:25,  1.28it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0823. T_Loss: 3.1491. Mask: 0.9633. :  34%|███▍      | 17/50 [00:11<00:25,  1.28it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0823. T_Loss: 3.1491. Mask: 0.9633. :  36%|███▌      | 18/50 [00:11<00:21,  1.50it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.40s. Batch: 0.63s. S_Loss: 1.0793. T_Loss: 3.1361. Mask: 0.9622. :  36%|███▌      | 18/50 [00:12<00:21,  1.50it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.40s. Batch: 0.63s. S_Loss: 1.0793. T_Loss: 3.1361. Mask: 0.9622. :  38%|███▊      | 19/50 [00:12<00:17,  1.75it/s]total : 1000  current step :  367
total : 1000  current step :  368
total : 1000  current step :  369
Train Iter: 370/1000. LR: 0.0231. Data: 0.43s. Batch: 0.67s. S_Loss: 1.0793. T_Loss: 3.1255. Mask: 0.9625. :  38%|███▊      | 19/50 [00:13<00:17,  1.75it/s]Train Iter: 370/1000. LR: 0.0231. Data: 0.43s. Batch: 0.67s. S_Loss: 1.0793. T_Loss: 3.1255. Mask: 0.9625. :  40%|████      | 20/50 [00:13<00:23,  1.27it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0779. T_Loss: 3.1268. Mask: 0.9617. :  40%|████      | 20/50 [00:13<00:23,  1.27it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0779. T_Loss: 3.1268. Mask: 0.9617. :  42%|████▏     | 21/50 [00:13<00:18,  1.60it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.39s. Batch: 0.63s. S_Loss: 1.0793. T_Loss: 3.1386. Mask: 0.9608. :  42%|████▏     | 21/50 [00:13<00:18,  1.60it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.39s. Batch: 0.63s. S_Loss: 1.0793. T_Loss: 3.1386. Mask: 0.9608. :  44%|████▍     | 22/50 [00:13<00:14,  1.91it/s]total : 1000  current step :  370
total : 1000  current step :  371
total : 1000  current step :  372
Train Iter: 373/1000. LR: 0.0233. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0780. T_Loss: 3.1553. Mask: 0.9614. :  44%|████▍     | 22/50 [00:15<00:14,  1.91it/s]Train Iter: 373/1000. LR: 0.0233. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0780. T_Loss: 3.1553. Mask: 0.9614. :  46%|████▌     | 23/50 [00:15<00:19,  1.36it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0763. T_Loss: 3.1550. Mask: 0.9618. :  46%|████▌     | 23/50 [00:15<00:19,  1.36it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0763. T_Loss: 3.1550. Mask: 0.9618. :  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0744. T_Loss: 3.1535. Mask: 0.9613. :  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0744. T_Loss: 3.1535. Mask: 0.9613. :  50%|█████     | 25/50 [00:15<00:14,  1.74it/s]total : 1000  current step :  373
total : 1000  current step :  374
total : 1000  current step :  375
Train Iter: 376/1000. LR: 0.0235. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0742. T_Loss: 3.1522. Mask: 0.9611. :  50%|█████     | 25/50 [00:17<00:14,  1.74it/s]Train Iter: 376/1000. LR: 0.0235. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0742. T_Loss: 3.1522. Mask: 0.9611. :  52%|█████▏    | 26/50 [00:17<00:17,  1.34it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0726. T_Loss: 3.1479. Mask: 0.9612. :  52%|█████▏    | 26/50 [00:17<00:17,  1.34it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0726. T_Loss: 3.1479. Mask: 0.9612. :  54%|█████▍    | 27/50 [00:17<00:14,  1.61it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0709. T_Loss: 3.1476. Mask: 0.9614. :  54%|█████▍    | 27/50 [00:17<00:14,  1.61it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0709. T_Loss: 3.1476. Mask: 0.9614. :  56%|█████▌    | 28/50 [00:17<00:12,  1.78it/s]total : 1000  current step :  376
total : 1000  current step :  377
total : 1000  current step :  378
Train Iter: 379/1000. LR: 0.0237. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0684. T_Loss: 3.1374. Mask: 0.9619. :  56%|█████▌    | 28/50 [00:19<00:12,  1.78it/s]Train Iter: 379/1000. LR: 0.0237. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0684. T_Loss: 3.1374. Mask: 0.9619. :  58%|█████▊    | 29/50 [00:19<00:16,  1.30it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0677. T_Loss: 3.1413. Mask: 0.9618. :  58%|█████▊    | 29/50 [00:19<00:16,  1.30it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0677. T_Loss: 3.1413. Mask: 0.9618. :  60%|██████    | 30/50 [00:19<00:12,  1.56it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0656. T_Loss: 3.1322. Mask: 0.9616. :  60%|██████    | 30/50 [00:19<00:12,  1.56it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0656. T_Loss: 3.1322. Mask: 0.9616. :  62%|██████▏   | 31/50 [00:19<00:10,  1.83it/s]total : 1000  current step :  379
total : 1000  current step :  380
total : 1000  current step :  381
Train Iter: 382/1000. LR: 0.0239. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0653. T_Loss: 3.1269. Mask: 0.9608. :  62%|██████▏   | 31/50 [00:20<00:10,  1.83it/s]Train Iter: 382/1000. LR: 0.0239. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0653. T_Loss: 3.1269. Mask: 0.9608. :  64%|██████▍   | 32/50 [00:20<00:13,  1.36it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0641. T_Loss: 3.1221. Mask: 0.9606. :  64%|██████▍   | 32/50 [00:21<00:13,  1.36it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0641. T_Loss: 3.1221. Mask: 0.9606. :  66%|██████▌   | 33/50 [00:21<00:10,  1.59it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0627. T_Loss: 3.1222. Mask: 0.9606. :  66%|██████▌   | 33/50 [00:21<00:10,  1.59it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0627. T_Loss: 3.1222. Mask: 0.9606. :  68%|██████▊   | 34/50 [00:21<00:08,  1.82it/s]total : 1000  current step :  382
total : 1000  current step :  383
total : 1000  current step :  384
Train Iter: 385/1000. LR: 0.0241. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0644. T_Loss: 3.1280. Mask: 0.9606. :  68%|██████▊   | 34/50 [00:22<00:08,  1.82it/s]Train Iter: 385/1000. LR: 0.0241. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0644. T_Loss: 3.1280. Mask: 0.9606. :  70%|███████   | 35/50 [00:22<00:10,  1.44it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0623. T_Loss: 3.1277. Mask: 0.9613. :  70%|███████   | 35/50 [00:23<00:10,  1.44it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0623. T_Loss: 3.1277. Mask: 0.9613. :  72%|███████▏  | 36/50 [00:23<00:08,  1.66it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0610. T_Loss: 3.1292. Mask: 0.9614. :  72%|███████▏  | 36/50 [00:23<00:08,  1.66it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0610. T_Loss: 3.1292. Mask: 0.9614. :  74%|███████▍  | 37/50 [00:23<00:07,  1.77it/s]total : 1000  current step :  385
total : 1000  current step :  386
total : 1000  current step :  387
Train Iter: 388/1000. LR: 0.0243. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0607. T_Loss: 3.1370. Mask: 0.9613. :  74%|███████▍  | 37/50 [00:24<00:07,  1.77it/s]Train Iter: 388/1000. LR: 0.0243. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0607. T_Loss: 3.1370. Mask: 0.9613. :  76%|███████▌  | 38/50 [00:24<00:08,  1.40it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0605. T_Loss: 3.1412. Mask: 0.9614. :  76%|███████▌  | 38/50 [00:25<00:08,  1.40it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0605. T_Loss: 3.1412. Mask: 0.9614. :  78%|███████▊  | 39/50 [00:25<00:06,  1.67it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.41s. Batch: 0.63s. S_Loss: 1.0591. T_Loss: 3.1414. Mask: 0.9614. :  78%|███████▊  | 39/50 [00:25<00:06,  1.67it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.41s. Batch: 0.63s. S_Loss: 1.0591. T_Loss: 3.1414. Mask: 0.9614. :  80%|████████  | 40/50 [00:25<00:05,  1.81it/s]total : 1000  current step :  388
total : 1000  current step :  389
total : 1000  current step :  390
Train Iter: 391/1000. LR: 0.0244. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0583. T_Loss: 3.1468. Mask: 0.9616. :  80%|████████  | 40/50 [00:26<00:05,  1.81it/s]Train Iter: 391/1000. LR: 0.0244. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0583. T_Loss: 3.1468. Mask: 0.9616. :  82%|████████▏ | 41/50 [00:26<00:06,  1.30it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0566. T_Loss: 3.1487. Mask: 0.9620. :  82%|████████▏ | 41/50 [00:27<00:06,  1.30it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0566. T_Loss: 3.1487. Mask: 0.9620. :  84%|████████▍ | 42/50 [00:27<00:05,  1.54it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0557. T_Loss: 3.1651. Mask: 0.9620. :  84%|████████▍ | 42/50 [00:27<00:05,  1.54it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0557. T_Loss: 3.1651. Mask: 0.9620. :  86%|████████▌ | 43/50 [00:27<00:04,  1.63it/s]total : 1000  current step :  391
total : 1000  current step :  392
total : 1000  current step :  393
Train Iter: 394/1000. LR: 0.0246. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0552. T_Loss: 3.1685. Mask: 0.9615. :  86%|████████▌ | 43/50 [00:28<00:04,  1.63it/s]Train Iter: 394/1000. LR: 0.0246. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0552. T_Loss: 3.1685. Mask: 0.9615. :  88%|████████▊ | 44/50 [00:28<00:04,  1.22it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0547. T_Loss: 3.1759. Mask: 0.9610. :  88%|████████▊ | 44/50 [00:29<00:04,  1.22it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0547. T_Loss: 3.1759. Mask: 0.9610. :  90%|█████████ | 45/50 [00:29<00:03,  1.49it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0545. T_Loss: 3.1747. Mask: 0.9610. :  90%|█████████ | 45/50 [00:29<00:03,  1.49it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0545. T_Loss: 3.1747. Mask: 0.9610. :  92%|█████████▏| 46/50 [00:29<00:02,  1.68it/s]total : 1000  current step :  394
total : 1000  current step :  395
total : 1000  current step :  396
Train Iter: 397/1000. LR: 0.0248. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0551. T_Loss: 3.1805. Mask: 0.9608. :  92%|█████████▏| 46/50 [00:30<00:02,  1.68it/s]Train Iter: 397/1000. LR: 0.0248. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0551. T_Loss: 3.1805. Mask: 0.9608. :  94%|█████████▍| 47/50 [00:30<00:02,  1.29it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0533. T_Loss: 3.1762. Mask: 0.9610. :  94%|█████████▍| 47/50 [00:31<00:02,  1.29it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0533. T_Loss: 3.1762. Mask: 0.9610. :  96%|█████████▌| 48/50 [00:31<00:01,  1.47it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0529. T_Loss: 3.1730. Mask: 0.9611. :  96%|█████████▌| 48/50 [00:31<00:01,  1.47it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0529. T_Loss: 3.1730. Mask: 0.9611. :  98%|█████████▊| 49/50 [00:31<00:00,  1.72it/s]total : 1000  current step :  397
total : 1000  current step :  398
total : 1000  current step :  399
Train Iter: 400/1000. LR: 0.0250. Data: 0.44s. Batch: 0.66s. S_Loss: 1.0521. T_Loss: 3.1651. Mask: 0.9610. :  98%|█████████▊| 49/50 [00:33<00:00,  1.72it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.44s. Batch: 0.66s. S_Loss: 1.0521. T_Loss: 3.1651. Mask: 0.9610. : 100%|██████████| 50/50 [00:33<00:00,  1.13it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.44s. Batch: 0.66s. S_Loss: 1.0521. T_Loss: 3.1651. Mask: 0.9610. : 100%|██████████| 50/50 [00:33<00:00,  1.50it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.2604. top1: 91.41. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.2604. top1: 91.41. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.2514. top1: 91.99. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.2514. top1: 91.99. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.08it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.2484. top1: 91.93. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.08it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.2484. top1: 91.93. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.2505. top1: 91.70. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.2505. top1: 91.70. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.46it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.3029. top1: 84.84. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.46it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.3029. top1: 84.84. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.3377. top1: 80.40. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.3377. top1: 80.40. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.86it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.3605. top1: 77.90. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  2.86it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.3605. top1: 77.90. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.3749. top1: 75.80. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.3749. top1: 75.80. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.3749. top1: 75.80. top5: 100.00. : 100%|██████████| 8/8 [00:03<00:00,  2.50it/s]
total : 1000  current step :  400
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.18s. S_Loss: 1.0320. T_Loss: 2.8697. Mask: 0.9570. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.18s. S_Loss: 1.0320. T_Loss: 2.8697. Mask: 0.9570. :   2%|▏         | 1/50 [00:00<00:08,  5.60it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.22s. S_Loss: 1.0353. T_Loss: 2.9753. Mask: 0.9648. :   2%|▏         | 1/50 [00:00<00:08,  5.60it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.22s. S_Loss: 1.0353. T_Loss: 2.9753. Mask: 0.9648. :   4%|▍         | 2/50 [00:00<00:11,  4.27it/s]total : 1000  current step :  401
total : 1000  current step :  402
Train Iter: 403/1000. LR: 0.0252. Data: 0.31s. Batch: 0.54s. S_Loss: 1.0495. T_Loss: 3.0938. Mask: 0.9701. :   4%|▍         | 2/50 [00:01<00:11,  4.27it/s]Train Iter: 403/1000. LR: 0.0252. Data: 0.31s. Batch: 0.54s. S_Loss: 1.0495. T_Loss: 3.0938. Mask: 0.9701. :   6%|▌         | 3/50 [00:01<00:31,  1.51it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0433. T_Loss: 3.0295. Mask: 0.9658. :   6%|▌         | 3/50 [00:01<00:31,  1.51it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.27s. Batch: 0.49s. S_Loss: 1.0433. T_Loss: 3.0295. Mask: 0.9658. :   8%|▊         | 4/50 [00:01<00:24,  1.88it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.25s. Batch: 0.46s. S_Loss: 1.0509. T_Loss: 3.1787. Mask: 0.9648. :   8%|▊         | 4/50 [00:02<00:24,  1.88it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.25s. Batch: 0.46s. S_Loss: 1.0509. T_Loss: 3.1787. Mask: 0.9648. :  10%|█         | 5/50 [00:02<00:21,  2.12it/s]total : 1000  current step :  403
total : 1000  current step :  404
total : 1000  current step :  405
Train Iter: 406/1000. LR: 0.0254. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0456. T_Loss: 3.1707. Mask: 0.9642. :  10%|█         | 5/50 [00:03<00:21,  2.12it/s]Train Iter: 406/1000. LR: 0.0254. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0456. T_Loss: 3.1707. Mask: 0.9642. :  12%|█▏        | 6/50 [00:03<00:29,  1.51it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.31s. Batch: 0.51s. S_Loss: 1.0458. T_Loss: 3.2224. Mask: 0.9626. :  12%|█▏        | 6/50 [00:03<00:29,  1.51it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.31s. Batch: 0.51s. S_Loss: 1.0458. T_Loss: 3.2224. Mask: 0.9626. :  14%|█▍        | 7/50 [00:03<00:22,  1.92it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.31s. Batch: 0.51s. S_Loss: 1.0439. T_Loss: 3.2950. Mask: 0.9614. :  14%|█▍        | 7/50 [00:04<00:22,  1.92it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.31s. Batch: 0.51s. S_Loss: 1.0439. T_Loss: 3.2950. Mask: 0.9614. :  16%|█▌        | 8/50 [00:04<00:21,  1.93it/s]total : 1000  current step :  406
total : 1000  current step :  407
total : 1000  current step :  408
Train Iter: 409/1000. LR: 0.0256. Data: 0.39s. Batch: 0.59s. S_Loss: 1.0419. T_Loss: 3.3630. Mask: 0.9596. :  16%|█▌        | 8/50 [00:05<00:21,  1.93it/s]Train Iter: 409/1000. LR: 0.0256. Data: 0.39s. Batch: 0.59s. S_Loss: 1.0419. T_Loss: 3.3630. Mask: 0.9596. :  18%|█▊        | 9/50 [00:05<00:30,  1.37it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0385. T_Loss: 3.4189. Mask: 0.9602. :  18%|█▊        | 9/50 [00:05<00:30,  1.37it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.38s. Batch: 0.58s. S_Loss: 1.0385. T_Loss: 3.4189. Mask: 0.9602. :  20%|██        | 10/50 [00:05<00:27,  1.47it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0376. T_Loss: 3.4383. Mask: 0.9609. :  20%|██        | 10/50 [00:06<00:27,  1.47it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.36s. Batch: 0.56s. S_Loss: 1.0376. T_Loss: 3.4383. Mask: 0.9609. :  22%|██▏       | 11/50 [00:06<00:22,  1.71it/s]total : 1000  current step :  409
total : 1000  current step :  410
total : 1000  current step :  411
Train Iter: 412/1000. LR: 0.0258. Data: 0.43s. Batch: 0.63s. S_Loss: 1.0332. T_Loss: 3.4233. Mask: 0.9609. :  22%|██▏       | 11/50 [00:07<00:22,  1.71it/s]Train Iter: 412/1000. LR: 0.0258. Data: 0.43s. Batch: 0.63s. S_Loss: 1.0332. T_Loss: 3.4233. Mask: 0.9609. :  24%|██▍       | 12/50 [00:07<00:31,  1.20it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.41s. Batch: 0.61s. S_Loss: 1.0333. T_Loss: 3.4124. Mask: 0.9603. :  24%|██▍       | 12/50 [00:07<00:31,  1.20it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.41s. Batch: 0.61s. S_Loss: 1.0333. T_Loss: 3.4124. Mask: 0.9603. :  26%|██▌       | 13/50 [00:07<00:25,  1.44it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.39s. Batch: 0.60s. S_Loss: 1.0304. T_Loss: 3.3894. Mask: 0.9607. :  26%|██▌       | 13/50 [00:08<00:25,  1.44it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.39s. Batch: 0.60s. S_Loss: 1.0304. T_Loss: 3.3894. Mask: 0.9607. :  28%|██▊       | 14/50 [00:08<00:21,  1.67it/s]total : 1000  current step :  412
total : 1000  current step :  413
total : 1000  current step :  414
Train Iter: 415/1000. LR: 0.0259. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0241. T_Loss: 3.3516. Mask: 0.9612. :  28%|██▊       | 14/50 [00:09<00:21,  1.67it/s]Train Iter: 415/1000. LR: 0.0259. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0241. T_Loss: 3.3516. Mask: 0.9612. :  30%|███       | 15/50 [00:09<00:28,  1.22it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0240. T_Loss: 3.3414. Mask: 0.9624. :  30%|███       | 15/50 [00:10<00:28,  1.22it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0240. T_Loss: 3.3414. Mask: 0.9624. :  32%|███▏      | 16/50 [00:10<00:22,  1.50it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.40s. Batch: 0.60s. S_Loss: 1.0232. T_Loss: 3.3344. Mask: 0.9630. :  32%|███▏      | 16/50 [00:10<00:22,  1.50it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.40s. Batch: 0.60s. S_Loss: 1.0232. T_Loss: 3.3344. Mask: 0.9630. :  34%|███▍      | 17/50 [00:10<00:18,  1.83it/s]total : 1000  current step :  415
total : 1000  current step :  416
total : 1000  current step :  417
Train Iter: 418/1000. LR: 0.0261. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0206. T_Loss: 3.3244. Mask: 0.9633. :  34%|███▍      | 17/50 [00:11<00:18,  1.83it/s]Train Iter: 418/1000. LR: 0.0261. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0206. T_Loss: 3.3244. Mask: 0.9633. :  36%|███▌      | 18/50 [00:11<00:24,  1.32it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0178. T_Loss: 3.2931. Mask: 0.9632. :  36%|███▌      | 18/50 [00:11<00:24,  1.32it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0178. T_Loss: 3.2931. Mask: 0.9632. :  38%|███▊      | 19/50 [00:11<00:20,  1.52it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0165. T_Loss: 3.2840. Mask: 0.9635. :  38%|███▊      | 19/50 [00:12<00:20,  1.52it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0165. T_Loss: 3.2840. Mask: 0.9635. :  40%|████      | 20/50 [00:12<00:17,  1.70it/s]total : 1000  current step :  418
total : 1000  current step :  419
total : 1000  current step :  420
Train Iter: 421/1000. LR: 0.0263. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0161. T_Loss: 3.2744. Mask: 0.9641. :  40%|████      | 20/50 [00:13<00:17,  1.70it/s]Train Iter: 421/1000. LR: 0.0263. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0161. T_Loss: 3.2744. Mask: 0.9641. :  42%|████▏     | 21/50 [00:13<00:23,  1.24it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.43s. Batch: 0.63s. S_Loss: 1.0162. T_Loss: 3.2804. Mask: 0.9643. :  42%|████▏     | 21/50 [00:14<00:23,  1.24it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.43s. Batch: 0.63s. S_Loss: 1.0162. T_Loss: 3.2804. Mask: 0.9643. :  44%|████▍     | 22/50 [00:14<00:18,  1.53it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0175. T_Loss: 3.3045. Mask: 0.9642. :  44%|████▍     | 22/50 [00:14<00:18,  1.53it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0175. T_Loss: 3.3045. Mask: 0.9642. :  46%|████▌     | 23/50 [00:14<00:14,  1.81it/s]total : 1000  current step :  421
total : 1000  current step :  422
total : 1000  current step :  423
Train Iter: 424/1000. LR: 0.0265. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0211. T_Loss: 3.3327. Mask: 0.9635. :  46%|████▌     | 23/50 [00:15<00:14,  1.81it/s]Train Iter: 424/1000. LR: 0.0265. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0211. T_Loss: 3.3327. Mask: 0.9635. :  48%|████▊     | 24/50 [00:15<00:20,  1.28it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0197. T_Loss: 3.3443. Mask: 0.9641. :  48%|████▊     | 24/50 [00:15<00:20,  1.28it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0197. T_Loss: 3.3443. Mask: 0.9641. :  50%|█████     | 25/50 [00:15<00:16,  1.55it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.42s. Batch: 0.62s. S_Loss: 1.0195. T_Loss: 3.3664. Mask: 0.9641. :  50%|█████     | 25/50 [00:16<00:16,  1.55it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.42s. Batch: 0.62s. S_Loss: 1.0195. T_Loss: 3.3664. Mask: 0.9641. :  52%|█████▏    | 26/50 [00:16<00:12,  1.86it/s]total : 1000  current step :  424
total : 1000  current step :  425
total : 1000  current step :  426
Train Iter: 427/1000. LR: 0.0267. Data: 0.44s. Batch: 0.64s. S_Loss: 1.0201. T_Loss: 3.3813. Mask: 0.9643. :  52%|█████▏    | 26/50 [00:17<00:12,  1.86it/s]Train Iter: 427/1000. LR: 0.0267. Data: 0.44s. Batch: 0.64s. S_Loss: 1.0201. T_Loss: 3.3813. Mask: 0.9643. :  54%|█████▍    | 27/50 [00:17<00:17,  1.35it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0196. T_Loss: 3.3903. Mask: 0.9641. :  54%|█████▍    | 27/50 [00:17<00:17,  1.35it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0196. T_Loss: 3.3903. Mask: 0.9641. :  56%|█████▌    | 28/50 [00:17<00:13,  1.63it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0176. T_Loss: 3.3951. Mask: 0.9650. :  56%|█████▌    | 28/50 [00:18<00:13,  1.63it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0176. T_Loss: 3.3951. Mask: 0.9650. :  58%|█████▊    | 29/50 [00:18<00:11,  1.88it/s]total : 1000  current step :  427
total : 1000  current step :  428
total : 1000  current step :  429
Train Iter: 430/1000. LR: 0.0269. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0169. T_Loss: 3.3913. Mask: 0.9650. :  58%|█████▊    | 29/50 [00:19<00:11,  1.88it/s]Train Iter: 430/1000. LR: 0.0269. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0169. T_Loss: 3.3913. Mask: 0.9650. :  60%|██████    | 30/50 [00:19<00:14,  1.37it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0156. T_Loss: 3.3784. Mask: 0.9651. :  60%|██████    | 30/50 [00:19<00:14,  1.37it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0156. T_Loss: 3.3784. Mask: 0.9651. :  62%|██████▏   | 31/50 [00:19<00:11,  1.65it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0139. T_Loss: 3.3658. Mask: 0.9652. :  62%|██████▏   | 31/50 [00:19<00:11,  1.65it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0139. T_Loss: 3.3658. Mask: 0.9652. :  64%|██████▍   | 32/50 [00:19<00:09,  1.89it/s]total : 1000  current step :  430
total : 1000  current step :  431
total : 1000  current step :  432
Train Iter: 433/1000. LR: 0.0271. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0140. T_Loss: 3.3583. Mask: 0.9648. :  64%|██████▍   | 32/50 [00:21<00:09,  1.89it/s]Train Iter: 433/1000. LR: 0.0271. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0140. T_Loss: 3.3583. Mask: 0.9648. :  66%|██████▌   | 33/50 [00:21<00:11,  1.43it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0133. T_Loss: 3.3490. Mask: 0.9652. :  66%|██████▌   | 33/50 [00:21<00:11,  1.43it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0133. T_Loss: 3.3490. Mask: 0.9652. :  68%|██████▊   | 34/50 [00:21<00:09,  1.68it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0120. T_Loss: 3.3476. Mask: 0.9654. :  68%|██████▊   | 34/50 [00:21<00:09,  1.68it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0120. T_Loss: 3.3476. Mask: 0.9654. :  70%|███████   | 35/50 [00:21<00:08,  1.79it/s]total : 1000  current step :  433
total : 1000  current step :  434
total : 1000  current step :  435
Train Iter: 436/1000. LR: 0.0273. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0115. T_Loss: 3.3455. Mask: 0.9655. :  70%|███████   | 35/50 [00:23<00:08,  1.79it/s]Train Iter: 436/1000. LR: 0.0273. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0115. T_Loss: 3.3455. Mask: 0.9655. :  72%|███████▏  | 36/50 [00:23<00:10,  1.32it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0098. T_Loss: 3.3428. Mask: 0.9653. :  72%|███████▏  | 36/50 [00:23<00:10,  1.32it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0098. T_Loss: 3.3428. Mask: 0.9653. :  74%|███████▍  | 37/50 [00:23<00:07,  1.66it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0090. T_Loss: 3.3435. Mask: 0.9650. :  74%|███████▍  | 37/50 [00:23<00:07,  1.66it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.41s. Batch: 0.62s. S_Loss: 1.0090. T_Loss: 3.3435. Mask: 0.9650. :  76%|███████▌  | 38/50 [00:23<00:06,  1.84it/s]total : 1000  current step :  436
total : 1000  current step :  437
total : 1000  current step :  438
Train Iter: 439/1000. LR: 0.0274. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0090. T_Loss: 3.3544. Mask: 0.9652. :  76%|███████▌  | 38/50 [00:24<00:06,  1.84it/s]Train Iter: 439/1000. LR: 0.0274. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0090. T_Loss: 3.3544. Mask: 0.9652. :  78%|███████▊  | 39/50 [00:24<00:07,  1.39it/s]total : 1000  current step :  439
Train Iter: 440/1000. LR: 0.0275. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0089. T_Loss: 3.3567. Mask: 0.9649. :  78%|███████▊  | 39/50 [00:25<00:07,  1.39it/s]Train Iter: 440/1000. LR: 0.0275. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0089. T_Loss: 3.3567. Mask: 0.9649. :  80%|████████  | 40/50 [00:25<00:06,  1.45it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0092. T_Loss: 3.3566. Mask: 0.9650. :  80%|████████  | 40/50 [00:26<00:06,  1.45it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0092. T_Loss: 3.3566. Mask: 0.9650. :  82%|████████▏ | 41/50 [00:26<00:06,  1.38it/s]total : 1000  current step :  440
total : 1000  current step :  441
Train Iter: 442/1000. LR: 0.0276. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0094. T_Loss: 3.3546. Mask: 0.9650. :  82%|████████▏ | 41/50 [00:27<00:06,  1.38it/s]Train Iter: 442/1000. LR: 0.0276. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0094. T_Loss: 3.3546. Mask: 0.9650. :  84%|████████▍ | 42/50 [00:27<00:06,  1.19it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0090. T_Loss: 3.3498. Mask: 0.9654. :  84%|████████▍ | 42/50 [00:27<00:06,  1.19it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0090. T_Loss: 3.3498. Mask: 0.9654. :  86%|████████▌ | 43/50 [00:27<00:04,  1.48it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0089. T_Loss: 3.3471. Mask: 0.9651. :  86%|████████▌ | 43/50 [00:28<00:04,  1.48it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.42s. Batch: 0.64s. S_Loss: 1.0089. T_Loss: 3.3471. Mask: 0.9651. :  88%|████████▊ | 44/50 [00:28<00:03,  1.71it/s]total : 1000  current step :  442
total : 1000  current step :  443
total : 1000  current step :  444
Train Iter: 445/1000. LR: 0.0278. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0083. T_Loss: 3.3552. Mask: 0.9654. :  88%|████████▊ | 44/50 [00:29<00:03,  1.71it/s]Train Iter: 445/1000. LR: 0.0278. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0083. T_Loss: 3.3552. Mask: 0.9654. :  90%|█████████ | 45/50 [00:29<00:03,  1.38it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0081. T_Loss: 3.3576. Mask: 0.9650. :  90%|█████████ | 45/50 [00:29<00:03,  1.38it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0081. T_Loss: 3.3576. Mask: 0.9650. :  92%|█████████▏| 46/50 [00:29<00:02,  1.67it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0082. T_Loss: 3.3584. Mask: 0.9648. :  92%|█████████▏| 46/50 [00:29<00:02,  1.67it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0082. T_Loss: 3.3584. Mask: 0.9648. :  94%|█████████▍| 47/50 [00:29<00:01,  1.93it/s]total : 1000  current step :  445
total : 1000  current step :  446
total : 1000  current step :  447
Train Iter: 448/1000. LR: 0.0280. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0074. T_Loss: 3.3571. Mask: 0.9646. :  94%|█████████▍| 47/50 [00:30<00:01,  1.93it/s]Train Iter: 448/1000. LR: 0.0280. Data: 0.43s. Batch: 0.64s. S_Loss: 1.0074. T_Loss: 3.3571. Mask: 0.9646. :  96%|█████████▌| 48/50 [00:30<00:01,  1.50it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0070. T_Loss: 3.3575. Mask: 0.9647. :  96%|█████████▌| 48/50 [00:31<00:01,  1.50it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0070. T_Loss: 3.3575. Mask: 0.9647. :  98%|█████████▊| 49/50 [00:31<00:00,  1.76it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0062. T_Loss: 3.3607. Mask: 0.9646. :  98%|█████████▊| 49/50 [00:31<00:00,  1.76it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0062. T_Loss: 3.3607. Mask: 0.9646. : 100%|██████████| 50/50 [00:31<00:00,  2.00it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0062. T_Loss: 3.3607. Mask: 0.9646. : 100%|██████████| 50/50 [00:31<00:00,  1.59it/s]
total : 1000  current step :  448
total : 1000  current step :  449
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.0774. top1: 92.97. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.0774. top1: 92.97. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0703. top1: 93.16. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0703. top1: 93.16. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0663. top1: 92.84. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0663. top1: 92.84. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0701. top1: 92.68. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.94it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0701. top1: 92.68. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.17it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1340. top1: 86.56. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.17it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1340. top1: 86.56. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.34it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1752. top1: 82.68. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.34it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1752. top1: 82.68. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.52it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2022. top1: 79.74. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.52it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.2022. top1: 79.74. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.2192. top1: 77.70. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.2192. top1: 77.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.2192. top1: 77.70. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]
total : 1000  current step :  450
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.78s. Batch: 0.92s. S_Loss: 0.9624. T_Loss: 3.2938. Mask: 0.9609. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.78s. Batch: 0.92s. S_Loss: 0.9624. T_Loss: 3.2938. Mask: 0.9609. :   2%|▏         | 1/50 [00:00<00:45,  1.08it/s]Train Iter: 452/1000. LR: 0.0282. Data: 0.51s. Batch: 0.67s. S_Loss: 0.9758. T_Loss: 3.3353. Mask: 0.9629. :   2%|▏         | 1/50 [00:01<00:45,  1.08it/s]Train Iter: 452/1000. LR: 0.0282. Data: 0.51s. Batch: 0.67s. S_Loss: 0.9758. T_Loss: 3.3353. Mask: 0.9629. :   4%|▍         | 2/50 [00:01<00:29,  1.61it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.42s. Batch: 0.60s. S_Loss: 0.9686. T_Loss: 3.3261. Mask: 0.9596. :   4%|▍         | 2/50 [00:01<00:29,  1.61it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.42s. Batch: 0.60s. S_Loss: 0.9686. T_Loss: 3.3261. Mask: 0.9596. :   6%|▌         | 3/50 [00:01<00:26,  1.79it/s]total : 1000  current step :  451
total : 1000  current step :  452
total : 1000  current step :  453
Train Iter: 454/1000. LR: 0.0284. Data: 0.52s. Batch: 0.71s. S_Loss: 0.9645. T_Loss: 3.2448. Mask: 0.9609. :   6%|▌         | 3/50 [00:02<00:26,  1.79it/s]Train Iter: 454/1000. LR: 0.0284. Data: 0.52s. Batch: 0.71s. S_Loss: 0.9645. T_Loss: 3.2448. Mask: 0.9609. :   8%|▊         | 4/50 [00:02<00:34,  1.33it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9719. T_Loss: 3.2409. Mask: 0.9617. :   8%|▊         | 4/50 [00:03<00:34,  1.33it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9719. T_Loss: 3.2409. Mask: 0.9617. :  10%|█         | 5/50 [00:03<00:26,  1.67it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.38s. Batch: 0.59s. S_Loss: 0.9843. T_Loss: 3.1794. Mask: 0.9635. :  10%|█         | 5/50 [00:03<00:26,  1.67it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.38s. Batch: 0.59s. S_Loss: 0.9843. T_Loss: 3.1794. Mask: 0.9635. :  12%|█▏        | 6/50 [00:03<00:22,  1.94it/s]total : 1000  current step :  454
total : 1000  current step :  455
total : 1000  current step :  456
Train Iter: 457/1000. LR: 0.0286. Data: 0.46s. Batch: 0.66s. S_Loss: 1.0012. T_Loss: 3.1801. Mask: 0.9632. :  12%|█▏        | 6/50 [00:04<00:22,  1.94it/s]Train Iter: 457/1000. LR: 0.0286. Data: 0.46s. Batch: 0.66s. S_Loss: 1.0012. T_Loss: 3.1801. Mask: 0.9632. :  14%|█▍        | 7/50 [00:04<00:30,  1.41it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0006. T_Loss: 3.1938. Mask: 0.9619. :  14%|█▍        | 7/50 [00:05<00:30,  1.41it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.42s. Batch: 0.63s. S_Loss: 1.0006. T_Loss: 3.1938. Mask: 0.9619. :  16%|█▌        | 8/50 [00:05<00:25,  1.62it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9981. T_Loss: 3.2366. Mask: 0.9622. :  16%|█▌        | 8/50 [00:05<00:25,  1.62it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9981. T_Loss: 3.2366. Mask: 0.9622. :  18%|█▊        | 9/50 [00:05<00:22,  1.85it/s]total : 1000  current step :  457
total : 1000  current step :  458
total : 1000  current step :  459
Train Iter: 460/1000. LR: 0.0287. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0011. T_Loss: 3.2935. Mask: 0.9621. :  18%|█▊        | 9/50 [00:06<00:22,  1.85it/s]Train Iter: 460/1000. LR: 0.0287. Data: 0.44s. Batch: 0.65s. S_Loss: 1.0011. T_Loss: 3.2935. Mask: 0.9621. :  20%|██        | 10/50 [00:06<00:28,  1.42it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9974. T_Loss: 3.3307. Mask: 0.9613. :  20%|██        | 10/50 [00:07<00:28,  1.42it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9974. T_Loss: 3.3307. Mask: 0.9613. :  22%|██▏       | 11/50 [00:07<00:25,  1.56it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9968. T_Loss: 3.3548. Mask: 0.9606. :  22%|██▏       | 11/50 [00:07<00:25,  1.56it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9968. T_Loss: 3.3548. Mask: 0.9606. :  24%|██▍       | 12/50 [00:07<00:22,  1.72it/s]total : 1000  current step :  460
total : 1000  current step :  461
total : 1000  current step :  462
Train Iter: 463/1000. LR: 0.0289. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9978. T_Loss: 3.3793. Mask: 0.9594. :  24%|██▍       | 12/50 [00:08<00:22,  1.72it/s]Train Iter: 463/1000. LR: 0.0289. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9978. T_Loss: 3.3793. Mask: 0.9594. :  26%|██▌       | 13/50 [00:08<00:27,  1.34it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9964. T_Loss: 3.4196. Mask: 0.9598. :  26%|██▌       | 13/50 [00:08<00:27,  1.34it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9964. T_Loss: 3.4196. Mask: 0.9598. :  28%|██▊       | 14/50 [00:08<00:22,  1.59it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9947. T_Loss: 3.4285. Mask: 0.9607. :  28%|██▊       | 14/50 [00:09<00:22,  1.59it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9947. T_Loss: 3.4285. Mask: 0.9607. :  30%|███       | 15/50 [00:09<00:18,  1.86it/s]total : 1000  current step :  463
total : 1000  current step :  464
total : 1000  current step :  465
Train Iter: 466/1000. LR: 0.0291. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9955. T_Loss: 3.4379. Mask: 0.9607. :  30%|███       | 15/50 [00:10<00:18,  1.86it/s]Train Iter: 466/1000. LR: 0.0291. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9955. T_Loss: 3.4379. Mask: 0.9607. :  32%|███▏      | 16/50 [00:10<00:25,  1.34it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9928. T_Loss: 3.4416. Mask: 0.9619. :  32%|███▏      | 16/50 [00:10<00:25,  1.34it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9928. T_Loss: 3.4416. Mask: 0.9619. :  34%|███▍      | 17/50 [00:10<00:20,  1.59it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9946. T_Loss: 3.4664. Mask: 0.9618. :  34%|███▍      | 17/50 [00:11<00:20,  1.59it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9946. T_Loss: 3.4664. Mask: 0.9618. :  36%|███▌      | 18/50 [00:11<00:18,  1.71it/s]total : 1000  current step :  466
total : 1000  current step :  467
total : 1000  current step :  468
Train Iter: 469/1000. LR: 0.0293. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9957. T_Loss: 3.4825. Mask: 0.9628. :  36%|███▌      | 18/50 [00:12<00:18,  1.71it/s]Train Iter: 469/1000. LR: 0.0293. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9957. T_Loss: 3.4825. Mask: 0.9628. :  38%|███▊      | 19/50 [00:12<00:24,  1.26it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9957. T_Loss: 3.5013. Mask: 0.9631. :  38%|███▊      | 19/50 [00:13<00:24,  1.26it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9957. T_Loss: 3.5013. Mask: 0.9631. :  40%|████      | 20/50 [00:13<00:20,  1.47it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9952. T_Loss: 3.5240. Mask: 0.9635. :  40%|████      | 20/50 [00:13<00:20,  1.47it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9952. T_Loss: 3.5240. Mask: 0.9635. :  42%|████▏     | 21/50 [00:13<00:17,  1.70it/s]total : 1000  current step :  469
total : 1000  current step :  470
total : 1000  current step :  471
Train Iter: 472/1000. LR: 0.0295. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9951. T_Loss: 3.5600. Mask: 0.9641. :  42%|████▏     | 21/50 [00:14<00:17,  1.70it/s]Train Iter: 472/1000. LR: 0.0295. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9951. T_Loss: 3.5600. Mask: 0.9641. :  44%|████▍     | 22/50 [00:14<00:20,  1.35it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9939. T_Loss: 3.5655. Mask: 0.9637. :  44%|████▍     | 22/50 [00:14<00:20,  1.35it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9939. T_Loss: 3.5655. Mask: 0.9637. :  46%|████▌     | 23/50 [00:14<00:17,  1.50it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9932. T_Loss: 3.5469. Mask: 0.9621. :  46%|████▌     | 23/50 [00:15<00:17,  1.50it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9932. T_Loss: 3.5469. Mask: 0.9621. :  48%|████▊     | 24/50 [00:15<00:15,  1.66it/s]total : 1000  current step :  472
total : 1000  current step :  473
total : 1000  current step :  474
Train Iter: 475/1000. LR: 0.0297. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9937. T_Loss: 3.5394. Mask: 0.9622. :  48%|████▊     | 24/50 [00:16<00:15,  1.66it/s]Train Iter: 475/1000. LR: 0.0297. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9937. T_Loss: 3.5394. Mask: 0.9622. :  50%|█████     | 25/50 [00:16<00:19,  1.26it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9953. T_Loss: 3.5553. Mask: 0.9624. :  50%|█████     | 25/50 [00:16<00:19,  1.26it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9953. T_Loss: 3.5553. Mask: 0.9624. :  52%|█████▏    | 26/50 [00:16<00:15,  1.55it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9936. T_Loss: 3.5475. Mask: 0.9630. :  52%|█████▏    | 26/50 [00:17<00:15,  1.55it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9936. T_Loss: 3.5475. Mask: 0.9630. :  54%|█████▍    | 27/50 [00:17<00:13,  1.76it/s]total : 1000  current step :  475
total : 1000  current step :  476
total : 1000  current step :  477
Train Iter: 478/1000. LR: 0.0299. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9920. T_Loss: 3.5410. Mask: 0.9626. :  54%|█████▍    | 27/50 [00:18<00:13,  1.76it/s]Train Iter: 478/1000. LR: 0.0299. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9920. T_Loss: 3.5410. Mask: 0.9626. :  56%|█████▌    | 28/50 [00:18<00:16,  1.37it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9914. T_Loss: 3.5445. Mask: 0.9623. :  56%|█████▌    | 28/50 [00:18<00:16,  1.37it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9914. T_Loss: 3.5445. Mask: 0.9623. :  58%|█████▊    | 29/50 [00:18<00:13,  1.59it/s]total : 1000  current step :  478
total : 1000  current step :  479
Train Iter: 480/1000. LR: 0.0300. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9904. T_Loss: 3.5439. Mask: 0.9629. :  58%|█████▊    | 29/50 [00:19<00:13,  1.59it/s]Train Iter: 480/1000. LR: 0.0300. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9904. T_Loss: 3.5439. Mask: 0.9629. :  60%|██████    | 30/50 [00:19<00:12,  1.58it/s]total : 1000  current step :  480
Train Iter: 481/1000. LR: 0.0301. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9893. T_Loss: 3.5364. Mask: 0.9624. :  60%|██████    | 30/50 [00:20<00:12,  1.58it/s]Train Iter: 481/1000. LR: 0.0301. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9893. T_Loss: 3.5364. Mask: 0.9624. :  62%|██████▏   | 31/50 [00:20<00:15,  1.23it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9894. T_Loss: 3.5289. Mask: 0.9625. :  62%|██████▏   | 31/50 [00:20<00:15,  1.23it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9894. T_Loss: 3.5289. Mask: 0.9625. :  64%|██████▍   | 32/50 [00:20<00:11,  1.56it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9889. T_Loss: 3.5258. Mask: 0.9625. :  64%|██████▍   | 32/50 [00:21<00:11,  1.56it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9889. T_Loss: 3.5258. Mask: 0.9625. :  66%|██████▌   | 33/50 [00:21<00:09,  1.84it/s]total : 1000  current step :  481
total : 1000  current step :  482
total : 1000  current step :  483
Train Iter: 484/1000. LR: 0.0302. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9882. T_Loss: 3.5201. Mask: 0.9628. :  66%|██████▌   | 33/50 [00:22<00:09,  1.84it/s]Train Iter: 484/1000. LR: 0.0302. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9882. T_Loss: 3.5201. Mask: 0.9628. :  68%|██████▊   | 34/50 [00:22<00:12,  1.32it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9875. T_Loss: 3.5192. Mask: 0.9629. :  68%|██████▊   | 34/50 [00:22<00:12,  1.32it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9875. T_Loss: 3.5192. Mask: 0.9629. :  70%|███████   | 35/50 [00:22<00:09,  1.59it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9867. T_Loss: 3.5132. Mask: 0.9633. :  70%|███████   | 35/50 [00:23<00:09,  1.59it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9867. T_Loss: 3.5132. Mask: 0.9633. :  72%|███████▏  | 36/50 [00:23<00:07,  1.85it/s]total : 1000  current step :  484
total : 1000  current step :  485
total : 1000  current step :  486
Train Iter: 487/1000. LR: 0.0304. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9867. T_Loss: 3.5103. Mask: 0.9636. :  72%|███████▏  | 36/50 [00:24<00:07,  1.85it/s]Train Iter: 487/1000. LR: 0.0304. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9867. T_Loss: 3.5103. Mask: 0.9636. :  74%|███████▍  | 37/50 [00:24<00:09,  1.36it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9862. T_Loss: 3.5249. Mask: 0.9636. :  74%|███████▍  | 37/50 [00:24<00:09,  1.36it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9862. T_Loss: 3.5249. Mask: 0.9636. :  76%|███████▌  | 38/50 [00:24<00:07,  1.58it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9858. T_Loss: 3.5294. Mask: 0.9634. :  76%|███████▌  | 38/50 [00:25<00:07,  1.58it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9858. T_Loss: 3.5294. Mask: 0.9634. :  78%|███████▊  | 39/50 [00:25<00:06,  1.78it/s]total : 1000  current step :  487
total : 1000  current step :  488
total : 1000  current step :  489
Train Iter: 490/1000. LR: 0.0306. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9846. T_Loss: 3.5288. Mask: 0.9632. :  78%|███████▊  | 39/50 [00:26<00:06,  1.78it/s]Train Iter: 490/1000. LR: 0.0306. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9846. T_Loss: 3.5288. Mask: 0.9632. :  80%|████████  | 40/50 [00:26<00:07,  1.30it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9833. T_Loss: 3.5264. Mask: 0.9637. :  80%|████████  | 40/50 [00:26<00:07,  1.30it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9833. T_Loss: 3.5264. Mask: 0.9637. :  82%|████████▏ | 41/50 [00:26<00:05,  1.65it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9823. T_Loss: 3.5270. Mask: 0.9639. :  82%|████████▏ | 41/50 [00:27<00:05,  1.65it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9823. T_Loss: 3.5270. Mask: 0.9639. :  84%|████████▍ | 42/50 [00:27<00:04,  1.89it/s]total : 1000  current step :  490
total : 1000  current step :  491
total : 1000  current step :  492
Train Iter: 493/1000. LR: 0.0308. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9813. T_Loss: 3.5336. Mask: 0.9644. :  84%|████████▍ | 42/50 [00:28<00:04,  1.89it/s]Train Iter: 493/1000. LR: 0.0308. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9813. T_Loss: 3.5336. Mask: 0.9644. :  86%|████████▌ | 43/50 [00:28<00:05,  1.37it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9804. T_Loss: 3.5361. Mask: 0.9644. :  86%|████████▌ | 43/50 [00:28<00:05,  1.37it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9804. T_Loss: 3.5361. Mask: 0.9644. :  88%|████████▊ | 44/50 [00:28<00:03,  1.65it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9805. T_Loss: 3.5468. Mask: 0.9644. :  88%|████████▊ | 44/50 [00:28<00:03,  1.65it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9805. T_Loss: 3.5468. Mask: 0.9644. :  90%|█████████ | 45/50 [00:28<00:02,  1.90it/s]total : 1000  current step :  493
total : 1000  current step :  494
total : 1000  current step :  495
Train Iter: 496/1000. LR: 0.0310. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9800. T_Loss: 3.5459. Mask: 0.9648. :  90%|█████████ | 45/50 [00:30<00:02,  1.90it/s]Train Iter: 496/1000. LR: 0.0310. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9800. T_Loss: 3.5459. Mask: 0.9648. :  92%|█████████▏| 46/50 [00:30<00:02,  1.41it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9806. T_Loss: 3.5473. Mask: 0.9649. :  92%|█████████▏| 46/50 [00:30<00:02,  1.41it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9806. T_Loss: 3.5473. Mask: 0.9649. :  94%|█████████▍| 47/50 [00:30<00:01,  1.64it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9804. T_Loss: 3.5492. Mask: 0.9648. :  94%|█████████▍| 47/50 [00:30<00:01,  1.64it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9804. T_Loss: 3.5492. Mask: 0.9648. :  96%|█████████▌| 48/50 [00:30<00:01,  1.85it/s]total : 1000  current step :  496
total : 1000  current step :  497
total : 1000  current step :  498
Train Iter: 499/1000. LR: 0.0312. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9810. T_Loss: 3.5613. Mask: 0.9650. :  96%|█████████▌| 48/50 [00:31<00:01,  1.85it/s]Train Iter: 499/1000. LR: 0.0312. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9810. T_Loss: 3.5613. Mask: 0.9650. :  98%|█████████▊| 49/50 [00:31<00:00,  1.44it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9808. T_Loss: 3.5628. Mask: 0.9650. :  98%|█████████▊| 49/50 [00:32<00:00,  1.44it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9808. T_Loss: 3.5628. Mask: 0.9650. : 100%|██████████| 50/50 [00:32<00:00,  1.74it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9808. T_Loss: 3.5628. Mask: 0.9650. : 100%|██████████| 50/50 [00:32<00:00,  1.56it/s]
total : 1000  current step :  499
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.0227. top1: 91.80. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.0227. top1: 91.80. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0142. top1: 91.99. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0142. top1: 91.99. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0091. top1: 92.06. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0091. top1: 92.06. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0096. top1: 91.89. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0096. top1: 91.89. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0516. top1: 88.44. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0516. top1: 88.44. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0769. top1: 85.87. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0769. top1: 85.87. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0919. top1: 84.82. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0919. top1: 84.82. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1016. top1: 83.80. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1016. top1: 83.80. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.59it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1016. top1: 83.80. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.73it/s]
total : 1000  current step :  500
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.18s. S_Loss: 1.0126. T_Loss: 3.4701. Mask: 0.9648. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.18s. S_Loss: 1.0126. T_Loss: 3.4701. Mask: 0.9648. :   2%|▏         | 1/50 [00:00<00:08,  5.53it/s]total : 1000  current step :  501
Train Iter: 502/1000. LR: 0.0314. Data: 0.44s. Batch: 0.62s. S_Loss: 1.0085. T_Loss: 3.6462. Mask: 0.9629. :   2%|▏         | 1/50 [00:01<00:08,  5.53it/s]Train Iter: 502/1000. LR: 0.0314. Data: 0.44s. Batch: 0.62s. S_Loss: 1.0085. T_Loss: 3.6462. Mask: 0.9629. :   4%|▍         | 2/50 [00:01<00:33,  1.42it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.35s. Batch: 0.53s. S_Loss: 0.9891. T_Loss: 3.6582. Mask: 0.9596. :   4%|▍         | 2/50 [00:01<00:33,  1.42it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.35s. Batch: 0.53s. S_Loss: 0.9891. T_Loss: 3.6582. Mask: 0.9596. :   6%|▌         | 3/50 [00:01<00:25,  1.84it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9933. T_Loss: 3.6992. Mask: 0.9600. :   6%|▌         | 3/50 [00:01<00:25,  1.84it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.28s. Batch: 0.49s. S_Loss: 0.9933. T_Loss: 3.6992. Mask: 0.9600. :   8%|▊         | 4/50 [00:01<00:21,  2.13it/s]total : 1000  current step :  502
total : 1000  current step :  503
total : 1000  current step :  504
Train Iter: 505/1000. LR: 0.0316. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9818. T_Loss: 3.6985. Mask: 0.9602. :   8%|▊         | 4/50 [00:03<00:21,  2.13it/s]Train Iter: 505/1000. LR: 0.0316. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9818. T_Loss: 3.6985. Mask: 0.9602. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9811. T_Loss: 3.7167. Mask: 0.9629. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9811. T_Loss: 3.7167. Mask: 0.9629. :  12%|█▏        | 6/50 [00:03<00:26,  1.67it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9855. T_Loss: 3.7243. Mask: 0.9615. :  12%|█▏        | 6/50 [00:03<00:26,  1.67it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.32s. Batch: 0.54s. S_Loss: 0.9855. T_Loss: 3.7243. Mask: 0.9615. :  14%|█▍        | 7/50 [00:03<00:21,  2.02it/s]total : 1000  current step :  505
total : 1000  current step :  506
total : 1000  current step :  507
Train Iter: 508/1000. LR: 0.0318. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9834. T_Loss: 3.6866. Mask: 0.9614. :  14%|█▍        | 7/50 [00:05<00:21,  2.02it/s]Train Iter: 508/1000. LR: 0.0318. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9834. T_Loss: 3.6866. Mask: 0.9614. :  16%|█▌        | 8/50 [00:05<00:32,  1.29it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9854. T_Loss: 3.6712. Mask: 0.9622. :  16%|█▌        | 8/50 [00:05<00:32,  1.29it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9854. T_Loss: 3.6712. Mask: 0.9622. :  18%|█▊        | 9/50 [00:05<00:25,  1.64it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.35s. Batch: 0.57s. S_Loss: 0.9872. T_Loss: 3.6723. Mask: 0.9621. :  18%|█▊        | 9/50 [00:05<00:25,  1.64it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.35s. Batch: 0.57s. S_Loss: 0.9872. T_Loss: 3.6723. Mask: 0.9621. :  20%|██        | 10/50 [00:05<00:20,  1.93it/s]total : 1000  current step :  508
total : 1000  current step :  509
total : 1000  current step :  510
Train Iter: 511/1000. LR: 0.0319. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9865. T_Loss: 3.7242. Mask: 0.9631. :  20%|██        | 10/50 [00:06<00:20,  1.93it/s]Train Iter: 511/1000. LR: 0.0319. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9865. T_Loss: 3.7242. Mask: 0.9631. :  22%|██▏       | 11/50 [00:06<00:28,  1.35it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9845. T_Loss: 3.7049. Mask: 0.9622. :  22%|██▏       | 11/50 [00:07<00:28,  1.35it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9845. T_Loss: 3.7049. Mask: 0.9622. :  24%|██▍       | 12/50 [00:07<00:24,  1.57it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9820. T_Loss: 3.7384. Mask: 0.9618. :  24%|██▍       | 12/50 [00:07<00:24,  1.57it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9820. T_Loss: 3.7384. Mask: 0.9618. :  26%|██▌       | 13/50 [00:07<00:19,  1.90it/s]total : 1000  current step :  511
total : 1000  current step :  512
total : 1000  current step :  513
Train Iter: 514/1000. LR: 0.0321. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9819. T_Loss: 3.7369. Mask: 0.9626. :  26%|██▌       | 13/50 [00:08<00:19,  1.90it/s]Train Iter: 514/1000. LR: 0.0321. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9819. T_Loss: 3.7369. Mask: 0.9626. :  28%|██▊       | 14/50 [00:08<00:24,  1.48it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9830. T_Loss: 3.7376. Mask: 0.9622. :  28%|██▊       | 14/50 [00:08<00:24,  1.48it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9830. T_Loss: 3.7376. Mask: 0.9622. :  30%|███       | 15/50 [00:08<00:19,  1.78it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9829. T_Loss: 3.7335. Mask: 0.9626. :  30%|███       | 15/50 [00:09<00:19,  1.78it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9829. T_Loss: 3.7335. Mask: 0.9626. :  32%|███▏      | 16/50 [00:09<00:16,  2.04it/s]total : 1000  current step :  514
total : 1000  current step :  515
total : 1000  current step :  516
Train Iter: 517/1000. LR: 0.0323. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9812. T_Loss: 3.7012. Mask: 0.9637. :  32%|███▏      | 16/50 [00:10<00:16,  2.04it/s]Train Iter: 517/1000. LR: 0.0323. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9812. T_Loss: 3.7012. Mask: 0.9637. :  34%|███▍      | 17/50 [00:10<00:24,  1.34it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9787. T_Loss: 3.6766. Mask: 0.9638. :  34%|███▍      | 17/50 [00:10<00:24,  1.34it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9787. T_Loss: 3.6766. Mask: 0.9638. :  36%|███▌      | 18/50 [00:10<00:20,  1.59it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9780. T_Loss: 3.6712. Mask: 0.9640. :  36%|███▌      | 18/50 [00:11<00:20,  1.59it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9780. T_Loss: 3.6712. Mask: 0.9640. :  38%|███▊      | 19/50 [00:11<00:15,  1.98it/s]total : 1000  current step :  517
total : 1000  current step :  518
total : 1000  current step :  519
Train Iter: 520/1000. LR: 0.0325. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9755. T_Loss: 3.6480. Mask: 0.9658. :  38%|███▊      | 19/50 [00:12<00:15,  1.98it/s]Train Iter: 520/1000. LR: 0.0325. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9755. T_Loss: 3.6480. Mask: 0.9658. :  40%|████      | 20/50 [00:12<00:24,  1.23it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9752. T_Loss: 3.6466. Mask: 0.9654. :  40%|████      | 20/50 [00:13<00:24,  1.23it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9752. T_Loss: 3.6466. Mask: 0.9654. :  42%|████▏     | 21/50 [00:13<00:21,  1.34it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9751. T_Loss: 3.6386. Mask: 0.9654. :  42%|████▏     | 21/50 [00:14<00:21,  1.34it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9751. T_Loss: 3.6386. Mask: 0.9654. :  44%|████▍     | 22/50 [00:14<00:20,  1.36it/s]total : 1000  current step :  520
total : 1000  current step :  521
total : 1000  current step :  522
Train Iter: 523/1000. LR: 0.0327. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9740. T_Loss: 3.6142. Mask: 0.9652. :  44%|████▍     | 22/50 [00:14<00:20,  1.36it/s]Train Iter: 523/1000. LR: 0.0327. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9740. T_Loss: 3.6142. Mask: 0.9652. :  46%|████▌     | 23/50 [00:14<00:21,  1.26it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9741. T_Loss: 3.6032. Mask: 0.9652. :  46%|████▌     | 23/50 [00:15<00:21,  1.26it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9741. T_Loss: 3.6032. Mask: 0.9652. :  48%|████▊     | 24/50 [00:15<00:16,  1.56it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9747. T_Loss: 3.6011. Mask: 0.9647. :  48%|████▊     | 24/50 [00:15<00:16,  1.56it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9747. T_Loss: 3.6011. Mask: 0.9647. :  50%|█████     | 25/50 [00:15<00:15,  1.66it/s]total : 1000  current step :  523
total : 1000  current step :  524
total : 1000  current step :  525
Train Iter: 526/1000. LR: 0.0329. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9764. T_Loss: 3.5997. Mask: 0.9654. :  50%|█████     | 25/50 [00:16<00:15,  1.66it/s]Train Iter: 526/1000. LR: 0.0329. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9764. T_Loss: 3.5997. Mask: 0.9654. :  52%|█████▏    | 26/50 [00:16<00:17,  1.37it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9759. T_Loss: 3.6180. Mask: 0.9654. :  52%|█████▏    | 26/50 [00:17<00:17,  1.37it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9759. T_Loss: 3.6180. Mask: 0.9654. :  54%|█████▍    | 27/50 [00:17<00:13,  1.69it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9757. T_Loss: 3.6419. Mask: 0.9648. :  54%|█████▍    | 27/50 [00:17<00:13,  1.69it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9757. T_Loss: 3.6419. Mask: 0.9648. :  56%|█████▌    | 28/50 [00:17<00:11,  1.86it/s]total : 1000  current step :  526
total : 1000  current step :  527
total : 1000  current step :  528
Train Iter: 529/1000. LR: 0.0331. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9744. T_Loss: 3.6534. Mask: 0.9650. :  56%|█████▌    | 28/50 [00:18<00:11,  1.86it/s]Train Iter: 529/1000. LR: 0.0331. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9744. T_Loss: 3.6534. Mask: 0.9650. :  58%|█████▊    | 29/50 [00:18<00:14,  1.41it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9748. T_Loss: 3.6732. Mask: 0.9651. :  58%|█████▊    | 29/50 [00:18<00:14,  1.41it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9748. T_Loss: 3.6732. Mask: 0.9651. :  60%|██████    | 30/50 [00:18<00:11,  1.79it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9745. T_Loss: 3.6801. Mask: 0.9653. :  60%|██████    | 30/50 [00:19<00:11,  1.79it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9745. T_Loss: 3.6801. Mask: 0.9653. :  62%|██████▏   | 31/50 [00:19<00:09,  1.96it/s]total : 1000  current step :  529
total : 1000  current step :  530
total : 1000  current step :  531
Train Iter: 532/1000. LR: 0.0333. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9759. T_Loss: 3.6911. Mask: 0.9659. :  62%|██████▏   | 31/50 [00:20<00:09,  1.96it/s]Train Iter: 532/1000. LR: 0.0333. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9759. T_Loss: 3.6911. Mask: 0.9659. :  64%|██████▍   | 32/50 [00:20<00:12,  1.48it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9753. T_Loss: 3.6964. Mask: 0.9664. :  64%|██████▍   | 32/50 [00:20<00:12,  1.48it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9753. T_Loss: 3.6964. Mask: 0.9664. :  66%|██████▌   | 33/50 [00:20<00:09,  1.74it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9754. T_Loss: 3.6901. Mask: 0.9660. :  66%|██████▌   | 33/50 [00:20<00:09,  1.74it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9754. T_Loss: 3.6901. Mask: 0.9660. :  68%|██████▊   | 34/50 [00:20<00:08,  1.97it/s]total : 1000  current step :  532
total : 1000  current step :  533
total : 1000  current step :  534
Train Iter: 535/1000. LR: 0.0334. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9745. T_Loss: 3.6829. Mask: 0.9654. :  68%|██████▊   | 34/50 [00:21<00:08,  1.97it/s]Train Iter: 535/1000. LR: 0.0334. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9745. T_Loss: 3.6829. Mask: 0.9654. :  70%|███████   | 35/50 [00:21<00:10,  1.47it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9732. T_Loss: 3.6782. Mask: 0.9656. :  70%|███████   | 35/50 [00:22<00:10,  1.47it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9732. T_Loss: 3.6782. Mask: 0.9656. :  72%|███████▏  | 36/50 [00:22<00:08,  1.72it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9727. T_Loss: 3.6711. Mask: 0.9661. :  72%|███████▏  | 36/50 [00:22<00:08,  1.72it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9727. T_Loss: 3.6711. Mask: 0.9661. :  74%|███████▍  | 37/50 [00:22<00:06,  2.07it/s]total : 1000  current step :  535
total : 1000  current step :  536
total : 1000  current step :  537
Train Iter: 538/1000. LR: 0.0336. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9722. T_Loss: 3.6671. Mask: 0.9660. :  74%|███████▍  | 37/50 [00:23<00:06,  2.07it/s]Train Iter: 538/1000. LR: 0.0336. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9722. T_Loss: 3.6671. Mask: 0.9660. :  76%|███████▌  | 38/50 [00:23<00:08,  1.42it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9717. T_Loss: 3.6625. Mask: 0.9662. :  76%|███████▌  | 38/50 [00:24<00:08,  1.42it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9717. T_Loss: 3.6625. Mask: 0.9662. :  78%|███████▊  | 39/50 [00:24<00:06,  1.65it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9708. T_Loss: 3.6602. Mask: 0.9663. :  78%|███████▊  | 39/50 [00:24<00:06,  1.65it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9708. T_Loss: 3.6602. Mask: 0.9663. :  80%|████████  | 40/50 [00:24<00:05,  1.90it/s]total : 1000  current step :  538
total : 1000  current step :  539
total : 1000  current step :  540
Train Iter: 541/1000. LR: 0.0338. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9702. T_Loss: 3.6476. Mask: 0.9665. :  80%|████████  | 40/50 [00:25<00:05,  1.90it/s]Train Iter: 541/1000. LR: 0.0338. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9702. T_Loss: 3.6476. Mask: 0.9665. :  82%|████████▏ | 41/50 [00:25<00:06,  1.40it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9698. T_Loss: 3.6412. Mask: 0.9669. :  82%|████████▏ | 41/50 [00:26<00:06,  1.40it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9698. T_Loss: 3.6412. Mask: 0.9669. :  84%|████████▍ | 42/50 [00:26<00:04,  1.64it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9695. T_Loss: 3.6328. Mask: 0.9673. :  84%|████████▍ | 42/50 [00:26<00:04,  1.64it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9695. T_Loss: 3.6328. Mask: 0.9673. :  86%|████████▌ | 43/50 [00:26<00:03,  1.87it/s]total : 1000  current step :  541
total : 1000  current step :  542
total : 1000  current step :  543
Train Iter: 544/1000. LR: 0.0340. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9703. T_Loss: 3.6230. Mask: 0.9672. :  86%|████████▌ | 43/50 [00:27<00:03,  1.87it/s]Train Iter: 544/1000. LR: 0.0340. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9703. T_Loss: 3.6230. Mask: 0.9672. :  88%|████████▊ | 44/50 [00:27<00:04,  1.44it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9692. T_Loss: 3.6165. Mask: 0.9673. :  88%|████████▊ | 44/50 [00:27<00:04,  1.44it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9692. T_Loss: 3.6165. Mask: 0.9673. :  90%|█████████ | 45/50 [00:27<00:02,  1.70it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9701. T_Loss: 3.6211. Mask: 0.9672. :  90%|█████████ | 45/50 [00:28<00:02,  1.70it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9701. T_Loss: 3.6211. Mask: 0.9672. :  92%|█████████▏| 46/50 [00:28<00:02,  1.92it/s]total : 1000  current step :  544
total : 1000  current step :  545
total : 1000  current step :  546
Train Iter: 547/1000. LR: 0.0342. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9701. T_Loss: 3.6158. Mask: 0.9673. :  92%|█████████▏| 46/50 [00:29<00:02,  1.92it/s]Train Iter: 547/1000. LR: 0.0342. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9701. T_Loss: 3.6158. Mask: 0.9673. :  94%|█████████▍| 47/50 [00:29<00:02,  1.34it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9690. T_Loss: 3.6239. Mask: 0.9674. :  94%|█████████▍| 47/50 [00:29<00:02,  1.34it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9690. T_Loss: 3.6239. Mask: 0.9674. :  96%|█████████▌| 48/50 [00:29<00:01,  1.61it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9690. T_Loss: 3.6284. Mask: 0.9672. :  96%|█████████▌| 48/50 [00:30<00:01,  1.61it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9690. T_Loss: 3.6284. Mask: 0.9672. :  98%|█████████▊| 49/50 [00:30<00:00,  1.90it/s]total : 1000  current step :  547
total : 1000  current step :  548
total : 1000  current step :  549
Train Iter: 550/1000. LR: 0.0344. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9683. T_Loss: 3.6270. Mask: 0.9670. :  98%|█████████▊| 49/50 [00:31<00:00,  1.90it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9683. T_Loss: 3.6270. Mask: 0.9670. : 100%|██████████| 50/50 [00:31<00:00,  1.30it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9683. T_Loss: 3.6270. Mask: 0.9670. : 100%|██████████| 50/50 [00:31<00:00,  1.59it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 1.0366. top1: 89.84. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 1.0366. top1: 89.84. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.21it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0259. top1: 89.65. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.21it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0259. top1: 89.65. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.85it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0189. top1: 89.97. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.85it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0189. top1: 89.97. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.26it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0127. top1: 90.23. top5: 99.90. :  38%|███▊      | 3/8 [00:01<00:02,  2.26it/s] Test Iter:   4/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0127. top1: 90.23. top5: 99.90. :  50%|█████     | 4/8 [00:01<00:01,  2.40it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0203. top1: 89.06. top5: 99.92. :  50%|█████     | 4/8 [00:02<00:01,  2.40it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0203. top1: 89.06. top5: 99.92. :  62%|██████▎   | 5/8 [00:02<00:01,  2.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0228. top1: 88.74. top5: 99.93. :  62%|██████▎   | 5/8 [00:02<00:01,  2.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0228. top1: 88.74. top5: 99.93. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0205. top1: 88.50. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0205. top1: 88.50. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.28it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0200. top1: 88.35. top5: 99.95. :  88%|████████▊ | 7/8 [00:02<00:00,  3.28it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0200. top1: 88.35. top5: 99.95. : 100%|██████████| 8/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0200. top1: 88.35. top5: 99.95. : 100%|██████████| 8/8 [00:03<00:00,  2.54it/s]
total : 1000  current step :  550
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.31s. S_Loss: 0.9125. T_Loss: 3.4588. Mask: 0.9648. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.31s. S_Loss: 0.9125. T_Loss: 3.4588. Mask: 0.9648. :   2%|▏         | 1/50 [00:00<00:15,  3.18it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.33s. S_Loss: 0.9352. T_Loss: 3.5174. Mask: 0.9609. :   2%|▏         | 1/50 [00:00<00:15,  3.18it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.33s. S_Loss: 0.9352. T_Loss: 3.5174. Mask: 0.9609. :   4%|▍         | 2/50 [00:00<00:16,  2.97it/s]total : 1000  current step :  551
total : 1000  current step :  552
Train Iter: 553/1000. LR: 0.0346. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9399. T_Loss: 3.4978. Mask: 0.9648. :   4%|▍         | 2/50 [00:01<00:16,  2.97it/s]Train Iter: 553/1000. LR: 0.0346. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9399. T_Loss: 3.4978. Mask: 0.9648. :   6%|▌         | 3/50 [00:01<00:32,  1.47it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.25s. Batch: 0.53s. S_Loss: 0.9393. T_Loss: 3.5298. Mask: 0.9658. :   6%|▌         | 3/50 [00:02<00:32,  1.47it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.25s. Batch: 0.53s. S_Loss: 0.9393. T_Loss: 3.5298. Mask: 0.9658. :   8%|▊         | 4/50 [00:02<00:25,  1.79it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.24s. Batch: 0.52s. S_Loss: 0.9470. T_Loss: 3.5274. Mask: 0.9633. :   8%|▊         | 4/50 [00:02<00:25,  1.79it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.24s. Batch: 0.52s. S_Loss: 0.9470. T_Loss: 3.5274. Mask: 0.9633. :  10%|█         | 5/50 [00:02<00:23,  1.89it/s]total : 1000  current step :  553
total : 1000  current step :  554
total : 1000  current step :  555
Train Iter: 556/1000. LR: 0.0347. Data: 0.38s. Batch: 0.67s. S_Loss: 0.9461. T_Loss: 3.5039. Mask: 0.9596. :  10%|█         | 5/50 [00:04<00:23,  1.89it/s]Train Iter: 556/1000. LR: 0.0347. Data: 0.38s. Batch: 0.67s. S_Loss: 0.9461. T_Loss: 3.5039. Mask: 0.9596. :  12%|█▏        | 6/50 [00:04<00:36,  1.21it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.34s. Batch: 0.62s. S_Loss: 0.9547. T_Loss: 3.5804. Mask: 0.9604. :  12%|█▏        | 6/50 [00:04<00:36,  1.21it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.34s. Batch: 0.62s. S_Loss: 0.9547. T_Loss: 3.5804. Mask: 0.9604. :  14%|█▍        | 7/50 [00:04<00:28,  1.49it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.33s. Batch: 0.60s. S_Loss: 0.9547. T_Loss: 3.5813. Mask: 0.9624. :  14%|█▍        | 7/50 [00:04<00:28,  1.49it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.33s. Batch: 0.60s. S_Loss: 0.9547. T_Loss: 3.5813. Mask: 0.9624. :  16%|█▌        | 8/50 [00:04<00:24,  1.68it/s]total : 1000  current step :  556
total : 1000  current step :  557
total : 1000  current step :  558
Train Iter: 559/1000. LR: 0.0349. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9544. T_Loss: 3.6003. Mask: 0.9648. :  16%|█▌        | 8/50 [00:05<00:24,  1.68it/s]Train Iter: 559/1000. LR: 0.0349. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9544. T_Loss: 3.6003. Mask: 0.9648. :  18%|█▊        | 9/50 [00:05<00:31,  1.29it/s]total : 1000  current step :  559
Train Iter: 560/1000. LR: 0.0350. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9530. T_Loss: 3.5906. Mask: 0.9672. :  18%|█▊        | 9/50 [00:06<00:31,  1.29it/s]Train Iter: 560/1000. LR: 0.0350. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9530. T_Loss: 3.5906. Mask: 0.9672. :  20%|██        | 10/50 [00:06<00:30,  1.29it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9554. T_Loss: 3.5848. Mask: 0.9677. :  20%|██        | 10/50 [00:07<00:30,  1.29it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9554. T_Loss: 3.5848. Mask: 0.9677. :  22%|██▏       | 11/50 [00:07<00:29,  1.30it/s]total : 1000  current step :  560
total : 1000  current step :  561
Train Iter: 562/1000. LR: 0.0351. Data: 0.50s. Batch: 0.75s. S_Loss: 0.9565. T_Loss: 3.5623. Mask: 0.9688. :  22%|██▏       | 11/50 [00:09<00:29,  1.30it/s]Train Iter: 562/1000. LR: 0.0351. Data: 0.50s. Batch: 0.75s. S_Loss: 0.9565. T_Loss: 3.5623. Mask: 0.9688. :  24%|██▍       | 12/50 [00:09<00:38,  1.00s/it]Train Iter: 563/1000. LR: 0.0352. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9615. T_Loss: 3.5674. Mask: 0.9697. :  24%|██▍       | 12/50 [00:09<00:38,  1.00s/it]Train Iter: 563/1000. LR: 0.0352. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9615. T_Loss: 3.5674. Mask: 0.9697. :  26%|██▌       | 13/50 [00:09<00:29,  1.24it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9612. T_Loss: 3.5445. Mask: 0.9710. :  26%|██▌       | 13/50 [00:09<00:29,  1.24it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9612. T_Loss: 3.5445. Mask: 0.9710. :  28%|██▊       | 14/50 [00:09<00:24,  1.49it/s]total : 1000  current step :  562
total : 1000  current step :  563
total : 1000  current step :  564
Train Iter: 565/1000. LR: 0.0353. Data: 0.49s. Batch: 0.73s. S_Loss: 0.9615. T_Loss: 3.5233. Mask: 0.9711. :  28%|██▊       | 14/50 [00:11<00:24,  1.49it/s]Train Iter: 565/1000. LR: 0.0353. Data: 0.49s. Batch: 0.73s. S_Loss: 0.9615. T_Loss: 3.5233. Mask: 0.9711. :  30%|███       | 15/50 [00:11<00:29,  1.18it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.47s. Batch: 0.71s. S_Loss: 0.9613. T_Loss: 3.5181. Mask: 0.9700. :  30%|███       | 15/50 [00:11<00:29,  1.18it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.47s. Batch: 0.71s. S_Loss: 0.9613. T_Loss: 3.5181. Mask: 0.9700. :  32%|███▏      | 16/50 [00:11<00:24,  1.40it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9614. T_Loss: 3.5257. Mask: 0.9713. :  32%|███▏      | 16/50 [00:11<00:24,  1.40it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9614. T_Loss: 3.5257. Mask: 0.9713. :  34%|███▍      | 17/50 [00:11<00:20,  1.62it/s]total : 1000  current step :  565
total : 1000  current step :  566
total : 1000  current step :  567
Train Iter: 568/1000. LR: 0.0355. Data: 0.48s. Batch: 0.72s. S_Loss: 0.9611. T_Loss: 3.5400. Mask: 0.9718. :  34%|███▍      | 17/50 [00:12<00:20,  1.62it/s]Train Iter: 568/1000. LR: 0.0355. Data: 0.48s. Batch: 0.72s. S_Loss: 0.9611. T_Loss: 3.5400. Mask: 0.9718. :  36%|███▌      | 18/50 [00:12<00:24,  1.30it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9603. T_Loss: 3.5317. Mask: 0.9712. :  36%|███▌      | 18/50 [00:13<00:24,  1.30it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9603. T_Loss: 3.5317. Mask: 0.9712. :  38%|███▊      | 19/50 [00:13<00:19,  1.62it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9590. T_Loss: 3.5323. Mask: 0.9709. :  38%|███▊      | 19/50 [00:13<00:19,  1.62it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9590. T_Loss: 3.5323. Mask: 0.9709. :  40%|████      | 20/50 [00:13<00:15,  1.92it/s]total : 1000  current step :  568
total : 1000  current step :  569
total : 1000  current step :  570
Train Iter: 571/1000. LR: 0.0357. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9591. T_Loss: 3.5336. Mask: 0.9719. :  40%|████      | 20/50 [00:14<00:15,  1.92it/s]Train Iter: 571/1000. LR: 0.0357. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9591. T_Loss: 3.5336. Mask: 0.9719. :  42%|████▏     | 21/50 [00:14<00:20,  1.38it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9589. T_Loss: 3.5501. Mask: 0.9727. :  42%|████▏     | 21/50 [00:14<00:20,  1.38it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9589. T_Loss: 3.5501. Mask: 0.9727. :  44%|████▍     | 22/50 [00:14<00:15,  1.77it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9578. T_Loss: 3.5515. Mask: 0.9725. :  44%|████▍     | 22/50 [00:15<00:15,  1.77it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9578. T_Loss: 3.5515. Mask: 0.9725. :  46%|████▌     | 23/50 [00:15<00:12,  2.09it/s]total : 1000  current step :  571
total : 1000  current step :  572
total : 1000  current step :  573
Train Iter: 574/1000. LR: 0.0359. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9554. T_Loss: 3.5441. Mask: 0.9722. :  46%|████▌     | 23/50 [00:16<00:12,  2.09it/s]Train Iter: 574/1000. LR: 0.0359. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9554. T_Loss: 3.5441. Mask: 0.9722. :  48%|████▊     | 24/50 [00:16<00:17,  1.45it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9540. T_Loss: 3.5366. Mask: 0.9723. :  48%|████▊     | 24/50 [00:16<00:17,  1.45it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9540. T_Loss: 3.5366. Mask: 0.9723. :  50%|█████     | 25/50 [00:16<00:14,  1.75it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9542. T_Loss: 3.5455. Mask: 0.9721. :  50%|█████     | 25/50 [00:16<00:14,  1.75it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9542. T_Loss: 3.5455. Mask: 0.9721. :  52%|█████▏    | 26/50 [00:16<00:11,  2.05it/s]total : 1000  current step :  574
total : 1000  current step :  575
total : 1000  current step :  576
Train Iter: 577/1000. LR: 0.0361. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9545. T_Loss: 3.5460. Mask: 0.9721. :  52%|█████▏    | 26/50 [00:18<00:11,  2.05it/s]Train Iter: 577/1000. LR: 0.0361. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9545. T_Loss: 3.5460. Mask: 0.9721. :  54%|█████▍    | 27/50 [00:18<00:15,  1.47it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9540. T_Loss: 3.5436. Mask: 0.9722. :  54%|█████▍    | 27/50 [00:18<00:15,  1.47it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9540. T_Loss: 3.5436. Mask: 0.9722. :  56%|█████▌    | 28/50 [00:18<00:13,  1.66it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9546. T_Loss: 3.5553. Mask: 0.9716. :  56%|█████▌    | 28/50 [00:18<00:13,  1.66it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9546. T_Loss: 3.5553. Mask: 0.9716. :  58%|█████▊    | 29/50 [00:18<00:10,  2.06it/s]total : 1000  current step :  577
total : 1000  current step :  578
total : 1000  current step :  579
Train Iter: 580/1000. LR: 0.0362. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9563. T_Loss: 3.5881. Mask: 0.9719. :  58%|█████▊    | 29/50 [00:19<00:10,  2.06it/s]Train Iter: 580/1000. LR: 0.0362. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9563. T_Loss: 3.5881. Mask: 0.9719. :  60%|██████    | 30/50 [00:19<00:13,  1.49it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9561. T_Loss: 3.6005. Mask: 0.9715. :  60%|██████    | 30/50 [00:20<00:13,  1.49it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9561. T_Loss: 3.6005. Mask: 0.9715. :  62%|██████▏   | 31/50 [00:20<00:10,  1.77it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9560. T_Loss: 3.6187. Mask: 0.9718. :  62%|██████▏   | 31/50 [00:20<00:10,  1.77it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9560. T_Loss: 3.6187. Mask: 0.9718. :  64%|██████▍   | 32/50 [00:20<00:09,  1.99it/s]total : 1000  current step :  580
total : 1000  current step :  581
total : 1000  current step :  582
Train Iter: 583/1000. LR: 0.0364. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9550. T_Loss: 3.6211. Mask: 0.9716. :  64%|██████▍   | 32/50 [00:21<00:09,  1.99it/s]Train Iter: 583/1000. LR: 0.0364. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9550. T_Loss: 3.6211. Mask: 0.9716. :  66%|██████▌   | 33/50 [00:21<00:11,  1.46it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9555. T_Loss: 3.6314. Mask: 0.9714. :  66%|██████▌   | 33/50 [00:21<00:11,  1.46it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9555. T_Loss: 3.6314. Mask: 0.9714. :  68%|██████▊   | 34/50 [00:21<00:08,  1.83it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9546. T_Loss: 3.6258. Mask: 0.9717. :  68%|██████▊   | 34/50 [00:22<00:08,  1.83it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9546. T_Loss: 3.6258. Mask: 0.9717. :  70%|███████   | 35/50 [00:22<00:07,  1.91it/s]total : 1000  current step :  583
total : 1000  current step :  584
total : 1000  current step :  585
Train Iter: 586/1000. LR: 0.0366. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9547. T_Loss: 3.6217. Mask: 0.9715. :  70%|███████   | 35/50 [00:23<00:07,  1.91it/s]Train Iter: 586/1000. LR: 0.0366. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9547. T_Loss: 3.6217. Mask: 0.9715. :  72%|███████▏  | 36/50 [00:23<00:09,  1.48it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9549. T_Loss: 3.6165. Mask: 0.9711. :  72%|███████▏  | 36/50 [00:23<00:09,  1.48it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9549. T_Loss: 3.6165. Mask: 0.9711. :  74%|███████▍  | 37/50 [00:23<00:07,  1.67it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9550. T_Loss: 3.6121. Mask: 0.9711. :  74%|███████▍  | 37/50 [00:24<00:07,  1.67it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9550. T_Loss: 3.6121. Mask: 0.9711. :  76%|███████▌  | 38/50 [00:24<00:06,  1.77it/s]total : 1000  current step :  586
total : 1000  current step :  587
total : 1000  current step :  588
Train Iter: 589/1000. LR: 0.0368. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9538. T_Loss: 3.6052. Mask: 0.9716. :  76%|███████▌  | 38/50 [00:25<00:06,  1.77it/s]Train Iter: 589/1000. LR: 0.0368. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9538. T_Loss: 3.6052. Mask: 0.9716. :  78%|███████▊  | 39/50 [00:25<00:08,  1.37it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9531. T_Loss: 3.5986. Mask: 0.9718. :  78%|███████▊  | 39/50 [00:25<00:08,  1.37it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9531. T_Loss: 3.5986. Mask: 0.9718. :  80%|████████  | 40/50 [00:25<00:05,  1.69it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9525. T_Loss: 3.5868. Mask: 0.9718. :  80%|████████  | 40/50 [00:25<00:05,  1.69it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9525. T_Loss: 3.5868. Mask: 0.9718. :  82%|████████▏ | 41/50 [00:25<00:04,  1.90it/s]total : 1000  current step :  589
total : 1000  current step :  590
total : 1000  current step :  591
Train Iter: 592/1000. LR: 0.0370. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9538. T_Loss: 3.5861. Mask: 0.9717. :  82%|████████▏ | 41/50 [00:27<00:04,  1.90it/s]Train Iter: 592/1000. LR: 0.0370. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9538. T_Loss: 3.5861. Mask: 0.9717. :  84%|████████▍ | 42/50 [00:27<00:05,  1.46it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9524. T_Loss: 3.5771. Mask: 0.9720. :  84%|████████▍ | 42/50 [00:27<00:05,  1.46it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9524. T_Loss: 3.5771. Mask: 0.9720. :  86%|████████▌ | 43/50 [00:27<00:03,  1.83it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9533. T_Loss: 3.5755. Mask: 0.9723. :  86%|████████▌ | 43/50 [00:27<00:03,  1.83it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9533. T_Loss: 3.5755. Mask: 0.9723. :  88%|████████▊ | 44/50 [00:27<00:02,  2.03it/s]total : 1000  current step :  592
total : 1000  current step :  593
total : 1000  current step :  594
Train Iter: 595/1000. LR: 0.0372. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9521. T_Loss: 3.5652. Mask: 0.9722. :  88%|████████▊ | 44/50 [00:28<00:02,  2.03it/s]Train Iter: 595/1000. LR: 0.0372. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9521. T_Loss: 3.5652. Mask: 0.9722. :  90%|█████████ | 45/50 [00:28<00:03,  1.46it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9515. T_Loss: 3.5627. Mask: 0.9725. :  90%|█████████ | 45/50 [00:29<00:03,  1.46it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9515. T_Loss: 3.5627. Mask: 0.9725. :  92%|█████████▏| 46/50 [00:29<00:02,  1.72it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9506. T_Loss: 3.5592. Mask: 0.9724. :  92%|█████████▏| 46/50 [00:29<00:02,  1.72it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9506. T_Loss: 3.5592. Mask: 0.9724. :  94%|█████████▍| 47/50 [00:29<00:01,  2.10it/s]total : 1000  current step :  595
total : 1000  current step :  596
total : 1000  current step :  597
Train Iter: 598/1000. LR: 0.0374. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9516. T_Loss: 3.5681. Mask: 0.9727. :  94%|█████████▍| 47/50 [00:30<00:01,  2.10it/s]Train Iter: 598/1000. LR: 0.0374. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9516. T_Loss: 3.5681. Mask: 0.9727. :  96%|█████████▌| 48/50 [00:30<00:01,  1.53it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9511. T_Loss: 3.5673. Mask: 0.9727. :  96%|█████████▌| 48/50 [00:30<00:01,  1.53it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9511. T_Loss: 3.5673. Mask: 0.9727. :  98%|█████████▊| 49/50 [00:30<00:00,  1.77it/s]total : 1000  current step :  598
total : 1000  current step :  599
Train Iter: 600/1000. LR: 0.0375. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9503. T_Loss: 3.5599. Mask: 0.9727. :  98%|█████████▊| 49/50 [00:31<00:00,  1.77it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9503. T_Loss: 3.5599. Mask: 0.9727. : 100%|██████████| 50/50 [00:31<00:00,  1.81it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.39s. Batch: 0.62s. S_Loss: 0.9503. T_Loss: 3.5599. Mask: 0.9727. : 100%|██████████| 50/50 [00:31<00:00,  1.60it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.0865. top1: 83.20. top5: 99.61. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.0865. top1: 83.20. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0748. top1: 84.77. top5: 99.61. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0748. top1: 84.77. top5: 99.61. :  25%|██▌       | 2/8 [00:00<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0654. top1: 85.68. top5: 99.61. :  25%|██▌       | 2/8 [00:01<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0654. top1: 85.68. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0507. top1: 86.13. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0507. top1: 86.13. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.33it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0271. top1: 87.03. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.33it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0271. top1: 87.03. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.48it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0096. top1: 87.76. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.48it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0096. top1: 87.76. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.55it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9925. top1: 88.67. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.55it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9925. top1: 88.67. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9828. top1: 89.30. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9828. top1: 89.30. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  4.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9828. top1: 89.30. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.30it/s]
total : 1000  current step :  600
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.75s. Batch: 0.95s. S_Loss: 0.9247. T_Loss: 3.7898. Mask: 0.9766. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.75s. Batch: 0.95s. S_Loss: 0.9247. T_Loss: 3.7898. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:46,  1.05it/s]Train Iter: 602/1000. LR: 0.0376. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9862. T_Loss: 3.7175. Mask: 0.9727. :   2%|▏         | 1/50 [00:01<00:46,  1.05it/s]Train Iter: 602/1000. LR: 0.0376. Data: 0.43s. Batch: 0.64s. S_Loss: 0.9862. T_Loss: 3.7175. Mask: 0.9727. :   4%|▍         | 2/50 [00:01<00:28,  1.71it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.32s. Batch: 0.55s. S_Loss: 1.0016. T_Loss: 3.7128. Mask: 0.9688. :   4%|▍         | 2/50 [00:01<00:28,  1.71it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.32s. Batch: 0.55s. S_Loss: 1.0016. T_Loss: 3.7128. Mask: 0.9688. :   6%|▌         | 3/50 [00:01<00:22,  2.05it/s]total : 1000  current step :  601
total : 1000  current step :  602
total : 1000  current step :  603
Train Iter: 604/1000. LR: 0.0378. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9797. T_Loss: 3.5859. Mask: 0.9668. :   6%|▌         | 3/50 [00:02<00:22,  2.05it/s]Train Iter: 604/1000. LR: 0.0378. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9797. T_Loss: 3.5859. Mask: 0.9668. :   8%|▊         | 4/50 [00:02<00:32,  1.43it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.33s. Batch: 0.61s. S_Loss: 0.9772. T_Loss: 3.5877. Mask: 0.9711. :   8%|▊         | 4/50 [00:03<00:32,  1.43it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.33s. Batch: 0.61s. S_Loss: 0.9772. T_Loss: 3.5877. Mask: 0.9711. :  10%|█         | 5/50 [00:03<00:26,  1.72it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.28s. Batch: 0.57s. S_Loss: 0.9764. T_Loss: 3.5860. Mask: 0.9720. :  10%|█         | 5/50 [00:03<00:26,  1.72it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.28s. Batch: 0.57s. S_Loss: 0.9764. T_Loss: 3.5860. Mask: 0.9720. :  12%|█▏        | 6/50 [00:03<00:22,  1.97it/s]total : 1000  current step :  604
total : 1000  current step :  605
total : 1000  current step :  606
Train Iter: 607/1000. LR: 0.0379. Data: 0.34s. Batch: 0.63s. S_Loss: 0.9698. T_Loss: 3.5739. Mask: 0.9732. :  12%|█▏        | 6/50 [00:04<00:22,  1.97it/s]Train Iter: 607/1000. LR: 0.0379. Data: 0.34s. Batch: 0.63s. S_Loss: 0.9698. T_Loss: 3.5739. Mask: 0.9732. :  14%|█▍        | 7/50 [00:04<00:29,  1.48it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9679. T_Loss: 3.5754. Mask: 0.9702. :  14%|█▍        | 7/50 [00:04<00:29,  1.48it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9679. T_Loss: 3.5754. Mask: 0.9702. :  16%|█▌        | 8/50 [00:04<00:23,  1.79it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.27s. Batch: 0.57s. S_Loss: 0.9654. T_Loss: 3.5721. Mask: 0.9718. :  16%|█▌        | 8/50 [00:05<00:23,  1.79it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.27s. Batch: 0.57s. S_Loss: 0.9654. T_Loss: 3.5721. Mask: 0.9718. :  18%|█▊        | 9/50 [00:05<00:20,  2.01it/s]total : 1000  current step :  607
total : 1000  current step :  608
total : 1000  current step :  609
Train Iter: 610/1000. LR: 0.0381. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9643. T_Loss: 3.6045. Mask: 0.9707. :  18%|█▊        | 9/50 [00:05<00:20,  2.01it/s]Train Iter: 610/1000. LR: 0.0381. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9643. T_Loss: 3.6045. Mask: 0.9707. :  20%|██        | 10/50 [00:05<00:24,  1.66it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9609. T_Loss: 3.6083. Mask: 0.9702. :  20%|██        | 10/50 [00:06<00:24,  1.66it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9609. T_Loss: 3.6083. Mask: 0.9702. :  22%|██▏       | 11/50 [00:06<00:21,  1.79it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.26s. Batch: 0.56s. S_Loss: 0.9578. T_Loss: 3.6118. Mask: 0.9694. :  22%|██▏       | 11/50 [00:06<00:21,  1.79it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.26s. Batch: 0.56s. S_Loss: 0.9578. T_Loss: 3.6118. Mask: 0.9694. :  24%|██▍       | 12/50 [00:06<00:18,  2.03it/s]total : 1000  current step :  610
total : 1000  current step :  611
total : 1000  current step :  612
Train Iter: 613/1000. LR: 0.0383. Data: 0.30s. Batch: 0.60s. S_Loss: 0.9565. T_Loss: 3.6003. Mask: 0.9700. :  24%|██▍       | 12/50 [00:07<00:18,  2.03it/s]Train Iter: 613/1000. LR: 0.0383. Data: 0.30s. Batch: 0.60s. S_Loss: 0.9565. T_Loss: 3.6003. Mask: 0.9700. :  26%|██▌       | 13/50 [00:07<00:24,  1.53it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9529. T_Loss: 3.5941. Mask: 0.9704. :  26%|██▌       | 13/50 [00:08<00:24,  1.53it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9529. T_Loss: 3.5941. Mask: 0.9704. :  28%|██▊       | 14/50 [00:08<00:21,  1.70it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.27s. Batch: 0.57s. S_Loss: 0.9533. T_Loss: 3.6003. Mask: 0.9701. :  28%|██▊       | 14/50 [00:08<00:21,  1.70it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.27s. Batch: 0.57s. S_Loss: 0.9533. T_Loss: 3.6003. Mask: 0.9701. :  30%|███       | 15/50 [00:08<00:18,  1.92it/s]total : 1000  current step :  613
total : 1000  current step :  614
total : 1000  current step :  615
Train Iter: 616/1000. LR: 0.0385. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9524. T_Loss: 3.6008. Mask: 0.9712. :  30%|███       | 15/50 [00:09<00:18,  1.92it/s]Train Iter: 616/1000. LR: 0.0385. Data: 0.30s. Batch: 0.59s. S_Loss: 0.9524. T_Loss: 3.6008. Mask: 0.9712. :  32%|███▏      | 16/50 [00:09<00:21,  1.57it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9514. T_Loss: 3.5919. Mask: 0.9710. :  32%|███▏      | 16/50 [00:09<00:21,  1.57it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9514. T_Loss: 3.5919. Mask: 0.9710. :  34%|███▍      | 17/50 [00:09<00:19,  1.70it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.27s. Batch: 0.56s. S_Loss: 0.9506. T_Loss: 3.5712. Mask: 0.9707. :  34%|███▍      | 17/50 [00:10<00:19,  1.70it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.27s. Batch: 0.56s. S_Loss: 0.9506. T_Loss: 3.5712. Mask: 0.9707. :  36%|███▌      | 18/50 [00:10<00:15,  2.08it/s]total : 1000  current step :  616
total : 1000  current step :  617
total : 1000  current step :  618
Train Iter: 619/1000. LR: 0.0387. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9512. T_Loss: 3.5703. Mask: 0.9702. :  36%|███▌      | 18/50 [00:11<00:15,  2.08it/s]Train Iter: 619/1000. LR: 0.0387. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9512. T_Loss: 3.5703. Mask: 0.9702. :  38%|███▊      | 19/50 [00:11<00:18,  1.67it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.28s. Batch: 0.57s. S_Loss: 0.9500. T_Loss: 3.5597. Mask: 0.9709. :  38%|███▊      | 19/50 [00:11<00:18,  1.67it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.28s. Batch: 0.57s. S_Loss: 0.9500. T_Loss: 3.5597. Mask: 0.9709. :  40%|████      | 20/50 [00:11<00:15,  1.89it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.27s. Batch: 0.56s. S_Loss: 0.9498. T_Loss: 3.5405. Mask: 0.9701. :  40%|████      | 20/50 [00:11<00:15,  1.89it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.27s. Batch: 0.56s. S_Loss: 0.9498. T_Loss: 3.5405. Mask: 0.9701. :  42%|████▏     | 21/50 [00:11<00:13,  2.08it/s]total : 1000  current step :  619
total : 1000  current step :  620
total : 1000  current step :  621
Train Iter: 622/1000. LR: 0.0389. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9504. T_Loss: 3.5429. Mask: 0.9700. :  42%|████▏     | 21/50 [00:12<00:13,  2.08it/s]Train Iter: 622/1000. LR: 0.0389. Data: 0.29s. Batch: 0.58s. S_Loss: 0.9504. T_Loss: 3.5429. Mask: 0.9700. :  44%|████▍     | 22/50 [00:12<00:17,  1.64it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9500. T_Loss: 3.5507. Mask: 0.9698. :  44%|████▍     | 22/50 [00:13<00:17,  1.64it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9500. T_Loss: 3.5507. Mask: 0.9698. :  46%|████▌     | 23/50 [00:13<00:15,  1.75it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.28s. Batch: 0.56s. S_Loss: 0.9505. T_Loss: 3.5588. Mask: 0.9694. :  46%|████▌     | 23/50 [00:13<00:15,  1.75it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.28s. Batch: 0.56s. S_Loss: 0.9505. T_Loss: 3.5588. Mask: 0.9694. :  48%|████▊     | 24/50 [00:13<00:13,  1.92it/s]total : 1000  current step :  622
total : 1000  current step :  623
total : 1000  current step :  624
Train Iter: 625/1000. LR: 0.0391. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9491. T_Loss: 3.5512. Mask: 0.9692. :  48%|████▊     | 24/50 [00:14<00:13,  1.92it/s]Train Iter: 625/1000. LR: 0.0391. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9491. T_Loss: 3.5512. Mask: 0.9692. :  50%|█████     | 25/50 [00:14<00:16,  1.53it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9498. T_Loss: 3.5446. Mask: 0.9694. :  50%|█████     | 25/50 [00:15<00:16,  1.53it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9498. T_Loss: 3.5446. Mask: 0.9694. :  52%|█████▏    | 26/50 [00:15<00:14,  1.64it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9499. T_Loss: 3.5399. Mask: 0.9695. :  52%|█████▏    | 26/50 [00:15<00:14,  1.64it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9499. T_Loss: 3.5399. Mask: 0.9695. :  54%|█████▍    | 27/50 [00:15<00:12,  1.86it/s]total : 1000  current step :  625
total : 1000  current step :  626
total : 1000  current step :  627
Train Iter: 628/1000. LR: 0.0393. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9484. T_Loss: 3.5274. Mask: 0.9696. :  54%|█████▍    | 27/50 [00:16<00:12,  1.86it/s]Train Iter: 628/1000. LR: 0.0393. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9484. T_Loss: 3.5274. Mask: 0.9696. :  56%|█████▌    | 28/50 [00:16<00:14,  1.50it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9484. T_Loss: 3.5195. Mask: 0.9700. :  56%|█████▌    | 28/50 [00:16<00:14,  1.50it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9484. T_Loss: 3.5195. Mask: 0.9700. :  58%|█████▊    | 29/50 [00:16<00:12,  1.65it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9470. T_Loss: 3.5122. Mask: 0.9704. :  58%|█████▊    | 29/50 [00:17<00:12,  1.65it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9470. T_Loss: 3.5122. Mask: 0.9704. :  60%|██████    | 30/50 [00:17<00:10,  1.92it/s]total : 1000  current step :  628
total : 1000  current step :  629
total : 1000  current step :  630
Train Iter: 631/1000. LR: 0.0394. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9469. T_Loss: 3.5029. Mask: 0.9704. :  60%|██████    | 30/50 [00:18<00:10,  1.92it/s]Train Iter: 631/1000. LR: 0.0394. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9469. T_Loss: 3.5029. Mask: 0.9704. :  62%|██████▏   | 31/50 [00:18<00:12,  1.54it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9461. T_Loss: 3.4908. Mask: 0.9702. :  62%|██████▏   | 31/50 [00:18<00:12,  1.54it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9461. T_Loss: 3.4908. Mask: 0.9702. :  64%|██████▍   | 32/50 [00:18<00:09,  1.81it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9466. T_Loss: 3.4814. Mask: 0.9698. :  64%|██████▍   | 32/50 [00:18<00:09,  1.81it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9466. T_Loss: 3.4814. Mask: 0.9698. :  66%|██████▌   | 33/50 [00:18<00:08,  2.10it/s]total : 1000  current step :  631
total : 1000  current step :  632
total : 1000  current step :  633
Train Iter: 634/1000. LR: 0.0396. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9465. T_Loss: 3.4774. Mask: 0.9699. :  66%|██████▌   | 33/50 [00:19<00:08,  2.10it/s]Train Iter: 634/1000. LR: 0.0396. Data: 0.31s. Batch: 0.58s. S_Loss: 0.9465. T_Loss: 3.4774. Mask: 0.9699. :  68%|██████▊   | 34/50 [00:19<00:09,  1.65it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9460. T_Loss: 3.4799. Mask: 0.9696. :  68%|██████▊   | 34/50 [00:20<00:09,  1.65it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9460. T_Loss: 3.4799. Mask: 0.9696. :  70%|███████   | 35/50 [00:20<00:08,  1.84it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9457. T_Loss: 3.4730. Mask: 0.9694. :  70%|███████   | 35/50 [00:20<00:08,  1.84it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9457. T_Loss: 3.4730. Mask: 0.9694. :  72%|███████▏  | 36/50 [00:20<00:07,  1.92it/s]total : 1000  current step :  634
total : 1000  current step :  635
total : 1000  current step :  636
Train Iter: 637/1000. LR: 0.0398. Data: 0.31s. Batch: 0.59s. S_Loss: 0.9460. T_Loss: 3.4732. Mask: 0.9694. :  72%|███████▏  | 36/50 [00:21<00:07,  1.92it/s]Train Iter: 637/1000. LR: 0.0398. Data: 0.31s. Batch: 0.59s. S_Loss: 0.9460. T_Loss: 3.4732. Mask: 0.9694. :  74%|███████▍  | 37/50 [00:21<00:09,  1.37it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9456. T_Loss: 3.4792. Mask: 0.9692. :  74%|███████▍  | 37/50 [00:22<00:09,  1.37it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.30s. Batch: 0.58s. S_Loss: 0.9456. T_Loss: 3.4792. Mask: 0.9692. :  76%|███████▌  | 38/50 [00:22<00:07,  1.61it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9456. T_Loss: 3.4849. Mask: 0.9692. :  76%|███████▌  | 38/50 [00:22<00:07,  1.61it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.30s. Batch: 0.57s. S_Loss: 0.9456. T_Loss: 3.4849. Mask: 0.9692. :  78%|███████▊  | 39/50 [00:22<00:05,  2.01it/s]total : 1000  current step :  637
total : 1000  current step :  638
total : 1000  current step :  639
Train Iter: 640/1000. LR: 0.0400. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9454. T_Loss: 3.4829. Mask: 0.9691. :  78%|███████▊  | 39/50 [00:23<00:05,  2.01it/s]Train Iter: 640/1000. LR: 0.0400. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9454. T_Loss: 3.4829. Mask: 0.9691. :  80%|████████  | 40/50 [00:23<00:07,  1.37it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9454. T_Loss: 3.4843. Mask: 0.9693. :  80%|████████  | 40/50 [00:24<00:07,  1.37it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9454. T_Loss: 3.4843. Mask: 0.9693. :  82%|████████▏ | 41/50 [00:24<00:06,  1.40it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9449. T_Loss: 3.4856. Mask: 0.9695. :  82%|████████▏ | 41/50 [00:24<00:06,  1.40it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9449. T_Loss: 3.4856. Mask: 0.9695. :  84%|████████▍ | 42/50 [00:24<00:04,  1.62it/s]total : 1000  current step :  640
total : 1000  current step :  641
total : 1000  current step :  642
Train Iter: 643/1000. LR: 0.0402. Data: 0.33s. Batch: 0.59s. S_Loss: 0.9448. T_Loss: 3.4831. Mask: 0.9691. :  84%|████████▍ | 42/50 [00:25<00:04,  1.62it/s]Train Iter: 643/1000. LR: 0.0402. Data: 0.33s. Batch: 0.59s. S_Loss: 0.9448. T_Loss: 3.4831. Mask: 0.9691. :  86%|████████▌ | 43/50 [00:25<00:05,  1.38it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9449. T_Loss: 3.4825. Mask: 0.9692. :  86%|████████▌ | 43/50 [00:25<00:05,  1.38it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9449. T_Loss: 3.4825. Mask: 0.9692. :  88%|████████▊ | 44/50 [00:25<00:03,  1.63it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9456. T_Loss: 3.4859. Mask: 0.9694. :  88%|████████▊ | 44/50 [00:26<00:03,  1.63it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9456. T_Loss: 3.4859. Mask: 0.9694. :  90%|█████████ | 45/50 [00:26<00:02,  1.85it/s]total : 1000  current step :  643
total : 1000  current step :  644
total : 1000  current step :  645
Train Iter: 646/1000. LR: 0.0404. Data: 0.33s. Batch: 0.60s. S_Loss: 0.9456. T_Loss: 3.4840. Mask: 0.9697. :  90%|█████████ | 45/50 [00:27<00:02,  1.85it/s]Train Iter: 646/1000. LR: 0.0404. Data: 0.33s. Batch: 0.60s. S_Loss: 0.9456. T_Loss: 3.4840. Mask: 0.9697. :  92%|█████████▏| 46/50 [00:27<00:02,  1.40it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.33s. Batch: 0.59s. S_Loss: 0.9464. T_Loss: 3.4808. Mask: 0.9699. :  92%|█████████▏| 46/50 [00:27<00:02,  1.40it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.33s. Batch: 0.59s. S_Loss: 0.9464. T_Loss: 3.4808. Mask: 0.9699. :  94%|█████████▍| 47/50 [00:27<00:01,  1.54it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9458. T_Loss: 3.4827. Mask: 0.9701. :  94%|█████████▍| 47/50 [00:28<00:01,  1.54it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.32s. Batch: 0.59s. S_Loss: 0.9458. T_Loss: 3.4827. Mask: 0.9701. :  96%|█████████▌| 48/50 [00:28<00:01,  1.75it/s]total : 1000  current step :  646
total : 1000  current step :  647
total : 1000  current step :  648
Train Iter: 649/1000. LR: 0.0406. Data: 0.33s. Batch: 0.60s. S_Loss: 0.9466. T_Loss: 3.4980. Mask: 0.9701. :  96%|█████████▌| 48/50 [00:29<00:01,  1.75it/s]Train Iter: 649/1000. LR: 0.0406. Data: 0.33s. Batch: 0.60s. S_Loss: 0.9466. T_Loss: 3.4980. Mask: 0.9701. :  98%|█████████▊| 49/50 [00:29<00:00,  1.40it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.33s. Batch: 0.59s. S_Loss: 0.9465. T_Loss: 3.4992. Mask: 0.9701. :  98%|█████████▊| 49/50 [00:29<00:00,  1.40it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.33s. Batch: 0.59s. S_Loss: 0.9465. T_Loss: 3.4992. Mask: 0.9701. : 100%|██████████| 50/50 [00:29<00:00,  1.59it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.33s. Batch: 0.59s. S_Loss: 0.9465. T_Loss: 3.4992. Mask: 0.9701. : 100%|██████████| 50/50 [00:29<00:00,  1.68it/s]
total : 1000  current step :  649
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.1373. top1: 80.47. top5: 98.83. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.1373. top1: 80.47. top5: 98.83. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.1247. top1: 81.25. top5: 99.02. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.1247. top1: 81.25. top5: 99.02. :  25%|██▌       | 2/8 [00:00<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.1128. top1: 82.42. top5: 99.22. :  25%|██▌       | 2/8 [00:01<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.1128. top1: 82.42. top5: 99.22. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0916. top1: 83.30. top5: 99.22. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0916. top1: 83.30. top5: 99.22. :  50%|█████     | 4/8 [00:01<00:01,  2.93it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0455. top1: 85.47. top5: 99.38. :  50%|█████     | 4/8 [00:01<00:01,  2.93it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0455. top1: 85.47. top5: 99.38. :  62%|██████▎   | 5/8 [00:01<00:00,  3.00it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0135. top1: 86.85. top5: 99.48. :  62%|██████▎   | 5/8 [00:02<00:00,  3.00it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0135. top1: 86.85. top5: 99.48. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9860. top1: 88.28. top5: 99.55. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9860. top1: 88.28. top5: 99.55. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9699. top1: 89.20. top5: 99.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9699. top1: 89.20. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9699. top1: 89.20. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
total : 1000  current step :  650
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.01s. Batch: 0.26s. S_Loss: 0.9215. T_Loss: 3.3902. Mask: 0.9492. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.01s. Batch: 0.26s. S_Loss: 0.9215. T_Loss: 3.3902. Mask: 0.9492. :   2%|▏         | 1/50 [00:00<00:12,  3.79it/s]total : 1000  current step :  651
Train Iter: 652/1000. LR: 0.0408. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9039. T_Loss: 3.4638. Mask: 0.9609. :   2%|▏         | 1/50 [00:01<00:12,  3.79it/s]Train Iter: 652/1000. LR: 0.0408. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9039. T_Loss: 3.4638. Mask: 0.9609. :   4%|▍         | 2/50 [00:01<00:36,  1.31it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.33s. Batch: 0.56s. S_Loss: 0.9249. T_Loss: 3.5982. Mask: 0.9648. :   4%|▍         | 2/50 [00:01<00:36,  1.31it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.33s. Batch: 0.56s. S_Loss: 0.9249. T_Loss: 3.5982. Mask: 0.9648. :   6%|▌         | 3/50 [00:01<00:26,  1.79it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9355. T_Loss: 3.7142. Mask: 0.9658. :   6%|▌         | 3/50 [00:02<00:26,  1.79it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.29s. Batch: 0.51s. S_Loss: 0.9355. T_Loss: 3.7142. Mask: 0.9658. :   8%|▊         | 4/50 [00:02<00:22,  2.08it/s]total : 1000  current step :  652
total : 1000  current step :  653
total : 1000  current step :  654
Train Iter: 655/1000. LR: 0.0409. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9475. T_Loss: 3.7109. Mask: 0.9656. :   8%|▊         | 4/50 [00:03<00:22,  2.08it/s]Train Iter: 655/1000. LR: 0.0409. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9475. T_Loss: 3.7109. Mask: 0.9656. :  10%|█         | 5/50 [00:03<00:30,  1.46it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9414. T_Loss: 3.6913. Mask: 0.9655. :  10%|█         | 5/50 [00:03<00:30,  1.46it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9414. T_Loss: 3.6913. Mask: 0.9655. :  12%|█▏        | 6/50 [00:03<00:25,  1.69it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9413. T_Loss: 3.7334. Mask: 0.9654. :  12%|█▏        | 6/50 [00:03<00:25,  1.69it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.33s. Batch: 0.55s. S_Loss: 0.9413. T_Loss: 3.7334. Mask: 0.9654. :  14%|█▍        | 7/50 [00:03<00:21,  1.98it/s]total : 1000  current step :  655
total : 1000  current step :  656
total : 1000  current step :  657
Train Iter: 658/1000. LR: 0.0411. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9369. T_Loss: 3.7718. Mask: 0.9683. :  14%|█▍        | 7/50 [00:05<00:21,  1.98it/s]Train Iter: 658/1000. LR: 0.0411. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9369. T_Loss: 3.7718. Mask: 0.9683. :  16%|█▌        | 8/50 [00:05<00:31,  1.34it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9317. T_Loss: 3.7424. Mask: 0.9692. :  16%|█▌        | 8/50 [00:05<00:31,  1.34it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9317. T_Loss: 3.7424. Mask: 0.9692. :  18%|█▊        | 9/50 [00:05<00:24,  1.66it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9273. T_Loss: 3.7079. Mask: 0.9695. :  18%|█▊        | 9/50 [00:05<00:24,  1.66it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9273. T_Loss: 3.7079. Mask: 0.9695. :  20%|██        | 10/50 [00:05<00:21,  1.84it/s]total : 1000  current step :  658
total : 1000  current step :  659
total : 1000  current step :  660
Train Iter: 661/1000. LR: 0.0413. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9265. T_Loss: 3.6653. Mask: 0.9691. :  20%|██        | 10/50 [00:07<00:21,  1.84it/s]Train Iter: 661/1000. LR: 0.0413. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9265. T_Loss: 3.6653. Mask: 0.9691. :  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9251. T_Loss: 3.6560. Mask: 0.9710. :  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9251. T_Loss: 3.6560. Mask: 0.9710. :  24%|██▍       | 12/50 [00:07<00:23,  1.62it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9243. T_Loss: 3.6182. Mask: 0.9712. :  24%|██▍       | 12/50 [00:07<00:23,  1.62it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9243. T_Loss: 3.6182. Mask: 0.9712. :  26%|██▌       | 13/50 [00:07<00:20,  1.78it/s]total : 1000  current step :  661
total : 1000  current step :  662
total : 1000  current step :  663
Train Iter: 664/1000. LR: 0.0415. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9234. T_Loss: 3.5847. Mask: 0.9713. :  26%|██▌       | 13/50 [00:09<00:20,  1.78it/s]Train Iter: 664/1000. LR: 0.0415. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9234. T_Loss: 3.5847. Mask: 0.9713. :  28%|██▊       | 14/50 [00:09<00:28,  1.26it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9286. T_Loss: 3.5874. Mask: 0.9719. :  28%|██▊       | 14/50 [00:09<00:28,  1.26it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9286. T_Loss: 3.5874. Mask: 0.9719. :  30%|███       | 15/50 [00:09<00:23,  1.50it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9309. T_Loss: 3.5624. Mask: 0.9717. :  30%|███       | 15/50 [00:09<00:23,  1.50it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9309. T_Loss: 3.5624. Mask: 0.9717. :  32%|███▏      | 16/50 [00:09<00:19,  1.74it/s]total : 1000  current step :  664
total : 1000  current step :  665
total : 1000  current step :  666
Train Iter: 667/1000. LR: 0.0417. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9291. T_Loss: 3.5449. Mask: 0.9715. :  32%|███▏      | 16/50 [00:11<00:19,  1.74it/s]Train Iter: 667/1000. LR: 0.0417. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9291. T_Loss: 3.5449. Mask: 0.9715. :  34%|███▍      | 17/50 [00:11<00:27,  1.18it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9290. T_Loss: 3.5152. Mask: 0.9716. :  34%|███▍      | 17/50 [00:11<00:27,  1.18it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9290. T_Loss: 3.5152. Mask: 0.9716. :  36%|███▌      | 18/50 [00:11<00:22,  1.41it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9259. T_Loss: 3.4826. Mask: 0.9716. :  36%|███▌      | 18/50 [00:11<00:22,  1.41it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9259. T_Loss: 3.4826. Mask: 0.9716. :  38%|███▊      | 19/50 [00:11<00:18,  1.71it/s]total : 1000  current step :  667
total : 1000  current step :  668
total : 1000  current step :  669
Train Iter: 670/1000. LR: 0.0419. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9245. T_Loss: 3.4645. Mask: 0.9719. :  38%|███▊      | 19/50 [00:13<00:18,  1.71it/s]Train Iter: 670/1000. LR: 0.0419. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9245. T_Loss: 3.4645. Mask: 0.9719. :  40%|████      | 20/50 [00:13<00:25,  1.17it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9241. T_Loss: 3.4387. Mask: 0.9721. :  40%|████      | 20/50 [00:13<00:25,  1.17it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9241. T_Loss: 3.4387. Mask: 0.9721. :  42%|████▏     | 21/50 [00:13<00:21,  1.38it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9228. T_Loss: 3.4377. Mask: 0.9725. :  42%|████▏     | 21/50 [00:14<00:21,  1.38it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9228. T_Loss: 3.4377. Mask: 0.9725. :  44%|████▍     | 22/50 [00:14<00:17,  1.61it/s]total : 1000  current step :  670
total : 1000  current step :  671
total : 1000  current step :  672
Train Iter: 673/1000. LR: 0.0421. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9239. T_Loss: 3.4372. Mask: 0.9727. :  44%|████▍     | 22/50 [00:15<00:17,  1.61it/s]Train Iter: 673/1000. LR: 0.0421. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9239. T_Loss: 3.4372. Mask: 0.9727. :  46%|████▌     | 23/50 [00:15<00:22,  1.20it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9237. T_Loss: 3.4220. Mask: 0.9717. :  46%|████▌     | 23/50 [00:16<00:22,  1.20it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9237. T_Loss: 3.4220. Mask: 0.9717. :  48%|████▊     | 24/50 [00:16<00:18,  1.42it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9231. T_Loss: 3.4048. Mask: 0.9714. :  48%|████▊     | 24/50 [00:16<00:18,  1.42it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9231. T_Loss: 3.4048. Mask: 0.9714. :  50%|█████     | 25/50 [00:16<00:15,  1.63it/s]total : 1000  current step :  673
total : 1000  current step :  674
total : 1000  current step :  675
Train Iter: 676/1000. LR: 0.0423. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9216. T_Loss: 3.4045. Mask: 0.9718. :  50%|█████     | 25/50 [00:17<00:15,  1.63it/s]Train Iter: 676/1000. LR: 0.0423. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9216. T_Loss: 3.4045. Mask: 0.9718. :  52%|█████▏    | 26/50 [00:17<00:19,  1.24it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9244. T_Loss: 3.4289. Mask: 0.9715. :  52%|█████▏    | 26/50 [00:18<00:19,  1.24it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9244. T_Loss: 3.4289. Mask: 0.9715. :  54%|█████▍    | 27/50 [00:18<00:16,  1.41it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9248. T_Loss: 3.4358. Mask: 0.9718. :  54%|█████▍    | 27/50 [00:18<00:16,  1.41it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9248. T_Loss: 3.4358. Mask: 0.9718. :  56%|█████▌    | 28/50 [00:18<00:13,  1.64it/s]total : 1000  current step :  676
total : 1000  current step :  677
total : 1000  current step :  678
Train Iter: 679/1000. LR: 0.0424. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9252. T_Loss: 3.4557. Mask: 0.9717. :  56%|█████▌    | 28/50 [00:19<00:13,  1.64it/s]Train Iter: 679/1000. LR: 0.0424. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9252. T_Loss: 3.4557. Mask: 0.9717. :  58%|█████▊    | 29/50 [00:19<00:16,  1.25it/s]total : 1000  current step :  679
Train Iter: 680/1000. LR: 0.0425. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9252. T_Loss: 3.4711. Mask: 0.9714. :  58%|█████▊    | 29/50 [00:20<00:16,  1.25it/s]Train Iter: 680/1000. LR: 0.0425. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9252. T_Loss: 3.4711. Mask: 0.9714. :  60%|██████    | 30/50 [00:20<00:16,  1.21it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9250. T_Loss: 3.4951. Mask: 0.9711. :  60%|██████    | 30/50 [00:21<00:16,  1.21it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9250. T_Loss: 3.4951. Mask: 0.9711. :  62%|██████▏   | 31/50 [00:21<00:13,  1.37it/s]total : 1000  current step :  680
total : 1000  current step :  681
Train Iter: 682/1000. LR: 0.0426. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9257. T_Loss: 3.5196. Mask: 0.9702. :  62%|██████▏   | 31/50 [00:22<00:13,  1.37it/s]Train Iter: 682/1000. LR: 0.0426. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9257. T_Loss: 3.5196. Mask: 0.9702. :  64%|██████▍   | 32/50 [00:22<00:15,  1.19it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9262. T_Loss: 3.5411. Mask: 0.9698. :  64%|██████▍   | 32/50 [00:22<00:15,  1.19it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9262. T_Loss: 3.5411. Mask: 0.9698. :  66%|██████▌   | 33/50 [00:22<00:12,  1.39it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9268. T_Loss: 3.5565. Mask: 0.9696. :  66%|██████▌   | 33/50 [00:23<00:12,  1.39it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9268. T_Loss: 3.5565. Mask: 0.9696. :  68%|██████▊   | 34/50 [00:23<00:09,  1.63it/s]total : 1000  current step :  682
total : 1000  current step :  683
total : 1000  current step :  684
Train Iter: 685/1000. LR: 0.0428. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9272. T_Loss: 3.5642. Mask: 0.9694. :  68%|██████▊   | 34/50 [00:24<00:09,  1.63it/s]Train Iter: 685/1000. LR: 0.0428. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9272. T_Loss: 3.5642. Mask: 0.9694. :  70%|███████   | 35/50 [00:24<00:12,  1.23it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9278. T_Loss: 3.5709. Mask: 0.9696. :  70%|███████   | 35/50 [00:24<00:12,  1.23it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9278. T_Loss: 3.5709. Mask: 0.9696. :  72%|███████▏  | 36/50 [00:24<00:09,  1.47it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9282. T_Loss: 3.5710. Mask: 0.9698. :  72%|███████▏  | 36/50 [00:25<00:09,  1.47it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9282. T_Loss: 3.5710. Mask: 0.9698. :  74%|███████▍  | 37/50 [00:25<00:07,  1.76it/s]total : 1000  current step :  685
total : 1000  current step :  686
total : 1000  current step :  687
Train Iter: 688/1000. LR: 0.0430. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9282. T_Loss: 3.5558. Mask: 0.9700. :  74%|███████▍  | 37/50 [00:26<00:07,  1.76it/s]Train Iter: 688/1000. LR: 0.0430. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9282. T_Loss: 3.5558. Mask: 0.9700. :  76%|███████▌  | 38/50 [00:26<00:08,  1.42it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9279. T_Loss: 3.5420. Mask: 0.9697. :  76%|███████▌  | 38/50 [00:26<00:08,  1.42it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9279. T_Loss: 3.5420. Mask: 0.9697. :  78%|███████▊  | 39/50 [00:26<00:06,  1.63it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9276. T_Loss: 3.5430. Mask: 0.9698. :  78%|███████▊  | 39/50 [00:26<00:06,  1.63it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9276. T_Loss: 3.5430. Mask: 0.9698. :  80%|████████  | 40/50 [00:26<00:05,  1.90it/s]total : 1000  current step :  688
total : 1000  current step :  689
total : 1000  current step :  690
Train Iter: 691/1000. LR: 0.0432. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9270. T_Loss: 3.5304. Mask: 0.9698. :  80%|████████  | 40/50 [00:27<00:05,  1.90it/s]Train Iter: 691/1000. LR: 0.0432. Data: 0.43s. Batch: 0.68s. S_Loss: 0.9270. T_Loss: 3.5304. Mask: 0.9698. :  82%|████████▏ | 41/50 [00:27<00:06,  1.42it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9274. T_Loss: 3.5266. Mask: 0.9701. :  82%|████████▏ | 41/50 [00:28<00:06,  1.42it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9274. T_Loss: 3.5266. Mask: 0.9701. :  84%|████████▍ | 42/50 [00:28<00:04,  1.63it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9266. T_Loss: 3.5160. Mask: 0.9700. :  84%|████████▍ | 42/50 [00:28<00:04,  1.63it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9266. T_Loss: 3.5160. Mask: 0.9700. :  86%|████████▌ | 43/50 [00:28<00:03,  1.93it/s]total : 1000  current step :  691
total : 1000  current step :  692
total : 1000  current step :  693
Train Iter: 694/1000. LR: 0.0434. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9266. T_Loss: 3.5134. Mask: 0.9702. :  86%|████████▌ | 43/50 [00:29<00:03,  1.93it/s]Train Iter: 694/1000. LR: 0.0434. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9266. T_Loss: 3.5134. Mask: 0.9702. :  88%|████████▊ | 44/50 [00:29<00:04,  1.35it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9268. T_Loss: 3.5101. Mask: 0.9704. :  88%|████████▊ | 44/50 [00:30<00:04,  1.35it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9268. T_Loss: 3.5101. Mask: 0.9704. :  90%|█████████ | 45/50 [00:30<00:03,  1.56it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9266. T_Loss: 3.5059. Mask: 0.9708. :  90%|█████████ | 45/50 [00:30<00:03,  1.56it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9266. T_Loss: 3.5059. Mask: 0.9708. :  92%|█████████▏| 46/50 [00:30<00:02,  1.68it/s]total : 1000  current step :  694
total : 1000  current step :  695
total : 1000  current step :  696
Train Iter: 697/1000. LR: 0.0436. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9263. T_Loss: 3.4971. Mask: 0.9707. :  92%|█████████▏| 46/50 [00:32<00:02,  1.68it/s]Train Iter: 697/1000. LR: 0.0436. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9263. T_Loss: 3.4971. Mask: 0.9707. :  94%|█████████▍| 47/50 [00:32<00:02,  1.24it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9259. T_Loss: 3.4963. Mask: 0.9708. :  94%|█████████▍| 47/50 [00:32<00:02,  1.24it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9259. T_Loss: 3.4963. Mask: 0.9708. :  96%|█████████▌| 48/50 [00:32<00:01,  1.47it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9273. T_Loss: 3.5015. Mask: 0.9707. :  96%|█████████▌| 48/50 [00:32<00:01,  1.47it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9273. T_Loss: 3.5015. Mask: 0.9707. :  98%|█████████▊| 49/50 [00:32<00:00,  1.68it/s]total : 1000  current step :  697
total : 1000  current step :  698
total : 1000  current step :  699
Train Iter: 700/1000. LR: 0.0438. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9270. T_Loss: 3.5015. Mask: 0.9709. :  98%|█████████▊| 49/50 [00:34<00:00,  1.68it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9270. T_Loss: 3.5015. Mask: 0.9709. : 100%|██████████| 50/50 [00:34<00:00,  1.26it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9270. T_Loss: 3.5015. Mask: 0.9709. : 100%|██████████| 50/50 [00:34<00:00,  1.47it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.2122. top1: 76.17. top5: 98.05. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.2122. top1: 76.17. top5: 98.05. :  12%|█▎        | 1/8 [00:00<00:03,  1.78it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.2021. top1: 76.56. top5: 98.44. :  12%|█▎        | 1/8 [00:00<00:03,  1.78it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.2021. top1: 76.56. top5: 98.44. :  25%|██▌       | 2/8 [00:00<00:02,  2.49it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.1869. top1: 78.26. top5: 98.57. :  25%|██▌       | 2/8 [00:01<00:02,  2.49it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.1869. top1: 78.26. top5: 98.57. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1575. top1: 79.59. top5: 98.63. :  38%|███▊      | 3/8 [00:01<00:01,  2.98it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1575. top1: 79.59. top5: 98.63. :  50%|█████     | 4/8 [00:01<00:01,  3.46it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0878. top1: 82.89. top5: 98.91. :  50%|█████     | 4/8 [00:01<00:01,  3.46it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0878. top1: 82.89. top5: 98.91. :  62%|██████▎   | 5/8 [00:01<00:00,  3.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0408. top1: 85.29. top5: 99.09. :  62%|██████▎   | 5/8 [00:01<00:00,  3.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0408. top1: 85.29. top5: 99.09. :  75%|███████▌  | 6/8 [00:01<00:00,  3.71it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0031. top1: 87.22. top5: 99.22. :  75%|███████▌  | 6/8 [00:02<00:00,  3.71it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0031. top1: 87.22. top5: 99.22. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9806. top1: 88.35. top5: 99.30. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9806. top1: 88.35. top5: 99.30. : 100%|██████████| 8/8 [00:02<00:00,  3.59it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9806. top1: 88.35. top5: 99.30. : 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]
total : 1000  current step :  700
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.32s. S_Loss: 0.9051. T_Loss: 3.5613. Mask: 0.9766. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.32s. S_Loss: 0.9051. T_Loss: 3.5613. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:15,  3.07it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.32s. S_Loss: 0.9079. T_Loss: 3.5386. Mask: 0.9707. :   2%|▏         | 1/50 [00:00<00:15,  3.07it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.32s. S_Loss: 0.9079. T_Loss: 3.5386. Mask: 0.9707. :   4%|▍         | 2/50 [00:00<00:15,  3.08it/s]total : 1000  current step :  701
total : 1000  current step :  702
Train Iter: 703/1000. LR: 0.0439. Data: 0.29s. Batch: 0.59s. S_Loss: 0.9138. T_Loss: 3.4957. Mask: 0.9688. :   4%|▍         | 2/50 [00:01<00:15,  3.08it/s]Train Iter: 703/1000. LR: 0.0439. Data: 0.29s. Batch: 0.59s. S_Loss: 0.9138. T_Loss: 3.4957. Mask: 0.9688. :   6%|▌         | 3/50 [00:01<00:32,  1.45it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.25s. Batch: 0.55s. S_Loss: 0.9418. T_Loss: 3.6127. Mask: 0.9736. :   6%|▌         | 3/50 [00:02<00:32,  1.45it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.25s. Batch: 0.55s. S_Loss: 0.9418. T_Loss: 3.6127. Mask: 0.9736. :   8%|▊         | 4/50 [00:02<00:27,  1.70it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.20s. Batch: 0.51s. S_Loss: 0.9396. T_Loss: 3.6174. Mask: 0.9734. :   8%|▊         | 4/50 [00:02<00:27,  1.70it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.20s. Batch: 0.51s. S_Loss: 0.9396. T_Loss: 3.6174. Mask: 0.9734. :  10%|█         | 5/50 [00:02<00:22,  2.00it/s]total : 1000  current step :  703
total : 1000  current step :  704
total : 1000  current step :  705
Train Iter: 706/1000. LR: 0.0441. Data: 0.33s. Batch: 0.64s. S_Loss: 0.9435. T_Loss: 3.6703. Mask: 0.9733. :  10%|█         | 5/50 [00:03<00:22,  2.00it/s]Train Iter: 706/1000. LR: 0.0441. Data: 0.33s. Batch: 0.64s. S_Loss: 0.9435. T_Loss: 3.6703. Mask: 0.9733. :  12%|█▏        | 6/50 [00:03<00:33,  1.31it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.29s. Batch: 0.60s. S_Loss: 0.9453. T_Loss: 3.6805. Mask: 0.9715. :  12%|█▏        | 6/50 [00:04<00:33,  1.31it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.29s. Batch: 0.60s. S_Loss: 0.9453. T_Loss: 3.6805. Mask: 0.9715. :  14%|█▍        | 7/50 [00:04<00:27,  1.56it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.27s. Batch: 0.58s. S_Loss: 0.9462. T_Loss: 3.7112. Mask: 0.9707. :  14%|█▍        | 7/50 [00:04<00:27,  1.56it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.27s. Batch: 0.58s. S_Loss: 0.9462. T_Loss: 3.7112. Mask: 0.9707. :  16%|█▌        | 8/50 [00:04<00:23,  1.76it/s]total : 1000  current step :  706
total : 1000  current step :  707
total : 1000  current step :  708
Train Iter: 709/1000. LR: 0.0443. Data: 0.35s. Batch: 0.66s. S_Loss: 0.9447. T_Loss: 3.6995. Mask: 0.9714. :  16%|█▌        | 8/50 [00:05<00:23,  1.76it/s]Train Iter: 709/1000. LR: 0.0443. Data: 0.35s. Batch: 0.66s. S_Loss: 0.9447. T_Loss: 3.6995. Mask: 0.9714. :  18%|█▊        | 9/50 [00:05<00:33,  1.24it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.32s. Batch: 0.63s. S_Loss: 0.9418. T_Loss: 3.6830. Mask: 0.9715. :  18%|█▊        | 9/50 [00:06<00:33,  1.24it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.32s. Batch: 0.63s. S_Loss: 0.9418. T_Loss: 3.6830. Mask: 0.9715. :  20%|██        | 10/50 [00:06<00:26,  1.53it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.30s. Batch: 0.61s. S_Loss: 0.9408. T_Loss: 3.6708. Mask: 0.9716. :  20%|██        | 10/50 [00:06<00:26,  1.53it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.30s. Batch: 0.61s. S_Loss: 0.9408. T_Loss: 3.6708. Mask: 0.9716. :  22%|██▏       | 11/50 [00:06<00:22,  1.72it/s]total : 1000  current step :  709
total : 1000  current step :  710
total : 1000  current step :  711
Train Iter: 712/1000. LR: 0.0445. Data: 0.34s. Batch: 0.65s. S_Loss: 0.9401. T_Loss: 3.6239. Mask: 0.9710. :  22%|██▏       | 11/50 [00:07<00:22,  1.72it/s]Train Iter: 712/1000. LR: 0.0445. Data: 0.34s. Batch: 0.65s. S_Loss: 0.9401. T_Loss: 3.6239. Mask: 0.9710. :  24%|██▍       | 12/50 [00:07<00:28,  1.35it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.33s. Batch: 0.64s. S_Loss: 0.9382. T_Loss: 3.5909. Mask: 0.9718. :  24%|██▍       | 12/50 [00:08<00:28,  1.35it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.33s. Batch: 0.64s. S_Loss: 0.9382. T_Loss: 3.5909. Mask: 0.9718. :  26%|██▌       | 13/50 [00:08<00:25,  1.46it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.31s. Batch: 0.61s. S_Loss: 0.9381. T_Loss: 3.5648. Mask: 0.9727. :  26%|██▌       | 13/50 [00:08<00:25,  1.46it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.31s. Batch: 0.61s. S_Loss: 0.9381. T_Loss: 3.5648. Mask: 0.9727. :  28%|██▊       | 14/50 [00:08<00:19,  1.85it/s]total : 1000  current step :  712
total : 1000  current step :  713
total : 1000  current step :  714
Train Iter: 715/1000. LR: 0.0447. Data: 0.36s. Batch: 0.66s. S_Loss: 0.9352. T_Loss: 3.5166. Mask: 0.9740. :  28%|██▊       | 14/50 [00:09<00:19,  1.85it/s]Train Iter: 715/1000. LR: 0.0447. Data: 0.36s. Batch: 0.66s. S_Loss: 0.9352. T_Loss: 3.5166. Mask: 0.9740. :  30%|███       | 15/50 [00:09<00:27,  1.27it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.35s. Batch: 0.64s. S_Loss: 0.9340. T_Loss: 3.4912. Mask: 0.9744. :  30%|███       | 15/50 [00:10<00:27,  1.27it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.35s. Batch: 0.64s. S_Loss: 0.9340. T_Loss: 3.4912. Mask: 0.9744. :  32%|███▏      | 16/50 [00:10<00:21,  1.55it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.34s. Batch: 0.62s. S_Loss: 0.9323. T_Loss: 3.4607. Mask: 0.9740. :  32%|███▏      | 16/50 [00:10<00:21,  1.55it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.34s. Batch: 0.62s. S_Loss: 0.9323. T_Loss: 3.4607. Mask: 0.9740. :  34%|███▍      | 17/50 [00:10<00:18,  1.78it/s]total : 1000  current step :  715
total : 1000  current step :  716
total : 1000  current step :  717
Train Iter: 718/1000. LR: 0.0449. Data: 0.38s. Batch: 0.66s. S_Loss: 0.9302. T_Loss: 3.4508. Mask: 0.9742. :  34%|███▍      | 17/50 [00:11<00:18,  1.78it/s]Train Iter: 718/1000. LR: 0.0449. Data: 0.38s. Batch: 0.66s. S_Loss: 0.9302. T_Loss: 3.4508. Mask: 0.9742. :  36%|███▌      | 18/50 [00:11<00:25,  1.24it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.36s. Batch: 0.64s. S_Loss: 0.9288. T_Loss: 3.4428. Mask: 0.9743. :  36%|███▌      | 18/50 [00:12<00:25,  1.24it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.36s. Batch: 0.64s. S_Loss: 0.9288. T_Loss: 3.4428. Mask: 0.9743. :  38%|███▊      | 19/50 [00:12<00:20,  1.54it/s]total : 1000  current step :  718
total : 1000  current step :  719
Train Iter: 720/1000. LR: 0.0450. Data: 0.37s. Batch: 0.65s. S_Loss: 0.9268. T_Loss: 3.4271. Mask: 0.9744. :  38%|███▊      | 19/50 [00:12<00:20,  1.54it/s]Train Iter: 720/1000. LR: 0.0450. Data: 0.37s. Batch: 0.65s. S_Loss: 0.9268. T_Loss: 3.4271. Mask: 0.9744. :  40%|████      | 20/50 [00:12<00:20,  1.50it/s]total : 1000  current step :  720
Train Iter: 721/1000. LR: 0.0451. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9267. T_Loss: 3.4318. Mask: 0.9745. :  40%|████      | 20/50 [00:14<00:20,  1.50it/s]Train Iter: 721/1000. LR: 0.0451. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9267. T_Loss: 3.4318. Mask: 0.9745. :  42%|████▏     | 21/50 [00:14<00:24,  1.20it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.38s. Batch: 0.66s. S_Loss: 0.9263. T_Loss: 3.4297. Mask: 0.9748. :  42%|████▏     | 21/50 [00:14<00:24,  1.20it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.38s. Batch: 0.66s. S_Loss: 0.9263. T_Loss: 3.4297. Mask: 0.9748. :  44%|████▍     | 22/50 [00:14<00:20,  1.40it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9254. T_Loss: 3.4147. Mask: 0.9744. :  44%|████▍     | 22/50 [00:15<00:20,  1.40it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9254. T_Loss: 3.4147. Mask: 0.9744. :  46%|████▌     | 23/50 [00:15<00:17,  1.57it/s]total : 1000  current step :  721
total : 1000  current step :  722
total : 1000  current step :  723
Train Iter: 724/1000. LR: 0.0453. Data: 0.40s. Batch: 0.68s. S_Loss: 0.9248. T_Loss: 3.4054. Mask: 0.9741. :  46%|████▌     | 23/50 [00:16<00:17,  1.57it/s]Train Iter: 724/1000. LR: 0.0453. Data: 0.40s. Batch: 0.68s. S_Loss: 0.9248. T_Loss: 3.4054. Mask: 0.9741. :  48%|████▊     | 24/50 [00:16<00:21,  1.21it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.39s. Batch: 0.67s. S_Loss: 0.9243. T_Loss: 3.4179. Mask: 0.9742. :  48%|████▊     | 24/50 [00:16<00:21,  1.21it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.39s. Batch: 0.67s. S_Loss: 0.9243. T_Loss: 3.4179. Mask: 0.9742. :  50%|█████     | 25/50 [00:16<00:17,  1.47it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9242. T_Loss: 3.4336. Mask: 0.9742. :  50%|█████     | 25/50 [00:17<00:17,  1.47it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9242. T_Loss: 3.4336. Mask: 0.9742. :  52%|█████▏    | 26/50 [00:17<00:13,  1.74it/s]total : 1000  current step :  724
total : 1000  current step :  725
total : 1000  current step :  726
Train Iter: 727/1000. LR: 0.0454. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9248. T_Loss: 3.4525. Mask: 0.9734. :  52%|█████▏    | 26/50 [00:18<00:13,  1.74it/s]Train Iter: 727/1000. LR: 0.0454. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9248. T_Loss: 3.4525. Mask: 0.9734. :  54%|█████▍    | 27/50 [00:18<00:17,  1.34it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9264. T_Loss: 3.4789. Mask: 0.9731. :  54%|█████▍    | 27/50 [00:18<00:17,  1.34it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9264. T_Loss: 3.4789. Mask: 0.9731. :  56%|█████▌    | 28/50 [00:18<00:15,  1.46it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9259. T_Loss: 3.4862. Mask: 0.9721. :  56%|█████▌    | 28/50 [00:19<00:15,  1.46it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9259. T_Loss: 3.4862. Mask: 0.9721. :  58%|█████▊    | 29/50 [00:19<00:12,  1.73it/s]total : 1000  current step :  727
total : 1000  current step :  728
total : 1000  current step :  729
Train Iter: 730/1000. LR: 0.0456. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9259. T_Loss: 3.5035. Mask: 0.9721. :  58%|█████▊    | 29/50 [00:20<00:12,  1.73it/s]Train Iter: 730/1000. LR: 0.0456. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9259. T_Loss: 3.5035. Mask: 0.9721. :  60%|██████    | 30/50 [00:20<00:15,  1.31it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9269. T_Loss: 3.5242. Mask: 0.9714. :  60%|██████    | 30/50 [00:20<00:15,  1.31it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9269. T_Loss: 3.5242. Mask: 0.9714. :  62%|██████▏   | 31/50 [00:20<00:11,  1.62it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.39s. Batch: 0.66s. S_Loss: 0.9276. T_Loss: 3.5433. Mask: 0.9716. :  62%|██████▏   | 31/50 [00:21<00:11,  1.62it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.39s. Batch: 0.66s. S_Loss: 0.9276. T_Loss: 3.5433. Mask: 0.9716. :  64%|██████▍   | 32/50 [00:21<00:10,  1.66it/s]total : 1000  current step :  730
total : 1000  current step :  731
total : 1000  current step :  732
Train Iter: 733/1000. LR: 0.0458. Data: 0.41s. Batch: 0.68s. S_Loss: 0.9269. T_Loss: 3.5379. Mask: 0.9717. :  64%|██████▍   | 32/50 [00:22<00:10,  1.66it/s]Train Iter: 733/1000. LR: 0.0458. Data: 0.41s. Batch: 0.68s. S_Loss: 0.9269. T_Loss: 3.5379. Mask: 0.9717. :  66%|██████▌   | 33/50 [00:22<00:14,  1.21it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9266. T_Loss: 3.5328. Mask: 0.9712. :  66%|██████▌   | 33/50 [00:22<00:14,  1.21it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9266. T_Loss: 3.5328. Mask: 0.9712. :  68%|██████▊   | 34/50 [00:22<00:10,  1.52it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9278. T_Loss: 3.5322. Mask: 0.9704. :  68%|██████▊   | 34/50 [00:23<00:10,  1.52it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9278. T_Loss: 3.5322. Mask: 0.9704. :  70%|███████   | 35/50 [00:23<00:08,  1.73it/s]total : 1000  current step :  733
total : 1000  current step :  734
total : 1000  current step :  735
Train Iter: 736/1000. LR: 0.0460. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9267. T_Loss: 3.5282. Mask: 0.9707. :  70%|███████   | 35/50 [00:24<00:08,  1.73it/s]Train Iter: 736/1000. LR: 0.0460. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9267. T_Loss: 3.5282. Mask: 0.9707. :  72%|███████▏  | 36/50 [00:24<00:10,  1.29it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9269. T_Loss: 3.5191. Mask: 0.9704. :  72%|███████▏  | 36/50 [00:24<00:10,  1.29it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9269. T_Loss: 3.5191. Mask: 0.9704. :  74%|███████▍  | 37/50 [00:24<00:08,  1.50it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9266. T_Loss: 3.5160. Mask: 0.9709. :  74%|███████▍  | 37/50 [00:25<00:08,  1.50it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9266. T_Loss: 3.5160. Mask: 0.9709. :  76%|███████▌  | 38/50 [00:25<00:07,  1.71it/s]total : 1000  current step :  736
total : 1000  current step :  737
total : 1000  current step :  738
Train Iter: 739/1000. LR: 0.0462. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9266. T_Loss: 3.5089. Mask: 0.9713. :  76%|███████▌  | 38/50 [00:26<00:07,  1.71it/s]Train Iter: 739/1000. LR: 0.0462. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9266. T_Loss: 3.5089. Mask: 0.9713. :  78%|███████▊  | 39/50 [00:26<00:08,  1.31it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9264. T_Loss: 3.5020. Mask: 0.9713. :  78%|███████▊  | 39/50 [00:26<00:08,  1.31it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9264. T_Loss: 3.5020. Mask: 0.9713. :  80%|████████  | 40/50 [00:26<00:06,  1.59it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9271. T_Loss: 3.4962. Mask: 0.9716. :  80%|████████  | 40/50 [00:26<00:06,  1.59it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9271. T_Loss: 3.4962. Mask: 0.9716. :  82%|████████▏ | 41/50 [00:26<00:04,  1.82it/s]total : 1000  current step :  739
total : 1000  current step :  740
total : 1000  current step :  741
Train Iter: 742/1000. LR: 0.0464. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9280. T_Loss: 3.5016. Mask: 0.9717. :  82%|████████▏ | 41/50 [00:28<00:04,  1.82it/s]Train Iter: 742/1000. LR: 0.0464. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9280. T_Loss: 3.5016. Mask: 0.9717. :  84%|████████▍ | 42/50 [00:28<00:06,  1.27it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9278. T_Loss: 3.5022. Mask: 0.9719. :  84%|████████▍ | 42/50 [00:28<00:06,  1.27it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9278. T_Loss: 3.5022. Mask: 0.9719. :  86%|████████▌ | 43/50 [00:28<00:04,  1.52it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9266. T_Loss: 3.4953. Mask: 0.9722. :  86%|████████▌ | 43/50 [00:28<00:04,  1.52it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9266. T_Loss: 3.4953. Mask: 0.9722. :  88%|████████▊ | 44/50 [00:28<00:03,  1.80it/s]total : 1000  current step :  742
total : 1000  current step :  743
total : 1000  current step :  744
Train Iter: 745/1000. LR: 0.0466. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9269. T_Loss: 3.5021. Mask: 0.9723. :  88%|████████▊ | 44/50 [00:30<00:03,  1.80it/s]Train Iter: 745/1000. LR: 0.0466. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9269. T_Loss: 3.5021. Mask: 0.9723. :  90%|█████████ | 45/50 [00:30<00:03,  1.28it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9264. T_Loss: 3.4938. Mask: 0.9723. :  90%|█████████ | 45/50 [00:30<00:03,  1.28it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9264. T_Loss: 3.4938. Mask: 0.9723. :  92%|█████████▏| 46/50 [00:30<00:02,  1.62it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9259. T_Loss: 3.4856. Mask: 0.9722. :  92%|█████████▏| 46/50 [00:30<00:02,  1.62it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9259. T_Loss: 3.4856. Mask: 0.9722. :  94%|█████████▍| 47/50 [00:30<00:01,  1.83it/s]total : 1000  current step :  745
total : 1000  current step :  746
total : 1000  current step :  747
Train Iter: 748/1000. LR: 0.0468. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9261. T_Loss: 3.4798. Mask: 0.9723. :  94%|█████████▍| 47/50 [00:32<00:01,  1.83it/s]Train Iter: 748/1000. LR: 0.0468. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9261. T_Loss: 3.4798. Mask: 0.9723. :  96%|█████████▌| 48/50 [00:32<00:01,  1.37it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9258. T_Loss: 3.4717. Mask: 0.9723. :  96%|█████████▌| 48/50 [00:32<00:01,  1.37it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9258. T_Loss: 3.4717. Mask: 0.9723. :  98%|█████████▊| 49/50 [00:32<00:00,  1.65it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9253. T_Loss: 3.4638. Mask: 0.9724. :  98%|█████████▊| 49/50 [00:32<00:00,  1.65it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9253. T_Loss: 3.4638. Mask: 0.9724. : 100%|██████████| 50/50 [00:32<00:00,  1.78it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9253. T_Loss: 3.4638. Mask: 0.9724. : 100%|██████████| 50/50 [00:32<00:00,  1.52it/s]
total : 1000  current step :  748
total : 1000  current step :  749
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 1.2926. top1: 73.83. top5: 97.66. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 1.2926. top1: 73.83. top5: 97.66. :  12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.2905. top1: 73.24. top5: 97.85. :  12%|█▎        | 1/8 [00:01<00:05,  1.29it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.2905. top1: 73.24. top5: 97.85. :  25%|██▌       | 2/8 [00:01<00:02,  2.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.2736. top1: 74.09. top5: 97.79. :  25%|██▌       | 2/8 [00:01<00:02,  2.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.2736. top1: 74.09. top5: 97.79. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.2356. top1: 75.88. top5: 97.75. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.2356. top1: 75.88. top5: 97.75. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.1437. top1: 80.31. top5: 98.20. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.1437. top1: 80.31. top5: 98.20. :  62%|██████▎   | 5/8 [00:01<00:01,  2.93it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0828. top1: 83.20. top5: 98.50. :  62%|██████▎   | 5/8 [00:02<00:01,  2.93it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0828. top1: 83.20. top5: 98.50. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0357. top1: 85.55. top5: 98.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0357. top1: 85.55. top5: 98.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.23it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0074. top1: 86.90. top5: 98.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.23it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0074. top1: 86.90. top5: 98.85. : 100%|██████████| 8/8 [00:02<00:00,  3.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0074. top1: 86.90. top5: 98.85. : 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
total : 1000  current step :  750
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 1.08s. Batch: 1.29s. S_Loss: 0.8835. T_Loss: 3.1023. Mask: 0.9766. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 1.08s. Batch: 1.29s. S_Loss: 0.8835. T_Loss: 3.1023. Mask: 0.9766. :   2%|▏         | 1/50 [00:01<01:03,  1.29s/it]Train Iter: 752/1000. LR: 0.0470. Data: 0.60s. Batch: 0.83s. S_Loss: 0.8860. T_Loss: 3.1929. Mask: 0.9746. :   2%|▏         | 1/50 [00:01<01:03,  1.29s/it]Train Iter: 752/1000. LR: 0.0470. Data: 0.60s. Batch: 0.83s. S_Loss: 0.8860. T_Loss: 3.1929. Mask: 0.9746. :   4%|▍         | 2/50 [00:01<00:36,  1.33it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.45s. Batch: 0.70s. S_Loss: 0.8999. T_Loss: 3.1212. Mask: 0.9661. :   4%|▍         | 2/50 [00:02<00:36,  1.33it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.45s. Batch: 0.70s. S_Loss: 0.8999. T_Loss: 3.1212. Mask: 0.9661. :   6%|▌         | 3/50 [00:02<00:28,  1.64it/s]total : 1000  current step :  751
total : 1000  current step :  752
total : 1000  current step :  753
Train Iter: 754/1000. LR: 0.0471. Data: 0.60s. Batch: 0.85s. S_Loss: 0.9056. T_Loss: 3.1130. Mask: 0.9658. :   6%|▌         | 3/50 [00:03<00:28,  1.64it/s]Train Iter: 754/1000. LR: 0.0471. Data: 0.60s. Batch: 0.85s. S_Loss: 0.9056. T_Loss: 3.1130. Mask: 0.9658. :   8%|▊         | 4/50 [00:03<00:40,  1.14it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.51s. Batch: 0.75s. S_Loss: 0.9080. T_Loss: 3.1614. Mask: 0.9703. :   8%|▊         | 4/50 [00:03<00:40,  1.14it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.51s. Batch: 0.75s. S_Loss: 0.9080. T_Loss: 3.1614. Mask: 0.9703. :  10%|█         | 5/50 [00:03<00:30,  1.46it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.45s. Batch: 0.70s. S_Loss: 0.9032. T_Loss: 3.1913. Mask: 0.9707. :  10%|█         | 5/50 [00:04<00:30,  1.46it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.45s. Batch: 0.70s. S_Loss: 0.9032. T_Loss: 3.1913. Mask: 0.9707. :  12%|█▏        | 6/50 [00:04<00:26,  1.64it/s]total : 1000  current step :  754
total : 1000  current step :  755
total : 1000  current step :  756
Train Iter: 757/1000. LR: 0.0473. Data: 0.55s. Batch: 0.79s. S_Loss: 0.9019. T_Loss: 3.2175. Mask: 0.9699. :  12%|█▏        | 6/50 [00:05<00:26,  1.64it/s]Train Iter: 757/1000. LR: 0.0473. Data: 0.55s. Batch: 0.79s. S_Loss: 0.9019. T_Loss: 3.2175. Mask: 0.9699. :  14%|█▍        | 7/50 [00:05<00:36,  1.17it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.49s. Batch: 0.73s. S_Loss: 0.9016. T_Loss: 3.2346. Mask: 0.9707. :  14%|█▍        | 7/50 [00:05<00:36,  1.17it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.49s. Batch: 0.73s. S_Loss: 0.9016. T_Loss: 3.2346. Mask: 0.9707. :  16%|█▌        | 8/50 [00:05<00:28,  1.46it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9022. T_Loss: 3.2523. Mask: 0.9714. :  16%|█▌        | 8/50 [00:06<00:28,  1.46it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9022. T_Loss: 3.2523. Mask: 0.9714. :  18%|█▊        | 9/50 [00:06<00:23,  1.71it/s]total : 1000  current step :  757
total : 1000  current step :  758
total : 1000  current step :  759
Train Iter: 760/1000. LR: 0.0475. Data: 0.58s. Batch: 0.81s. S_Loss: 0.9016. T_Loss: 3.2667. Mask: 0.9711. :  18%|█▊        | 9/50 [00:08<00:23,  1.71it/s]Train Iter: 760/1000. LR: 0.0475. Data: 0.58s. Batch: 0.81s. S_Loss: 0.9016. T_Loss: 3.2667. Mask: 0.9711. :  20%|██        | 10/50 [00:08<00:39,  1.00it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.58s. Batch: 0.82s. S_Loss: 0.9023. T_Loss: 3.2870. Mask: 0.9698. :  20%|██        | 10/50 [00:09<00:39,  1.00it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.58s. Batch: 0.82s. S_Loss: 0.9023. T_Loss: 3.2870. Mask: 0.9698. :  22%|██▏       | 11/50 [00:09<00:37,  1.05it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.56s. Batch: 0.80s. S_Loss: 0.9025. T_Loss: 3.2869. Mask: 0.9701. :  22%|██▏       | 11/50 [00:09<00:37,  1.05it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.56s. Batch: 0.80s. S_Loss: 0.9025. T_Loss: 3.2869. Mask: 0.9701. :  24%|██▍       | 12/50 [00:09<00:32,  1.17it/s]total : 1000  current step :  760
total : 1000  current step :  761
total : 1000  current step :  762
Train Iter: 763/1000. LR: 0.0477. Data: 0.60s. Batch: 0.83s. S_Loss: 0.9031. T_Loss: 3.3078. Mask: 0.9712. :  24%|██▍       | 12/50 [00:10<00:32,  1.17it/s]Train Iter: 763/1000. LR: 0.0477. Data: 0.60s. Batch: 0.83s. S_Loss: 0.9031. T_Loss: 3.3078. Mask: 0.9712. :  26%|██▌       | 13/50 [00:10<00:35,  1.05it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.56s. Batch: 0.81s. S_Loss: 0.9022. T_Loss: 3.3292. Mask: 0.9715. :  26%|██▌       | 13/50 [00:11<00:35,  1.05it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.56s. Batch: 0.81s. S_Loss: 0.9022. T_Loss: 3.3292. Mask: 0.9715. :  28%|██▊       | 14/50 [00:11<00:29,  1.22it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.53s. Batch: 0.78s. S_Loss: 0.9045. T_Loss: 3.3704. Mask: 0.9719. :  28%|██▊       | 14/50 [00:11<00:29,  1.22it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.53s. Batch: 0.78s. S_Loss: 0.9045. T_Loss: 3.3704. Mask: 0.9719. :  30%|███       | 15/50 [00:11<00:23,  1.46it/s]total : 1000  current step :  763
total : 1000  current step :  764
total : 1000  current step :  765
Train Iter: 766/1000. LR: 0.0479. Data: 0.55s. Batch: 0.80s. S_Loss: 0.9068. T_Loss: 3.3801. Mask: 0.9724. :  30%|███       | 15/50 [00:12<00:23,  1.46it/s]Train Iter: 766/1000. LR: 0.0479. Data: 0.55s. Batch: 0.80s. S_Loss: 0.9068. T_Loss: 3.3801. Mask: 0.9724. :  32%|███▏      | 16/50 [00:12<00:28,  1.21it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.53s. Batch: 0.79s. S_Loss: 0.9089. T_Loss: 3.3892. Mask: 0.9724. :  32%|███▏      | 16/50 [00:13<00:28,  1.21it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.53s. Batch: 0.79s. S_Loss: 0.9089. T_Loss: 3.3892. Mask: 0.9724. :  34%|███▍      | 17/50 [00:13<00:24,  1.35it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.51s. Batch: 0.77s. S_Loss: 0.9103. T_Loss: 3.4146. Mask: 0.9731. :  34%|███▍      | 17/50 [00:13<00:24,  1.35it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.51s. Batch: 0.77s. S_Loss: 0.9103. T_Loss: 3.4146. Mask: 0.9731. :  36%|███▌      | 18/50 [00:13<00:21,  1.51it/s]total : 1000  current step :  766
total : 1000  current step :  767
total : 1000  current step :  768
Train Iter: 769/1000. LR: 0.0481. Data: 0.53s. Batch: 0.79s. S_Loss: 0.9121. T_Loss: 3.4359. Mask: 0.9729. :  36%|███▌      | 18/50 [00:15<00:21,  1.51it/s]Train Iter: 769/1000. LR: 0.0481. Data: 0.53s. Batch: 0.79s. S_Loss: 0.9121. T_Loss: 3.4359. Mask: 0.9729. :  38%|███▊      | 19/50 [00:15<00:25,  1.20it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.51s. Batch: 0.77s. S_Loss: 0.9122. T_Loss: 3.4351. Mask: 0.9732. :  38%|███▊      | 19/50 [00:15<00:25,  1.20it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.51s. Batch: 0.77s. S_Loss: 0.9122. T_Loss: 3.4351. Mask: 0.9732. :  40%|████      | 20/50 [00:15<00:21,  1.42it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.49s. Batch: 0.75s. S_Loss: 0.9104. T_Loss: 3.4315. Mask: 0.9728. :  40%|████      | 20/50 [00:15<00:21,  1.42it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.49s. Batch: 0.75s. S_Loss: 0.9104. T_Loss: 3.4315. Mask: 0.9728. :  42%|████▏     | 21/50 [00:15<00:17,  1.67it/s]total : 1000  current step :  769
total : 1000  current step :  770
total : 1000  current step :  771
Train Iter: 772/1000. LR: 0.0483. Data: 0.51s. Batch: 0.77s. S_Loss: 0.9096. T_Loss: 3.4178. Mask: 0.9730. :  42%|████▏     | 21/50 [00:17<00:17,  1.67it/s]Train Iter: 772/1000. LR: 0.0483. Data: 0.51s. Batch: 0.77s. S_Loss: 0.9096. T_Loss: 3.4178. Mask: 0.9730. :  44%|████▍     | 22/50 [00:17<00:21,  1.28it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.49s. Batch: 0.75s. S_Loss: 0.9099. T_Loss: 3.4267. Mask: 0.9733. :  44%|████▍     | 22/50 [00:17<00:21,  1.28it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.49s. Batch: 0.75s. S_Loss: 0.9099. T_Loss: 3.4267. Mask: 0.9733. :  46%|████▌     | 23/50 [00:17<00:16,  1.61it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.47s. Batch: 0.74s. S_Loss: 0.9098. T_Loss: 3.4154. Mask: 0.9733. :  46%|████▌     | 23/50 [00:17<00:16,  1.61it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.47s. Batch: 0.74s. S_Loss: 0.9098. T_Loss: 3.4154. Mask: 0.9733. :  48%|████▊     | 24/50 [00:17<00:14,  1.80it/s]total : 1000  current step :  772
total : 1000  current step :  773
total : 1000  current step :  774
Train Iter: 775/1000. LR: 0.0484. Data: 0.49s. Batch: 0.75s. S_Loss: 0.9100. T_Loss: 3.4162. Mask: 0.9733. :  48%|████▊     | 24/50 [00:18<00:14,  1.80it/s]Train Iter: 775/1000. LR: 0.0484. Data: 0.49s. Batch: 0.75s. S_Loss: 0.9100. T_Loss: 3.4162. Mask: 0.9733. :  50%|█████     | 25/50 [00:18<00:18,  1.37it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.48s. Batch: 0.74s. S_Loss: 0.9098. T_Loss: 3.4159. Mask: 0.9737. :  50%|█████     | 25/50 [00:19<00:18,  1.37it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.48s. Batch: 0.74s. S_Loss: 0.9098. T_Loss: 3.4159. Mask: 0.9737. :  52%|█████▏    | 26/50 [00:19<00:15,  1.51it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.47s. Batch: 0.73s. S_Loss: 0.9093. T_Loss: 3.4208. Mask: 0.9744. :  52%|█████▏    | 26/50 [00:19<00:15,  1.51it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.47s. Batch: 0.73s. S_Loss: 0.9093. T_Loss: 3.4208. Mask: 0.9744. :  54%|█████▍    | 27/50 [00:19<00:12,  1.80it/s]total : 1000  current step :  775
total : 1000  current step :  776
total : 1000  current step :  777
Train Iter: 778/1000. LR: 0.0486. Data: 0.48s. Batch: 0.74s. S_Loss: 0.9085. T_Loss: 3.4135. Mask: 0.9749. :  54%|█████▍    | 27/50 [00:20<00:12,  1.80it/s]Train Iter: 778/1000. LR: 0.0486. Data: 0.48s. Batch: 0.74s. S_Loss: 0.9085. T_Loss: 3.4135. Mask: 0.9749. :  56%|█████▌    | 28/50 [00:20<00:15,  1.40it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.47s. Batch: 0.73s. S_Loss: 0.9074. T_Loss: 3.4054. Mask: 0.9752. :  56%|█████▌    | 28/50 [00:21<00:15,  1.40it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.47s. Batch: 0.73s. S_Loss: 0.9074. T_Loss: 3.4054. Mask: 0.9752. :  58%|█████▊    | 29/50 [00:21<00:12,  1.68it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9078. T_Loss: 3.3932. Mask: 0.9750. :  58%|█████▊    | 29/50 [00:21<00:12,  1.68it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9078. T_Loss: 3.3932. Mask: 0.9750. :  60%|██████    | 30/50 [00:21<00:10,  1.93it/s]total : 1000  current step :  778
total : 1000  current step :  779
total : 1000  current step :  780
Train Iter: 781/1000. LR: 0.0488. Data: 0.48s. Batch: 0.72s. S_Loss: 0.9083. T_Loss: 3.3898. Mask: 0.9757. :  60%|██████    | 30/50 [00:22<00:10,  1.93it/s]Train Iter: 781/1000. LR: 0.0488. Data: 0.48s. Batch: 0.72s. S_Loss: 0.9083. T_Loss: 3.3898. Mask: 0.9757. :  62%|██████▏   | 31/50 [00:22<00:13,  1.44it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9071. T_Loss: 3.3858. Mask: 0.9761. :  62%|██████▏   | 31/50 [00:22<00:13,  1.44it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9071. T_Loss: 3.3858. Mask: 0.9761. :  64%|██████▍   | 32/50 [00:22<00:10,  1.64it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9070. T_Loss: 3.3858. Mask: 0.9766. :  64%|██████▍   | 32/50 [00:23<00:10,  1.64it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9070. T_Loss: 3.3858. Mask: 0.9766. :  66%|██████▌   | 33/50 [00:23<00:09,  1.79it/s]total : 1000  current step :  781
total : 1000  current step :  782
total : 1000  current step :  783
Train Iter: 784/1000. LR: 0.0490. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9061. T_Loss: 3.3772. Mask: 0.9768. :  66%|██████▌   | 33/50 [00:24<00:09,  1.79it/s]Train Iter: 784/1000. LR: 0.0490. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9061. T_Loss: 3.3772. Mask: 0.9768. :  68%|██████▊   | 34/50 [00:24<00:11,  1.37it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9057. T_Loss: 3.3755. Mask: 0.9761. :  68%|██████▊   | 34/50 [00:24<00:11,  1.37it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9057. T_Loss: 3.3755. Mask: 0.9761. :  70%|███████   | 35/50 [00:24<00:08,  1.69it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.45s. Batch: 0.70s. S_Loss: 0.9056. T_Loss: 3.3792. Mask: 0.9762. :  70%|███████   | 35/50 [00:25<00:08,  1.69it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.45s. Batch: 0.70s. S_Loss: 0.9056. T_Loss: 3.3792. Mask: 0.9762. :  72%|███████▏  | 36/50 [00:25<00:07,  1.85it/s]total : 1000  current step :  784
total : 1000  current step :  785
total : 1000  current step :  786
Train Iter: 787/1000. LR: 0.0492. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9052. T_Loss: 3.3756. Mask: 0.9764. :  72%|███████▏  | 36/50 [00:26<00:07,  1.85it/s]Train Iter: 787/1000. LR: 0.0492. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9052. T_Loss: 3.3756. Mask: 0.9764. :  74%|███████▍  | 37/50 [00:26<00:09,  1.40it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.46s. Batch: 0.70s. S_Loss: 0.9048. T_Loss: 3.3680. Mask: 0.9762. :  74%|███████▍  | 37/50 [00:26<00:09,  1.40it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.46s. Batch: 0.70s. S_Loss: 0.9048. T_Loss: 3.3680. Mask: 0.9762. :  76%|███████▌  | 38/50 [00:26<00:07,  1.70it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9054. T_Loss: 3.3680. Mask: 0.9766. :  76%|███████▌  | 38/50 [00:27<00:07,  1.70it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9054. T_Loss: 3.3680. Mask: 0.9766. :  78%|███████▊  | 39/50 [00:27<00:05,  1.88it/s]total : 1000  current step :  787
total : 1000  current step :  788
total : 1000  current step :  789
Train Iter: 790/1000. LR: 0.0494. Data: 0.46s. Batch: 0.70s. S_Loss: 0.9049. T_Loss: 3.3657. Mask: 0.9768. :  78%|███████▊  | 39/50 [00:28<00:05,  1.88it/s]Train Iter: 790/1000. LR: 0.0494. Data: 0.46s. Batch: 0.70s. S_Loss: 0.9049. T_Loss: 3.3657. Mask: 0.9768. :  80%|████████  | 40/50 [00:28<00:06,  1.46it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9053. T_Loss: 3.3696. Mask: 0.9765. :  80%|████████  | 40/50 [00:28<00:06,  1.46it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9053. T_Loss: 3.3696. Mask: 0.9765. :  82%|████████▏ | 41/50 [00:28<00:05,  1.72it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9050. T_Loss: 3.3647. Mask: 0.9765. :  82%|████████▏ | 41/50 [00:28<00:05,  1.72it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9050. T_Loss: 3.3647. Mask: 0.9765. :  84%|████████▍ | 42/50 [00:28<00:04,  1.92it/s]total : 1000  current step :  790
total : 1000  current step :  791
total : 1000  current step :  792
Train Iter: 793/1000. LR: 0.0496. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9056. T_Loss: 3.3651. Mask: 0.9765. :  84%|████████▍ | 42/50 [00:29<00:04,  1.92it/s]Train Iter: 793/1000. LR: 0.0496. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9056. T_Loss: 3.3651. Mask: 0.9765. :  86%|████████▌ | 43/50 [00:29<00:04,  1.42it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9070. T_Loss: 3.3767. Mask: 0.9763. :  86%|████████▌ | 43/50 [00:30<00:04,  1.42it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9070. T_Loss: 3.3767. Mask: 0.9763. :  88%|████████▊ | 44/50 [00:30<00:03,  1.67it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9070. T_Loss: 3.3692. Mask: 0.9762. :  88%|████████▊ | 44/50 [00:30<00:03,  1.67it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9070. T_Loss: 3.3692. Mask: 0.9762. :  90%|█████████ | 45/50 [00:30<00:02,  1.90it/s]total : 1000  current step :  793
total : 1000  current step :  794
total : 1000  current step :  795
Train Iter: 796/1000. LR: 0.0498. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9087. T_Loss: 3.3730. Mask: 0.9763. :  90%|█████████ | 45/50 [00:31<00:02,  1.90it/s]Train Iter: 796/1000. LR: 0.0498. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9087. T_Loss: 3.3730. Mask: 0.9763. :  92%|█████████▏| 46/50 [00:31<00:02,  1.39it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9086. T_Loss: 3.3774. Mask: 0.9765. :  92%|█████████▏| 46/50 [00:32<00:02,  1.39it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9086. T_Loss: 3.3774. Mask: 0.9765. :  94%|█████████▍| 47/50 [00:32<00:01,  1.61it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9085. T_Loss: 3.3782. Mask: 0.9765. :  94%|█████████▍| 47/50 [00:32<00:01,  1.61it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9085. T_Loss: 3.3782. Mask: 0.9765. :  96%|█████████▌| 48/50 [00:32<00:01,  1.95it/s]total : 1000  current step :  796
total : 1000  current step :  797
total : 1000  current step :  798
Train Iter: 799/1000. LR: 0.0499. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9097. T_Loss: 3.3753. Mask: 0.9762. :  96%|█████████▌| 48/50 [00:33<00:01,  1.95it/s]Train Iter: 799/1000. LR: 0.0499. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9097. T_Loss: 3.3753. Mask: 0.9762. :  98%|█████████▊| 49/50 [00:33<00:00,  1.42it/s]total : 1000  current step :  799
Train Iter: 800/1000. LR: 0.0500. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9093. T_Loss: 3.3709. Mask: 0.9762. :  98%|█████████▊| 49/50 [00:34<00:00,  1.42it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9093. T_Loss: 3.3709. Mask: 0.9762. : 100%|██████████| 50/50 [00:34<00:00,  1.36it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9093. T_Loss: 3.3709. Mask: 0.9762. : 100%|██████████| 50/50 [00:34<00:00,  1.45it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.3699. top1: 69.14. top5: 96.48. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.3699. top1: 69.14. top5: 96.48. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.3724. top1: 68.95. top5: 96.68. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.3724. top1: 68.95. top5: 96.68. :  25%|██▌       | 2/8 [00:00<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.3542. top1: 69.92. top5: 96.61. :  25%|██▌       | 2/8 [00:01<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.3542. top1: 69.92. top5: 96.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.3102. top1: 72.27. top5: 96.78. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.3102. top1: 72.27. top5: 96.78. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.1999. top1: 77.58. top5: 97.42. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.1999. top1: 77.58. top5: 97.42. :  62%|██████▎   | 5/8 [00:01<00:00,  3.09it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.1271. top1: 80.99. top5: 97.85. :  62%|██████▎   | 5/8 [00:02<00:00,  3.09it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.1271. top1: 80.99. top5: 97.85. :  75%|███████▌  | 6/8 [00:02<00:00,  3.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0717. top1: 83.65. top5: 98.16. :  75%|███████▌  | 6/8 [00:02<00:00,  3.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0717. top1: 83.65. top5: 98.16. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0383. top1: 85.25. top5: 98.35. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0383. top1: 85.25. top5: 98.35. : 100%|██████████| 8/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.0383. top1: 85.25. top5: 98.35. : 100%|██████████| 8/8 [00:02<00:00,  2.86it/s]
total : 1000  current step :  800
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.01s. Batch: 0.21s. S_Loss: 0.8867. T_Loss: 3.2556. Mask: 0.9844. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.01s. Batch: 0.21s. S_Loss: 0.8867. T_Loss: 3.2556. Mask: 0.9844. :   2%|▏         | 1/50 [00:00<00:10,  4.80it/s]total : 1000  current step :  801
Train Iter: 802/1000. LR: 0.0500. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8998. T_Loss: 3.3586. Mask: 0.9805. :   2%|▏         | 1/50 [00:01<00:10,  4.80it/s]Train Iter: 802/1000. LR: 0.0500. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8998. T_Loss: 3.3586. Mask: 0.9805. :   4%|▍         | 2/50 [00:01<00:32,  1.48it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.29s. Batch: 0.49s. S_Loss: 0.9025. T_Loss: 3.3682. Mask: 0.9779. :   4%|▍         | 2/50 [00:01<00:32,  1.48it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.29s. Batch: 0.49s. S_Loss: 0.9025. T_Loss: 3.3682. Mask: 0.9779. :   6%|▌         | 3/50 [00:01<00:22,  2.05it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.27s. Batch: 0.47s. S_Loss: 0.9102. T_Loss: 3.3658. Mask: 0.9785. :   6%|▌         | 3/50 [00:01<00:22,  2.05it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.27s. Batch: 0.47s. S_Loss: 0.9102. T_Loss: 3.3658. Mask: 0.9785. :   8%|▊         | 4/50 [00:01<00:20,  2.19it/s]total : 1000  current step :  802
total : 1000  current step :  803
total : 1000  current step :  804
Train Iter: 805/1000. LR: 0.0499. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9163. T_Loss: 3.3759. Mask: 0.9773. :   8%|▊         | 4/50 [00:02<00:20,  2.19it/s]Train Iter: 805/1000. LR: 0.0499. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9163. T_Loss: 3.3759. Mask: 0.9773. :  10%|█         | 5/50 [00:02<00:30,  1.49it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9144. T_Loss: 3.3608. Mask: 0.9785. :  10%|█         | 5/50 [00:03<00:30,  1.49it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.34s. Batch: 0.54s. S_Loss: 0.9144. T_Loss: 3.3608. Mask: 0.9785. :  12%|█▏        | 6/50 [00:03<00:24,  1.82it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.31s. Batch: 0.50s. S_Loss: 0.9132. T_Loss: 3.3551. Mask: 0.9794. :  12%|█▏        | 6/50 [00:03<00:24,  1.82it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.31s. Batch: 0.50s. S_Loss: 0.9132. T_Loss: 3.3551. Mask: 0.9794. :  14%|█▍        | 7/50 [00:03<00:19,  2.20it/s]total : 1000  current step :  805
total : 1000  current step :  806
total : 1000  current step :  807
Train Iter: 808/1000. LR: 0.0498. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9140. T_Loss: 3.3254. Mask: 0.9780. :  14%|█▍        | 7/50 [00:04<00:19,  2.20it/s]Train Iter: 808/1000. LR: 0.0498. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9140. T_Loss: 3.3254. Mask: 0.9780. :  16%|█▌        | 8/50 [00:04<00:28,  1.48it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9199. T_Loss: 3.3475. Mask: 0.9770. :  16%|█▌        | 8/50 [00:04<00:28,  1.48it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9199. T_Loss: 3.3475. Mask: 0.9770. :  18%|█▊        | 9/50 [00:04<00:22,  1.80it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9195. T_Loss: 3.3425. Mask: 0.9773. :  18%|█▊        | 9/50 [00:05<00:22,  1.80it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.32s. Batch: 0.52s. S_Loss: 0.9195. T_Loss: 3.3425. Mask: 0.9773. :  20%|██        | 10/50 [00:05<00:18,  2.18it/s]total : 1000  current step :  808
total : 1000  current step :  809
total : 1000  current step :  810
Train Iter: 811/1000. LR: 0.0496. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9188. T_Loss: 3.3498. Mask: 0.9780. :  20%|██        | 10/50 [00:06<00:18,  2.18it/s]Train Iter: 811/1000. LR: 0.0496. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9188. T_Loss: 3.3498. Mask: 0.9780. :  22%|██▏       | 11/50 [00:06<00:27,  1.44it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9157. T_Loss: 3.3325. Mask: 0.9782. :  22%|██▏       | 11/50 [00:06<00:27,  1.44it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.36s. Batch: 0.56s. S_Loss: 0.9157. T_Loss: 3.3325. Mask: 0.9782. :  24%|██▍       | 12/50 [00:06<00:22,  1.68it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9156. T_Loss: 3.3113. Mask: 0.9775. :  24%|██▍       | 12/50 [00:07<00:22,  1.68it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.35s. Batch: 0.55s. S_Loss: 0.9156. T_Loss: 3.3113. Mask: 0.9775. :  26%|██▌       | 13/50 [00:07<00:20,  1.84it/s]total : 1000  current step :  811
total : 1000  current step :  812
total : 1000  current step :  813
Train Iter: 814/1000. LR: 0.0494. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9160. T_Loss: 3.2897. Mask: 0.9766. :  26%|██▌       | 13/50 [00:08<00:20,  1.84it/s]Train Iter: 814/1000. LR: 0.0494. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9160. T_Loss: 3.2897. Mask: 0.9766. :  28%|██▊       | 14/50 [00:08<00:27,  1.33it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.38s. Batch: 0.59s. S_Loss: 0.9158. T_Loss: 3.2857. Mask: 0.9760. :  28%|██▊       | 14/50 [00:08<00:27,  1.33it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.38s. Batch: 0.59s. S_Loss: 0.9158. T_Loss: 3.2857. Mask: 0.9760. :  30%|███       | 15/50 [00:08<00:22,  1.56it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9164. T_Loss: 3.2925. Mask: 0.9771. :  30%|███       | 15/50 [00:09<00:22,  1.56it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9164. T_Loss: 3.2925. Mask: 0.9771. :  32%|███▏      | 16/50 [00:09<00:19,  1.74it/s]total : 1000  current step :  814
total : 1000  current step :  815
total : 1000  current step :  816
Train Iter: 817/1000. LR: 0.0491. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9166. T_Loss: 3.2991. Mask: 0.9777. :  32%|███▏      | 16/50 [00:10<00:19,  1.74it/s]Train Iter: 817/1000. LR: 0.0491. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9166. T_Loss: 3.2991. Mask: 0.9777. :  34%|███▍      | 17/50 [00:10<00:25,  1.30it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9153. T_Loss: 3.2855. Mask: 0.9781. :  34%|███▍      | 17/50 [00:10<00:25,  1.30it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9153. T_Loss: 3.2855. Mask: 0.9781. :  36%|███▌      | 18/50 [00:10<00:19,  1.63it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9143. T_Loss: 3.2741. Mask: 0.9774. :  36%|███▌      | 18/50 [00:11<00:19,  1.63it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.38s. Batch: 0.58s. S_Loss: 0.9143. T_Loss: 3.2741. Mask: 0.9774. :  38%|███▊      | 19/50 [00:11<00:16,  1.84it/s]total : 1000  current step :  817
total : 1000  current step :  818
total : 1000  current step :  819
Train Iter: 820/1000. LR: 0.0488. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9134. T_Loss: 3.2736. Mask: 0.9775. :  38%|███▊      | 19/50 [00:12<00:16,  1.84it/s]Train Iter: 820/1000. LR: 0.0488. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9134. T_Loss: 3.2736. Mask: 0.9775. :  40%|████      | 20/50 [00:12<00:21,  1.38it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9118. T_Loss: 3.2660. Mask: 0.9784. :  40%|████      | 20/50 [00:12<00:21,  1.38it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9118. T_Loss: 3.2660. Mask: 0.9784. :  42%|████▏     | 21/50 [00:12<00:18,  1.53it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9118. T_Loss: 3.2578. Mask: 0.9785. :  42%|████▏     | 21/50 [00:13<00:18,  1.53it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9118. T_Loss: 3.2578. Mask: 0.9785. :  44%|████▍     | 22/50 [00:13<00:15,  1.77it/s]total : 1000  current step :  820
total : 1000  current step :  821
total : 1000  current step :  822
Train Iter: 823/1000. LR: 0.0484. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9096. T_Loss: 3.2514. Mask: 0.9784. :  44%|████▍     | 22/50 [00:14<00:15,  1.77it/s]Train Iter: 823/1000. LR: 0.0484. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9096. T_Loss: 3.2514. Mask: 0.9784. :  46%|████▌     | 23/50 [00:14<00:20,  1.32it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9105. T_Loss: 3.2371. Mask: 0.9780. :  46%|████▌     | 23/50 [00:14<00:20,  1.32it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9105. T_Loss: 3.2371. Mask: 0.9780. :  48%|████▊     | 24/50 [00:14<00:16,  1.60it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9109. T_Loss: 3.2333. Mask: 0.9784. :  48%|████▊     | 24/50 [00:14<00:16,  1.60it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.39s. Batch: 0.59s. S_Loss: 0.9109. T_Loss: 3.2333. Mask: 0.9784. :  50%|█████     | 25/50 [00:14<00:13,  1.89it/s]total : 1000  current step :  823
total : 1000  current step :  824
total : 1000  current step :  825
Train Iter: 826/1000. LR: 0.0479. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9107. T_Loss: 3.2329. Mask: 0.9787. :  50%|█████     | 25/50 [00:16<00:13,  1.89it/s]Train Iter: 826/1000. LR: 0.0479. Data: 0.41s. Batch: 0.61s. S_Loss: 0.9107. T_Loss: 3.2329. Mask: 0.9787. :  52%|█████▏    | 26/50 [00:16<00:16,  1.42it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9108. T_Loss: 3.2299. Mask: 0.9784. :  52%|█████▏    | 26/50 [00:16<00:16,  1.42it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9108. T_Loss: 3.2299. Mask: 0.9784. :  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9116. T_Loss: 3.2430. Mask: 0.9787. :  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9116. T_Loss: 3.2430. Mask: 0.9787. :  56%|█████▌    | 28/50 [00:16<00:12,  1.78it/s]total : 1000  current step :  826
total : 1000  current step :  827
total : 1000  current step :  828
Train Iter: 829/1000. LR: 0.0475. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9124. T_Loss: 3.2436. Mask: 0.9787. :  56%|█████▌    | 28/50 [00:18<00:12,  1.78it/s]Train Iter: 829/1000. LR: 0.0475. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9124. T_Loss: 3.2436. Mask: 0.9787. :  58%|█████▊    | 29/50 [00:18<00:15,  1.33it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9127. T_Loss: 3.2556. Mask: 0.9784. :  58%|█████▊    | 29/50 [00:18<00:15,  1.33it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9127. T_Loss: 3.2556. Mask: 0.9784. :  60%|██████    | 30/50 [00:18<00:12,  1.63it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9122. T_Loss: 3.2694. Mask: 0.9783. :  60%|██████    | 30/50 [00:18<00:12,  1.63it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9122. T_Loss: 3.2694. Mask: 0.9783. :  62%|██████▏   | 31/50 [00:18<00:10,  1.83it/s]total : 1000  current step :  829
total : 1000  current step :  830
total : 1000  current step :  831
Train Iter: 832/1000. LR: 0.0469. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9130. T_Loss: 3.2755. Mask: 0.9786. :  62%|██████▏   | 31/50 [00:19<00:10,  1.83it/s]Train Iter: 832/1000. LR: 0.0469. Data: 0.41s. Batch: 0.62s. S_Loss: 0.9130. T_Loss: 3.2755. Mask: 0.9786. :  64%|██████▍   | 32/50 [00:19<00:12,  1.46it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9121. T_Loss: 3.2720. Mask: 0.9789. :  64%|██████▍   | 32/50 [00:20<00:12,  1.46it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9121. T_Loss: 3.2720. Mask: 0.9789. :  66%|██████▌   | 33/50 [00:20<00:10,  1.67it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9127. T_Loss: 3.2823. Mask: 0.9786. :  66%|██████▌   | 33/50 [00:20<00:10,  1.67it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9127. T_Loss: 3.2823. Mask: 0.9786. :  68%|██████▊   | 34/50 [00:20<00:08,  1.88it/s]total : 1000  current step :  832
total : 1000  current step :  833
total : 1000  current step :  834
Train Iter: 835/1000. LR: 0.0463. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9129. T_Loss: 3.2886. Mask: 0.9787. :  68%|██████▊   | 34/50 [00:21<00:08,  1.88it/s]Train Iter: 835/1000. LR: 0.0463. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9129. T_Loss: 3.2886. Mask: 0.9787. :  70%|███████   | 35/50 [00:21<00:11,  1.36it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9134. T_Loss: 3.2954. Mask: 0.9785. :  70%|███████   | 35/50 [00:22<00:11,  1.36it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9134. T_Loss: 3.2954. Mask: 0.9785. :  72%|███████▏  | 36/50 [00:22<00:09,  1.47it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9125. T_Loss: 3.2965. Mask: 0.9787. :  72%|███████▏  | 36/50 [00:22<00:09,  1.47it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9125. T_Loss: 3.2965. Mask: 0.9787. :  74%|███████▍  | 37/50 [00:22<00:07,  1.79it/s]total : 1000  current step :  835
total : 1000  current step :  836
total : 1000  current step :  837
Train Iter: 838/1000. LR: 0.0457. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9130. T_Loss: 3.2980. Mask: 0.9787. :  74%|███████▍  | 37/50 [00:23<00:07,  1.79it/s]Train Iter: 838/1000. LR: 0.0457. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9130. T_Loss: 3.2980. Mask: 0.9787. :  76%|███████▌  | 38/50 [00:23<00:08,  1.37it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9119. T_Loss: 3.3003. Mask: 0.9788. :  76%|███████▌  | 38/50 [00:23<00:08,  1.37it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9119. T_Loss: 3.3003. Mask: 0.9788. :  78%|███████▊  | 39/50 [00:23<00:06,  1.67it/s]total : 1000  current step :  838
total : 1000  current step :  839
Train Iter: 840/1000. LR: 0.0452. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9106. T_Loss: 3.3011. Mask: 0.9789. :  78%|███████▊  | 39/50 [00:24<00:06,  1.67it/s]Train Iter: 840/1000. LR: 0.0452. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9106. T_Loss: 3.3011. Mask: 0.9789. :  80%|████████  | 40/50 [00:24<00:05,  1.71it/s]total : 1000  current step :  840
Train Iter: 841/1000. LR: 0.0450. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9106. T_Loss: 3.3088. Mask: 0.9792. :  80%|████████  | 40/50 [00:25<00:05,  1.71it/s]Train Iter: 841/1000. LR: 0.0450. Data: 0.40s. Batch: 0.62s. S_Loss: 0.9106. T_Loss: 3.3088. Mask: 0.9792. :  82%|████████▏ | 41/50 [00:25<00:06,  1.45it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9102. T_Loss: 3.3168. Mask: 0.9794. :  82%|████████▏ | 41/50 [00:25<00:06,  1.45it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9102. T_Loss: 3.3168. Mask: 0.9794. :  84%|████████▍ | 42/50 [00:25<00:04,  1.81it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9100. T_Loss: 3.3247. Mask: 0.9793. :  84%|████████▍ | 42/50 [00:26<00:04,  1.81it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.39s. Batch: 0.60s. S_Loss: 0.9100. T_Loss: 3.3247. Mask: 0.9793. :  86%|████████▌ | 43/50 [00:26<00:03,  2.08it/s]total : 1000  current step :  841
total : 1000  current step :  842
total : 1000  current step :  843
Train Iter: 844/1000. LR: 0.0443. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9100. T_Loss: 3.3256. Mask: 0.9792. :  86%|████████▌ | 43/50 [00:27<00:03,  2.08it/s]Train Iter: 844/1000. LR: 0.0443. Data: 0.40s. Batch: 0.61s. S_Loss: 0.9100. T_Loss: 3.3256. Mask: 0.9792. :  88%|████████▊ | 44/50 [00:27<00:03,  1.52it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9111. T_Loss: 3.3337. Mask: 0.9793. :  88%|████████▊ | 44/50 [00:27<00:03,  1.52it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9111. T_Loss: 3.3337. Mask: 0.9793. :  90%|█████████ | 45/50 [00:27<00:02,  1.76it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.38s. Batch: 0.60s. S_Loss: 0.9112. T_Loss: 3.3312. Mask: 0.9789. :  90%|█████████ | 45/50 [00:27<00:02,  1.76it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.38s. Batch: 0.60s. S_Loss: 0.9112. T_Loss: 3.3312. Mask: 0.9789. :  92%|█████████▏| 46/50 [00:27<00:01,  2.08it/s]total : 1000  current step :  844
total : 1000  current step :  845
total : 1000  current step :  846
Train Iter: 847/1000. LR: 0.0435. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9111. T_Loss: 3.3272. Mask: 0.9787. :  92%|█████████▏| 46/50 [00:28<00:01,  2.08it/s]Train Iter: 847/1000. LR: 0.0435. Data: 0.39s. Batch: 0.61s. S_Loss: 0.9111. T_Loss: 3.3272. Mask: 0.9787. :  94%|█████████▍| 47/50 [00:28<00:02,  1.40it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9123. T_Loss: 3.3319. Mask: 0.9786. :  94%|█████████▍| 47/50 [00:29<00:02,  1.40it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9123. T_Loss: 3.3319. Mask: 0.9786. :  96%|█████████▌| 48/50 [00:29<00:01,  1.69it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.38s. Batch: 0.60s. S_Loss: 0.9111. T_Loss: 3.3216. Mask: 0.9783. :  96%|█████████▌| 48/50 [00:29<00:01,  1.69it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.38s. Batch: 0.60s. S_Loss: 0.9111. T_Loss: 3.3216. Mask: 0.9783. :  98%|█████████▊| 49/50 [00:29<00:00,  2.10it/s]total : 1000  current step :  847
total : 1000  current step :  848
total : 1000  current step :  849
Train Iter: 850/1000. LR: 0.0427. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9107. T_Loss: 3.3205. Mask: 0.9786. :  98%|█████████▊| 49/50 [00:30<00:00,  2.10it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9107. T_Loss: 3.3205. Mask: 0.9786. : 100%|██████████| 50/50 [00:30<00:00,  1.51it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9107. T_Loss: 3.3205. Mask: 0.9786. : 100%|██████████| 50/50 [00:30<00:00,  1.64it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.3699. top1: 69.53. top5: 96.09. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.3699. top1: 69.53. top5: 96.09. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.3717. top1: 68.75. top5: 96.48. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.3717. top1: 68.75. top5: 96.48. :  25%|██▌       | 2/8 [00:00<00:02,  2.64it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.3535. top1: 69.66. top5: 96.48. :  25%|██▌       | 2/8 [00:01<00:02,  2.64it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.3535. top1: 69.66. top5: 96.48. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.3095. top1: 71.88. top5: 96.88. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.3095. top1: 71.88. top5: 96.88. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1990. top1: 77.34. top5: 97.50. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1990. top1: 77.34. top5: 97.50. :  62%|██████▎   | 5/8 [00:01<00:00,  3.34it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1261. top1: 80.86. top5: 97.92. :  62%|██████▎   | 5/8 [00:01<00:00,  3.34it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.1261. top1: 80.86. top5: 97.92. :  75%|███████▌  | 6/8 [00:01<00:00,  3.66it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0706. top1: 83.54. top5: 98.21. :  75%|███████▌  | 6/8 [00:02<00:00,  3.66it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0706. top1: 83.54. top5: 98.21. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0371. top1: 85.10. top5: 98.40. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0371. top1: 85.10. top5: 98.40. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.0371. top1: 85.10. top5: 98.40. : 100%|██████████| 8/8 [00:02<00:00,  3.17it/s]
total : 1000  current step :  850
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.32s. S_Loss: 0.9600. T_Loss: 3.2181. Mask: 0.9531. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.32s. S_Loss: 0.9600. T_Loss: 3.2181. Mask: 0.9531. :   2%|▏         | 1/50 [00:00<00:15,  3.08it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9265. T_Loss: 3.2527. Mask: 0.9668. :   2%|▏         | 1/50 [00:00<00:15,  3.08it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9265. T_Loss: 3.2527. Mask: 0.9668. :   4%|▍         | 2/50 [00:00<00:14,  3.37it/s]total : 1000  current step :  851
total : 1000  current step :  852
Train Iter: 853/1000. LR: 0.0418. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9244. T_Loss: 3.2513. Mask: 0.9701. :   4%|▍         | 2/50 [00:01<00:14,  3.37it/s]Train Iter: 853/1000. LR: 0.0418. Data: 0.29s. Batch: 0.57s. S_Loss: 0.9244. T_Loss: 3.2513. Mask: 0.9701. :   6%|▌         | 3/50 [00:01<00:31,  1.49it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.22s. Batch: 0.52s. S_Loss: 0.9248. T_Loss: 3.2448. Mask: 0.9697. :   6%|▌         | 3/50 [00:02<00:31,  1.49it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.22s. Batch: 0.52s. S_Loss: 0.9248. T_Loss: 3.2448. Mask: 0.9697. :   8%|▊         | 4/50 [00:02<00:25,  1.82it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.18s. Batch: 0.46s. S_Loss: 0.9233. T_Loss: 3.2807. Mask: 0.9711. :   8%|▊         | 4/50 [00:02<00:25,  1.82it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.18s. Batch: 0.46s. S_Loss: 0.9233. T_Loss: 3.2807. Mask: 0.9711. :  10%|█         | 5/50 [00:02<00:19,  2.27it/s]total : 1000  current step :  853
total : 1000  current step :  854
total : 1000  current step :  855
Train Iter: 856/1000. LR: 0.0409. Data: 0.27s. Batch: 0.55s. S_Loss: 0.9179. T_Loss: 3.2781. Mask: 0.9707. :  10%|█         | 5/50 [00:03<00:19,  2.27it/s]Train Iter: 856/1000. LR: 0.0409. Data: 0.27s. Batch: 0.55s. S_Loss: 0.9179. T_Loss: 3.2781. Mask: 0.9707. :  12%|█▏        | 6/50 [00:03<00:27,  1.58it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.23s. Batch: 0.53s. S_Loss: 0.9151. T_Loss: 3.2844. Mask: 0.9727. :  12%|█▏        | 6/50 [00:03<00:27,  1.58it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.23s. Batch: 0.53s. S_Loss: 0.9151. T_Loss: 3.2844. Mask: 0.9727. :  14%|█▍        | 7/50 [00:03<00:23,  1.84it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9141. T_Loss: 3.2964. Mask: 0.9736. :  14%|█▍        | 7/50 [00:03<00:23,  1.84it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.20s. Batch: 0.49s. S_Loss: 0.9141. T_Loss: 3.2964. Mask: 0.9736. :  16%|█▌        | 8/50 [00:03<00:18,  2.25it/s]total : 1000  current step :  856
total : 1000  current step :  857
total : 1000  current step :  858
Train Iter: 859/1000. LR: 0.0400. Data: 0.27s. Batch: 0.55s. S_Loss: 0.9094. T_Loss: 3.2785. Mask: 0.9748. :  16%|█▌        | 8/50 [00:04<00:18,  2.25it/s]Train Iter: 859/1000. LR: 0.0400. Data: 0.27s. Batch: 0.55s. S_Loss: 0.9094. T_Loss: 3.2785. Mask: 0.9748. :  18%|█▊        | 9/50 [00:04<00:25,  1.60it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.25s. Batch: 0.53s. S_Loss: 0.9073. T_Loss: 3.2796. Mask: 0.9762. :  18%|█▊        | 9/50 [00:05<00:25,  1.60it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.25s. Batch: 0.53s. S_Loss: 0.9073. T_Loss: 3.2796. Mask: 0.9762. :  20%|██        | 10/50 [00:05<00:21,  1.87it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.23s. Batch: 0.51s. S_Loss: 0.9080. T_Loss: 3.3075. Mask: 0.9773. :  20%|██        | 10/50 [00:05<00:21,  1.87it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.23s. Batch: 0.51s. S_Loss: 0.9080. T_Loss: 3.3075. Mask: 0.9773. :  22%|██▏       | 11/50 [00:05<00:18,  2.13it/s]total : 1000  current step :  859
total : 1000  current step :  860
total : 1000  current step :  861
Train Iter: 862/1000. LR: 0.0391. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9047. T_Loss: 3.3080. Mask: 0.9769. :  22%|██▏       | 11/50 [00:06<00:18,  2.13it/s]Train Iter: 862/1000. LR: 0.0391. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9047. T_Loss: 3.3080. Mask: 0.9769. :  24%|██▍       | 12/50 [00:06<00:26,  1.45it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.28s. Batch: 0.55s. S_Loss: 0.9058. T_Loss: 3.3035. Mask: 0.9754. :  24%|██▍       | 12/50 [00:07<00:26,  1.45it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.28s. Batch: 0.55s. S_Loss: 0.9058. T_Loss: 3.3035. Mask: 0.9754. :  26%|██▌       | 13/50 [00:07<00:22,  1.66it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9095. T_Loss: 3.2981. Mask: 0.9749. :  26%|██▌       | 13/50 [00:07<00:22,  1.66it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9095. T_Loss: 3.2981. Mask: 0.9749. :  28%|██▊       | 14/50 [00:07<00:18,  1.94it/s]total : 1000  current step :  862
total : 1000  current step :  863
total : 1000  current step :  864
Train Iter: 865/1000. LR: 0.0381. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9091. T_Loss: 3.3085. Mask: 0.9755. :  28%|██▊       | 14/50 [00:08<00:18,  1.94it/s]Train Iter: 865/1000. LR: 0.0381. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9091. T_Loss: 3.3085. Mask: 0.9755. :  30%|███       | 15/50 [00:08<00:23,  1.50it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.29s. Batch: 0.55s. S_Loss: 0.9080. T_Loss: 3.3162. Mask: 0.9761. :  30%|███       | 15/50 [00:08<00:23,  1.50it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.29s. Batch: 0.55s. S_Loss: 0.9080. T_Loss: 3.3162. Mask: 0.9761. :  32%|███▏      | 16/50 [00:08<00:19,  1.73it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9069. T_Loss: 3.3275. Mask: 0.9761. :  32%|███▏      | 16/50 [00:09<00:19,  1.73it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9069. T_Loss: 3.3275. Mask: 0.9761. :  34%|███▍      | 17/50 [00:09<00:15,  2.13it/s]total : 1000  current step :  865
total : 1000  current step :  866
total : 1000  current step :  867
Train Iter: 868/1000. LR: 0.0370. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9068. T_Loss: 3.3287. Mask: 0.9761. :  34%|███▍      | 17/50 [00:10<00:15,  2.13it/s]Train Iter: 868/1000. LR: 0.0370. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9068. T_Loss: 3.3287. Mask: 0.9761. :  36%|███▌      | 18/50 [00:10<00:20,  1.56it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9061. T_Loss: 3.3396. Mask: 0.9766. :  36%|███▌      | 18/50 [00:10<00:20,  1.56it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9061. T_Loss: 3.3396. Mask: 0.9766. :  38%|███▊      | 19/50 [00:10<00:17,  1.80it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.29s. Batch: 0.54s. S_Loss: 0.9076. T_Loss: 3.3660. Mask: 0.9768. :  38%|███▊      | 19/50 [00:10<00:17,  1.80it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.29s. Batch: 0.54s. S_Loss: 0.9076. T_Loss: 3.3660. Mask: 0.9768. :  40%|████      | 20/50 [00:10<00:15,  1.98it/s]total : 1000  current step :  868
total : 1000  current step :  869
total : 1000  current step :  870
Train Iter: 871/1000. LR: 0.0360. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9067. T_Loss: 3.3684. Mask: 0.9754. :  40%|████      | 20/50 [00:11<00:15,  1.98it/s]Train Iter: 871/1000. LR: 0.0360. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9067. T_Loss: 3.3684. Mask: 0.9754. :  42%|████▏     | 21/50 [00:11<00:19,  1.50it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9051. T_Loss: 3.3755. Mask: 0.9762. :  42%|████▏     | 21/50 [00:12<00:19,  1.50it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9051. T_Loss: 3.3755. Mask: 0.9762. :  44%|████▍     | 22/50 [00:12<00:15,  1.78it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9061. T_Loss: 3.3840. Mask: 0.9757. :  44%|████▍     | 22/50 [00:12<00:15,  1.78it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9061. T_Loss: 3.3840. Mask: 0.9757. :  46%|████▌     | 23/50 [00:12<00:13,  1.93it/s]total : 1000  current step :  871
total : 1000  current step :  872
total : 1000  current step :  873
Train Iter: 874/1000. LR: 0.0349. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9063. T_Loss: 3.3786. Mask: 0.9757. :  46%|████▌     | 23/50 [00:13<00:13,  1.93it/s]Train Iter: 874/1000. LR: 0.0349. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9063. T_Loss: 3.3786. Mask: 0.9757. :  48%|████▊     | 24/50 [00:13<00:17,  1.48it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9067. T_Loss: 3.3777. Mask: 0.9761. :  48%|████▊     | 24/50 [00:14<00:17,  1.48it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9067. T_Loss: 3.3777. Mask: 0.9761. :  50%|█████     | 25/50 [00:14<00:14,  1.70it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9068. T_Loss: 3.3759. Mask: 0.9754. :  50%|█████     | 25/50 [00:14<00:14,  1.70it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.30s. Batch: 0.55s. S_Loss: 0.9068. T_Loss: 3.3759. Mask: 0.9754. :  52%|█████▏    | 26/50 [00:14<00:12,  2.00it/s]total : 1000  current step :  874
total : 1000  current step :  875
total : 1000  current step :  876
Train Iter: 877/1000. LR: 0.0338. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9074. T_Loss: 3.3890. Mask: 0.9755. :  52%|█████▏    | 26/50 [00:15<00:12,  2.00it/s]Train Iter: 877/1000. LR: 0.0338. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9074. T_Loss: 3.3890. Mask: 0.9755. :  54%|█████▍    | 27/50 [00:15<00:15,  1.51it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9054. T_Loss: 3.3754. Mask: 0.9754. :  54%|█████▍    | 27/50 [00:15<00:15,  1.51it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.31s. Batch: 0.56s. S_Loss: 0.9054. T_Loss: 3.3754. Mask: 0.9754. :  56%|█████▌    | 28/50 [00:15<00:12,  1.78it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9072. T_Loss: 3.3787. Mask: 0.9747. :  56%|█████▌    | 28/50 [00:16<00:12,  1.78it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.31s. Batch: 0.55s. S_Loss: 0.9072. T_Loss: 3.3787. Mask: 0.9747. :  58%|█████▊    | 29/50 [00:16<00:10,  2.00it/s]total : 1000  current step :  877
total : 1000  current step :  878
total : 1000  current step :  879
Train Iter: 880/1000. LR: 0.0327. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9069. T_Loss: 3.3729. Mask: 0.9741. :  58%|█████▊    | 29/50 [00:17<00:10,  2.00it/s]Train Iter: 880/1000. LR: 0.0327. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9069. T_Loss: 3.3729. Mask: 0.9741. :  60%|██████    | 30/50 [00:17<00:14,  1.33it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9079. T_Loss: 3.3810. Mask: 0.9740. :  60%|██████    | 30/50 [00:18<00:14,  1.33it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9079. T_Loss: 3.3810. Mask: 0.9740. :  62%|██████▏   | 31/50 [00:18<00:14,  1.35it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9091. T_Loss: 3.3925. Mask: 0.9740. :  62%|██████▏   | 31/50 [00:18<00:14,  1.35it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9091. T_Loss: 3.3925. Mask: 0.9740. :  64%|██████▍   | 32/50 [00:18<00:11,  1.58it/s]total : 1000  current step :  880
total : 1000  current step :  881
total : 1000  current step :  882
Train Iter: 883/1000. LR: 0.0316. Data: 0.35s. Batch: 0.60s. S_Loss: 0.9082. T_Loss: 3.3841. Mask: 0.9736. :  64%|██████▍   | 32/50 [00:19<00:11,  1.58it/s]Train Iter: 883/1000. LR: 0.0316. Data: 0.35s. Batch: 0.60s. S_Loss: 0.9082. T_Loss: 3.3841. Mask: 0.9736. :  66%|██████▌   | 33/50 [00:19<00:13,  1.25it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9082. T_Loss: 3.3796. Mask: 0.9735. :  66%|██████▌   | 33/50 [00:20<00:13,  1.25it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9082. T_Loss: 3.3796. Mask: 0.9735. :  68%|██████▊   | 34/50 [00:20<00:10,  1.54it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9076. T_Loss: 3.3795. Mask: 0.9733. :  68%|██████▊   | 34/50 [00:20<00:10,  1.54it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9076. T_Loss: 3.3795. Mask: 0.9733. :  70%|███████   | 35/50 [00:20<00:08,  1.68it/s]total : 1000  current step :  883
total : 1000  current step :  884
total : 1000  current step :  885
Train Iter: 886/1000. LR: 0.0305. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9075. T_Loss: 3.3794. Mask: 0.9735. :  70%|███████   | 35/50 [00:21<00:08,  1.68it/s]Train Iter: 886/1000. LR: 0.0305. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9075. T_Loss: 3.3794. Mask: 0.9735. :  72%|███████▏  | 36/50 [00:21<00:10,  1.29it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9065. T_Loss: 3.3727. Mask: 0.9734. :  72%|███████▏  | 36/50 [00:22<00:10,  1.29it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9065. T_Loss: 3.3727. Mask: 0.9734. :  74%|███████▍  | 37/50 [00:22<00:08,  1.56it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9060. T_Loss: 3.3740. Mask: 0.9737. :  74%|███████▍  | 37/50 [00:22<00:08,  1.56it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9060. T_Loss: 3.3740. Mask: 0.9737. :  76%|███████▌  | 38/50 [00:22<00:06,  1.79it/s]total : 1000  current step :  886
total : 1000  current step :  887
total : 1000  current step :  888
Train Iter: 889/1000. LR: 0.0293. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9050. T_Loss: 3.3649. Mask: 0.9737. :  76%|███████▌  | 38/50 [00:23<00:06,  1.79it/s]Train Iter: 889/1000. LR: 0.0293. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9050. T_Loss: 3.3649. Mask: 0.9737. :  78%|███████▊  | 39/50 [00:23<00:07,  1.42it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9051. T_Loss: 3.3621. Mask: 0.9736. :  78%|███████▊  | 39/50 [00:23<00:07,  1.42it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9051. T_Loss: 3.3621. Mask: 0.9736. :  80%|████████  | 40/50 [00:23<00:06,  1.66it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9052. T_Loss: 3.3574. Mask: 0.9736. :  80%|████████  | 40/50 [00:24<00:06,  1.66it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9052. T_Loss: 3.3574. Mask: 0.9736. :  82%|████████▏ | 41/50 [00:24<00:04,  1.97it/s]total : 1000  current step :  889
total : 1000  current step :  890
total : 1000  current step :  891
Train Iter: 892/1000. LR: 0.0281. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9056. T_Loss: 3.3566. Mask: 0.9735. :  82%|████████▏ | 41/50 [00:25<00:04,  1.97it/s]Train Iter: 892/1000. LR: 0.0281. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9056. T_Loss: 3.3566. Mask: 0.9735. :  84%|████████▍ | 42/50 [00:25<00:05,  1.42it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9059. T_Loss: 3.3575. Mask: 0.9735. :  84%|████████▍ | 42/50 [00:25<00:05,  1.42it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9059. T_Loss: 3.3575. Mask: 0.9735. :  86%|████████▌ | 43/50 [00:25<00:04,  1.67it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9061. T_Loss: 3.3575. Mask: 0.9735. :  86%|████████▌ | 43/50 [00:26<00:04,  1.67it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9061. T_Loss: 3.3575. Mask: 0.9735. :  88%|████████▊ | 44/50 [00:26<00:03,  1.80it/s]total : 1000  current step :  892
total : 1000  current step :  893
total : 1000  current step :  894
Train Iter: 895/1000. LR: 0.0270. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9051. T_Loss: 3.3506. Mask: 0.9735. :  88%|████████▊ | 44/50 [00:27<00:03,  1.80it/s]Train Iter: 895/1000. LR: 0.0270. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9051. T_Loss: 3.3506. Mask: 0.9735. :  90%|█████████ | 45/50 [00:27<00:03,  1.36it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9055. T_Loss: 3.3499. Mask: 0.9734. :  90%|█████████ | 45/50 [00:27<00:03,  1.36it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9055. T_Loss: 3.3499. Mask: 0.9734. :  92%|█████████▏| 46/50 [00:27<00:02,  1.62it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9055. T_Loss: 3.3504. Mask: 0.9734. :  92%|█████████▏| 46/50 [00:27<00:02,  1.62it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9055. T_Loss: 3.3504. Mask: 0.9734. :  94%|█████████▍| 47/50 [00:27<00:01,  1.85it/s]total : 1000  current step :  895
total : 1000  current step :  896
total : 1000  current step :  897
Train Iter: 898/1000. LR: 0.0258. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9044. T_Loss: 3.3490. Mask: 0.9736. :  94%|█████████▍| 47/50 [00:28<00:01,  1.85it/s]Train Iter: 898/1000. LR: 0.0258. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9044. T_Loss: 3.3490. Mask: 0.9736. :  96%|█████████▌| 48/50 [00:28<00:01,  1.47it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9050. T_Loss: 3.3504. Mask: 0.9736. :  96%|█████████▌| 48/50 [00:29<00:01,  1.47it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9050. T_Loss: 3.3504. Mask: 0.9736. :  98%|█████████▊| 49/50 [00:29<00:00,  1.68it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9054. T_Loss: 3.3487. Mask: 0.9736. :  98%|█████████▊| 49/50 [00:29<00:00,  1.68it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9054. T_Loss: 3.3487. Mask: 0.9736. : 100%|██████████| 50/50 [00:29<00:00,  1.96it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9054. T_Loss: 3.3487. Mask: 0.9736. : 100%|██████████| 50/50 [00:29<00:00,  1.69it/s]
total : 1000  current step :  898
total : 1000  current step :  899
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 1.3569. top1: 69.14. top5: 96.09. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 1.3569. top1: 69.14. top5: 96.09. :  12%|█▎        | 1/8 [00:00<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.3595. top1: 68.55. top5: 96.29. :  12%|█▎        | 1/8 [00:01<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.3595. top1: 68.55. top5: 96.29. :  25%|██▌       | 2/8 [00:01<00:02,  2.05it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.3417. top1: 69.14. top5: 96.35. :  25%|██▌       | 2/8 [00:01<00:02,  2.05it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.3417. top1: 69.14. top5: 96.35. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.2983. top1: 71.39. top5: 96.88. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.2983. top1: 71.39. top5: 96.88. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.1901. top1: 76.88. top5: 97.50. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.1901. top1: 76.88. top5: 97.50. :  62%|██████▎   | 5/8 [00:01<00:01,  2.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.1191. top1: 80.47. top5: 97.92. :  62%|██████▎   | 5/8 [00:02<00:01,  2.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.1191. top1: 80.47. top5: 97.92. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0649. top1: 83.26. top5: 98.21. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0649. top1: 83.26. top5: 98.21. :  88%|████████▊ | 7/8 [00:02<00:00,  3.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0322. top1: 84.85. top5: 98.40. :  88%|████████▊ | 7/8 [00:02<00:00,  3.12it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0322. top1: 84.85. top5: 98.40. : 100%|██████████| 8/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0322. top1: 84.85. top5: 98.40. : 100%|██████████| 8/8 [00:03<00:00,  2.65it/s]
total : 1000  current step :  900
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 1.01s. Batch: 1.31s. S_Loss: 0.9167. T_Loss: 3.0143. Mask: 0.9727. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 1.01s. Batch: 1.31s. S_Loss: 0.9167. T_Loss: 3.0143. Mask: 0.9727. :   2%|▏         | 1/50 [00:01<01:04,  1.31s/it]Train Iter: 902/1000. LR: 0.0242. Data: 0.60s. Batch: 0.91s. S_Loss: 0.9137. T_Loss: 3.1834. Mask: 0.9766. :   2%|▏         | 1/50 [00:01<01:04,  1.31s/it]Train Iter: 902/1000. LR: 0.0242. Data: 0.60s. Batch: 0.91s. S_Loss: 0.9137. T_Loss: 3.1834. Mask: 0.9766. :   4%|▍         | 2/50 [00:01<00:40,  1.19it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.45s. Batch: 0.77s. S_Loss: 0.8949. T_Loss: 3.0966. Mask: 0.9766. :   4%|▍         | 2/50 [00:02<00:40,  1.19it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.45s. Batch: 0.77s. S_Loss: 0.8949. T_Loss: 3.0966. Mask: 0.9766. :   6%|▌         | 3/50 [00:02<00:31,  1.48it/s]total : 1000  current step :  901
total : 1000  current step :  902
total : 1000  current step :  903
Train Iter: 904/1000. LR: 0.0234. Data: 0.60s. Batch: 0.91s. S_Loss: 0.8954. T_Loss: 3.1255. Mask: 0.9756. :   6%|▌         | 3/50 [00:03<00:31,  1.48it/s]Train Iter: 904/1000. LR: 0.0234. Data: 0.60s. Batch: 0.91s. S_Loss: 0.8954. T_Loss: 3.1255. Mask: 0.9756. :   8%|▊         | 4/50 [00:03<00:42,  1.07it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.50s. Batch: 0.80s. S_Loss: 0.8970. T_Loss: 3.1860. Mask: 0.9781. :   8%|▊         | 4/50 [00:03<00:42,  1.07it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.50s. Batch: 0.80s. S_Loss: 0.8970. T_Loss: 3.1860. Mask: 0.9781. :  10%|█         | 5/50 [00:03<00:32,  1.38it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.43s. Batch: 0.73s. S_Loss: 0.8985. T_Loss: 3.1968. Mask: 0.9779. :  10%|█         | 5/50 [00:04<00:32,  1.38it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.43s. Batch: 0.73s. S_Loss: 0.8985. T_Loss: 3.1968. Mask: 0.9779. :  12%|█▏        | 6/50 [00:04<00:27,  1.62it/s]total : 1000  current step :  904
total : 1000  current step :  905
total : 1000  current step :  906
Train Iter: 907/1000. LR: 0.0223. Data: 0.50s. Batch: 0.79s. S_Loss: 0.8996. T_Loss: 3.1978. Mask: 0.9743. :  12%|█▏        | 6/50 [00:05<00:27,  1.62it/s]Train Iter: 907/1000. LR: 0.0223. Data: 0.50s. Batch: 0.79s. S_Loss: 0.8996. T_Loss: 3.1978. Mask: 0.9743. :  14%|█▍        | 7/50 [00:05<00:33,  1.27it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.46s. Batch: 0.74s. S_Loss: 0.8958. T_Loss: 3.1724. Mask: 0.9751. :  14%|█▍        | 7/50 [00:05<00:33,  1.27it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.46s. Batch: 0.74s. S_Loss: 0.8958. T_Loss: 3.1724. Mask: 0.9751. :  16%|█▌        | 8/50 [00:05<00:28,  1.48it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.41s. Batch: 0.70s. S_Loss: 0.8997. T_Loss: 3.2154. Mask: 0.9753. :  16%|█▌        | 8/50 [00:06<00:28,  1.48it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.41s. Batch: 0.70s. S_Loss: 0.8997. T_Loss: 3.2154. Mask: 0.9753. :  18%|█▊        | 9/50 [00:06<00:23,  1.75it/s]total : 1000  current step :  907
total : 1000  current step :  908
total : 1000  current step :  909
Train Iter: 910/1000. LR: 0.0211. Data: 0.47s. Batch: 0.75s. S_Loss: 0.9022. T_Loss: 3.2379. Mask: 0.9754. :  18%|█▊        | 9/50 [00:07<00:23,  1.75it/s]Train Iter: 910/1000. LR: 0.0211. Data: 0.47s. Batch: 0.75s. S_Loss: 0.9022. T_Loss: 3.2379. Mask: 0.9754. :  20%|██        | 10/50 [00:07<00:30,  1.30it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.44s. Batch: 0.71s. S_Loss: 0.9015. T_Loss: 3.2215. Mask: 0.9751. :  20%|██        | 10/50 [00:07<00:30,  1.30it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.44s. Batch: 0.71s. S_Loss: 0.9015. T_Loss: 3.2215. Mask: 0.9751. :  22%|██▏       | 11/50 [00:07<00:24,  1.56it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.41s. Batch: 0.69s. S_Loss: 0.9004. T_Loss: 3.2197. Mask: 0.9756. :  22%|██▏       | 11/50 [00:08<00:24,  1.56it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.41s. Batch: 0.69s. S_Loss: 0.9004. T_Loss: 3.2197. Mask: 0.9756. :  24%|██▍       | 12/50 [00:08<00:21,  1.76it/s]total : 1000  current step :  910
total : 1000  current step :  911
total : 1000  current step :  912
Train Iter: 913/1000. LR: 0.0199. Data: 0.45s. Batch: 0.72s. S_Loss: 0.9022. T_Loss: 3.2438. Mask: 0.9760. :  24%|██▍       | 12/50 [00:09<00:21,  1.76it/s]Train Iter: 913/1000. LR: 0.0199. Data: 0.45s. Batch: 0.72s. S_Loss: 0.9022. T_Loss: 3.2438. Mask: 0.9760. :  26%|██▌       | 13/50 [00:09<00:27,  1.33it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9009. T_Loss: 3.2548. Mask: 0.9766. :  26%|██▌       | 13/50 [00:09<00:27,  1.33it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9009. T_Loss: 3.2548. Mask: 0.9766. :  28%|██▊       | 14/50 [00:09<00:22,  1.62it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.41s. Batch: 0.68s. S_Loss: 0.9009. T_Loss: 3.2468. Mask: 0.9755. :  28%|██▊       | 14/50 [00:10<00:22,  1.62it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.41s. Batch: 0.68s. S_Loss: 0.9009. T_Loss: 3.2468. Mask: 0.9755. :  30%|███       | 15/50 [00:10<00:19,  1.80it/s]total : 1000  current step :  913
total : 1000  current step :  914
total : 1000  current step :  915
Train Iter: 916/1000. LR: 0.0188. Data: 0.44s. Batch: 0.72s. S_Loss: 0.9030. T_Loss: 3.2662. Mask: 0.9749. :  30%|███       | 15/50 [00:11<00:19,  1.80it/s]Train Iter: 916/1000. LR: 0.0188. Data: 0.44s. Batch: 0.72s. S_Loss: 0.9030. T_Loss: 3.2662. Mask: 0.9749. :  32%|███▏      | 16/50 [00:11<00:26,  1.26it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9029. T_Loss: 3.2600. Mask: 0.9745. :  32%|███▏      | 16/50 [00:11<00:26,  1.26it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9029. T_Loss: 3.2600. Mask: 0.9745. :  34%|███▍      | 17/50 [00:11<00:21,  1.53it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9033. T_Loss: 3.2569. Mask: 0.9735. :  34%|███▍      | 17/50 [00:12<00:21,  1.53it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9033. T_Loss: 3.2569. Mask: 0.9735. :  36%|███▌      | 18/50 [00:12<00:17,  1.81it/s]total : 1000  current step :  916
total : 1000  current step :  917
total : 1000  current step :  918
Train Iter: 919/1000. LR: 0.0176. Data: 0.44s. Batch: 0.71s. S_Loss: 0.9033. T_Loss: 3.2653. Mask: 0.9725. :  36%|███▌      | 18/50 [00:13<00:17,  1.81it/s]Train Iter: 919/1000. LR: 0.0176. Data: 0.44s. Batch: 0.71s. S_Loss: 0.9033. T_Loss: 3.2653. Mask: 0.9725. :  38%|███▊      | 19/50 [00:13<00:25,  1.23it/s]total : 1000  current step :  919
Train Iter: 920/1000. LR: 0.0173. Data: 0.44s. Batch: 0.72s. S_Loss: 0.9039. T_Loss: 3.2810. Mask: 0.9719. :  38%|███▊      | 19/50 [00:14<00:25,  1.23it/s]Train Iter: 920/1000. LR: 0.0173. Data: 0.44s. Batch: 0.72s. S_Loss: 0.9039. T_Loss: 3.2810. Mask: 0.9719. :  40%|████      | 20/50 [00:14<00:24,  1.23it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.45s. Batch: 0.72s. S_Loss: 0.9056. T_Loss: 3.2817. Mask: 0.9715. :  40%|████      | 20/50 [00:15<00:24,  1.23it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.45s. Batch: 0.72s. S_Loss: 0.9056. T_Loss: 3.2817. Mask: 0.9715. :  42%|████▏     | 21/50 [00:15<00:23,  1.26it/s]total : 1000  current step :  920
total : 1000  current step :  921
Train Iter: 922/1000. LR: 0.0165. Data: 0.48s. Batch: 0.75s. S_Loss: 0.9060. T_Loss: 3.2831. Mask: 0.9718. :  42%|████▏     | 21/50 [00:16<00:23,  1.26it/s]Train Iter: 922/1000. LR: 0.0165. Data: 0.48s. Batch: 0.75s. S_Loss: 0.9060. T_Loss: 3.2831. Mask: 0.9718. :  44%|████▍     | 22/50 [00:16<00:26,  1.05it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.46s. Batch: 0.73s. S_Loss: 0.9054. T_Loss: 3.2788. Mask: 0.9716. :  44%|████▍     | 22/50 [00:16<00:26,  1.05it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.46s. Batch: 0.73s. S_Loss: 0.9054. T_Loss: 3.2788. Mask: 0.9716. :  46%|████▌     | 23/50 [00:16<00:20,  1.31it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.45s. Batch: 0.71s. S_Loss: 0.9041. T_Loss: 3.2828. Mask: 0.9722. :  46%|████▌     | 23/50 [00:17<00:20,  1.31it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.45s. Batch: 0.71s. S_Loss: 0.9041. T_Loss: 3.2828. Mask: 0.9722. :  48%|████▊     | 24/50 [00:17<00:16,  1.56it/s]total : 1000  current step :  922
total : 1000  current step :  923
total : 1000  current step :  924
Train Iter: 925/1000. LR: 0.0154. Data: 0.48s. Batch: 0.74s. S_Loss: 0.9028. T_Loss: 3.2797. Mask: 0.9725. :  48%|████▊     | 24/50 [00:18<00:16,  1.56it/s]Train Iter: 925/1000. LR: 0.0154. Data: 0.48s. Batch: 0.74s. S_Loss: 0.9028. T_Loss: 3.2797. Mask: 0.9725. :  50%|█████     | 25/50 [00:18<00:21,  1.17it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9021. T_Loss: 3.2698. Mask: 0.9725. :  50%|█████     | 25/50 [00:18<00:21,  1.17it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9021. T_Loss: 3.2698. Mask: 0.9725. :  52%|█████▏    | 26/50 [00:18<00:17,  1.41it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.46s. Batch: 0.72s. S_Loss: 0.9034. T_Loss: 3.2761. Mask: 0.9724. :  52%|█████▏    | 26/50 [00:19<00:17,  1.41it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.46s. Batch: 0.72s. S_Loss: 0.9034. T_Loss: 3.2761. Mask: 0.9724. :  54%|█████▍    | 27/50 [00:19<00:15,  1.53it/s]total : 1000  current step :  925
total : 1000  current step :  926
total : 1000  current step :  927
Train Iter: 928/1000. LR: 0.0144. Data: 0.48s. Batch: 0.73s. S_Loss: 0.9018. T_Loss: 3.2647. Mask: 0.9720. :  54%|█████▍    | 27/50 [00:20<00:15,  1.53it/s]Train Iter: 928/1000. LR: 0.0144. Data: 0.48s. Batch: 0.73s. S_Loss: 0.9018. T_Loss: 3.2647. Mask: 0.9720. :  56%|█████▌    | 28/50 [00:20<00:18,  1.22it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9014. T_Loss: 3.2671. Mask: 0.9720. :  56%|█████▌    | 28/50 [00:20<00:18,  1.22it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.47s. Batch: 0.72s. S_Loss: 0.9014. T_Loss: 3.2671. Mask: 0.9720. :  58%|█████▊    | 29/50 [00:20<00:14,  1.46it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.46s. Batch: 0.72s. S_Loss: 0.8999. T_Loss: 3.2600. Mask: 0.9723. :  58%|█████▊    | 29/50 [00:21<00:14,  1.46it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.46s. Batch: 0.72s. S_Loss: 0.8999. T_Loss: 3.2600. Mask: 0.9723. :  60%|██████    | 30/50 [00:21<00:13,  1.52it/s]total : 1000  current step :  928
total : 1000  current step :  929
total : 1000  current step :  930
Train Iter: 931/1000. LR: 0.0133. Data: 0.48s. Batch: 0.73s. S_Loss: 0.8996. T_Loss: 3.2616. Mask: 0.9722. :  60%|██████    | 30/50 [00:22<00:13,  1.52it/s]Train Iter: 931/1000. LR: 0.0133. Data: 0.48s. Batch: 0.73s. S_Loss: 0.8996. T_Loss: 3.2616. Mask: 0.9722. :  62%|██████▏   | 31/50 [00:22<00:15,  1.23it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.47s. Batch: 0.72s. S_Loss: 0.8991. T_Loss: 3.2604. Mask: 0.9727. :  62%|██████▏   | 31/50 [00:23<00:15,  1.23it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.47s. Batch: 0.72s. S_Loss: 0.8991. T_Loss: 3.2604. Mask: 0.9727. :  64%|██████▍   | 32/50 [00:23<00:12,  1.48it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.46s. Batch: 0.71s. S_Loss: 0.8992. T_Loss: 3.2626. Mask: 0.9729. :  64%|██████▍   | 32/50 [00:23<00:12,  1.48it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.46s. Batch: 0.71s. S_Loss: 0.8992. T_Loss: 3.2626. Mask: 0.9729. :  66%|██████▌   | 33/50 [00:23<00:10,  1.59it/s]total : 1000  current step :  931
total : 1000  current step :  932
total : 1000  current step :  933
Train Iter: 934/1000. LR: 0.0123. Data: 0.48s. Batch: 0.73s. S_Loss: 0.8995. T_Loss: 3.2714. Mask: 0.9731. :  66%|██████▌   | 33/50 [00:24<00:10,  1.59it/s]Train Iter: 934/1000. LR: 0.0123. Data: 0.48s. Batch: 0.73s. S_Loss: 0.8995. T_Loss: 3.2714. Mask: 0.9731. :  68%|██████▊   | 34/50 [00:24<00:13,  1.22it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.47s. Batch: 0.72s. S_Loss: 0.8993. T_Loss: 3.2724. Mask: 0.9732. :  68%|██████▊   | 34/50 [00:25<00:13,  1.22it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.47s. Batch: 0.72s. S_Loss: 0.8993. T_Loss: 3.2724. Mask: 0.9732. :  70%|███████   | 35/50 [00:25<00:10,  1.45it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.46s. Batch: 0.71s. S_Loss: 0.8991. T_Loss: 3.2696. Mask: 0.9733. :  70%|███████   | 35/50 [00:25<00:10,  1.45it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.46s. Batch: 0.71s. S_Loss: 0.8991. T_Loss: 3.2696. Mask: 0.9733. :  72%|███████▏  | 36/50 [00:25<00:08,  1.63it/s]total : 1000  current step :  934
total : 1000  current step :  935
total : 1000  current step :  936
Train Iter: 937/1000. LR: 0.0113. Data: 0.48s. Batch: 0.73s. S_Loss: 0.8984. T_Loss: 3.2635. Mask: 0.9735. :  72%|███████▏  | 36/50 [00:27<00:08,  1.63it/s]Train Iter: 937/1000. LR: 0.0113. Data: 0.48s. Batch: 0.73s. S_Loss: 0.8984. T_Loss: 3.2635. Mask: 0.9735. :  74%|███████▍  | 37/50 [00:27<00:10,  1.21it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.47s. Batch: 0.72s. S_Loss: 0.8982. T_Loss: 3.2609. Mask: 0.9737. :  74%|███████▍  | 37/50 [00:27<00:10,  1.21it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.47s. Batch: 0.72s. S_Loss: 0.8982. T_Loss: 3.2609. Mask: 0.9737. :  76%|███████▌  | 38/50 [00:27<00:08,  1.45it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8982. T_Loss: 3.2683. Mask: 0.9741. :  76%|███████▌  | 38/50 [00:27<00:08,  1.45it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8982. T_Loss: 3.2683. Mask: 0.9741. :  78%|███████▊  | 39/50 [00:27<00:06,  1.64it/s]total : 1000  current step :  937
total : 1000  current step :  938
total : 1000  current step :  939
Train Iter: 940/1000. LR: 0.0103. Data: 0.48s. Batch: 0.72s. S_Loss: 0.8986. T_Loss: 3.2746. Mask: 0.9741. :  78%|███████▊  | 39/50 [00:29<00:06,  1.64it/s]Train Iter: 940/1000. LR: 0.0103. Data: 0.48s. Batch: 0.72s. S_Loss: 0.8986. T_Loss: 3.2746. Mask: 0.9741. :  80%|████████  | 40/50 [00:29<00:08,  1.25it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.47s. Batch: 0.72s. S_Loss: 0.8996. T_Loss: 3.2820. Mask: 0.9739. :  80%|████████  | 40/50 [00:29<00:08,  1.25it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.47s. Batch: 0.72s. S_Loss: 0.8996. T_Loss: 3.2820. Mask: 0.9739. :  82%|████████▏ | 41/50 [00:29<00:06,  1.45it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8992. T_Loss: 3.2853. Mask: 0.9740. :  82%|████████▏ | 41/50 [00:29<00:06,  1.45it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8992. T_Loss: 3.2853. Mask: 0.9740. :  84%|████████▍ | 42/50 [00:29<00:04,  1.69it/s]total : 1000  current step :  940
total : 1000  current step :  941
total : 1000  current step :  942
Train Iter: 943/1000. LR: 0.0094. Data: 0.48s. Batch: 0.72s. S_Loss: 0.8984. T_Loss: 3.2874. Mask: 0.9746. :  84%|████████▍ | 42/50 [00:31<00:04,  1.69it/s]Train Iter: 943/1000. LR: 0.0094. Data: 0.48s. Batch: 0.72s. S_Loss: 0.8984. T_Loss: 3.2874. Mask: 0.9746. :  86%|████████▌ | 43/50 [00:31<00:05,  1.25it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8984. T_Loss: 3.2837. Mask: 0.9748. :  86%|████████▌ | 43/50 [00:31<00:05,  1.25it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8984. T_Loss: 3.2837. Mask: 0.9748. :  88%|████████▊ | 44/50 [00:31<00:03,  1.51it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.46s. Batch: 0.70s. S_Loss: 0.8981. T_Loss: 3.2908. Mask: 0.9748. :  88%|████████▊ | 44/50 [00:31<00:03,  1.51it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.46s. Batch: 0.70s. S_Loss: 0.8981. T_Loss: 3.2908. Mask: 0.9748. :  90%|█████████ | 45/50 [00:31<00:02,  1.78it/s]total : 1000  current step :  943
total : 1000  current step :  944
total : 1000  current step :  945
Train Iter: 946/1000. LR: 0.0085. Data: 0.48s. Batch: 0.72s. S_Loss: 0.8985. T_Loss: 3.2970. Mask: 0.9748. :  90%|█████████ | 45/50 [00:33<00:02,  1.78it/s]Train Iter: 946/1000. LR: 0.0085. Data: 0.48s. Batch: 0.72s. S_Loss: 0.8985. T_Loss: 3.2970. Mask: 0.9748. :  92%|█████████▏| 46/50 [00:33<00:03,  1.23it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8984. T_Loss: 3.2958. Mask: 0.9747. :  92%|█████████▏| 46/50 [00:33<00:03,  1.23it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8984. T_Loss: 3.2958. Mask: 0.9747. :  94%|█████████▍| 47/50 [00:33<00:02,  1.42it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8978. T_Loss: 3.2915. Mask: 0.9749. :  94%|█████████▍| 47/50 [00:33<00:02,  1.42it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8978. T_Loss: 3.2915. Mask: 0.9749. :  96%|█████████▌| 48/50 [00:33<00:01,  1.69it/s]total : 1000  current step :  946
total : 1000  current step :  947
total : 1000  current step :  948
Train Iter: 949/1000. LR: 0.0076. Data: 0.48s. Batch: 0.72s. S_Loss: 0.8975. T_Loss: 3.2881. Mask: 0.9750. :  96%|█████████▌| 48/50 [00:35<00:01,  1.69it/s]Train Iter: 949/1000. LR: 0.0076. Data: 0.48s. Batch: 0.72s. S_Loss: 0.8975. T_Loss: 3.2881. Mask: 0.9750. :  98%|█████████▊| 49/50 [00:35<00:00,  1.22it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8965. T_Loss: 3.2843. Mask: 0.9749. :  98%|█████████▊| 49/50 [00:35<00:00,  1.22it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8965. T_Loss: 3.2843. Mask: 0.9749. : 100%|██████████| 50/50 [00:35<00:00,  1.46it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8965. T_Loss: 3.2843. Mask: 0.9749. : 100%|██████████| 50/50 [00:35<00:00,  1.40it/s]
total : 1000  current step :  949
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 1.3225. top1: 69.53. top5: 96.48. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 1.3225. top1: 69.53. top5: 96.48. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.3261. top1: 69.14. top5: 96.88. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.3261. top1: 69.14. top5: 96.88. :  25%|██▌       | 2/8 [00:00<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.3093. top1: 69.66. top5: 97.14. :  25%|██▌       | 2/8 [00:01<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.3093. top1: 69.66. top5: 97.14. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.2684. top1: 71.58. top5: 97.46. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.2684. top1: 71.58. top5: 97.46. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.1666. top1: 77.03. top5: 97.97. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.1666. top1: 77.03. top5: 97.97. :  62%|██████▎   | 5/8 [00:01<00:01,  2.88it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.1002. top1: 80.60. top5: 98.31. :  62%|██████▎   | 5/8 [00:02<00:01,  2.88it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.1002. top1: 80.60. top5: 98.31. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0492. top1: 83.31. top5: 98.55. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0492. top1: 83.31. top5: 98.55. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0185. top1: 84.90. top5: 98.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0185. top1: 84.90. top5: 98.70. : 100%|██████████| 8/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0185. top1: 84.90. top5: 98.70. : 100%|██████████| 8/8 [00:02<00:00,  2.73it/s]
total : 1000  current step :  950
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.18s. S_Loss: 0.8852. T_Loss: 2.9936. Mask: 0.9688. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.18s. S_Loss: 0.8852. T_Loss: 2.9936. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:08,  5.56it/s]total : 1000  current step :  951
Train Iter: 952/1000. LR: 0.0068. Data: 0.51s. Batch: 0.69s. S_Loss: 0.8845. T_Loss: 3.0974. Mask: 0.9727. :   2%|▏         | 1/50 [00:01<00:08,  5.56it/s]Train Iter: 952/1000. LR: 0.0068. Data: 0.51s. Batch: 0.69s. S_Loss: 0.8845. T_Loss: 3.0974. Mask: 0.9727. :   4%|▍         | 2/50 [00:01<00:37,  1.28it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8795. T_Loss: 3.1471. Mask: 0.9714. :   4%|▍         | 2/50 [00:01<00:37,  1.28it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8795. T_Loss: 3.1471. Mask: 0.9714. :   6%|▌         | 3/50 [00:01<00:28,  1.64it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.34s. Batch: 0.54s. S_Loss: 0.8754. T_Loss: 3.0728. Mask: 0.9727. :   6%|▌         | 3/50 [00:02<00:28,  1.64it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.34s. Batch: 0.54s. S_Loss: 0.8754. T_Loss: 3.0728. Mask: 0.9727. :   8%|▊         | 4/50 [00:02<00:23,  1.95it/s]total : 1000  current step :  952
total : 1000  current step :  953
total : 1000  current step :  954
Train Iter: 955/1000. LR: 0.0060. Data: 0.45s. Batch: 0.64s. S_Loss: 0.8737. T_Loss: 3.0896. Mask: 0.9734. :   8%|▊         | 4/50 [00:03<00:23,  1.95it/s]Train Iter: 955/1000. LR: 0.0060. Data: 0.45s. Batch: 0.64s. S_Loss: 0.8737. T_Loss: 3.0896. Mask: 0.9734. :  10%|█         | 5/50 [00:03<00:32,  1.40it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.40s. Batch: 0.58s. S_Loss: 0.8759. T_Loss: 3.1085. Mask: 0.9714. :  10%|█         | 5/50 [00:03<00:32,  1.40it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.40s. Batch: 0.58s. S_Loss: 0.8759. T_Loss: 3.1085. Mask: 0.9714. :  12%|█▏        | 6/50 [00:03<00:25,  1.76it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.37s. Batch: 0.56s. S_Loss: 0.8740. T_Loss: 3.0674. Mask: 0.9721. :  12%|█▏        | 6/50 [00:03<00:25,  1.76it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.37s. Batch: 0.56s. S_Loss: 0.8740. T_Loss: 3.0674. Mask: 0.9721. :  14%|█▍        | 7/50 [00:03<00:22,  1.95it/s]total : 1000  current step :  955
total : 1000  current step :  956
total : 1000  current step :  957
Train Iter: 958/1000. LR: 0.0052. Data: 0.44s. Batch: 0.63s. S_Loss: 0.8774. T_Loss: 3.0852. Mask: 0.9707. :  14%|█▍        | 7/50 [00:05<00:22,  1.95it/s]Train Iter: 958/1000. LR: 0.0052. Data: 0.44s. Batch: 0.63s. S_Loss: 0.8774. T_Loss: 3.0852. Mask: 0.9707. :  16%|█▌        | 8/50 [00:05<00:30,  1.40it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.41s. Batch: 0.60s. S_Loss: 0.8750. T_Loss: 3.0799. Mask: 0.9722. :  16%|█▌        | 8/50 [00:05<00:30,  1.40it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.41s. Batch: 0.60s. S_Loss: 0.8750. T_Loss: 3.0799. Mask: 0.9722. :  18%|█▊        | 9/50 [00:05<00:24,  1.66it/s]total : 1000  current step :  958
total : 1000  current step :  959
Train Iter: 960/1000. LR: 0.0048. Data: 0.43s. Batch: 0.61s. S_Loss: 0.8722. T_Loss: 3.0719. Mask: 0.9727. :  18%|█▊        | 9/50 [00:06<00:24,  1.66it/s]Train Iter: 960/1000. LR: 0.0048. Data: 0.43s. Batch: 0.61s. S_Loss: 0.8722. T_Loss: 3.0719. Mask: 0.9727. :  20%|██        | 10/50 [00:06<00:25,  1.54it/s]total : 1000  current step :  960
Train Iter: 961/1000. LR: 0.0045. Data: 0.48s. Batch: 0.67s. S_Loss: 0.8697. T_Loss: 3.0732. Mask: 0.9737. :  20%|██        | 10/50 [00:07<00:25,  1.54it/s]Train Iter: 961/1000. LR: 0.0045. Data: 0.48s. Batch: 0.67s. S_Loss: 0.8697. T_Loss: 3.0732. Mask: 0.9737. :  22%|██▏       | 11/50 [00:07<00:32,  1.19it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.44s. Batch: 0.64s. S_Loss: 0.8691. T_Loss: 3.0530. Mask: 0.9743. :  22%|██▏       | 11/50 [00:07<00:32,  1.19it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.44s. Batch: 0.64s. S_Loss: 0.8691. T_Loss: 3.0530. Mask: 0.9743. :  24%|██▍       | 12/50 [00:07<00:25,  1.50it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.42s. Batch: 0.61s. S_Loss: 0.8727. T_Loss: 3.0769. Mask: 0.9739. :  24%|██▍       | 12/50 [00:07<00:25,  1.50it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.42s. Batch: 0.61s. S_Loss: 0.8727. T_Loss: 3.0769. Mask: 0.9739. :  26%|██▌       | 13/50 [00:07<00:19,  1.85it/s]total : 1000  current step :  961
total : 1000  current step :  962
total : 1000  current step :  963
Train Iter: 964/1000. LR: 0.0039. Data: 0.45s. Batch: 0.64s. S_Loss: 0.8750. T_Loss: 3.0986. Mask: 0.9741. :  26%|██▌       | 13/50 [00:09<00:19,  1.85it/s]Train Iter: 964/1000. LR: 0.0039. Data: 0.45s. Batch: 0.64s. S_Loss: 0.8750. T_Loss: 3.0986. Mask: 0.9741. :  28%|██▊       | 14/50 [00:09<00:25,  1.41it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.42s. Batch: 0.62s. S_Loss: 0.8772. T_Loss: 3.1118. Mask: 0.9742. :  28%|██▊       | 14/50 [00:09<00:25,  1.41it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.42s. Batch: 0.62s. S_Loss: 0.8772. T_Loss: 3.1118. Mask: 0.9742. :  30%|███       | 15/50 [00:09<00:20,  1.73it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8757. T_Loss: 3.0967. Mask: 0.9751. :  30%|███       | 15/50 [00:09<00:20,  1.73it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8757. T_Loss: 3.0967. Mask: 0.9751. :  32%|███▏      | 16/50 [00:09<00:16,  2.03it/s]total : 1000  current step :  964
total : 1000  current step :  965
total : 1000  current step :  966
Train Iter: 967/1000. LR: 0.0033. Data: 0.43s. Batch: 0.63s. S_Loss: 0.8766. T_Loss: 3.1067. Mask: 0.9752. :  32%|███▏      | 16/50 [00:10<00:16,  2.03it/s]Train Iter: 967/1000. LR: 0.0033. Data: 0.43s. Batch: 0.63s. S_Loss: 0.8766. T_Loss: 3.1067. Mask: 0.9752. :  34%|███▍      | 17/50 [00:10<00:21,  1.51it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.41s. Batch: 0.61s. S_Loss: 0.8790. T_Loss: 3.1222. Mask: 0.9750. :  34%|███▍      | 17/50 [00:10<00:21,  1.51it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.41s. Batch: 0.61s. S_Loss: 0.8790. T_Loss: 3.1222. Mask: 0.9750. :  36%|███▌      | 18/50 [00:10<00:17,  1.81it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8799. T_Loss: 3.1389. Mask: 0.9755. :  36%|███▌      | 18/50 [00:11<00:17,  1.81it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8799. T_Loss: 3.1389. Mask: 0.9755. :  38%|███▊      | 19/50 [00:11<00:14,  2.09it/s]total : 1000  current step :  967
total : 1000  current step :  968
total : 1000  current step :  969
Train Iter: 970/1000. LR: 0.0027. Data: 0.42s. Batch: 0.62s. S_Loss: 0.8800. T_Loss: 3.1329. Mask: 0.9750. :  38%|███▊      | 19/50 [00:12<00:14,  2.09it/s]Train Iter: 970/1000. LR: 0.0027. Data: 0.42s. Batch: 0.62s. S_Loss: 0.8800. T_Loss: 3.1329. Mask: 0.9750. :  40%|████      | 20/50 [00:12<00:20,  1.48it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8799. T_Loss: 3.1328. Mask: 0.9754. :  40%|████      | 20/50 [00:12<00:20,  1.48it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.40s. Batch: 0.61s. S_Loss: 0.8799. T_Loss: 3.1328. Mask: 0.9754. :  42%|████▏     | 21/50 [00:12<00:16,  1.74it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8795. T_Loss: 3.1381. Mask: 0.9751. :  42%|████▏     | 21/50 [00:13<00:16,  1.74it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8795. T_Loss: 3.1381. Mask: 0.9751. :  44%|████▍     | 22/50 [00:13<00:14,  1.96it/s]total : 1000  current step :  970
total : 1000  current step :  971
total : 1000  current step :  972
Train Iter: 973/1000. LR: 0.0022. Data: 0.41s. Batch: 0.61s. S_Loss: 0.8808. T_Loss: 3.1398. Mask: 0.9749. :  44%|████▍     | 22/50 [00:14<00:14,  1.96it/s]Train Iter: 973/1000. LR: 0.0022. Data: 0.41s. Batch: 0.61s. S_Loss: 0.8808. T_Loss: 3.1398. Mask: 0.9749. :  46%|████▌     | 23/50 [00:14<00:17,  1.50it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8806. T_Loss: 3.1373. Mask: 0.9746. :  46%|████▌     | 23/50 [00:14<00:17,  1.50it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8806. T_Loss: 3.1373. Mask: 0.9746. :  48%|████▊     | 24/50 [00:14<00:14,  1.79it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8821. T_Loss: 3.1488. Mask: 0.9738. :  48%|████▊     | 24/50 [00:14<00:14,  1.79it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8821. T_Loss: 3.1488. Mask: 0.9738. :  50%|█████     | 25/50 [00:14<00:11,  2.19it/s]total : 1000  current step :  973
total : 1000  current step :  974
total : 1000  current step :  975
Train Iter: 976/1000. LR: 0.0018. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8818. T_Loss: 3.1536. Mask: 0.9743. :  50%|█████     | 25/50 [00:15<00:11,  2.19it/s]Train Iter: 976/1000. LR: 0.0018. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8818. T_Loss: 3.1536. Mask: 0.9743. :  52%|█████▏    | 26/50 [00:15<00:15,  1.57it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8820. T_Loss: 3.1487. Mask: 0.9742. :  52%|█████▏    | 26/50 [00:16<00:15,  1.57it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8820. T_Loss: 3.1487. Mask: 0.9742. :  54%|█████▍    | 27/50 [00:16<00:12,  1.87it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8820. T_Loss: 3.1471. Mask: 0.9741. :  54%|█████▍    | 27/50 [00:16<00:12,  1.87it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8820. T_Loss: 3.1471. Mask: 0.9741. :  56%|█████▌    | 28/50 [00:16<00:10,  2.11it/s]total : 1000  current step :  976
total : 1000  current step :  977
total : 1000  current step :  978
Train Iter: 979/1000. LR: 0.0013. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8837. T_Loss: 3.1560. Mask: 0.9744. :  56%|█████▌    | 28/50 [00:17<00:10,  2.11it/s]Train Iter: 979/1000. LR: 0.0013. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8837. T_Loss: 3.1560. Mask: 0.9744. :  58%|█████▊    | 29/50 [00:17<00:13,  1.56it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8829. T_Loss: 3.1522. Mask: 0.9747. :  58%|█████▊    | 29/50 [00:17<00:13,  1.56it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8829. T_Loss: 3.1522. Mask: 0.9747. :  60%|██████    | 30/50 [00:17<00:10,  1.83it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8836. T_Loss: 3.1508. Mask: 0.9747. :  60%|██████    | 30/50 [00:18<00:10,  1.83it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8836. T_Loss: 3.1508. Mask: 0.9747. :  62%|██████▏   | 31/50 [00:18<00:09,  1.94it/s]total : 1000  current step :  979
total : 1000  current step :  980
total : 1000  current step :  981
Train Iter: 982/1000. LR: 0.0010. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8839. T_Loss: 3.1519. Mask: 0.9749. :  62%|██████▏   | 31/50 [00:19<00:09,  1.94it/s]Train Iter: 982/1000. LR: 0.0010. Data: 0.39s. Batch: 0.60s. S_Loss: 0.8839. T_Loss: 3.1519. Mask: 0.9749. :  64%|██████▍   | 32/50 [00:19<00:11,  1.51it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8832. T_Loss: 3.1467. Mask: 0.9750. :  64%|██████▍   | 32/50 [00:19<00:11,  1.51it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.38s. Batch: 0.59s. S_Loss: 0.8832. T_Loss: 3.1467. Mask: 0.9750. :  66%|██████▌   | 33/50 [00:19<00:09,  1.85it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8825. T_Loss: 3.1509. Mask: 0.9755. :  66%|██████▌   | 33/50 [00:19<00:09,  1.85it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8825. T_Loss: 3.1509. Mask: 0.9755. :  68%|██████▊   | 34/50 [00:19<00:07,  2.21it/s]total : 1000  current step :  982
total : 1000  current step :  983
total : 1000  current step :  984
Train Iter: 985/1000. LR: 0.0007. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8818. T_Loss: 3.1495. Mask: 0.9758. :  68%|██████▊   | 34/50 [00:20<00:07,  2.21it/s]Train Iter: 985/1000. LR: 0.0007. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8818. T_Loss: 3.1495. Mask: 0.9758. :  70%|███████   | 35/50 [00:20<00:09,  1.64it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8820. T_Loss: 3.1548. Mask: 0.9760. :  70%|███████   | 35/50 [00:20<00:09,  1.64it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8820. T_Loss: 3.1548. Mask: 0.9760. :  72%|███████▏  | 36/50 [00:20<00:07,  1.91it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8811. T_Loss: 3.1491. Mask: 0.9761. :  72%|███████▏  | 36/50 [00:21<00:07,  1.91it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8811. T_Loss: 3.1491. Mask: 0.9761. :  74%|███████▍  | 37/50 [00:21<00:06,  2.03it/s]total : 1000  current step :  985
total : 1000  current step :  986
total : 1000  current step :  987
Train Iter: 988/1000. LR: 0.0004. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8808. T_Loss: 3.1496. Mask: 0.9760. :  74%|███████▍  | 37/50 [00:22<00:06,  2.03it/s]Train Iter: 988/1000. LR: 0.0004. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8808. T_Loss: 3.1496. Mask: 0.9760. :  76%|███████▌  | 38/50 [00:22<00:08,  1.43it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8804. T_Loss: 3.1467. Mask: 0.9762. :  76%|███████▌  | 38/50 [00:22<00:08,  1.43it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8804. T_Loss: 3.1467. Mask: 0.9762. :  78%|███████▊  | 39/50 [00:22<00:06,  1.67it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8799. T_Loss: 3.1465. Mask: 0.9763. :  78%|███████▊  | 39/50 [00:23<00:06,  1.67it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.38s. Batch: 0.58s. S_Loss: 0.8799. T_Loss: 3.1465. Mask: 0.9763. :  80%|████████  | 40/50 [00:23<00:05,  1.91it/s]total : 1000  current step :  988
total : 1000  current step :  989
total : 1000  current step :  990
Train Iter: 991/1000. LR: 0.0002. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8793. T_Loss: 3.1454. Mask: 0.9760. :  80%|████████  | 40/50 [00:24<00:05,  1.91it/s]Train Iter: 991/1000. LR: 0.0002. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8793. T_Loss: 3.1454. Mask: 0.9760. :  82%|████████▏ | 41/50 [00:24<00:06,  1.41it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8791. T_Loss: 3.1465. Mask: 0.9762. :  82%|████████▏ | 41/50 [00:24<00:06,  1.41it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8791. T_Loss: 3.1465. Mask: 0.9762. :  84%|████████▍ | 42/50 [00:24<00:04,  1.64it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8790. T_Loss: 3.1485. Mask: 0.9763. :  84%|████████▍ | 42/50 [00:25<00:04,  1.64it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8790. T_Loss: 3.1485. Mask: 0.9763. :  86%|████████▌ | 43/50 [00:25<00:03,  1.77it/s]total : 1000  current step :  991
total : 1000  current step :  992
total : 1000  current step :  993
Train Iter: 994/1000. LR: 0.0001. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8789. T_Loss: 3.1436. Mask: 0.9765. :  86%|████████▌ | 43/50 [00:26<00:03,  1.77it/s]Train Iter: 994/1000. LR: 0.0001. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8789. T_Loss: 3.1436. Mask: 0.9765. :  88%|████████▊ | 44/50 [00:26<00:04,  1.39it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8787. T_Loss: 3.1372. Mask: 0.9761. :  88%|████████▊ | 44/50 [00:26<00:04,  1.39it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8787. T_Loss: 3.1372. Mask: 0.9761. :  90%|█████████ | 45/50 [00:26<00:03,  1.60it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8791. T_Loss: 3.1413. Mask: 0.9761. :  90%|█████████ | 45/50 [00:27<00:03,  1.60it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.39s. Batch: 0.59s. S_Loss: 0.8791. T_Loss: 3.1413. Mask: 0.9761. :  92%|█████████▏| 46/50 [00:27<00:02,  1.71it/s]total : 1000  current step :  994
total : 1000  current step :  995
total : 1000  current step :  996
Train Iter: 997/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 0.8789. T_Loss: 3.1360. Mask: 0.9761. :  92%|█████████▏| 46/50 [00:28<00:02,  1.71it/s]Train Iter: 997/1000. LR: 0.0000. Data: 0.41s. Batch: 0.60s. S_Loss: 0.8789. T_Loss: 3.1360. Mask: 0.9761. :  94%|█████████▍| 47/50 [00:28<00:02,  1.30it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8788. T_Loss: 3.1408. Mask: 0.9762. :  94%|█████████▍| 47/50 [00:28<00:02,  1.30it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.40s. Batch: 0.60s. S_Loss: 0.8788. T_Loss: 3.1408. Mask: 0.9762. :  96%|█████████▌| 48/50 [00:28<00:01,  1.58it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8787. T_Loss: 3.1367. Mask: 0.9764. :  96%|█████████▌| 48/50 [00:29<00:01,  1.58it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.40s. Batch: 0.59s. S_Loss: 0.8787. T_Loss: 3.1367. Mask: 0.9764. :  98%|█████████▊| 49/50 [00:29<00:00,  1.83it/s]total : 1000  current step :  997
total : 1000  current step :  998
total : 1000  current step :  999
Train Iter: 1000/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 0.8788. T_Loss: 3.1323. Mask: 0.9766. :  98%|█████████▊| 49/50 [00:30<00:00,  1.83it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 0.8788. T_Loss: 3.1323. Mask: 0.9766. : 100%|██████████| 50/50 [00:30<00:00,  1.16it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.42s. Batch: 0.61s. S_Loss: 0.8788. T_Loss: 3.1323. Mask: 0.9766. : 100%|██████████| 50/50 [00:30<00:00,  1.63it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 1.2741. top1: 71.09. top5: 97.66. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 1.2741. top1: 71.09. top5: 97.66. :  12%|█▎        | 1/8 [00:00<00:05,  1.21it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.2780. top1: 71.48. top5: 97.85. :  12%|█▎        | 1/8 [00:01<00:05,  1.21it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.2780. top1: 71.48. top5: 97.85. :  25%|██▌       | 2/8 [00:01<00:03,  1.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.2622. top1: 71.88. top5: 97.92. :  25%|██▌       | 2/8 [00:01<00:03,  1.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.2622. top1: 71.88. top5: 97.92. :  38%|███▊      | 3/8 [00:01<00:02,  2.25it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.2242. top1: 73.63. top5: 98.05. :  38%|███▊      | 3/8 [00:01<00:02,  2.25it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.2242. top1: 73.63. top5: 98.05. :  50%|█████     | 4/8 [00:01<00:01,  2.57it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.1319. top1: 78.59. top5: 98.44. :  50%|█████     | 4/8 [00:02<00:01,  2.57it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.1319. top1: 78.59. top5: 98.44. :  62%|██████▎   | 5/8 [00:02<00:01,  2.57it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0720. top1: 81.90. top5: 98.70. :  62%|██████▎   | 5/8 [00:02<00:01,  2.57it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0720. top1: 81.90. top5: 98.70. :  75%|███████▌  | 6/8 [00:02<00:00,  2.69it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0256. top1: 84.43. top5: 98.88. :  75%|███████▌  | 6/8 [00:02<00:00,  2.69it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0256. top1: 84.43. top5: 98.88. :  88%|████████▊ | 7/8 [00:02<00:00,  2.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9977. top1: 85.90. top5: 99.00. :  88%|████████▊ | 7/8 [00:03<00:00,  2.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9977. top1: 85.90. top5: 99.00. : 100%|██████████| 8/8 [00:03<00:00,  2.97it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9977. top1: 85.90. top5: 99.00. : 100%|██████████| 8/8 [00:03<00:00,  2.38it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.83s. Batch: 0.89s. Loss: 0.9000. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.83s. Batch: 0.89s. Loss: 0.9000. :  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Finetune Epoch:  1/70. Data: 0.99s. Batch: 1.06s. Loss: 0.9138. :  33%|███▎      | 1/3 [00:01<00:01,  1.12it/s]Finetune Epoch:  1/70. Data: 0.99s. Batch: 1.06s. Loss: 0.9138. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch:  1/70. Data: 1.16s. Batch: 1.22s. Loss: 0.9285. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch:  1/70. Data: 1.16s. Batch: 1.22s. Loss: 0.9285. : 100%|██████████| 3/3 [00:01<00:00,  2.19it/s]Finetune Epoch:  1/70. Data: 1.16s. Batch: 1.22s. Loss: 0.9285. : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.1186. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.1186. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.1082. top1: 82.03. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.1082. top1: 82.03. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0989. top1: 83.33. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0989. top1: 83.33. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0823. top1: 84.28. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0823. top1: 84.28. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0467. top1: 85.94. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.97it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0467. top1: 85.94. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.89it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0227. top1: 87.24. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.89it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0227. top1: 87.24. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0009. top1: 88.45. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0009. top1: 88.45. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9888. top1: 89.05. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9888. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9888. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9347. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9347. :  33%|███▎      | 1/3 [00:00<00:01,  1.14it/s]Finetune Epoch:  2/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9366. :  33%|███▎      | 1/3 [00:01<00:01,  1.14it/s]Finetune Epoch:  2/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9366. :  67%|██████▋   | 2/3 [00:01<00:00,  1.69it/s]Finetune Epoch:  2/70. Data: 1.23s. Batch: 1.29s. Loss: 0.9308. :  67%|██████▋   | 2/3 [00:01<00:00,  1.69it/s]Finetune Epoch:  2/70. Data: 1.23s. Batch: 1.29s. Loss: 0.9308. : 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]Finetune Epoch:  2/70. Data: 1.23s. Batch: 1.29s. Loss: 0.9308. : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 1.1172. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 1.1172. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.47it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.1068. top1: 82.03. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.47it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.1068. top1: 82.03. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0975. top1: 83.33. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0975. top1: 83.33. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0810. top1: 84.28. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0810. top1: 84.28. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.59it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0456. top1: 85.94. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.59it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0456. top1: 85.94. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0218. top1: 87.24. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0218. top1: 87.24. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0000. top1: 88.45. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0000. top1: 88.45. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9879. top1: 89.05. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9879. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9879. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.55it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.72s. Batch: 0.78s. Loss: 0.9368. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.72s. Batch: 0.78s. Loss: 0.9368. :  33%|███▎      | 1/3 [00:00<00:01,  1.28it/s]Finetune Epoch:  3/70. Data: 0.92s. Batch: 0.98s. Loss: 0.9235. :  33%|███▎      | 1/3 [00:01<00:01,  1.28it/s]Finetune Epoch:  3/70. Data: 0.92s. Batch: 0.98s. Loss: 0.9235. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch:  3/70. Data: 1.11s. Batch: 1.18s. Loss: 0.9310. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch:  3/70. Data: 1.11s. Batch: 1.18s. Loss: 0.9310. : 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]Finetune Epoch:  3/70. Data: 1.11s. Batch: 1.18s. Loss: 0.9310. : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.1158. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.1158. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.38it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.1053. top1: 82.03. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:05,  1.38it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.1053. top1: 82.03. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0960. top1: 83.33. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0960. top1: 83.33. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0796. top1: 84.28. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0796. top1: 84.28. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.17it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0444. top1: 85.94. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.17it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0444. top1: 85.94. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0208. top1: 87.24. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0208. top1: 87.24. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9991. top1: 88.45. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9991. top1: 88.45. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9871. top1: 89.05. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9871. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9871. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.71s. Batch: 0.78s. Loss: 0.9628. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.71s. Batch: 0.78s. Loss: 0.9628. :  33%|███▎      | 1/3 [00:00<00:01,  1.28it/s]Finetune Epoch:  4/70. Data: 0.87s. Batch: 0.95s. Loss: 0.9368. :  33%|███▎      | 1/3 [00:01<00:01,  1.28it/s]Finetune Epoch:  4/70. Data: 0.87s. Batch: 0.95s. Loss: 0.9368. :  67%|██████▋   | 2/3 [00:01<00:00,  1.93it/s]Finetune Epoch:  4/70. Data: 1.03s. Batch: 1.10s. Loss: 0.9257. :  67%|██████▋   | 2/3 [00:01<00:00,  1.93it/s]Finetune Epoch:  4/70. Data: 1.03s. Batch: 1.10s. Loss: 0.9257. : 100%|██████████| 3/3 [00:01<00:00,  2.43it/s]Finetune Epoch:  4/70. Data: 1.03s. Batch: 1.10s. Loss: 0.9257. : 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.1141. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.1141. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.1036. top1: 82.03. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.48it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.1036. top1: 82.03. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.09it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0944. top1: 83.33. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.09it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0944. top1: 83.33. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0780. top1: 84.28. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0780. top1: 84.28. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.86it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0432. top1: 85.94. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.86it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0432. top1: 85.94. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0197. top1: 87.24. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.85it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0197. top1: 87.24. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9981. top1: 88.45. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9981. top1: 88.45. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9862. top1: 89.05. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9862. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.67it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9862. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.74s. Batch: 0.81s. Loss: 0.9167. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.74s. Batch: 0.81s. Loss: 0.9167. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch:  5/70. Data: 0.93s. Batch: 1.00s. Loss: 0.9484. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch:  5/70. Data: 0.93s. Batch: 1.00s. Loss: 0.9484. :  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s]Finetune Epoch:  5/70. Data: 1.12s. Batch: 1.18s. Loss: 0.9415. :  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s]Finetune Epoch:  5/70. Data: 1.12s. Batch: 1.18s. Loss: 0.9415. : 100%|██████████| 3/3 [00:01<00:00,  2.17it/s]Finetune Epoch:  5/70. Data: 1.12s. Batch: 1.18s. Loss: 0.9415. : 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 1.1126. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 1.1126. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.36it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.1020. top1: 82.03. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.36it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.1020. top1: 82.03. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0928. top1: 83.33. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0928. top1: 83.33. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0765. top1: 84.28. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0765. top1: 84.28. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0419. top1: 85.94. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0419. top1: 85.94. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0186. top1: 87.24. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0186. top1: 87.24. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.33it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9972. top1: 88.45. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.33it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9972. top1: 88.45. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9853. top1: 89.05. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9853. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9853. top1: 89.05. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.77s. Batch: 0.84s. Loss: 0.9307. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.77s. Batch: 0.84s. Loss: 0.9307. :  33%|███▎      | 1/3 [00:00<00:01,  1.19it/s]Finetune Epoch:  6/70. Data: 0.92s. Batch: 0.98s. Loss: 0.9496. :  33%|███▎      | 1/3 [00:01<00:01,  1.19it/s]Finetune Epoch:  6/70. Data: 0.92s. Batch: 0.98s. Loss: 0.9496. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch:  6/70. Data: 1.08s. Batch: 1.13s. Loss: 0.9337. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch:  6/70. Data: 1.08s. Batch: 1.13s. Loss: 0.9337. : 100%|██████████| 3/3 [00:01<00:00,  2.35it/s]Finetune Epoch:  6/70. Data: 1.08s. Batch: 1.13s. Loss: 0.9337. : 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.1110. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.1110. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.1005. top1: 82.23. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.1005. top1: 82.23. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0912. top1: 83.46. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0912. top1: 83.46. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0750. top1: 84.47. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0750. top1: 84.47. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0407. top1: 86.09. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0407. top1: 86.09. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0176. top1: 87.37. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.80it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0176. top1: 87.37. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9963. top1: 88.56. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9963. top1: 88.56. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9844. top1: 89.15. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9844. top1: 89.15. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9844. top1: 89.15. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.73s. Batch: 0.79s. Loss: 0.9205. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.73s. Batch: 0.79s. Loss: 0.9205. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch:  7/70. Data: 0.92s. Batch: 0.97s. Loss: 0.9237. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch:  7/70. Data: 0.92s. Batch: 0.97s. Loss: 0.9237. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch:  7/70. Data: 1.11s. Batch: 1.15s. Loss: 0.9257. :  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s]Finetune Epoch:  7/70. Data: 1.11s. Batch: 1.15s. Loss: 0.9257. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]Finetune Epoch:  7/70. Data: 1.11s. Batch: 1.15s. Loss: 0.9257. : 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.1095. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.1095. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.55it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0989. top1: 82.23. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.55it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0989. top1: 82.23. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0896. top1: 83.46. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0896. top1: 83.46. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0735. top1: 84.47. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0735. top1: 84.47. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0395. top1: 86.09. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0395. top1: 86.09. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0166. top1: 87.37. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0166. top1: 87.37. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9953. top1: 88.56. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9953. top1: 88.56. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9836. top1: 89.15. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9836. top1: 89.15. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9836. top1: 89.15. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.92s. Batch: 0.99s. Loss: 0.9038. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.92s. Batch: 0.99s. Loss: 0.9038. :  33%|███▎      | 1/3 [00:00<00:01,  1.01it/s]Finetune Epoch:  8/70. Data: 1.10s. Batch: 1.17s. Loss: 0.9431. :  33%|███▎      | 1/3 [00:01<00:01,  1.01it/s]Finetune Epoch:  8/70. Data: 1.10s. Batch: 1.17s. Loss: 0.9431. :  67%|██████▋   | 2/3 [00:01<00:00,  1.60it/s]Finetune Epoch:  8/70. Data: 1.27s. Batch: 1.35s. Loss: 0.9299. :  67%|██████▋   | 2/3 [00:01<00:00,  1.60it/s]Finetune Epoch:  8/70. Data: 1.27s. Batch: 1.35s. Loss: 0.9299. : 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]Finetune Epoch:  8/70. Data: 1.27s. Batch: 1.35s. Loss: 0.9299. : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.1079. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.1079. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0972. top1: 82.23. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.44it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0972. top1: 82.23. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0880. top1: 83.46. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.97it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0880. top1: 83.46. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0720. top1: 84.47. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0720. top1: 84.47. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0383. top1: 86.09. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0383. top1: 86.09. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0155. top1: 87.37. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.91it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0155. top1: 87.37. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9944. top1: 88.56. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9944. top1: 88.56. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9827. top1: 89.15. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  2.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9827. top1: 89.15. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9827. top1: 89.15. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.52it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.63s. Batch: 0.70s. Loss: 0.8893. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.63s. Batch: 0.70s. Loss: 0.8893. :  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]Finetune Epoch:  9/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9155. :  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]Finetune Epoch:  9/70. Data: 0.78s. Batch: 0.84s. Loss: 0.9155. :  67%|██████▋   | 2/3 [00:00<00:00,  2.21it/s]Finetune Epoch:  9/70. Data: 0.93s. Batch: 0.99s. Loss: 0.9247. :  67%|██████▋   | 2/3 [00:01<00:00,  2.21it/s]Finetune Epoch:  9/70. Data: 0.93s. Batch: 0.99s. Loss: 0.9247. : 100%|██████████| 3/3 [00:01<00:00,  2.57it/s]Finetune Epoch:  9/70. Data: 0.93s. Batch: 0.99s. Loss: 0.9247. : 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.1063. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.1063. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0956. top1: 82.23. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0956. top1: 82.23. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0864. top1: 83.46. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0864. top1: 83.46. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0705. top1: 84.47. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0705. top1: 84.47. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0371. top1: 86.09. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0371. top1: 86.09. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0145. top1: 87.37. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0145. top1: 87.37. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9935. top1: 88.56. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9935. top1: 88.56. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9818. top1: 89.15. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9818. top1: 89.15. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9818. top1: 89.15. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.60it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.73s. Batch: 0.81s. Loss: 0.9084. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.73s. Batch: 0.81s. Loss: 0.9084. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch: 10/70. Data: 0.92s. Batch: 0.98s. Loss: 0.9081. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch: 10/70. Data: 0.92s. Batch: 0.98s. Loss: 0.9081. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 10/70. Data: 1.12s. Batch: 1.18s. Loss: 0.9268. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 10/70. Data: 1.12s. Batch: 1.18s. Loss: 0.9268. : 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 10/70. Data: 1.12s. Batch: 1.18s. Loss: 0.9268. : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 1.1049. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 1.1049. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.31it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0942. top1: 82.42. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.31it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0942. top1: 82.42. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0850. top1: 83.59. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0850. top1: 83.59. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0691. top1: 84.57. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0691. top1: 84.57. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0359. top1: 86.17. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0359. top1: 86.17. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0135. top1: 87.43. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0135. top1: 87.43. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9926. top1: 88.62. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9926. top1: 88.62. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9810. top1: 89.25. top5: 99.80. :  88%|████████▊ | 7/8 [00:03<00:00,  2.48it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9810. top1: 89.25. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.77it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9810. top1: 89.25. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.36it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.83s. Batch: 0.90s. Loss: 0.9283. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.83s. Batch: 0.90s. Loss: 0.9283. :  33%|███▎      | 1/3 [00:00<00:01,  1.11it/s]Finetune Epoch: 11/70. Data: 1.00s. Batch: 1.07s. Loss: 0.9325. :  33%|███▎      | 1/3 [00:01<00:01,  1.11it/s]Finetune Epoch: 11/70. Data: 1.00s. Batch: 1.07s. Loss: 0.9325. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 11/70. Data: 1.19s. Batch: 1.25s. Loss: 0.9342. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 11/70. Data: 1.19s. Batch: 1.25s. Loss: 0.9342. : 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 11/70. Data: 1.19s. Batch: 1.25s. Loss: 0.9342. : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 1.1033. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 1.1033. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.0925. top1: 82.42. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.34it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.0925. top1: 82.42. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0833. top1: 83.72. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0833. top1: 83.72. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.42it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0676. top1: 84.67. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.42it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0676. top1: 84.67. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0347. top1: 86.25. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0347. top1: 86.25. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0124. top1: 87.50. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0124. top1: 87.50. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9916. top1: 88.67. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9916. top1: 88.67. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9801. top1: 89.30. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9801. top1: 89.30. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9801. top1: 89.30. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.51it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.88s. Batch: 0.96s. Loss: 0.9103. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.88s. Batch: 0.96s. Loss: 0.9103. :  33%|███▎      | 1/3 [00:00<00:01,  1.04it/s]Finetune Epoch: 12/70. Data: 1.07s. Batch: 1.14s. Loss: 0.9280. :  33%|███▎      | 1/3 [00:01<00:01,  1.04it/s]Finetune Epoch: 12/70. Data: 1.07s. Batch: 1.14s. Loss: 0.9280. :  67%|██████▋   | 2/3 [00:01<00:00,  1.66it/s]Finetune Epoch: 12/70. Data: 1.26s. Batch: 1.33s. Loss: 0.9299. :  67%|██████▋   | 2/3 [00:01<00:00,  1.66it/s]Finetune Epoch: 12/70. Data: 1.26s. Batch: 1.33s. Loss: 0.9299. : 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]Finetune Epoch: 12/70. Data: 1.26s. Batch: 1.33s. Loss: 0.9299. : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 1.1019. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 1.1019. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.33it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0911. top1: 82.62. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.33it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0911. top1: 82.62. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.05it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0819. top1: 83.85. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.05it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0819. top1: 83.85. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.47it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0663. top1: 84.77. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.47it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0663. top1: 84.77. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.68it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0336. top1: 86.25. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.68it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0336. top1: 86.25. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.92it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0115. top1: 87.50. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.92it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0115. top1: 87.50. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9908. top1: 88.67. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9908. top1: 88.67. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9793. top1: 89.30. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9793. top1: 89.30. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9793. top1: 89.30. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.66it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.75s. Batch: 0.83s. Loss: 0.9072. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.75s. Batch: 0.83s. Loss: 0.9072. :  33%|███▎      | 1/3 [00:00<00:01,  1.21it/s]Finetune Epoch: 13/70. Data: 0.91s. Batch: 0.98s. Loss: 0.9190. :  33%|███▎      | 1/3 [00:01<00:01,  1.21it/s]Finetune Epoch: 13/70. Data: 0.91s. Batch: 0.98s. Loss: 0.9190. :  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s]Finetune Epoch: 13/70. Data: 1.10s. Batch: 1.17s. Loss: 0.9236. :  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s]Finetune Epoch: 13/70. Data: 1.10s. Batch: 1.17s. Loss: 0.9236. : 100%|██████████| 3/3 [00:01<00:00,  2.13it/s]Finetune Epoch: 13/70. Data: 1.10s. Batch: 1.17s. Loss: 0.9236. : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.86s. Loss: 1.1005. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.86s. Loss: 1.1005. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:06,  1.16it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0896. top1: 82.81. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:06,  1.16it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0896. top1: 82.81. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0805. top1: 83.98. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0805. top1: 83.98. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.32it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0649. top1: 84.86. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.32it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0649. top1: 84.86. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0325. top1: 86.33. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.51it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0325. top1: 86.33. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0105. top1: 87.57. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0105. top1: 87.57. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9899. top1: 88.73. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9899. top1: 88.73. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.81it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9785. top1: 89.40. top5: 99.80. :  88%|████████▊ | 7/8 [00:03<00:00,  2.81it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9785. top1: 89.40. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9785. top1: 89.40. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.40it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.71s. Batch: 0.77s. Loss: 0.9202. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.71s. Batch: 0.77s. Loss: 0.9202. :  33%|███▎      | 1/3 [00:00<00:01,  1.30it/s]Finetune Epoch: 14/70. Data: 0.91s. Batch: 0.96s. Loss: 0.9369. :  33%|███▎      | 1/3 [00:01<00:01,  1.30it/s]Finetune Epoch: 14/70. Data: 0.91s. Batch: 0.96s. Loss: 0.9369. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 14/70. Data: 1.16s. Batch: 1.21s. Loss: 0.9310. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 14/70. Data: 1.16s. Batch: 1.21s. Loss: 0.9310. : 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]Finetune Epoch: 14/70. Data: 1.16s. Batch: 1.21s. Loss: 0.9310. : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 1.0988. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 1.0988. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0879. top1: 82.81. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.37it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0879. top1: 82.81. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0788. top1: 83.98. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.03it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0788. top1: 83.98. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0633. top1: 84.86. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0633. top1: 84.86. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.66it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0312. top1: 86.25. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.66it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0312. top1: 86.25. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.83it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0094. top1: 87.50. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.83it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0094. top1: 87.50. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.80it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9889. top1: 88.67. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.80it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9889. top1: 88.67. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9776. top1: 89.35. top5: 99.80. :  88%|████████▊ | 7/8 [00:03<00:00,  2.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9776. top1: 89.35. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.94it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9776. top1: 89.35. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.46it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.77s. Batch: 0.85s. Loss: 0.9351. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.77s. Batch: 0.85s. Loss: 0.9351. :  33%|███▎      | 1/3 [00:00<00:01,  1.18it/s]Finetune Epoch: 15/70. Data: 0.93s. Batch: 0.99s. Loss: 0.9284. :  33%|███▎      | 1/3 [00:01<00:01,  1.18it/s]Finetune Epoch: 15/70. Data: 0.93s. Batch: 0.99s. Loss: 0.9284. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 15/70. Data: 1.10s. Batch: 1.16s. Loss: 0.9194. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 15/70. Data: 1.10s. Batch: 1.16s. Loss: 0.9194. : 100%|██████████| 3/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 15/70. Data: 1.10s. Batch: 1.16s. Loss: 0.9194. : 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 1.0974. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 1.0974. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0864. top1: 82.81. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0864. top1: 82.81. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0773. top1: 83.98. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0773. top1: 83.98. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.37it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0619. top1: 84.86. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.37it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0619. top1: 84.86. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0301. top1: 86.25. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0301. top1: 86.25. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0084. top1: 87.50. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0084. top1: 87.50. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9880. top1: 88.67. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9880. top1: 88.67. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9767. top1: 89.35. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9767. top1: 89.35. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9767. top1: 89.35. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.56it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9344. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9344. :  33%|███▎      | 1/3 [00:00<00:01,  1.06it/s]Finetune Epoch: 16/70. Data: 1.09s. Batch: 1.15s. Loss: 0.9384. :  33%|███▎      | 1/3 [00:01<00:01,  1.06it/s]Finetune Epoch: 16/70. Data: 1.09s. Batch: 1.15s. Loss: 0.9384. :  67%|██████▋   | 2/3 [00:01<00:00,  1.60it/s]Finetune Epoch: 16/70. Data: 1.26s. Batch: 1.32s. Loss: 0.9306. :  67%|██████▋   | 2/3 [00:01<00:00,  1.60it/s]Finetune Epoch: 16/70. Data: 1.26s. Batch: 1.32s. Loss: 0.9306. : 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 16/70. Data: 1.26s. Batch: 1.32s. Loss: 0.9306. : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 1.0961. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 1.0961. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.42it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0851. top1: 82.81. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.42it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0851. top1: 82.81. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0760. top1: 84.11. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0760. top1: 84.11. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0607. top1: 85.06. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0607. top1: 85.06. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0290. top1: 86.41. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0290. top1: 86.41. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0075. top1: 87.63. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0075. top1: 87.63. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9871. top1: 88.84. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9871. top1: 88.84. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9759. top1: 89.50. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9759. top1: 89.50. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9759. top1: 89.50. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.73it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.63s. Batch: 0.70s. Loss: 0.9296. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.63s. Batch: 0.70s. Loss: 0.9296. :  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]Finetune Epoch: 17/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9230. :  33%|███▎      | 1/3 [00:01<00:01,  1.43it/s]Finetune Epoch: 17/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9230. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 17/70. Data: 1.05s. Batch: 1.10s. Loss: 0.9268. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 17/70. Data: 1.05s. Batch: 1.10s. Loss: 0.9268. : 100%|██████████| 3/3 [00:01<00:00,  2.14it/s]Finetune Epoch: 17/70. Data: 1.05s. Batch: 1.10s. Loss: 0.9268. : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.0946. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.0946. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0836. top1: 82.81. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0836. top1: 82.81. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.35it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0745. top1: 84.11. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.35it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0745. top1: 84.11. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0593. top1: 85.06. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0593. top1: 85.06. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0279. top1: 86.41. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0279. top1: 86.41. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0065. top1: 87.63. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0065. top1: 87.63. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.32it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9862. top1: 88.84. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.32it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9862. top1: 88.84. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9751. top1: 89.50. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9751. top1: 89.50. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9751. top1: 89.50. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.64s. Batch: 0.70s. Loss: 0.9420. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.64s. Batch: 0.70s. Loss: 0.9420. :  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]Finetune Epoch: 18/70. Data: 0.86s. Batch: 0.91s. Loss: 0.9230. :  33%|███▎      | 1/3 [00:01<00:01,  1.43it/s]Finetune Epoch: 18/70. Data: 0.86s. Batch: 0.91s. Loss: 0.9230. :  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s]Finetune Epoch: 18/70. Data: 1.06s. Batch: 1.12s. Loss: 0.9226. :  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s]Finetune Epoch: 18/70. Data: 1.06s. Batch: 1.12s. Loss: 0.9226. : 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 18/70. Data: 1.06s. Batch: 1.12s. Loss: 0.9226. : 100%|██████████| 3/3 [00:01<00:00,  1.72it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 1.0934. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 1.0934. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0823. top1: 83.01. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0823. top1: 83.01. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0732. top1: 84.24. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0732. top1: 84.24. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0580. top1: 85.16. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0580. top1: 85.16. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0268. top1: 86.48. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0268. top1: 86.48. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0056. top1: 87.70. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.91it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0056. top1: 87.70. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9854. top1: 88.90. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9854. top1: 88.90. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9743. top1: 89.55. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9743. top1: 89.55. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9743. top1: 89.55. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.70s. Batch: 0.76s. Loss: 0.9329. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.70s. Batch: 0.76s. Loss: 0.9329. :  33%|███▎      | 1/3 [00:00<00:01,  1.32it/s]Finetune Epoch: 19/70. Data: 0.85s. Batch: 0.90s. Loss: 0.9272. :  33%|███▎      | 1/3 [00:01<00:01,  1.32it/s]Finetune Epoch: 19/70. Data: 0.85s. Batch: 0.90s. Loss: 0.9272. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 19/70. Data: 1.00s. Batch: 1.05s. Loss: 0.9298. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 19/70. Data: 1.00s. Batch: 1.05s. Loss: 0.9298. : 100%|██████████| 3/3 [00:01<00:00,  2.54it/s]Finetune Epoch: 19/70. Data: 1.00s. Batch: 1.05s. Loss: 0.9298. : 100%|██████████| 3/3 [00:01<00:00,  1.95it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.0920. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.0920. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0809. top1: 83.01. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0809. top1: 83.01. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0718. top1: 84.38. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.34it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0718. top1: 84.38. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.96it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0568. top1: 85.25. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.96it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0568. top1: 85.25. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.31it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0257. top1: 86.56. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.31it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0257. top1: 86.56. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0047. top1: 87.76. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0047. top1: 87.76. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9845. top1: 88.95. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.28it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9845. top1: 88.95. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9734. top1: 89.60. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9734. top1: 89.60. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9734. top1: 89.60. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.97it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.68s. Batch: 0.75s. Loss: 0.9010. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.68s. Batch: 0.75s. Loss: 0.9010. :  33%|███▎      | 1/3 [00:00<00:01,  1.34it/s]Finetune Epoch: 20/70. Data: 0.87s. Batch: 0.93s. Loss: 0.9105. :  33%|███▎      | 1/3 [00:01<00:01,  1.34it/s]Finetune Epoch: 20/70. Data: 0.87s. Batch: 0.93s. Loss: 0.9105. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 20/70. Data: 1.05s. Batch: 1.11s. Loss: 0.9296. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 20/70. Data: 1.05s. Batch: 1.11s. Loss: 0.9296. : 100%|██████████| 3/3 [00:01<00:00,  2.23it/s]Finetune Epoch: 20/70. Data: 1.05s. Batch: 1.11s. Loss: 0.9296. : 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 1.0908. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 1.0908. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0796. top1: 83.20. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0796. top1: 83.20. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0706. top1: 84.51. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0706. top1: 84.51. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0556. top1: 85.35. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0556. top1: 85.35. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0247. top1: 86.64. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0247. top1: 86.64. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.35it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0038. top1: 87.83. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.35it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0038. top1: 87.83. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9837. top1: 89.01. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9837. top1: 89.01. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9727. top1: 89.65. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9727. top1: 89.65. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9727. top1: 89.65. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.71s. Batch: 0.81s. Loss: 0.9187. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.71s. Batch: 0.81s. Loss: 0.9187. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch: 21/70. Data: 0.87s. Batch: 0.95s. Loss: 0.9212. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch: 21/70. Data: 0.87s. Batch: 0.95s. Loss: 0.9212. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 21/70. Data: 1.02s. Batch: 1.09s. Loss: 0.9236. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 21/70. Data: 1.02s. Batch: 1.09s. Loss: 0.9236. : 100%|██████████| 3/3 [00:01<00:00,  2.52it/s]Finetune Epoch: 21/70. Data: 1.02s. Batch: 1.09s. Loss: 0.9236. : 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.0895. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 1.0895. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0782. top1: 83.20. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0782. top1: 83.20. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0692. top1: 84.51. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0692. top1: 84.51. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.83it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0543. top1: 85.35. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.83it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0543. top1: 85.35. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.36it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0237. top1: 86.64. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.36it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0237. top1: 86.64. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.55it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0029. top1: 87.83. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.55it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0029. top1: 87.83. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.38it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9829. top1: 89.01. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.38it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9829. top1: 89.01. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9719. top1: 89.65. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9719. top1: 89.65. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9719. top1: 89.65. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.55s. Batch: 0.61s. Loss: 0.9382. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.55s. Batch: 0.61s. Loss: 0.9382. :  33%|███▎      | 1/3 [00:00<00:01,  1.63it/s]Finetune Epoch: 22/70. Data: 0.71s. Batch: 0.77s. Loss: 0.9163. :  33%|███▎      | 1/3 [00:00<00:01,  1.63it/s]Finetune Epoch: 22/70. Data: 0.71s. Batch: 0.77s. Loss: 0.9163. :  67%|██████▋   | 2/3 [00:00<00:00,  2.31it/s]Finetune Epoch: 22/70. Data: 0.87s. Batch: 0.92s. Loss: 0.9291. :  67%|██████▋   | 2/3 [00:01<00:00,  2.31it/s]Finetune Epoch: 22/70. Data: 0.87s. Batch: 0.92s. Loss: 0.9291. : 100%|██████████| 3/3 [00:01<00:00,  2.63it/s]Finetune Epoch: 22/70. Data: 0.87s. Batch: 0.92s. Loss: 0.9291. : 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.0881. top1: 82.42. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.0881. top1: 82.42. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0769. top1: 83.20. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0769. top1: 83.20. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.62it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0679. top1: 84.64. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.62it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0679. top1: 84.64. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0531. top1: 85.45. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0531. top1: 85.45. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.24it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0227. top1: 86.72. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.24it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0227. top1: 86.72. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.33it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0020. top1: 87.89. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.33it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0020. top1: 87.89. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9821. top1: 89.06. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9821. top1: 89.06. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9712. top1: 89.70. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9712. top1: 89.70. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.89it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9712. top1: 89.70. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.62s. Batch: 0.69s. Loss: 0.9157. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.62s. Batch: 0.69s. Loss: 0.9157. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch: 23/70. Data: 0.76s. Batch: 0.83s. Loss: 0.9316. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch: 23/70. Data: 0.76s. Batch: 0.83s. Loss: 0.9316. :  67%|██████▋   | 2/3 [00:00<00:00,  2.24it/s]Finetune Epoch: 23/70. Data: 0.91s. Batch: 0.97s. Loss: 0.9285. :  67%|██████▋   | 2/3 [00:01<00:00,  2.24it/s]Finetune Epoch: 23/70. Data: 0.91s. Batch: 0.97s. Loss: 0.9285. : 100%|██████████| 3/3 [00:01<00:00,  2.67it/s]Finetune Epoch: 23/70. Data: 0.91s. Batch: 0.97s. Loss: 0.9285. : 100%|██████████| 3/3 [00:01<00:00,  2.14it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0870. top1: 82.81. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0870. top1: 82.81. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0757. top1: 83.59. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0757. top1: 83.59. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.56it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0667. top1: 84.90. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.56it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0667. top1: 84.90. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0519. top1: 85.64. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  3.19it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0519. top1: 85.64. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.46it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0217. top1: 86.88. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.46it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0217. top1: 86.88. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.67it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0012. top1: 88.02. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.67it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0012. top1: 88.02. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.50it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9814. top1: 89.17. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.50it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9814. top1: 89.17. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9705. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9705. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9705. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.56s. Batch: 0.61s. Loss: 0.9363. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.56s. Batch: 0.61s. Loss: 0.9363. :  33%|███▎      | 1/3 [00:00<00:01,  1.63it/s]Finetune Epoch: 24/70. Data: 0.72s. Batch: 0.79s. Loss: 0.9212. :  33%|███▎      | 1/3 [00:00<00:01,  1.63it/s]Finetune Epoch: 24/70. Data: 0.72s. Batch: 0.79s. Loss: 0.9212. :  67%|██████▋   | 2/3 [00:00<00:00,  2.20it/s]Finetune Epoch: 24/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9215. :  67%|██████▋   | 2/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 24/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9215. : 100%|██████████| 3/3 [00:01<00:00,  2.53it/s]Finetune Epoch: 24/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9215. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0856. top1: 82.81. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0856. top1: 82.81. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.84it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0743. top1: 83.59. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.84it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0743. top1: 83.59. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0654. top1: 84.90. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0654. top1: 84.90. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0506. top1: 85.64. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0506. top1: 85.64. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.20it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0207. top1: 86.88. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.20it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0207. top1: 86.88. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.29it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0003. top1: 88.02. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.29it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0003. top1: 88.02. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9805. top1: 89.17. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9805. top1: 89.17. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9697. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9697. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.81it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9697. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.66s. Batch: 0.73s. Loss: 0.9502. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.66s. Batch: 0.73s. Loss: 0.9502. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 25/70. Data: 0.84s. Batch: 0.89s. Loss: 0.9295. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 25/70. Data: 0.84s. Batch: 0.89s. Loss: 0.9295. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch: 25/70. Data: 1.02s. Batch: 1.07s. Loss: 0.9231. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch: 25/70. Data: 1.02s. Batch: 1.07s. Loss: 0.9231. : 100%|██████████| 3/3 [00:01<00:00,  2.28it/s]Finetune Epoch: 25/70. Data: 1.02s. Batch: 1.07s. Loss: 0.9231. : 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 1.0843. top1: 82.81. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 1.0843. top1: 82.81. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.70it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0729. top1: 83.59. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0729. top1: 83.59. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0640. top1: 84.90. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0640. top1: 84.90. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.89it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0493. top1: 85.64. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.89it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0493. top1: 85.64. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0196. top1: 86.88. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0196. top1: 86.88. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9994. top1: 88.02. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9994. top1: 88.02. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.61it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9797. top1: 89.17. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.61it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9797. top1: 89.17. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.64it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9689. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.64it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9689. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9689. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.69s. Batch: 0.75s. Loss: 0.9519. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.69s. Batch: 0.75s. Loss: 0.9519. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch: 26/70. Data: 0.85s. Batch: 0.91s. Loss: 0.9346. :  33%|███▎      | 1/3 [00:01<00:01,  1.33it/s]Finetune Epoch: 26/70. Data: 0.85s. Batch: 0.91s. Loss: 0.9346. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 26/70. Data: 1.00s. Batch: 1.06s. Loss: 0.9181. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 26/70. Data: 1.00s. Batch: 1.06s. Loss: 0.9181. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]Finetune Epoch: 26/70. Data: 1.00s. Batch: 1.06s. Loss: 0.9181. : 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 1.0830. top1: 82.81. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 1.0830. top1: 82.81. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0716. top1: 83.79. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0716. top1: 83.79. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0627. top1: 85.03. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0627. top1: 85.03. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.86it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0481. top1: 85.74. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.86it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0481. top1: 85.74. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.10it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0186. top1: 86.95. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.10it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0186. top1: 86.95. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9985. top1: 88.09. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9985. top1: 88.09. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9789. top1: 89.17. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9789. top1: 89.17. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9682. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9682. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9682. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.64s. Batch: 0.70s. Loss: 0.9320. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.64s. Batch: 0.70s. Loss: 0.9320. :  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]Finetune Epoch: 27/70. Data: 0.82s. Batch: 0.87s. Loss: 0.9307. :  33%|███▎      | 1/3 [00:01<00:01,  1.43it/s]Finetune Epoch: 27/70. Data: 0.82s. Batch: 0.87s. Loss: 0.9307. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 27/70. Data: 0.99s. Batch: 1.05s. Loss: 0.9268. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 27/70. Data: 0.99s. Batch: 1.05s. Loss: 0.9268. : 100%|██████████| 3/3 [00:01<00:00,  2.35it/s]Finetune Epoch: 27/70. Data: 0.99s. Batch: 1.05s. Loss: 0.9268. : 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 1.0817. top1: 82.81. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 1.0817. top1: 82.81. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0703. top1: 83.79. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0703. top1: 83.79. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0613. top1: 85.03. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0613. top1: 85.03. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0469. top1: 85.74. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0469. top1: 85.74. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.18it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0176. top1: 86.95. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.18it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0176. top1: 86.95. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.37it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9976. top1: 88.09. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.37it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9976. top1: 88.09. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9781. top1: 89.17. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.64it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9781. top1: 89.17. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9674. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9674. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9674. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9213. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9213. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 28/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9373. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 28/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9373. :  67%|██████▋   | 2/3 [00:01<00:00,  2.00it/s]Finetune Epoch: 28/70. Data: 1.00s. Batch: 1.06s. Loss: 0.9279. :  67%|██████▋   | 2/3 [00:01<00:00,  2.00it/s]Finetune Epoch: 28/70. Data: 1.00s. Batch: 1.06s. Loss: 0.9279. : 100%|██████████| 3/3 [00:01<00:00,  2.44it/s]Finetune Epoch: 28/70. Data: 1.00s. Batch: 1.06s. Loss: 0.9279. : 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.0804. top1: 82.81. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 1.0804. top1: 82.81. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0689. top1: 83.79. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0689. top1: 83.79. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0600. top1: 85.03. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0600. top1: 85.03. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0456. top1: 85.74. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0456. top1: 85.74. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  3.18it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0166. top1: 86.95. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.18it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0166. top1: 86.95. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9968. top1: 88.09. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9968. top1: 88.09. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9773. top1: 89.17. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9773. top1: 89.17. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9667. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9667. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.76it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9667. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.93it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9095. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.66s. Batch: 0.71s. Loss: 0.9095. :  33%|███▎      | 1/3 [00:00<00:01,  1.40it/s]Finetune Epoch: 29/70. Data: 0.84s. Batch: 0.89s. Loss: 0.9166. :  33%|███▎      | 1/3 [00:01<00:01,  1.40it/s]Finetune Epoch: 29/70. Data: 0.84s. Batch: 0.89s. Loss: 0.9166. :  67%|██████▋   | 2/3 [00:01<00:00,  1.98it/s]Finetune Epoch: 29/70. Data: 1.01s. Batch: 1.06s. Loss: 0.9208. :  67%|██████▋   | 2/3 [00:01<00:00,  1.98it/s]Finetune Epoch: 29/70. Data: 1.01s. Batch: 1.06s. Loss: 0.9208. : 100%|██████████| 3/3 [00:01<00:00,  2.35it/s]Finetune Epoch: 29/70. Data: 1.01s. Batch: 1.06s. Loss: 0.9208. : 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.0792. top1: 82.81. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 1.0792. top1: 82.81. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0678. top1: 83.79. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0678. top1: 83.79. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0589. top1: 85.03. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0589. top1: 85.03. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0445. top1: 85.74. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0445. top1: 85.74. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0157. top1: 87.03. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0157. top1: 87.03. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.07it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9960. top1: 88.15. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.07it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9960. top1: 88.15. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9766. top1: 89.23. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9766. top1: 89.23. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9660. top1: 89.85. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9660. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9660. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.81s. Batch: 0.87s. Loss: 0.9393. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.81s. Batch: 0.87s. Loss: 0.9393. :  33%|███▎      | 1/3 [00:00<00:01,  1.15it/s]Finetune Epoch: 30/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9396. :  33%|███▎      | 1/3 [00:01<00:01,  1.15it/s]Finetune Epoch: 30/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9396. :  67%|██████▋   | 2/3 [00:01<00:00,  1.68it/s]Finetune Epoch: 30/70. Data: 1.20s. Batch: 1.28s. Loss: 0.9349. :  67%|██████▋   | 2/3 [00:01<00:00,  1.68it/s]Finetune Epoch: 30/70. Data: 1.20s. Batch: 1.28s. Loss: 0.9349. : 100%|██████████| 3/3 [00:01<00:00,  1.95it/s]Finetune Epoch: 30/70. Data: 1.20s. Batch: 1.28s. Loss: 0.9349. : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0780. top1: 82.81. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0780. top1: 82.81. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0665. top1: 83.79. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.45it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0665. top1: 83.79. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0576. top1: 85.03. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0576. top1: 85.03. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.51it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0433. top1: 85.74. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.51it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0433. top1: 85.74. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0147. top1: 86.95. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0147. top1: 86.95. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9951. top1: 88.09. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9951. top1: 88.09. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9758. top1: 89.17. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.88it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9758. top1: 89.17. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9653. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  2.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9653. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9653. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.54it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9302. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.73s. Batch: 0.80s. Loss: 0.9302. :  33%|███▎      | 1/3 [00:00<00:01,  1.26it/s]Finetune Epoch: 31/70. Data: 0.93s. Batch: 0.99s. Loss: 0.9125. :  33%|███▎      | 1/3 [00:01<00:01,  1.26it/s]Finetune Epoch: 31/70. Data: 0.93s. Batch: 0.99s. Loss: 0.9125. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 31/70. Data: 1.16s. Batch: 1.21s. Loss: 0.9277. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 31/70. Data: 1.16s. Batch: 1.21s. Loss: 0.9277. : 100%|██████████| 3/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 31/70. Data: 1.16s. Batch: 1.21s. Loss: 0.9277. : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 1.0767. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 1.0767. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.43it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0652. top1: 83.98. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.43it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0652. top1: 83.98. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0563. top1: 85.16. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0563. top1: 85.16. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0421. top1: 85.84. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0421. top1: 85.84. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0137. top1: 87.03. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0137. top1: 87.03. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9942. top1: 88.15. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.90it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9942. top1: 88.15. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9750. top1: 89.23. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9750. top1: 89.23. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9645. top1: 89.85. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9645. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9645. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.61it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9153. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9153. :  33%|███▎      | 1/3 [00:00<00:01,  1.22it/s]Finetune Epoch: 32/70. Data: 0.94s. Batch: 1.01s. Loss: 0.9220. :  33%|███▎      | 1/3 [00:01<00:01,  1.22it/s]Finetune Epoch: 32/70. Data: 0.94s. Batch: 1.01s. Loss: 0.9220. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 32/70. Data: 1.11s. Batch: 1.17s. Loss: 0.9271. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 32/70. Data: 1.11s. Batch: 1.17s. Loss: 0.9271. : 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]Finetune Epoch: 32/70. Data: 1.11s. Batch: 1.17s. Loss: 0.9271. : 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 1.0756. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 1.0756. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.28it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0640. top1: 83.98. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.28it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0640. top1: 83.98. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0551. top1: 85.16. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0551. top1: 85.16. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0410. top1: 85.84. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.39it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0410. top1: 85.84. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.69it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0128. top1: 87.03. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.69it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0128. top1: 87.03. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9934. top1: 88.15. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.94it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9934. top1: 88.15. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9743. top1: 89.23. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9743. top1: 89.23. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9638. top1: 89.85. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9638. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.59it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9638. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.87s. Batch: 0.92s. Loss: 0.9315. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.87s. Batch: 0.92s. Loss: 0.9315. :  33%|███▎      | 1/3 [00:00<00:01,  1.09it/s]Finetune Epoch: 33/70. Data: 1.06s. Batch: 1.12s. Loss: 0.9343. :  33%|███▎      | 1/3 [00:01<00:01,  1.09it/s]Finetune Epoch: 33/70. Data: 1.06s. Batch: 1.12s. Loss: 0.9343. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 33/70. Data: 1.27s. Batch: 1.32s. Loss: 0.9282. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 33/70. Data: 1.27s. Batch: 1.32s. Loss: 0.9282. : 100%|██████████| 3/3 [00:01<00:00,  1.93it/s]Finetune Epoch: 33/70. Data: 1.27s. Batch: 1.32s. Loss: 0.9282. : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0742. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0742. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0626. top1: 83.98. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0626. top1: 83.98. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0538. top1: 85.16. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0538. top1: 85.16. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.80it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0397. top1: 85.84. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.80it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0397. top1: 85.84. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0117. top1: 87.03. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0117. top1: 87.03. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9926. top1: 88.15. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.96it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9926. top1: 88.15. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9735. top1: 89.23. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9735. top1: 89.23. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9631. top1: 89.85. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9631. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9631. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.60it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.79s. Batch: 0.85s. Loss: 0.9271. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.79s. Batch: 0.85s. Loss: 0.9271. :  33%|███▎      | 1/3 [00:00<00:01,  1.17it/s]Finetune Epoch: 34/70. Data: 0.98s. Batch: 1.04s. Loss: 0.9324. :  33%|███▎      | 1/3 [00:01<00:01,  1.17it/s]Finetune Epoch: 34/70. Data: 0.98s. Batch: 1.04s. Loss: 0.9324. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 34/70. Data: 1.16s. Batch: 1.22s. Loss: 0.9263. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 34/70. Data: 1.16s. Batch: 1.22s. Loss: 0.9263. : 100%|██████████| 3/3 [00:01<00:00,  2.12it/s]Finetune Epoch: 34/70. Data: 1.16s. Batch: 1.22s. Loss: 0.9263. : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 1.0731. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 1.0731. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0615. top1: 83.98. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.67it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0615. top1: 83.98. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0526. top1: 85.16. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0526. top1: 85.16. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0387. top1: 85.74. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0387. top1: 85.74. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0108. top1: 86.95. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0108. top1: 86.95. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9918. top1: 88.15. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9918. top1: 88.15. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9728. top1: 89.23. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9728. top1: 89.23. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9624. top1: 89.85. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9624. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.27it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9624. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.79s. Batch: 0.86s. Loss: 0.9342. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.79s. Batch: 0.86s. Loss: 0.9342. :  33%|███▎      | 1/3 [00:00<00:01,  1.16it/s]Finetune Epoch: 35/70. Data: 0.98s. Batch: 1.04s. Loss: 0.9356. :  33%|███▎      | 1/3 [00:01<00:01,  1.16it/s]Finetune Epoch: 35/70. Data: 0.98s. Batch: 1.04s. Loss: 0.9356. :  67%|██████▋   | 2/3 [00:01<00:00,  1.76it/s]Finetune Epoch: 35/70. Data: 1.17s. Batch: 1.23s. Loss: 0.9266. :  67%|██████▋   | 2/3 [00:01<00:00,  1.76it/s]Finetune Epoch: 35/70. Data: 1.17s. Batch: 1.23s. Loss: 0.9266. : 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 35/70. Data: 1.17s. Batch: 1.23s. Loss: 0.9266. : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 1.0720. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 1.0720. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.33it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.0603. top1: 84.18. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.33it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 1.0603. top1: 84.18. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0515. top1: 85.29. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0515. top1: 85.29. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0376. top1: 85.84. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0376. top1: 85.84. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.93it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0099. top1: 87.03. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.93it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0099. top1: 87.03. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9910. top1: 88.22. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9910. top1: 88.22. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9720. top1: 89.29. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9720. top1: 89.29. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9617. top1: 89.90. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9617. top1: 89.90. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.47it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9617. top1: 89.90. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.85s. Batch: 0.90s. Loss: 0.9450. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.85s. Batch: 0.90s. Loss: 0.9450. :  33%|███▎      | 1/3 [00:00<00:01,  1.11it/s]Finetune Epoch: 36/70. Data: 1.04s. Batch: 1.10s. Loss: 0.9307. :  33%|███▎      | 1/3 [00:01<00:01,  1.11it/s]Finetune Epoch: 36/70. Data: 1.04s. Batch: 1.10s. Loss: 0.9307. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 36/70. Data: 1.20s. Batch: 1.27s. Loss: 0.9207. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 36/70. Data: 1.20s. Batch: 1.27s. Loss: 0.9207. : 100%|██████████| 3/3 [00:01<00:00,  2.17it/s]Finetune Epoch: 36/70. Data: 1.20s. Batch: 1.27s. Loss: 0.9207. : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 1.0710. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 1.0710. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.0593. top1: 84.18. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.29it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.0593. top1: 84.18. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0505. top1: 85.42. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0505. top1: 85.42. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0366. top1: 85.94. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0366. top1: 85.94. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0091. top1: 87.11. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0091. top1: 87.11. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9903. top1: 88.28. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9903. top1: 88.28. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.32it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9714. top1: 89.34. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.32it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9714. top1: 89.34. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9610. top1: 89.95. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9610. top1: 89.95. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9610. top1: 89.95. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  2.71it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9397. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9397. :  33%|███▎      | 1/3 [00:00<00:01,  1.36it/s]Finetune Epoch: 37/70. Data: 0.85s. Batch: 0.91s. Loss: 0.9406. :  33%|███▎      | 1/3 [00:01<00:01,  1.36it/s]Finetune Epoch: 37/70. Data: 0.85s. Batch: 0.91s. Loss: 0.9406. :  67%|██████▋   | 2/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 37/70. Data: 1.01s. Batch: 1.06s. Loss: 0.9288. :  67%|██████▋   | 2/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 37/70. Data: 1.01s. Batch: 1.06s. Loss: 0.9288. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]Finetune Epoch: 37/70. Data: 1.01s. Batch: 1.06s. Loss: 0.9288. : 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0698. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0698. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0581. top1: 84.18. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0581. top1: 84.18. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0493. top1: 85.42. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0493. top1: 85.42. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0355. top1: 85.94. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0355. top1: 85.94. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0082. top1: 87.11. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0082. top1: 87.11. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9894. top1: 88.22. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9894. top1: 88.22. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9706. top1: 89.29. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9706. top1: 89.29. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.89it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9604. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  2.89it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9604. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9604. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.58it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.72s. Batch: 0.79s. Loss: 0.9178. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.72s. Batch: 0.79s. Loss: 0.9178. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 38/70. Data: 0.91s. Batch: 0.98s. Loss: 0.9254. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 38/70. Data: 0.91s. Batch: 0.98s. Loss: 0.9254. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 38/70. Data: 1.09s. Batch: 1.16s. Loss: 0.9222. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 38/70. Data: 1.09s. Batch: 1.16s. Loss: 0.9222. : 100%|██████████| 3/3 [00:01<00:00,  2.19it/s]Finetune Epoch: 38/70. Data: 1.09s. Batch: 1.16s. Loss: 0.9222. : 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 1.0685. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 1.0685. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.43it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0568. top1: 84.18. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.43it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0568. top1: 84.18. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0480. top1: 85.42. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0480. top1: 85.42. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.35it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0343. top1: 85.94. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.35it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0343. top1: 85.94. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.55it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0072. top1: 87.11. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.55it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0072. top1: 87.11. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9886. top1: 88.22. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9886. top1: 88.22. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9699. top1: 89.29. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9699. top1: 89.29. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9597. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9597. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9597. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.51it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.84s. Batch: 0.92s. Loss: 0.9090. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.84s. Batch: 0.92s. Loss: 0.9090. :  33%|███▎      | 1/3 [00:00<00:01,  1.09it/s]Finetune Epoch: 39/70. Data: 1.00s. Batch: 1.07s. Loss: 0.9239. :  33%|███▎      | 1/3 [00:01<00:01,  1.09it/s]Finetune Epoch: 39/70. Data: 1.00s. Batch: 1.07s. Loss: 0.9239. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 39/70. Data: 1.18s. Batch: 1.24s. Loss: 0.9225. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 39/70. Data: 1.18s. Batch: 1.24s. Loss: 0.9225. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]Finetune Epoch: 39/70. Data: 1.18s. Batch: 1.24s. Loss: 0.9225. : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.81s. Loss: 1.0674. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.81s. Loss: 1.0674. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.23it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.0557. top1: 84.18. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.23it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.0557. top1: 84.18. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0469. top1: 85.42. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0469. top1: 85.42. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0333. top1: 85.94. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.38it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0333. top1: 85.94. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.48it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0063. top1: 87.11. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.48it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0063. top1: 87.11. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.75it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9879. top1: 88.22. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.75it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9879. top1: 88.22. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9692. top1: 89.29. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9692. top1: 89.29. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9590. top1: 89.80. top5: 99.80. :  88%|████████▊ | 7/8 [00:03<00:00,  2.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9590. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  3.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9590. top1: 89.80. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.44it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 40/70. Data: 0.96s. Batch: 1.06s. Loss: 0.9402. :   0%|          | 0/3 [00:01<?, ?it/s]Finetune Epoch: 40/70. Data: 0.96s. Batch: 1.06s. Loss: 0.9402. :  33%|███▎      | 1/3 [00:01<00:02,  1.06s/it]Finetune Epoch: 40/70. Data: 1.19s. Batch: 1.30s. Loss: 0.9156. :  33%|███▎      | 1/3 [00:01<00:02,  1.06s/it]Finetune Epoch: 40/70. Data: 1.19s. Batch: 1.30s. Loss: 0.9156. :  67%|██████▋   | 2/3 [00:01<00:00,  1.40it/s]Finetune Epoch: 40/70. Data: 1.41s. Batch: 1.50s. Loss: 0.9160. :  67%|██████▋   | 2/3 [00:01<00:00,  1.40it/s]Finetune Epoch: 40/70. Data: 1.41s. Batch: 1.50s. Loss: 0.9160. : 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 40/70. Data: 1.41s. Batch: 1.50s. Loss: 0.9160. : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.85s. Loss: 1.0663. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.85s. Loss: 1.0663. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.17it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.0545. top1: 84.38. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.17it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.0545. top1: 84.38. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0457. top1: 85.55. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.92it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0457. top1: 85.55. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.40it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0322. top1: 86.04. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.40it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0322. top1: 86.04. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.67it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0055. top1: 87.19. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.67it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0055. top1: 87.19. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9871. top1: 88.28. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9871. top1: 88.28. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9685. top1: 89.34. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9685. top1: 89.34. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9584. top1: 89.85. top5: 99.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9584. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:02<00:00,  3.41it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9584. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.59it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.70s. Batch: 0.77s. Loss: 0.9206. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.70s. Batch: 0.77s. Loss: 0.9206. :  33%|███▎      | 1/3 [00:00<00:01,  1.31it/s]Finetune Epoch: 41/70. Data: 0.85s. Batch: 0.92s. Loss: 0.9219. :  33%|███▎      | 1/3 [00:01<00:01,  1.31it/s]Finetune Epoch: 41/70. Data: 0.85s. Batch: 0.92s. Loss: 0.9219. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 41/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9228. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 41/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9228. : 100%|██████████| 3/3 [00:01<00:00,  2.46it/s]Finetune Epoch: 41/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9228. : 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.90s. Loss: 1.0653. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.90s. Loss: 1.0653. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:06,  1.11it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0535. top1: 84.38. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:06,  1.11it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0535. top1: 84.38. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.89it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0447. top1: 85.55. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.89it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0447. top1: 85.55. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.44it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0312. top1: 86.04. top5: 99.61. :  38%|███▊      | 3/8 [00:01<00:02,  2.44it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0312. top1: 86.04. top5: 99.61. :  50%|█████     | 4/8 [00:01<00:01,  2.58it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0046. top1: 87.19. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.58it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0046. top1: 87.19. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9864. top1: 88.28. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9864. top1: 88.28. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.68it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9678. top1: 89.34. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.68it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9678. top1: 89.34. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9577. top1: 89.85. top5: 99.80. :  88%|████████▊ | 7/8 [00:03<00:00,  2.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9577. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  3.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9577. top1: 89.85. top5: 99.80. : 100%|██████████| 8/8 [00:03<00:00,  2.42it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.80s. Batch: 0.89s. Loss: 0.9410. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.80s. Batch: 0.89s. Loss: 0.9410. :  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Finetune Epoch: 42/70. Data: 0.98s. Batch: 1.06s. Loss: 0.9339. :  33%|███▎      | 1/3 [00:01<00:01,  1.12it/s]Finetune Epoch: 42/70. Data: 0.98s. Batch: 1.06s. Loss: 0.9339. :  67%|██████▋   | 2/3 [00:01<00:00,  1.77it/s]Finetune Epoch: 42/70. Data: 1.18s. Batch: 1.25s. Loss: 0.9250. :  67%|██████▋   | 2/3 [00:01<00:00,  1.77it/s]Finetune Epoch: 42/70. Data: 1.18s. Batch: 1.25s. Loss: 0.9250. : 100%|██████████| 3/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 42/70. Data: 1.18s. Batch: 1.25s. Loss: 0.9250. : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0641. top1: 83.20. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0641. top1: 83.20. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.40it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0523. top1: 84.38. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:05,  1.40it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0523. top1: 84.38. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0435. top1: 85.55. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0435. top1: 85.55. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0301. top1: 86.04. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.57it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0301. top1: 86.04. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.71it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0037. top1: 87.19. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.71it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0037. top1: 87.19. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9856. top1: 88.28. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9856. top1: 88.28. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9671. top1: 89.34. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9671. top1: 89.34. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9570. top1: 89.85. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9570. top1: 89.85. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.16it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9570. top1: 89.85. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.61it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.88s. Batch: 0.96s. Loss: 0.9285. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.88s. Batch: 0.96s. Loss: 0.9285. :  33%|███▎      | 1/3 [00:00<00:01,  1.04it/s]Finetune Epoch: 43/70. Data: 1.09s. Batch: 1.16s. Loss: 0.9268. :  33%|███▎      | 1/3 [00:01<00:01,  1.04it/s]Finetune Epoch: 43/70. Data: 1.09s. Batch: 1.16s. Loss: 0.9268. :  67%|██████▋   | 2/3 [00:01<00:00,  1.60it/s]Finetune Epoch: 43/70. Data: 1.27s. Batch: 1.33s. Loss: 0.9172. :  67%|██████▋   | 2/3 [00:01<00:00,  1.60it/s]Finetune Epoch: 43/70. Data: 1.27s. Batch: 1.33s. Loss: 0.9172. : 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 43/70. Data: 1.27s. Batch: 1.33s. Loss: 0.9172. : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0631. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0631. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0513. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0513. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0425. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0425. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0292. top1: 86.13. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0292. top1: 86.13. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0029. top1: 87.27. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0029. top1: 87.27. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9849. top1: 88.35. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9849. top1: 88.35. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9664. top1: 89.40. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9664. top1: 89.40. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9564. top1: 89.90. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9564. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9564. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.61s. Batch: 0.67s. Loss: 0.9347. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.61s. Batch: 0.67s. Loss: 0.9347. :  33%|███▎      | 1/3 [00:00<00:01,  1.49it/s]Finetune Epoch: 44/70. Data: 0.79s. Batch: 0.84s. Loss: 0.9195. :  33%|███▎      | 1/3 [00:01<00:01,  1.49it/s]Finetune Epoch: 44/70. Data: 0.79s. Batch: 0.84s. Loss: 0.9195. :  67%|██████▋   | 2/3 [00:01<00:00,  2.11it/s]Finetune Epoch: 44/70. Data: 0.95s. Batch: 1.00s. Loss: 0.9225. :  67%|██████▋   | 2/3 [00:01<00:00,  2.11it/s]Finetune Epoch: 44/70. Data: 0.95s. Batch: 1.00s. Loss: 0.9225. : 100%|██████████| 3/3 [00:01<00:00,  2.48it/s]Finetune Epoch: 44/70. Data: 0.95s. Batch: 1.00s. Loss: 0.9225. : 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0622. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0622. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.42it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0503. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.42it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0503. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0416. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0416. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0283. top1: 86.13. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0283. top1: 86.13. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0022. top1: 87.27. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0022. top1: 87.27. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9842. top1: 88.35. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9842. top1: 88.35. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9658. top1: 89.40. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.25it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9658. top1: 89.40. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9558. top1: 89.90. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9558. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.77it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9558. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9019. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.67s. Batch: 0.73s. Loss: 0.9019. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 45/70. Data: 0.83s. Batch: 0.88s. Loss: 0.9120. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 45/70. Data: 0.83s. Batch: 0.88s. Loss: 0.9120. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 45/70. Data: 0.99s. Batch: 1.04s. Loss: 0.9163. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 45/70. Data: 0.99s. Batch: 1.04s. Loss: 0.9163. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]Finetune Epoch: 45/70. Data: 0.99s. Batch: 1.04s. Loss: 0.9163. : 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0612. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0612. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0493. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0493. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0406. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0406. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0273. top1: 86.13. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0273. top1: 86.13. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.12it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0014. top1: 87.27. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.12it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0014. top1: 87.27. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9835. top1: 88.35. top5: 99.80. :  62%|██████▎   | 5/8 [00:01<00:00,  3.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9835. top1: 88.35. top5: 99.80. :  75%|███████▌  | 6/8 [00:01<00:00,  3.42it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9651. top1: 89.40. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.42it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9651. top1: 89.40. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9551. top1: 89.90. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9551. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.89it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9551. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9544. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.63s. Batch: 0.69s. Loss: 0.9544. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch: 46/70. Data: 0.77s. Batch: 0.82s. Loss: 0.9219. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch: 46/70. Data: 0.77s. Batch: 0.82s. Loss: 0.9219. :  67%|██████▋   | 2/3 [00:00<00:00,  2.26it/s]Finetune Epoch: 46/70. Data: 0.94s. Batch: 0.99s. Loss: 0.9199. :  67%|██████▋   | 2/3 [00:01<00:00,  2.26it/s]Finetune Epoch: 46/70. Data: 0.94s. Batch: 0.99s. Loss: 0.9199. : 100%|██████████| 3/3 [00:01<00:00,  2.48it/s]Finetune Epoch: 46/70. Data: 0.94s. Batch: 0.99s. Loss: 0.9199. : 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0601. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0601. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.89it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0481. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.89it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0481. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.87it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0394. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:00<00:02,  2.87it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0394. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:00<00:01,  3.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0262. top1: 86.13. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0262. top1: 86.13. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.62it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0005. top1: 87.27. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.62it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0005. top1: 87.27. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9827. top1: 88.35. top5: 99.80. :  62%|██████▎   | 5/8 [00:01<00:00,  3.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9827. top1: 88.35. top5: 99.80. :  75%|███████▌  | 6/8 [00:01<00:00,  3.81it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9644. top1: 89.40. top5: 99.83. :  75%|███████▌  | 6/8 [00:01<00:00,  3.81it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9644. top1: 89.40. top5: 99.83. :  88%|████████▊ | 7/8 [00:01<00:00,  3.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9545. top1: 89.90. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9545. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  4.23it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.27s. Loss: 0.9545. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.47it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.69s. Batch: 0.75s. Loss: 0.9009. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.69s. Batch: 0.75s. Loss: 0.9009. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch: 47/70. Data: 0.86s. Batch: 0.91s. Loss: 0.9168. :  33%|███▎      | 1/3 [00:01<00:01,  1.33it/s]Finetune Epoch: 47/70. Data: 0.86s. Batch: 0.91s. Loss: 0.9168. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 47/70. Data: 1.02s. Batch: 1.07s. Loss: 0.9290. :  67%|██████▋   | 2/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 47/70. Data: 1.02s. Batch: 1.07s. Loss: 0.9290. : 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]Finetune Epoch: 47/70. Data: 1.02s. Batch: 1.07s. Loss: 0.9290. : 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0590. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0590. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0470. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0470. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0383. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0383. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0252. top1: 86.13. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0252. top1: 86.13. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.12it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9996. top1: 87.27. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.12it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9996. top1: 87.27. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9820. top1: 88.35. top5: 99.80. :  62%|██████▎   | 5/8 [00:01<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9820. top1: 88.35. top5: 99.80. :  75%|███████▌  | 6/8 [00:01<00:00,  3.54it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9638. top1: 89.40. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.54it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9638. top1: 89.40. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9539. top1: 89.90. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9539. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9539. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.94it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.49s. Batch: 0.54s. Loss: 0.8996. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.49s. Batch: 0.54s. Loss: 0.8996. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch: 48/70. Data: 0.67s. Batch: 0.72s. Loss: 0.9421. :  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]Finetune Epoch: 48/70. Data: 0.67s. Batch: 0.72s. Loss: 0.9421. :  67%|██████▋   | 2/3 [00:00<00:00,  2.27it/s]Finetune Epoch: 48/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9249. :  67%|██████▋   | 2/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 48/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9249. : 100%|██████████| 3/3 [00:01<00:00,  2.58it/s]Finetune Epoch: 48/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9249. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0580. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.54s. Loss: 1.0580. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.85it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0460. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.85it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0460. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.76it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0373. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.76it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0373. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  3.10it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0243. top1: 86.13. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.10it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.0243. top1: 86.13. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.37it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9988. top1: 87.27. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.37it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9988. top1: 87.27. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.47it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9813. top1: 88.35. top5: 99.80. :  62%|██████▎   | 5/8 [00:01<00:00,  3.47it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9813. top1: 88.35. top5: 99.80. :  75%|███████▌  | 6/8 [00:01<00:00,  3.42it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9631. top1: 89.40. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.42it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9631. top1: 89.40. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9533. top1: 89.90. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9533. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.9533. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.68s. Batch: 0.75s. Loss: 0.9300. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.68s. Batch: 0.75s. Loss: 0.9300. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch: 49/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9025. :  33%|███▎      | 1/3 [00:01<00:01,  1.33it/s]Finetune Epoch: 49/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9025. :  67%|██████▋   | 2/3 [00:01<00:00,  2.08it/s]Finetune Epoch: 49/70. Data: 0.99s. Batch: 1.05s. Loss: 0.9123. :  67%|██████▋   | 2/3 [00:01<00:00,  2.08it/s]Finetune Epoch: 49/70. Data: 0.99s. Batch: 1.05s. Loss: 0.9123. : 100%|██████████| 3/3 [00:01<00:00,  2.46it/s]Finetune Epoch: 49/70. Data: 0.99s. Batch: 1.05s. Loss: 0.9123. : 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 1.0570. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 1.0570. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0450. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0450. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0363. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0363. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0233. top1: 86.13. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0233. top1: 86.13. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9981. top1: 87.27. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9981. top1: 87.27. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9806. top1: 88.35. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9806. top1: 88.35. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9625. top1: 89.40. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9625. top1: 89.40. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9527. top1: 89.90. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9527. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.47it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9527. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.70s. Batch: 0.77s. Loss: 0.8872. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.70s. Batch: 0.77s. Loss: 0.8872. :  33%|███▎      | 1/3 [00:00<00:01,  1.30it/s]Finetune Epoch: 50/70. Data: 0.86s. Batch: 0.92s. Loss: 0.9025. :  33%|███▎      | 1/3 [00:01<00:01,  1.30it/s]Finetune Epoch: 50/70. Data: 0.86s. Batch: 0.92s. Loss: 0.9025. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 50/70. Data: 1.04s. Batch: 1.10s. Loss: 0.9106. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 50/70. Data: 1.04s. Batch: 1.10s. Loss: 0.9106. : 100%|██████████| 3/3 [00:01<00:00,  2.23it/s]Finetune Epoch: 50/70. Data: 1.04s. Batch: 1.10s. Loss: 0.9106. : 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.0558. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.0558. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0438. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.80it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0438. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.64it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0352. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.64it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0352. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.92it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0222. top1: 86.13. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.92it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0222. top1: 86.13. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9972. top1: 87.27. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9972. top1: 87.27. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.15it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9799. top1: 88.35. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.15it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9799. top1: 88.35. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9618. top1: 89.40. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9618. top1: 89.40. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9520. top1: 89.90. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9520. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9520. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.86it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.58s. Batch: 0.66s. Loss: 0.9264. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.58s. Batch: 0.66s. Loss: 0.9264. :  33%|███▎      | 1/3 [00:00<00:01,  1.52it/s]Finetune Epoch: 51/70. Data: 0.80s. Batch: 0.88s. Loss: 0.9145. :  33%|███▎      | 1/3 [00:01<00:01,  1.52it/s]Finetune Epoch: 51/70. Data: 0.80s. Batch: 0.88s. Loss: 0.9145. :  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s]Finetune Epoch: 51/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9193. :  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s]Finetune Epoch: 51/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9193. : 100%|██████████| 3/3 [00:01<00:00,  2.21it/s]Finetune Epoch: 51/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9193. : 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0549. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0549. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0429. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.41it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0429. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0342. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 1.0342. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:02,  2.28it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0213. top1: 86.13. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.28it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0213. top1: 86.13. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9964. top1: 87.27. top5: 99.77. :  50%|█████     | 4/8 [00:02<00:01,  2.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9964. top1: 87.27. top5: 99.77. :  62%|██████▎   | 5/8 [00:02<00:01,  2.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9792. top1: 88.35. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.69it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9792. top1: 88.35. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9612. top1: 89.40. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9612. top1: 89.40. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9514. top1: 89.90. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9514. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9514. top1: 89.90. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.66it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9211. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.76s. Batch: 0.82s. Loss: 0.9211. :  33%|███▎      | 1/3 [00:00<00:01,  1.22it/s]Finetune Epoch: 52/70. Data: 0.91s. Batch: 0.97s. Loss: 0.9224. :  33%|███▎      | 1/3 [00:01<00:01,  1.22it/s]Finetune Epoch: 52/70. Data: 0.91s. Batch: 0.97s. Loss: 0.9224. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 52/70. Data: 1.08s. Batch: 1.14s. Loss: 0.9271. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 52/70. Data: 1.08s. Batch: 1.14s. Loss: 0.9271. : 100%|██████████| 3/3 [00:01<00:00,  2.28it/s]Finetune Epoch: 52/70. Data: 1.08s. Batch: 1.14s. Loss: 0.9271. : 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0539. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.0539. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0419. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0419. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0332. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0332. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0204. top1: 86.23. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0204. top1: 86.23. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9956. top1: 87.34. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9956. top1: 87.34. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.23it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9785. top1: 88.41. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.23it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9785. top1: 88.41. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9605. top1: 89.45. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.43it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9605. top1: 89.45. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9508. top1: 89.95. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9508. top1: 89.95. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9508. top1: 89.95. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.75s. Batch: 0.83s. Loss: 0.9334. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.75s. Batch: 0.83s. Loss: 0.9334. :  33%|███▎      | 1/3 [00:00<00:01,  1.21it/s]Finetune Epoch: 53/70. Data: 0.92s. Batch: 0.99s. Loss: 0.9219. :  33%|███▎      | 1/3 [00:01<00:01,  1.21it/s]Finetune Epoch: 53/70. Data: 0.92s. Batch: 0.99s. Loss: 0.9219. :  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s]Finetune Epoch: 53/70. Data: 1.09s. Batch: 1.16s. Loss: 0.9131. :  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s]Finetune Epoch: 53/70. Data: 1.09s. Batch: 1.16s. Loss: 0.9131. : 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]Finetune Epoch: 53/70. Data: 1.09s. Batch: 1.16s. Loss: 0.9131. : 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0529. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0529. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0408. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0408. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.44it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0322. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.44it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0322. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0195. top1: 86.23. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.93it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0195. top1: 86.23. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.27it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9949. top1: 87.34. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.27it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9949. top1: 87.34. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9778. top1: 88.41. top5: 99.80. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9778. top1: 88.41. top5: 99.80. :  75%|███████▌  | 6/8 [00:01<00:00,  3.41it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9599. top1: 89.45. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.41it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9599. top1: 89.45. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9503. top1: 89.95. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9503. top1: 89.95. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.67it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9503. top1: 89.95. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9034. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9034. :  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Finetune Epoch: 54/70. Data: 0.81s. Batch: 0.87s. Loss: 0.9209. :  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Finetune Epoch: 54/70. Data: 0.81s. Batch: 0.87s. Loss: 0.9209. :  67%|██████▋   | 2/3 [00:00<00:00,  2.20it/s]Finetune Epoch: 54/70. Data: 0.98s. Batch: 1.03s. Loss: 0.9163. :  67%|██████▋   | 2/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 54/70. Data: 0.98s. Batch: 1.03s. Loss: 0.9163. : 100%|██████████| 3/3 [00:01<00:00,  2.42it/s]Finetune Epoch: 54/70. Data: 0.98s. Batch: 1.03s. Loss: 0.9163. : 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 1.0520. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 1.0520. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0399. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 1.0399. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0313. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0313. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0186. top1: 86.23. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0186. top1: 86.23. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.08it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9941. top1: 87.34. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.08it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9941. top1: 87.34. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.34it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9772. top1: 88.41. top5: 99.80. :  62%|██████▎   | 5/8 [00:01<00:00,  3.34it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9772. top1: 88.41. top5: 99.80. :  75%|███████▌  | 6/8 [00:01<00:00,  3.51it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9593. top1: 89.45. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.51it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9593. top1: 89.45. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9497. top1: 89.95. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9497. top1: 89.95. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9497. top1: 89.95. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.54s. Batch: 0.60s. Loss: 0.9069. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.54s. Batch: 0.60s. Loss: 0.9069. :  33%|███▎      | 1/3 [00:00<00:01,  1.68it/s]Finetune Epoch: 55/70. Data: 0.70s. Batch: 0.75s. Loss: 0.9188. :  33%|███▎      | 1/3 [00:00<00:01,  1.68it/s]Finetune Epoch: 55/70. Data: 0.70s. Batch: 0.75s. Loss: 0.9188. :  67%|██████▋   | 2/3 [00:00<00:00,  2.37it/s]Finetune Epoch: 55/70. Data: 0.85s. Batch: 0.90s. Loss: 0.9288. :  67%|██████▋   | 2/3 [00:01<00:00,  2.37it/s]Finetune Epoch: 55/70. Data: 0.85s. Batch: 0.90s. Loss: 0.9288. : 100%|██████████| 3/3 [00:01<00:00,  2.72it/s]Finetune Epoch: 55/70. Data: 0.85s. Batch: 0.90s. Loss: 0.9288. : 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 1.0511. top1: 83.59. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 1.0511. top1: 83.59. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0390. top1: 84.57. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0390. top1: 84.57. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0304. top1: 85.68. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.50it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0304. top1: 85.68. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0177. top1: 86.23. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0177. top1: 86.23. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9934. top1: 87.34. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9934. top1: 87.34. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9765. top1: 88.41. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.95it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9765. top1: 88.41. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9587. top1: 89.45. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9587. top1: 89.45. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9491. top1: 89.95. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9491. top1: 89.95. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9491. top1: 89.95. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.62it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.77s. Batch: 0.83s. Loss: 0.9117. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.77s. Batch: 0.83s. Loss: 0.9117. :  33%|███▎      | 1/3 [00:00<00:01,  1.20it/s]Finetune Epoch: 56/70. Data: 0.93s. Batch: 0.98s. Loss: 0.9345. :  33%|███▎      | 1/3 [00:01<00:01,  1.20it/s]Finetune Epoch: 56/70. Data: 0.93s. Batch: 0.98s. Loss: 0.9345. :  67%|██████▋   | 2/3 [00:01<00:00,  1.93it/s]Finetune Epoch: 56/70. Data: 1.10s. Batch: 1.15s. Loss: 0.9189. :  67%|██████▋   | 2/3 [00:01<00:00,  1.93it/s]Finetune Epoch: 56/70. Data: 1.10s. Batch: 1.15s. Loss: 0.9189. : 100%|██████████| 3/3 [00:01<00:00,  2.22it/s]Finetune Epoch: 56/70. Data: 1.10s. Batch: 1.15s. Loss: 0.9189. : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0499. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0499. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0378. top1: 84.77. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0378. top1: 84.77. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0293. top1: 85.81. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0293. top1: 85.81. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0167. top1: 86.33. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0167. top1: 86.33. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.73it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9925. top1: 87.42. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.73it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9925. top1: 87.42. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9758. top1: 88.48. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.86it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9758. top1: 88.48. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9581. top1: 89.51. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9581. top1: 89.51. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9485. top1: 90.00. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9485. top1: 90.00. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.52it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9485. top1: 90.00. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.62it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.74s. Batch: 0.80s. Loss: 0.9287. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.74s. Batch: 0.80s. Loss: 0.9287. :  33%|███▎      | 1/3 [00:00<00:01,  1.25it/s]Finetune Epoch: 57/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9389. :  33%|███▎      | 1/3 [00:01<00:01,  1.25it/s]Finetune Epoch: 57/70. Data: 0.89s. Batch: 0.95s. Loss: 0.9389. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch: 57/70. Data: 1.08s. Batch: 1.14s. Loss: 0.9302. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch: 57/70. Data: 1.08s. Batch: 1.14s. Loss: 0.9302. : 100%|██████████| 3/3 [00:01<00:00,  2.14it/s]Finetune Epoch: 57/70. Data: 1.08s. Batch: 1.14s. Loss: 0.9302. : 100%|██████████| 3/3 [00:01<00:00,  1.75it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0489. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0489. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0368. top1: 84.77. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0368. top1: 84.77. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0282. top1: 85.94. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0282. top1: 85.94. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0157. top1: 86.43. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0157. top1: 86.43. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.85it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9917. top1: 87.50. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.85it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9917. top1: 87.50. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  3.00it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9751. top1: 88.54. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  3.00it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9751. top1: 88.54. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9575. top1: 89.56. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9575. top1: 89.56. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9479. top1: 90.05. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9479. top1: 90.05. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9479. top1: 90.05. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.71it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.67s. Batch: 0.73s. Loss: 0.8915. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.67s. Batch: 0.73s. Loss: 0.8915. :  33%|███▎      | 1/3 [00:00<00:01,  1.37it/s]Finetune Epoch: 58/70. Data: 0.85s. Batch: 0.91s. Loss: 0.9165. :  33%|███▎      | 1/3 [00:01<00:01,  1.37it/s]Finetune Epoch: 58/70. Data: 0.85s. Batch: 0.91s. Loss: 0.9165. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch: 58/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9132. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch: 58/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9132. : 100%|██████████| 3/3 [00:01<00:00,  2.39it/s]Finetune Epoch: 58/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9132. : 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 1.0478. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 1.0478. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.35it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0357. top1: 84.77. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.35it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0357. top1: 84.77. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0272. top1: 85.94. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0272. top1: 85.94. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0147. top1: 86.43. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0147. top1: 86.43. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.18it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9909. top1: 87.50. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.18it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9909. top1: 87.50. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9744. top1: 88.54. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.44it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9744. top1: 88.54. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.42it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9568. top1: 89.56. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.42it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9568. top1: 89.56. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9473. top1: 90.05. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9473. top1: 90.05. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.9473. top1: 90.05. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.91it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9144. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.68s. Batch: 0.74s. Loss: 0.9144. :  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Finetune Epoch: 59/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9165. :  33%|███▎      | 1/3 [00:01<00:01,  1.35it/s]Finetune Epoch: 59/70. Data: 0.84s. Batch: 0.90s. Loss: 0.9165. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 59/70. Data: 1.02s. Batch: 1.07s. Loss: 0.9207. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 59/70. Data: 1.02s. Batch: 1.07s. Loss: 0.9207. : 100%|██████████| 3/3 [00:01<00:00,  2.34it/s]Finetune Epoch: 59/70. Data: 1.02s. Batch: 1.07s. Loss: 0.9207. : 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.0470. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 1.0470. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0349. top1: 84.96. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0349. top1: 84.96. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0263. top1: 86.07. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0263. top1: 86.07. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0139. top1: 86.52. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0139. top1: 86.52. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9903. top1: 87.58. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9903. top1: 87.58. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9738. top1: 88.61. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9738. top1: 88.61. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9563. top1: 89.62. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9563. top1: 89.62. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9468. top1: 90.10. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9468. top1: 90.10. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9468. top1: 90.10. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.62it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.73s. Batch: 0.82s. Loss: 0.9303. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.73s. Batch: 0.82s. Loss: 0.9303. :  33%|███▎      | 1/3 [00:00<00:01,  1.23it/s]Finetune Epoch: 60/70. Data: 0.95s. Batch: 1.02s. Loss: 0.9161. :  33%|███▎      | 1/3 [00:01<00:01,  1.23it/s]Finetune Epoch: 60/70. Data: 0.95s. Batch: 1.02s. Loss: 0.9161. :  67%|██████▋   | 2/3 [00:01<00:00,  1.73it/s]Finetune Epoch: 60/70. Data: 1.13s. Batch: 1.20s. Loss: 0.9181. :  67%|██████▋   | 2/3 [00:01<00:00,  1.73it/s]Finetune Epoch: 60/70. Data: 1.13s. Batch: 1.20s. Loss: 0.9181. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]Finetune Epoch: 60/70. Data: 1.13s. Batch: 1.20s. Loss: 0.9181. : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 1.0462. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 1.0462. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0341. top1: 84.96. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0341. top1: 84.96. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0255. top1: 86.07. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0255. top1: 86.07. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0132. top1: 86.52. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0132. top1: 86.52. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9896. top1: 87.58. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.89it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9896. top1: 87.58. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9732. top1: 88.61. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.85it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9732. top1: 88.61. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9557. top1: 89.62. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9557. top1: 89.62. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9463. top1: 90.10. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9463. top1: 90.10. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9463. top1: 90.10. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.75s. Batch: 0.81s. Loss: 0.9143. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.75s. Batch: 0.81s. Loss: 0.9143. :  33%|███▎      | 1/3 [00:00<00:01,  1.23it/s]Finetune Epoch: 61/70. Data: 0.90s. Batch: 0.97s. Loss: 0.9168. :  33%|███▎      | 1/3 [00:01<00:01,  1.23it/s]Finetune Epoch: 61/70. Data: 0.90s. Batch: 0.97s. Loss: 0.9168. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 61/70. Data: 1.06s. Batch: 1.13s. Loss: 0.9199. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 61/70. Data: 1.06s. Batch: 1.13s. Loss: 0.9199. : 100%|██████████| 3/3 [00:01<00:00,  2.39it/s]Finetune Epoch: 61/70. Data: 1.06s. Batch: 1.13s. Loss: 0.9199. : 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.0452. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.0452. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0330. top1: 84.96. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0330. top1: 84.96. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0245. top1: 86.07. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0245. top1: 86.07. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.61it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0122. top1: 86.52. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.61it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0122. top1: 86.52. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.86it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9888. top1: 87.58. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.86it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9888. top1: 87.58. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9726. top1: 88.61. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9726. top1: 88.61. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  2.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9551. top1: 89.62. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  2.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9551. top1: 89.62. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9457. top1: 90.10. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9457. top1: 90.10. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.61it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9457. top1: 90.10. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9333. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.75s. Batch: 0.80s. Loss: 0.9333. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch: 62/70. Data: 0.88s. Batch: 0.93s. Loss: 0.9250. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch: 62/70. Data: 0.88s. Batch: 0.93s. Loss: 0.9250. :  67%|██████▋   | 2/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 62/70. Data: 1.04s. Batch: 1.11s. Loss: 0.9245. :  67%|██████▋   | 2/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 62/70. Data: 1.04s. Batch: 1.11s. Loss: 0.9245. : 100%|██████████| 3/3 [00:01<00:00,  2.24it/s]Finetune Epoch: 62/70. Data: 1.04s. Batch: 1.11s. Loss: 0.9245. : 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0442. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0442. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0320. top1: 84.96. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0320. top1: 84.96. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0235. top1: 86.20. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0235. top1: 86.20. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0113. top1: 86.62. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0113. top1: 86.62. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.12it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9880. top1: 87.66. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.12it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9880. top1: 87.66. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.29it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9719. top1: 88.67. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.29it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9719. top1: 88.67. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9545. top1: 89.68. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9545. top1: 89.68. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9451. top1: 90.15. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9451. top1: 90.15. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9451. top1: 90.15. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.64s. Batch: 0.70s. Loss: 0.9111. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.64s. Batch: 0.70s. Loss: 0.9111. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch: 63/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9163. :  33%|███▎      | 1/3 [00:01<00:01,  1.44it/s]Finetune Epoch: 63/70. Data: 0.82s. Batch: 0.88s. Loss: 0.9163. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch: 63/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9111. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch: 63/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9111. : 100%|██████████| 3/3 [00:01<00:00,  2.19it/s]Finetune Epoch: 63/70. Data: 1.01s. Batch: 1.07s. Loss: 0.9111. : 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0433. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 1.0433. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0312. top1: 85.16. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 1.0312. top1: 85.16. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0227. top1: 86.33. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 1.0227. top1: 86.33. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0104. top1: 86.72. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.49it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0104. top1: 86.72. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9874. top1: 87.73. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9874. top1: 87.73. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9713. top1: 88.74. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9713. top1: 88.74. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.20it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9540. top1: 89.73. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.20it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9540. top1: 89.73. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9446. top1: 90.20. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9446. top1: 90.20. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9446. top1: 90.20. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.64it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.85s. Batch: 0.92s. Loss: 0.9084. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.85s. Batch: 0.92s. Loss: 0.9084. :  33%|███▎      | 1/3 [00:00<00:01,  1.08it/s]Finetune Epoch: 64/70. Data: 1.05s. Batch: 1.11s. Loss: 0.9142. :  33%|███▎      | 1/3 [00:01<00:01,  1.08it/s]Finetune Epoch: 64/70. Data: 1.05s. Batch: 1.11s. Loss: 0.9142. :  67%|██████▋   | 2/3 [00:01<00:00,  1.67it/s]Finetune Epoch: 64/70. Data: 1.22s. Batch: 1.29s. Loss: 0.9141. :  67%|██████▋   | 2/3 [00:01<00:00,  1.67it/s]Finetune Epoch: 64/70. Data: 1.22s. Batch: 1.29s. Loss: 0.9141. : 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 64/70. Data: 1.22s. Batch: 1.29s. Loss: 0.9141. : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0425. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0425. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.38it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0303. top1: 85.16. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.38it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 1.0303. top1: 85.16. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0218. top1: 86.33. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0218. top1: 86.33. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0097. top1: 86.72. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0097. top1: 86.72. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9867. top1: 87.73. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9867. top1: 87.73. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.88it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9707. top1: 88.74. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.88it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9707. top1: 88.74. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9534. top1: 89.73. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9534. top1: 89.73. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9441. top1: 90.20. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9441. top1: 90.20. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9441. top1: 90.20. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.52it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.71s. Batch: 0.80s. Loss: 0.9148. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.71s. Batch: 0.80s. Loss: 0.9148. :  33%|███▎      | 1/3 [00:00<00:01,  1.25it/s]Finetune Epoch: 65/70. Data: 0.86s. Batch: 0.94s. Loss: 0.9162. :  33%|███▎      | 1/3 [00:01<00:01,  1.25it/s]Finetune Epoch: 65/70. Data: 0.86s. Batch: 0.94s. Loss: 0.9162. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 65/70. Data: 1.00s. Batch: 1.07s. Loss: 0.9242. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 65/70. Data: 1.00s. Batch: 1.07s. Loss: 0.9242. : 100%|██████████| 3/3 [00:01<00:00,  2.59it/s]Finetune Epoch: 65/70. Data: 1.00s. Batch: 1.07s. Loss: 0.9242. : 100%|██████████| 3/3 [00:01<00:00,  1.95it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0416. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0416. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.0295. top1: 85.16. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.39it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.0295. top1: 85.16. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0210. top1: 86.33. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0210. top1: 86.33. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0089. top1: 86.72. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0089. top1: 86.72. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.18it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9861. top1: 87.73. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.18it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9861. top1: 87.73. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.28it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9701. top1: 88.74. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.28it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9701. top1: 88.74. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9529. top1: 89.73. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9529. top1: 89.73. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9436. top1: 90.20. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9436. top1: 90.20. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9436. top1: 90.20. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.73it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.76s. Batch: 0.83s. Loss: 0.8958. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.76s. Batch: 0.83s. Loss: 0.8958. :  33%|███▎      | 1/3 [00:00<00:01,  1.21it/s]Finetune Epoch: 66/70. Data: 1.00s. Batch: 1.07s. Loss: 0.9170. :  33%|███▎      | 1/3 [00:01<00:01,  1.21it/s]Finetune Epoch: 66/70. Data: 1.00s. Batch: 1.07s. Loss: 0.9170. :  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s]Finetune Epoch: 66/70. Data: 1.21s. Batch: 1.27s. Loss: 0.9126. :  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s]Finetune Epoch: 66/70. Data: 1.21s. Batch: 1.27s. Loss: 0.9126. : 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]Finetune Epoch: 66/70. Data: 1.21s. Batch: 1.27s. Loss: 0.9126. : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 1.0407. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 1.0407. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.43it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0286. top1: 85.16. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.43it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0286. top1: 85.16. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0201. top1: 86.33. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0201. top1: 86.33. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0080. top1: 86.72. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.0080. top1: 86.72. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9853. top1: 87.73. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9853. top1: 87.73. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9695. top1: 88.74. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.91it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9695. top1: 88.74. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  2.87it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9523. top1: 89.73. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  2.87it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9523. top1: 89.73. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9430. top1: 90.20. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9430. top1: 90.20. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9430. top1: 90.20. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8848. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8848. :  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Finetune Epoch: 67/70. Data: 1.02s. Batch: 1.08s. Loss: 0.9085. :  33%|███▎      | 1/3 [00:01<00:01,  1.13it/s]Finetune Epoch: 67/70. Data: 1.02s. Batch: 1.08s. Loss: 0.9085. :  67%|██████▋   | 2/3 [00:01<00:00,  1.69it/s]Finetune Epoch: 67/70. Data: 1.24s. Batch: 1.30s. Loss: 0.9274. :  67%|██████▋   | 2/3 [00:01<00:00,  1.69it/s]Finetune Epoch: 67/70. Data: 1.24s. Batch: 1.30s. Loss: 0.9274. : 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]Finetune Epoch: 67/70. Data: 1.24s. Batch: 1.30s. Loss: 0.9274. : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 1.09s. Loss: 1.0398. top1: 84.38. top5: 100.00. :   0%|          | 0/8 [00:01<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 1.09s. Loss: 1.0398. top1: 84.38. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:07,  1.10s/it]Test Iter:   2/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0277. top1: 85.35. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:07,  1.10s/it] Test Iter:   2/  8. Data: 0.00s. Batch: 0.72s. Loss: 1.0277. top1: 85.35. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.53it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0192. top1: 86.46. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:03,  1.53it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.58s. Loss: 1.0192. top1: 86.46. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:02,  2.01it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0072. top1: 86.82. top5: 99.71. :  38%|███▊      | 3/8 [00:02<00:02,  2.01it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.53s. Loss: 1.0072. top1: 86.82. top5: 99.71. :  50%|█████     | 4/8 [00:02<00:01,  2.23it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.9846. top1: 87.81. top5: 99.77. :  50%|█████     | 4/8 [00:02<00:01,  2.23it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.9846. top1: 87.81. top5: 99.77. :  62%|██████▎   | 5/8 [00:02<00:01,  2.42it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.9689. top1: 88.80. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.42it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.9689. top1: 88.80. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  2.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.9517. top1: 89.79. top5: 99.83. :  75%|███████▌  | 6/8 [00:03<00:00,  2.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.9517. top1: 89.79. top5: 99.83. :  88%|████████▊ | 7/8 [00:03<00:00,  2.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.9425. top1: 90.25. top5: 99.85. :  88%|████████▊ | 7/8 [00:03<00:00,  2.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.9425. top1: 90.25. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.9425. top1: 90.25. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.21it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 68/70. Data: 0.95s. Batch: 1.03s. Loss: 0.9400. :   0%|          | 0/3 [00:01<?, ?it/s]Finetune Epoch: 68/70. Data: 0.95s. Batch: 1.03s. Loss: 0.9400. :  33%|███▎      | 1/3 [00:01<00:02,  1.03s/it]Finetune Epoch: 68/70. Data: 1.12s. Batch: 1.19s. Loss: 0.9270. :  33%|███▎      | 1/3 [00:01<00:02,  1.03s/it]Finetune Epoch: 68/70. Data: 1.12s. Batch: 1.19s. Loss: 0.9270. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 68/70. Data: 1.31s. Batch: 1.39s. Loss: 0.9164. :  67%|██████▋   | 2/3 [00:01<00:00,  1.64it/s]Finetune Epoch: 68/70. Data: 1.31s. Batch: 1.39s. Loss: 0.9164. : 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]Finetune Epoch: 68/70. Data: 1.31s. Batch: 1.39s. Loss: 0.9164. : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.0391. top1: 84.38. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.0391. top1: 84.38. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.55it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0269. top1: 85.35. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.55it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0269. top1: 85.35. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0185. top1: 86.46. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0185. top1: 86.46. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0065. top1: 86.82. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.55it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.0065. top1: 86.82. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9840. top1: 87.81. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.81it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9840. top1: 87.81. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9683. top1: 88.80. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.84it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9683. top1: 88.80. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  2.83it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9512. top1: 89.79. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  2.83it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9512. top1: 89.79. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9419. top1: 90.30. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9419. top1: 90.30. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.15it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9419. top1: 90.30. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.56it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.84s. Batch: 0.92s. Loss: 0.9388. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.84s. Batch: 0.92s. Loss: 0.9388. :  33%|███▎      | 1/3 [00:00<00:01,  1.09it/s]Finetune Epoch: 69/70. Data: 1.04s. Batch: 1.11s. Loss: 0.9357. :  33%|███▎      | 1/3 [00:01<00:01,  1.09it/s]Finetune Epoch: 69/70. Data: 1.04s. Batch: 1.11s. Loss: 0.9357. :  67%|██████▋   | 2/3 [00:01<00:00,  1.65it/s]Finetune Epoch: 69/70. Data: 1.24s. Batch: 1.30s. Loss: 0.9280. :  67%|██████▋   | 2/3 [00:01<00:00,  1.65it/s]Finetune Epoch: 69/70. Data: 1.24s. Batch: 1.30s. Loss: 0.9280. : 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 69/70. Data: 1.24s. Batch: 1.30s. Loss: 0.9280. : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 1.0382. top1: 84.38. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 1.0382. top1: 84.38. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.0261. top1: 85.35. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.34it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 1.0261. top1: 85.35. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0176. top1: 86.46. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:03,  1.95it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.0176. top1: 86.46. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:02,  2.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0057. top1: 86.82. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 1.0057. top1: 86.82. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.63it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9833. top1: 87.81. top5: 99.77. :  50%|█████     | 4/8 [00:02<00:01,  2.63it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.9833. top1: 87.81. top5: 99.77. :  62%|██████▎   | 5/8 [00:02<00:01,  2.80it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9677. top1: 88.80. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.80it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9677. top1: 88.80. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  2.82it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9506. top1: 89.79. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  2.82it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9506. top1: 89.79. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9414. top1: 90.30. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9414. top1: 90.30. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9414. top1: 90.30. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.47it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.81s. Batch: 0.86s. Loss: 0.8955. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.81s. Batch: 0.86s. Loss: 0.8955. :  33%|███▎      | 1/3 [00:00<00:01,  1.16it/s]Finetune Epoch: 70/70. Data: 1.03s. Batch: 1.09s. Loss: 0.9298. :  33%|███▎      | 1/3 [00:01<00:01,  1.16it/s]Finetune Epoch: 70/70. Data: 1.03s. Batch: 1.09s. Loss: 0.9298. :  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s]Finetune Epoch: 70/70. Data: 1.29s. Batch: 1.34s. Loss: 0.9183. :  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s]Finetune Epoch: 70/70. Data: 1.29s. Batch: 1.34s. Loss: 0.9183. : 100%|██████████| 3/3 [00:01<00:00,  1.72it/s]Finetune Epoch: 70/70. Data: 1.29s. Batch: 1.34s. Loss: 0.9183. : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.0375. top1: 84.38. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 1.0375. top1: 84.38. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.56it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0253. top1: 85.35. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.56it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 1.0253. top1: 85.35. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0168. top1: 86.46. top5: 99.87. :  25%|██▌       | 2/8 [00:01<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0168. top1: 86.46. top5: 99.87. :  38%|███▊      | 3/8 [00:01<00:01,  2.83it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0050. top1: 86.82. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.83it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0050. top1: 86.82. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9827. top1: 87.73. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9827. top1: 87.73. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9672. top1: 88.74. top5: 99.80. :  62%|██████▎   | 5/8 [00:02<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9672. top1: 88.74. top5: 99.80. :  75%|███████▌  | 6/8 [00:02<00:00,  3.27it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9501. top1: 89.73. top5: 99.83. :  75%|███████▌  | 6/8 [00:02<00:00,  3.27it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9501. top1: 89.73. top5: 99.83. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9409. top1: 90.25. top5: 99.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9409. top1: 90.25. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9409. top1: 90.25. top5: 99.85. : 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
total 9984 correct 8132 accuracy 81.45032051282051
[INFO] main.py:349 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 10 -> 10
[INFO] main.py:357 > [2-3] Start to train under online
[INFO] main.py:372 > Train over streamed data once
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42 pseudo_stream_size 42
num_stuff 237
[INFO] rainbow_memory.py:120 > Streamed samples: 800
[INFO] rainbow_memory.py:121 > In-memory samples: 500
[INFO] rainbow_memory.py:122 > Pseudo samples: 9984
[INFO] rainbow_memory.py:128 > Train samples: 11284
[INFO] rainbow_memory.py:129 > Test samples: 8000
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([38, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
last_idx 17
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
final_idx 237
task3/train/loss 0.5632104777237948 0
task3/test/loss 6.9079158789590815 0
task3/test/acc 0.233375 0
task3/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 1/1 | train_loss 0.5632 | train_acc 0.7159 | test_loss 6.9079 | test_acc 0.2334 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[INFO] finetune.py:679 > Compute uncertainty by vr_randaug!
[WARNING] finetune.py:639 > Fill the unused slots by breaking the equilibrium.
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
airplane      97
horse         78
frog          60
truck         59
bird          54
dog           51
automobile    51
deer          50
Name: klass, dtype: int64
[INFO] main.py:388 > Train over memory
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42 pseudo_stream_size 42
num_stuff 0
[INFO] rainbow_memory.py:120 > Streamed samples: 0
[INFO] rainbow_memory.py:121 > In-memory samples: 500
[INFO] rainbow_memory.py:122 > Pseudo samples: 0
[INFO] rainbow_memory.py:128 > Train samples: 500
[INFO] rainbow_memory.py:129 > Test samples: 8000
last_idx 11
final_idx 0
task3/train/loss 3.60877925157547 0
task3/test/loss 2.40489763709215 0
task3/test/acc 0.22675 0
task3/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 1/256 | train_loss 3.6088 | train_acc 0.2980 | test_loss 2.4049 | test_acc 0.2268 | lr 0.0050
last_idx 11
final_idx 0
task3/train/loss 2.6289943555990853 1
task3/test/loss 4.713474453478069 1
task3/test/acc 0.318 1
task3/train/lr 0.05 1
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 2/256 | train_loss 2.6290 | train_acc 0.3800 | test_loss 4.7135 | test_acc 0.3180 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 2.2098066012064614 2
task3/test/loss 2.0429395076992747 2
task3/test/acc 0.428375 2
task3/train/lr 0.05 2
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 3/256 | train_loss 2.2098 | train_acc 0.4100 | test_loss 2.0429 | test_acc 0.4284 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 1.9906574189662933 3
task3/test/loss 1.7235788506465954 3
task3/test/acc 0.464625 3
task3/train/lr 0.02525 3
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 4/256 | train_loss 1.9907 | train_acc 0.4520 | test_loss 1.7236 | test_acc 0.4646 | lr 0.0253
last_idx 11
final_idx 0
task3/train/loss 1.6418342689673107 4
task3/test/loss 1.4784308822600396 4
task3/test/acc 0.513 4
task3/train/lr 0.05 4
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 5/256 | train_loss 1.6418 | train_acc 0.5120 | test_loss 1.4784 | test_acc 0.5130 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 1.5115526020526886 5
task3/test/loss 1.2614786258110633 5
task3/test/acc 0.58075 5
task3/train/lr 0.04275089283436705 5
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 6/256 | train_loss 1.5116 | train_acc 0.4600 | test_loss 1.2615 | test_acc 0.5807 | lr 0.0428
last_idx 11
final_idx 0
task3/train/loss 1.3654521306355794 6
task3/test/loss 1.2406333742233424 6
task3/test/acc 0.58525 6
task3/train/lr 0.02525 6
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 7/256 | train_loss 1.3655 | train_acc 0.5360 | test_loss 1.2406 | test_acc 0.5853 | lr 0.0253
last_idx 11
final_idx 0
task3/train/loss 1.1072928756475449 7
task3/test/loss 1.2808005346374198 7
task3/test/acc 0.577125 7
task3/train/lr 0.00774910716563295 7
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 8/256 | train_loss 1.1073 | train_acc 0.6320 | test_loss 1.2808 | test_acc 0.5771 | lr 0.0077
last_idx 11
final_idx 0
task3/train/loss 1.3377009083827336 8
task3/test/loss 1.418769054196693 8
task3/test/acc 0.526875 8
task3/train/lr 0.05 8
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 9/256 | train_loss 1.3377 | train_acc 0.5820 | test_loss 1.4188 | test_acc 0.5269 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 1.150447557369868 9
task3/test/loss 1.2573553762593113 9
task3/test/acc 0.57125 9
task3/train/lr 0.04811601842965435 9
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 10/256 | train_loss 1.1504 | train_acc 0.6540 | test_loss 1.2574 | test_acc 0.5713 | lr 0.0481
last_idx 11
final_idx 0
task3/train/loss 1.3711032569408417 10
task3/test/loss 1.3754063153659903 10
task3/test/acc 0.542375 10
task3/train/lr 0.04275089283436705 10
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 11/256 | train_loss 1.3711 | train_acc 0.5820 | test_loss 1.3754 | test_acc 0.5424 | lr 0.0428
last_idx 11
final_idx 0
task3/train/loss 1.1935837765534718 11
task3/test/loss 1.2369606580053056 11
task3/test/acc 0.58175 11
task3/train/lr 0.03472141495103598 11
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 12/256 | train_loss 1.1936 | train_acc 0.6020 | test_loss 1.2370 | test_acc 0.5817 | lr 0.0347
last_idx 11
final_idx 0
task3/train/loss 1.2688768307367961 12
task3/test/loss 1.1590273488353897 12
task3/test/acc 0.600125 12
task3/train/lr 0.02525 12
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 13/256 | train_loss 1.2689 | train_acc 0.5460 | test_loss 1.1590 | test_acc 0.6001 | lr 0.0253
last_idx 11
final_idx 0
task3/train/loss 0.9090508123238882 13
task3/test/loss 1.2886082636458533 13
task3/test/acc 0.56625 13
task3/train/lr 0.01577858504896403 13
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 14/256 | train_loss 0.9091 | train_acc 0.7260 | test_loss 1.2886 | test_acc 0.5663 | lr 0.0158
last_idx 11
final_idx 0
task3/train/loss 0.9530380914608637 14
task3/test/loss 1.166047861287882 14
task3/test/acc 0.614375 14
task3/train/lr 0.00774910716563295 14
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 15/256 | train_loss 0.9530 | train_acc 0.6900 | test_loss 1.1660 | test_acc 0.6144 | lr 0.0077
last_idx 11
final_idx 0
task3/train/loss 0.9167924299836159 15
task3/test/loss 1.094346128322266 15
task3/test/acc 0.633 15
task3/train/lr 0.0023839815703456534 15
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 16/256 | train_loss 0.9168 | train_acc 0.6760 | test_loss 1.0943 | test_acc 0.6330 | lr 0.0024
last_idx 11
final_idx 0
task3/train/loss 1.1955623875061672 16
task3/test/loss 1.2112730160191818 16
task3/test/acc 0.592875 16
task3/train/lr 0.05 16
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 17/256 | train_loss 1.1956 | train_acc 0.6260 | test_loss 1.2113 | test_acc 0.5929 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 1.1491114497184753 17
task3/test/loss 2.0377733691053077 17
task3/test/acc 0.430625 17
task3/train/lr 0.049524435689979954 17
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 18/256 | train_loss 1.1491 | train_acc 0.5940 | test_loss 2.0378 | test_acc 0.4306 | lr 0.0495
last_idx 11
final_idx 0
task3/train/loss 1.3047038167715073 18
task3/test/loss 1.2473357586742757 18
task3/test/acc 0.564125 18
task3/train/lr 0.04811601842965435 18
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 19/256 | train_loss 1.3047 | train_acc 0.6000 | test_loss 1.2473 | test_acc 0.5641 | lr 0.0481
last_idx 11
final_idx 0
task3/train/loss 1.1854224900404613 19
task3/test/loss 1.4958075813867233 19
task3/test/acc 0.501125 19
task3/train/lr 0.045828872904488 19
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 20/256 | train_loss 1.1854 | train_acc 0.6300 | test_loss 1.4958 | test_acc 0.5011 | lr 0.0458
last_idx 11
final_idx 0
task3/train/loss 1.2163978765408199 20
task3/test/loss 1.4923517021995325 20
task3/test/acc 0.5215 20
task3/train/lr 0.04275089283436705 20
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 21/256 | train_loss 1.2164 | train_acc 0.5900 | test_loss 1.4924 | test_acc 0.5215 | lr 0.0428
last_idx 11
final_idx 0
task3/train/loss 0.875837246576945 21
task3/test/loss 1.7377285886924345 21
task3/test/acc 0.47275 21
task3/train/lr 0.039000363267235154 21
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 22/256 | train_loss 0.8758 | train_acc 0.7120 | test_loss 1.7377 | test_acc 0.4728 | lr 0.0390
last_idx 11
final_idx 0
task3/train/loss 1.1774597018957138 22
task3/test/loss 1.1717443687247706 22
task3/test/acc 0.593625 22
task3/train/lr 0.03472141495103598 22
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 23/256 | train_loss 1.1775 | train_acc 0.6380 | test_loss 1.1717 | test_acc 0.5936 | lr 0.0347
last_idx 11
final_idx 0
task3/train/loss 0.9777474850416183 23
task3/test/loss 1.0997933527598014 23
task3/test/acc 0.627125 23
task3/train/lr 0.03007848546989918 23
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 24/256 | train_loss 0.9777 | train_acc 0.6980 | test_loss 1.0998 | test_acc 0.6271 | lr 0.0301
last_idx 11
final_idx 0
task3/train/loss 0.9120587011178335 24
task3/test/loss 1.1148345734064395 24
task3/test/acc 0.63 24
task3/train/lr 0.02525 24
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 25/256 | train_loss 0.9121 | train_acc 0.6960 | test_loss 1.1148 | test_acc 0.6300 | lr 0.0253
last_idx 11
final_idx 0
task3/train/loss 0.8590992937485377 25
task3/test/loss 1.3211860545388945 25
task3/test/acc 0.5815 25
task3/train/lr 0.02042151453010083 25
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 26/256 | train_loss 0.8591 | train_acc 0.7580 | test_loss 1.3212 | test_acc 0.5815 | lr 0.0204
last_idx 11
final_idx 0
task3/train/loss 0.8617027426759402 26
task3/test/loss 1.1545507133334547 26
task3/test/acc 0.61775 26
task3/train/lr 0.01577858504896403 26
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 27/256 | train_loss 0.8617 | train_acc 0.7360 | test_loss 1.1546 | test_acc 0.6178 | lr 0.0158
last_idx 11
final_idx 0
task3/train/loss 0.9219314356644949 27
task3/test/loss 1.054153336600943 27
task3/test/acc 0.653625 27
task3/train/lr 0.011499636732764853 27
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 28/256 | train_loss 0.9219 | train_acc 0.7180 | test_loss 1.0542 | test_acc 0.6536 | lr 0.0115
last_idx 11
final_idx 0
task3/train/loss 0.9951948250333468 28
task3/test/loss 1.052612677871526 28
task3/test/acc 0.65225 28
task3/train/lr 0.00774910716563295 28
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 29/256 | train_loss 0.9952 | train_acc 0.6800 | test_loss 1.0526 | test_acc 0.6522 | lr 0.0077
last_idx 11
final_idx 0
task3/train/loss 0.6819152968625227 29
task3/test/loss 1.0090929720100466 29
task3/test/acc 0.66375 29
task3/train/lr 0.004671127095512003 29
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 30/256 | train_loss 0.6819 | train_acc 0.7920 | test_loss 1.0091 | test_acc 0.6637 | lr 0.0047
last_idx 11
final_idx 0
task3/train/loss 0.8467267453670502 30
task3/test/loss 0.9939159249866402 30
task3/test/acc 0.667625 30
task3/train/lr 0.0023839815703456534 30
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 31/256 | train_loss 0.8467 | train_acc 0.7280 | test_loss 0.9939 | test_acc 0.6676 | lr 0.0024
last_idx 11
final_idx 0
task3/train/loss 0.8395969246824583 31
task3/test/loss 1.0182330729542197 31
task3/test/acc 0.6615 31
task3/train/lr 0.0009755643100200469 31
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 32/256 | train_loss 0.8396 | train_acc 0.7580 | test_loss 1.0182 | test_acc 0.6615 | lr 0.0010
last_idx 11
final_idx 0
task3/train/loss 0.8089361737171809 32
task3/test/loss 1.4673286650207016 32
task3/test/acc 0.570625 32
task3/train/lr 0.05 32
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 33/256 | train_loss 0.8089 | train_acc 0.7380 | test_loss 1.4673 | test_acc 0.5706 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 0.8550740852952003 33
task3/test/loss 1.4858490766568497 33
task3/test/acc 0.569625 33
task3/train/lr 0.049880821985136874 33
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 34/256 | train_loss 0.8551 | train_acc 0.7100 | test_loss 1.4858 | test_acc 0.5696 | lr 0.0499
last_idx 11
final_idx 0
task3/train/loss 1.2257509802778561 34
task3/test/loss 1.3231184521874229 34
task3/test/acc 0.578875 34
task3/train/lr 0.049524435689979954 34
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 35/256 | train_loss 1.2258 | train_acc 0.5820 | test_loss 1.3231 | test_acc 0.5789 | lr 0.0495
last_idx 11
final_idx 0
task3/train/loss 1.1226724485556285 35
task3/test/loss 1.3521849147074825 35
task3/test/acc 0.560875 35
task3/train/lr 0.048934273309372174 35
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 36/256 | train_loss 1.1227 | train_acc 0.6020 | test_loss 1.3522 | test_acc 0.5609 | lr 0.0489
last_idx 11
final_idx 0
task3/train/loss 1.3005936741828918 36
task3/test/loss 1.1466164394066884 36
task3/test/acc 0.599375 36
task3/train/lr 0.04811601842965435 36
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 37/256 | train_loss 1.3006 | train_acc 0.6080 | test_loss 1.1466 | test_acc 0.5994 | lr 0.0481
last_idx 11
final_idx 0
task3/train/loss 1.1831810176372528 37
task3/test/loss 1.6550594653402055 37
task3/test/acc 0.481125 37
task3/train/lr 0.04707755129262179 37
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 38/256 | train_loss 1.1832 | train_acc 0.6140 | test_loss 1.6551 | test_acc 0.4811 | lr 0.0471
last_idx 11
final_idx 0
task3/train/loss 0.9641385252277056 38
task3/test/loss 1.1082858726546005 38
task3/test/acc 0.624875 38
task3/train/lr 0.045828872904488 38
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 39/256 | train_loss 0.9641 | train_acc 0.7120 | test_loss 1.1083 | test_acc 0.6249 | lr 0.0458
last_idx 11
final_idx 0
task3/train/loss 0.875714917977651 39
task3/test/loss 1.442299985541747 39
task3/test/acc 0.58625 39
task3/train/lr 0.04438200872072774 39
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 40/256 | train_loss 0.8757 | train_acc 0.7360 | test_loss 1.4423 | test_acc 0.5863 | lr 0.0444
last_idx 11
final_idx 0
task3/train/loss 1.152937412261963 40
task3/test/loss 1.1608564195397135 40
task3/test/acc 0.61625 40
task3/train/lr 0.04275089283436705 40
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 41/256 | train_loss 1.1529 | train_acc 0.6300 | test_loss 1.1609 | test_acc 0.6162 | lr 0.0428
last_idx 11
final_idx 0
task3/train/loss 0.7861999347805977 41
task3/test/loss 1.2613997032436042 41
task3/test/acc 0.59525 41
task3/train/lr 0.040951233783050225 41
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 42/256 | train_loss 0.7862 | train_acc 0.7960 | test_loss 1.2614 | test_acc 0.5952 | lr 0.0410
last_idx 11
final_idx 0
task3/train/loss 0.7934990922609965 42
task3/test/loss 1.5331657044180147 42
task3/test/acc 0.562125 42
task3/train/lr 0.039000363267235154 42
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 43/256 | train_loss 0.7935 | train_acc 0.7780 | test_loss 1.5332 | test_acc 0.5621 | lr 0.0390
last_idx 11
final_idx 0
task3/train/loss 1.208611475924651 43
task3/test/loss 1.1564270978445534 43
task3/test/acc 0.62075 43
task3/train/lr 0.03691706923644345 43
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 44/256 | train_loss 1.2086 | train_acc 0.6060 | test_loss 1.1564 | test_acc 0.6208 | lr 0.0369
last_idx 11
final_idx 0
task3/train/loss 0.9787177890539169 44
task3/test/loss 1.1435675398334042 44
task3/test/acc 0.61 44
task3/train/lr 0.03472141495103598 44
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 45/256 | train_loss 0.9787 | train_acc 0.7080 | test_loss 1.1436 | test_acc 0.6100 | lr 0.0347
last_idx 11
final_idx 0
task3/train/loss 0.9604476739962896 45
task3/test/loss 1.280331328019991 45
task3/test/acc 0.588 45
task3/train/lr 0.03243454576204794 45
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 46/256 | train_loss 0.9604 | train_acc 0.6700 | test_loss 1.2803 | test_acc 0.5880 | lr 0.0324
last_idx 11
final_idx 0
task3/train/loss 0.8901118089755377 46
task3/test/loss 1.299099247943569 46
task3/test/acc 0.58425 46
task3/train/lr 0.03007848546989918 46
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 47/256 | train_loss 0.8901 | train_acc 0.7380 | test_loss 1.2991 | test_acc 0.5843 | lr 0.0301
last_idx 11
final_idx 0
task3/train/loss 0.7897719318668047 47
task3/test/loss 1.153526165000685 47
task3/test/acc 0.621875 47
task3/train/lr 0.027675924223156633 47
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 48/256 | train_loss 0.7898 | train_acc 0.7880 | test_loss 1.1535 | test_acc 0.6219 | lr 0.0277
last_idx 11
final_idx 0
task3/train/loss 0.7551233197251955 48
task3/test/loss 1.1956308904912445 48
task3/test/acc 0.623625 48
task3/train/lr 0.02525 48
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 49/256 | train_loss 0.7551 | train_acc 0.8200 | test_loss 1.1956 | test_acc 0.6236 | lr 0.0253
last_idx 11
final_idx 0
task3/train/loss 0.6413315087556839 49
task3/test/loss 1.0983816612880308 49
task3/test/acc 0.648125 49
task3/train/lr 0.022824075776843374 49
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 50/256 | train_loss 0.6413 | train_acc 0.8020 | test_loss 1.0984 | test_acc 0.6481 | lr 0.0228
last_idx 11
final_idx 0
task3/train/loss 1.1690696527560551 50
task3/test/loss 1.3029141209118968 50
task3/test/acc 0.588625 50
task3/train/lr 0.02042151453010083 50
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 51/256 | train_loss 1.1691 | train_acc 0.6420 | test_loss 1.3029 | test_acc 0.5886 | lr 0.0204
last_idx 11
final_idx 0
task3/train/loss 1.1706805527210236 51
task3/test/loss 1.2094213048835376 51
task3/test/acc 0.607625 51
task3/train/lr 0.018065454237952062 51
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 52/256 | train_loss 1.1707 | train_acc 0.5880 | test_loss 1.2094 | test_acc 0.6076 | lr 0.0181
last_idx 11
final_idx 0
task3/train/loss 0.8320519129435221 52
task3/test/loss 1.0859467659022781 52
task3/test/acc 0.6485 52
task3/train/lr 0.01577858504896403 52
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 53/256 | train_loss 0.8321 | train_acc 0.7820 | test_loss 1.0859 | test_acc 0.6485 | lr 0.0158
last_idx 11
final_idx 0
task3/train/loss 0.7146251226464907 53
task3/test/loss 1.0710282014621484 53
task3/test/acc 0.655875 53
task3/train/lr 0.013582930763556558 53
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 54/256 | train_loss 0.7146 | train_acc 0.7980 | test_loss 1.0710 | test_acc 0.6559 | lr 0.0136
last_idx 11
final_idx 0
task3/train/loss 0.8779679611325264 54
task3/test/loss 1.0374502458578938 54
task3/test/acc 0.660125 54
task3/train/lr 0.011499636732764853 54
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 55/256 | train_loss 0.8780 | train_acc 0.7740 | test_loss 1.0375 | test_acc 0.6601 | lr 0.0115
last_idx 11
final_idx 0
task3/train/loss 0.7086110798021158 55
task3/test/loss 1.034230031809964 55
task3/test/acc 0.6655 55
task3/train/lr 0.009548766216949778 55
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 56/256 | train_loss 0.7086 | train_acc 0.7960 | test_loss 1.0342 | test_acc 0.6655 | lr 0.0095
last_idx 11
final_idx 0
task3/train/loss 0.9486246456702551 56
task3/test/loss 1.0217380962529026 56
task3/test/acc 0.668875 56
task3/train/lr 0.00774910716563295 56
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 57/256 | train_loss 0.9486 | train_acc 0.7080 | test_loss 1.0217 | test_acc 0.6689 | lr 0.0077
last_idx 11
final_idx 0
task3/train/loss 0.8502248624960581 57
task3/test/loss 1.0036821276931973 57
task3/test/acc 0.672625 57
task3/train/lr 0.0061179912792722595 57
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 58/256 | train_loss 0.8502 | train_acc 0.7500 | test_loss 1.0037 | test_acc 0.6726 | lr 0.0061
last_idx 11
final_idx 0
task3/train/loss 0.9083741493523121 58
task3/test/loss 1.0310281870456843 58
task3/test/acc 0.666375 58
task3/train/lr 0.004671127095512003 58
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 59/256 | train_loss 0.9084 | train_acc 0.7420 | test_loss 1.0310 | test_acc 0.6664 | lr 0.0047
last_idx 11
final_idx 0
task3/train/loss 0.7862884514033794 59
task3/test/loss 1.0306866493198898 59
task3/test/acc 0.665375 59
task3/train/lr 0.0034224487073782153 59
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 60/256 | train_loss 0.7863 | train_acc 0.7920 | test_loss 1.0307 | test_acc 0.6654 | lr 0.0034
last_idx 11
final_idx 0
task3/train/loss 0.8800973743200302 60
task3/test/loss 1.0105544061778666 60
task3/test/acc 0.672625 60
task3/train/lr 0.0023839815703456534 60
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 61/256 | train_loss 0.8801 | train_acc 0.7340 | test_loss 1.0106 | test_acc 0.6726 | lr 0.0024
last_idx 11
final_idx 0
task3/train/loss 0.8823662276069323 61
task3/test/loss 0.9948527593534071 61
task3/test/acc 0.672125 61
task3/train/lr 0.0015657266906278318 61
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 62/256 | train_loss 0.8824 | train_acc 0.7560 | test_loss 0.9949 | test_acc 0.6721 | lr 0.0016
last_idx 11
final_idx 0
task3/train/loss 0.8155785401662191 62
task3/test/loss 0.9963470684303032 62
task3/test/acc 0.67125 62
task3/train/lr 0.0009755643100200469 62
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 63/256 | train_loss 0.8156 | train_acc 0.7780 | test_loss 0.9963 | test_acc 0.6713 | lr 0.0010
last_idx 11
final_idx 0
task3/train/loss 0.8006114661693573 63
task3/test/loss 0.9952670306294829 63
task3/test/acc 0.67475 63
task3/train/lr 0.0006191780148631288 63
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 64/256 | train_loss 0.8006 | train_acc 0.7800 | test_loss 0.9953 | test_acc 0.6747 | lr 0.0006
last_idx 11
final_idx 0
task3/train/loss 0.8891480614741644 64
task3/test/loss 1.5156219045703228 64
task3/test/acc 0.552 64
task3/train/lr 0.05 64
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 65/256 | train_loss 0.8891 | train_acc 0.7360 | test_loss 1.5156 | test_acc 0.5520 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 0.9424735854069392 65
task3/test/loss 1.7788466922529451 65
task3/test/acc 0.47725 65
task3/train/lr 0.04997018754107802 65
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 66/256 | train_loss 0.9425 | train_acc 0.6800 | test_loss 1.7788 | test_acc 0.4773 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 1.0717044373353322 66
task3/test/loss 1.5234423748903223 66
task3/test/acc 0.53 66
task3/train/lr 0.049880821985136874 66
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 67/256 | train_loss 1.0717 | train_acc 0.7000 | test_loss 1.5234 | test_acc 0.5300 | lr 0.0499
last_idx 11
final_idx 0
task3/train/loss 0.8318801547090212 67
task3/test/loss 1.2569564856015718 67
task3/test/acc 0.600875 67
task3/train/lr 0.04973211862162834 67
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 68/256 | train_loss 0.8319 | train_acc 0.7780 | test_loss 1.2570 | test_acc 0.6009 | lr 0.0497
last_idx 11
final_idx 0
task3/train/loss 0.8811769088109335 68
task3/test/loss 1.2796928862948993 68
task3/test/acc 0.599875 68
task3/train/lr 0.049524435689979954 68
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 69/256 | train_loss 0.8812 | train_acc 0.7660 | test_loss 1.2797 | test_acc 0.5999 | lr 0.0495
last_idx 11
final_idx 0
task3/train/loss 0.9236245428522428 69
task3/test/loss 1.4648770315797774 69
task3/test/acc 0.549625 69
task3/train/lr 0.04925827351656497 69
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 70/256 | train_loss 0.9236 | train_acc 0.7540 | test_loss 1.4649 | test_acc 0.5496 | lr 0.0493
last_idx 11
final_idx 0
task3/train/loss 0.8759893601139387 70
task3/test/loss 1.2651246136852674 70
task3/test/acc 0.59275 70
task3/train/lr 0.048934273309372174 70
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 71/256 | train_loss 0.8760 | train_acc 0.7520 | test_loss 1.2651 | test_acc 0.5927 | lr 0.0489
last_idx 11
final_idx 0
task3/train/loss 0.8724626749753952 71
task3/test/loss 1.1437147457848538 71
task3/test/acc 0.612125 71
task3/train/lr 0.048553215613279764 71
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 72/256 | train_loss 0.8725 | train_acc 0.7140 | test_loss 1.1437 | test_acc 0.6121 | lr 0.0486
last_idx 11
final_idx 0
task3/train/loss 0.8131465266148249 72
task3/test/loss 1.1805133519919364 72
task3/test/acc 0.615875 72
task3/train/lr 0.04811601842965435 72
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 73/256 | train_loss 0.8131 | train_acc 0.7340 | test_loss 1.1805 | test_acc 0.6159 | lr 0.0481
last_idx 11
final_idx 0
task3/train/loss 1.0677251865466435 73
task3/test/loss 1.705036338705283 73
task3/test/acc 0.5125 73
task3/train/lr 0.047623735004805226 73
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 74/256 | train_loss 1.0677 | train_acc 0.6640 | test_loss 1.7050 | test_acc 0.5125 | lr 0.0476
last_idx 11
final_idx 0
task3/train/loss 0.8302530298630396 74
task3/test/loss 1.239285606604356 74
task3/test/acc 0.6055 74
task3/train/lr 0.04707755129262179 74
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 75/256 | train_loss 0.8303 | train_acc 0.7560 | test_loss 1.2393 | test_acc 0.6055 | lr 0.0471
last_idx 11
final_idx 0
task3/train/loss 0.8399138152599335 75
task3/test/loss 1.2195970570811858 75
task3/test/acc 0.609125 75
task3/train/lr 0.046478783097506735 75
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 76/256 | train_loss 0.8399 | train_acc 0.7280 | test_loss 1.2196 | test_acc 0.6091 | lr 0.0465
last_idx 11
final_idx 0
task3/train/loss 1.198595220843951 76
task3/test/loss 1.1009187963637677 76
task3/test/acc 0.63275 76
task3/train/lr 0.045828872904488 76
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 77/256 | train_loss 1.1986 | train_acc 0.5800 | test_loss 1.1009 | test_acc 0.6328 | lr 0.0458
last_idx 11
final_idx 0
task3/train/loss 0.9267077172795931 77
task3/test/loss 1.2053777629530036 77
task3/test/acc 0.602125 77
task3/train/lr 0.04512938640414596 77
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 78/256 | train_loss 0.9267 | train_acc 0.6520 | test_loss 1.2054 | test_acc 0.6021 | lr 0.0451
last_idx 11
final_idx 0
task3/train/loss 0.8625070949395498 78
task3/test/loss 1.6002906131875383 78
task3/test/acc 0.559375 78
task3/train/lr 0.04438200872072774 78
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 79/256 | train_loss 0.8625 | train_acc 0.7520 | test_loss 1.6003 | test_acc 0.5594 | lr 0.0444
last_idx 11
final_idx 0
task3/train/loss 0.8571390385429064 79
task3/test/loss 1.1149213991977356 79
task3/test/acc 0.637125 79
task3/train/lr 0.043588540352535246 79
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 80/256 | train_loss 0.8571 | train_acc 0.7600 | test_loss 1.1149 | test_acc 0.6371 | lr 0.0436
last_idx 11
final_idx 0
task3/train/loss 1.0218449582656224 80
task3/test/loss 1.2886679801312122 80
task3/test/acc 0.570125 80
task3/train/lr 0.04275089283436705 80
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 81/256 | train_loss 1.0218 | train_acc 0.6360 | test_loss 1.2887 | test_acc 0.5701 | lr 0.0428
last_idx 11
final_idx 0
task3/train/loss 1.0710971901814144 81
task3/test/loss 1.013247469296822 81
task3/test/acc 0.66125 81
task3/train/lr 0.04187108413246371 81
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 82/256 | train_loss 1.0711 | train_acc 0.6780 | test_loss 1.0132 | test_acc 0.6613 | lr 0.0419
last_idx 11
final_idx 0
task3/train/loss 0.8680220892031988 82
task3/test/loss 1.160404044356975 82
task3/test/acc 0.620375 82
task3/train/lr 0.040951233783050225 82
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 83/256 | train_loss 0.8680 | train_acc 0.7800 | test_loss 1.1604 | test_acc 0.6204 | lr 0.0410
last_idx 11
final_idx 0
task3/train/loss 0.8668400831520557 83
task3/test/loss 1.108350056213337 83
task3/test/acc 0.639375 83
task3/train/lr 0.03999355778618773 83
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 84/256 | train_loss 0.8668 | train_acc 0.7620 | test_loss 1.1084 | test_acc 0.6394 | lr 0.0400
last_idx 11
final_idx 0
task3/train/loss 0.8200036262472471 84
task3/test/loss 1.633862745139625 84
task3/test/acc 0.51625 84
task3/train/lr 0.039000363267235154 84
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 85/256 | train_loss 0.8200 | train_acc 0.7560 | test_loss 1.6339 | test_acc 0.5162 | lr 0.0390
last_idx 11
final_idx 0
task3/train/loss 0.8213633547226588 85
task3/test/loss 1.1560494213641346 85
task3/test/acc 0.630875 85
task3/train/lr 0.03797404291878224 85
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 86/256 | train_loss 0.8214 | train_acc 0.7060 | test_loss 1.1560 | test_acc 0.6309 | lr 0.0380
last_idx 11
final_idx 0
task3/train/loss 0.7151216939091682 86
task3/test/loss 1.2432051345214739 86
task3/test/acc 0.60225 86
task3/train/lr 0.03691706923644345 86
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 87/256 | train_loss 0.7151 | train_acc 0.8240 | test_loss 1.2432 | test_acc 0.6022 | lr 0.0369
last_idx 11
final_idx 0
task3/train/loss 0.829254113137722 87
task3/test/loss 1.3393849176201191 87
task3/test/acc 0.59025 87
task3/train/lr 0.03583198856239948 87
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 88/256 | train_loss 0.8293 | train_acc 0.7600 | test_loss 1.3394 | test_acc 0.5903 | lr 0.0358
last_idx 11
final_idx 0
task3/train/loss 0.614470649510622 88
task3/test/loss 1.3431788095078625 88
task3/test/acc 0.598875 88
task3/train/lr 0.03472141495103598 88
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 89/256 | train_loss 0.6145 | train_acc 0.8300 | test_loss 1.3432 | test_acc 0.5989 | lr 0.0347
last_idx 11
final_idx 0
task3/train/loss 0.9119383990764618 89
task3/test/loss 1.6439365691372327 89
task3/test/acc 0.53825 89
task3/train/lr 0.03358802387145745 89
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 90/256 | train_loss 0.9119 | train_acc 0.7480 | test_loss 1.6439 | test_acc 0.5383 | lr 0.0336
last_idx 11
final_idx 0
task3/train/loss 1.107825753589471 90
task3/test/loss 1.0578919827610582 90
task3/test/acc 0.649125 90
task3/train/lr 0.03243454576204794 90
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 91/256 | train_loss 1.1078 | train_acc 0.6560 | test_loss 1.0579 | test_acc 0.6491 | lr 0.0324
last_idx 11
final_idx 0
task3/train/loss 0.9834466626246771 91
task3/test/loss 1.1603407698509458 91
task3/test/acc 0.60775 91
task3/train/lr 0.03126375945260579 91
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 92/256 | train_loss 0.9834 | train_acc 0.6820 | test_loss 1.1603 | test_acc 0.6078 | lr 0.0313
last_idx 11
final_idx 0
task3/train/loss 0.6370985197524229 92
task3/test/loss 1.0071611538693146 92
task3/test/acc 0.665 92
task3/train/lr 0.03007848546989918 92
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 93/256 | train_loss 0.6371 | train_acc 0.7840 | test_loss 1.0072 | test_acc 0.6650 | lr 0.0301
last_idx 11
final_idx 0
task3/train/loss 0.7006609415014585 93
task3/test/loss 1.0993004557523098 93
task3/test/acc 0.651875 93
task3/train/lr 0.028881579242770204 93
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 94/256 | train_loss 0.7007 | train_acc 0.8160 | test_loss 1.0993 | test_acc 0.6519 | lr 0.0289
last_idx 11
final_idx 0
task3/train/loss 0.7514638726909956 94
task3/test/loss 1.1604657135494463 94
task3/test/acc 0.631125 94
task3/train/lr 0.027675924223156633 94
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 95/256 | train_loss 0.7515 | train_acc 0.7960 | test_loss 1.1605 | test_acc 0.6311 | lr 0.0277
last_idx 11
final_idx 0
task3/train/loss 0.5051019787788391 95
task3/test/loss 1.2104512729815073 95
task3/test/acc 0.61775 95
task3/train/lr 0.0264644249396036 95
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 96/256 | train_loss 0.5051 | train_acc 0.8820 | test_loss 1.2105 | test_acc 0.6178 | lr 0.0265
last_idx 11
final_idx 0
task3/train/loss 0.8250739028056463 96
task3/test/loss 1.336874524806882 96
task3/test/acc 0.591625 96
task3/train/lr 0.02525 96
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 97/256 | train_loss 0.8251 | train_acc 0.6960 | test_loss 1.3369 | test_acc 0.5916 | lr 0.0253
last_idx 11
final_idx 0
task3/train/loss 0.8632627129554749 97
task3/test/loss 1.141952789091802 97
task3/test/acc 0.633 97
task3/train/lr 0.024035575060396407 97
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 98/256 | train_loss 0.8633 | train_acc 0.7880 | test_loss 1.1420 | test_acc 0.6330 | lr 0.0240
last_idx 11
final_idx 0
task3/train/loss 0.8661562638978163 98
task3/test/loss 1.0512014440782778 98
task3/test/acc 0.656125 98
task3/train/lr 0.022824075776843374 98
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 99/256 | train_loss 0.8662 | train_acc 0.8020 | test_loss 1.0512 | test_acc 0.6561 | lr 0.0228
last_idx 11
final_idx 0
task3/train/loss 0.715861457089583 99
task3/test/loss 1.0860169872150316 99
task3/test/acc 0.6405 99
task3/train/lr 0.0216184207572298 99
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 100/256 | train_loss 0.7159 | train_acc 0.8620 | test_loss 1.0860 | test_acc 0.6405 | lr 0.0216
last_idx 11
final_idx 0
task3/train/loss 0.5860246444741884 100
task3/test/loss 1.046434444467445 100
task3/test/acc 0.657375 100
task3/train/lr 0.02042151453010083 100
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 101/256 | train_loss 0.5860 | train_acc 0.8420 | test_loss 1.0464 | test_acc 0.6574 | lr 0.0204
last_idx 11
final_idx 0
task3/train/loss 0.8334762925903002 101
task3/test/loss 1.0428606196419223 101
task3/test/acc 0.6595 101
task3/train/lr 0.019236240547394222 101
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 102/256 | train_loss 0.8335 | train_acc 0.7740 | test_loss 1.0429 | test_acc 0.6595 | lr 0.0192
last_idx 11
final_idx 0
task3/train/loss 0.7065714014073213 102
task3/test/loss 1.0770808439497108 102
task3/test/acc 0.64675 102
task3/train/lr 0.018065454237952062 102
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 103/256 | train_loss 0.7066 | train_acc 0.7520 | test_loss 1.0771 | test_acc 0.6468 | lr 0.0181
last_idx 11
final_idx 0
task3/train/loss 0.6299142763018608 103
task3/test/loss 1.0722078712104441 103
task3/test/acc 0.653 103
task3/train/lr 0.016911976128542557 103
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 104/256 | train_loss 0.6299 | train_acc 0.8280 | test_loss 1.0722 | test_acc 0.6530 | lr 0.0169
last_idx 11
final_idx 0
task3/train/loss 0.5602984093129635 104
task3/test/loss 1.0198698528520354 104
task3/test/acc 0.67 104
task3/train/lr 0.01577858504896403 104
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 105/256 | train_loss 0.5603 | train_acc 0.8300 | test_loss 1.0199 | test_acc 0.6700 | lr 0.0158
last_idx 11
final_idx 0
task3/train/loss 0.7492790309091409 105
task3/test/loss 1.0739987087282505 105
task3/test/acc 0.656875 105
task3/train/lr 0.014668011437600525 105
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 106/256 | train_loss 0.7493 | train_acc 0.7880 | test_loss 1.0740 | test_acc 0.6569 | lr 0.0147
last_idx 11
final_idx 0
task3/train/loss 0.6971647068858147 106
task3/test/loss 1.070073474403266 106
task3/test/acc 0.65275 106
task3/train/lr 0.013582930763556558 106
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 107/256 | train_loss 0.6972 | train_acc 0.8360 | test_loss 1.0701 | test_acc 0.6528 | lr 0.0136
last_idx 11
final_idx 0
task3/train/loss 0.7706218038996061 107
task3/test/loss 1.1104024984351881 107
task3/test/acc 0.64675 107
task3/train/lr 0.012525957081217764 107
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 108/256 | train_loss 0.7706 | train_acc 0.8000 | test_loss 1.1104 | test_acc 0.6468 | lr 0.0125
last_idx 11
final_idx 0
task3/train/loss 0.8270617773135504 108
task3/test/loss 1.0216102945608097 108
task3/test/acc 0.660875 108
task3/train/lr 0.011499636732764853 108
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 109/256 | train_loss 0.8271 | train_acc 0.7300 | test_loss 1.0216 | test_acc 0.6609 | lr 0.0115
last_idx 11
final_idx 0
task3/train/loss 0.9501536165674528 109
task3/test/loss 1.0884773431570975 109
task3/test/acc 0.639125 109
task3/train/lr 0.010506442213812275 109
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 110/256 | train_loss 0.9502 | train_acc 0.7140 | test_loss 1.0885 | test_acc 0.6391 | lr 0.0105
last_idx 11
final_idx 0
task3/train/loss 0.7065582113961378 110
task3/test/loss 1.0865736303257418 110
task3/test/acc 0.64175 110
task3/train/lr 0.009548766216949778 110
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 111/256 | train_loss 0.7066 | train_acc 0.8260 | test_loss 1.0866 | test_acc 0.6418 | lr 0.0095
last_idx 11
final_idx 0
task3/train/loss 0.6963477047781149 111
task3/test/loss 1.0165776300888796 111
task3/test/acc 0.6655 111
task3/train/lr 0.008628915867536294 111
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 112/256 | train_loss 0.6963 | train_acc 0.7820 | test_loss 1.0166 | test_acc 0.6655 | lr 0.0086
last_idx 11
final_idx 0
task3/train/loss 0.9587136879563332 112
task3/test/loss 0.9950373246447071 112
task3/test/acc 0.6685 112
task3/train/lr 0.00774910716563295 112
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 113/256 | train_loss 0.9587 | train_acc 0.7240 | test_loss 0.9950 | test_acc 0.6685 | lr 0.0077
last_idx 11
final_idx 0
task3/train/loss 0.5062866111596426 113
task3/test/loss 0.9759484933955329 113
task3/test/acc 0.67325 113
task3/train/lr 0.006911459647464768 113
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 114/256 | train_loss 0.5063 | train_acc 0.8840 | test_loss 0.9759 | test_acc 0.6733 | lr 0.0069
last_idx 11
final_idx 0
task3/train/loss 0.6521861044069132 114
task3/test/loss 0.9849025828170253 114
task3/test/acc 0.675125 114
task3/train/lr 0.0061179912792722595 114
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 115/256 | train_loss 0.6522 | train_acc 0.8340 | test_loss 0.9849 | test_acc 0.6751 | lr 0.0061
last_idx 11
final_idx 0
task3/train/loss 0.8353108043471972 115
task3/test/loss 1.0091629848702923 115
task3/test/acc 0.66575 115
task3/train/lr 0.005370613595854041 115
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 116/256 | train_loss 0.8353 | train_acc 0.8140 | test_loss 1.0092 | test_acc 0.6657 | lr 0.0054
last_idx 11
final_idx 0
task3/train/loss 0.8276088461279869 116
task3/test/loss 0.9833315772178409 116
task3/test/acc 0.678125 116
task3/train/lr 0.004671127095512003 116
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 117/256 | train_loss 0.8276 | train_acc 0.8080 | test_loss 0.9833 | test_acc 0.6781 | lr 0.0047
last_idx 11
final_idx 0
task3/train/loss 0.6245544950167338 117
task3/test/loss 0.9840407786624772 117
task3/test/acc 0.6785 117
task3/train/lr 0.004021216902493268 117
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 118/256 | train_loss 0.6246 | train_acc 0.7640 | test_loss 0.9840 | test_acc 0.6785 | lr 0.0040
last_idx 11
final_idx 0
task3/train/loss 0.7114354036748409 118
task3/test/loss 1.0077391615608235 118
task3/test/acc 0.666875 118
task3/train/lr 0.0034224487073782153 118
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 119/256 | train_loss 0.7114 | train_acc 0.7880 | test_loss 1.0077 | test_acc 0.6669 | lr 0.0034
last_idx 11
final_idx 0
task3/train/loss 0.9078643073638281 119
task3/test/loss 1.013174403172273 119
task3/test/acc 0.66825 119
task3/train/lr 0.0028762649951947776 119
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 120/256 | train_loss 0.9079 | train_acc 0.7100 | test_loss 1.0132 | test_acc 0.6683 | lr 0.0029
last_idx 11
final_idx 0
task3/train/loss 0.43808047908047837 120
task3/test/loss 1.0122311348607251 120
task3/test/acc 0.667625 120
task3/train/lr 0.0023839815703456534 120
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 121/256 | train_loss 0.4381 | train_acc 0.9120 | test_loss 1.0122 | test_acc 0.6676 | lr 0.0024
last_idx 11
final_idx 0
task3/train/loss 0.7362188349167506 121
task3/test/loss 1.0109744482151755 121
task3/test/acc 0.66875 121
task3/train/lr 0.0019467843867202379 121
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 122/256 | train_loss 0.7362 | train_acc 0.8020 | test_loss 1.0110 | test_acc 0.6687 | lr 0.0019
last_idx 11
final_idx 0
task3/train/loss 0.8361297609905401 122
task3/test/loss 1.006642060292946 122
task3/test/acc 0.665875 122
task3/train/lr 0.0015657266906278318 122
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 123/256 | train_loss 0.8361 | train_acc 0.7000 | test_loss 1.0066 | test_acc 0.6659 | lr 0.0016
last_idx 11
final_idx 0
task3/train/loss 0.4365175850689411 123
task3/test/loss 1.0106041599269753 123
task3/test/acc 0.66725 123
task3/train/lr 0.0012417264834350366 123
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 124/256 | train_loss 0.4365 | train_acc 0.9060 | test_loss 1.0106 | test_acc 0.6673 | lr 0.0012
last_idx 11
final_idx 0
task3/train/loss 0.5358162000775337 124
task3/test/loss 1.0067817540122912 124
task3/test/acc 0.66875 124
task3/train/lr 0.0009755643100200469 124
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 125/256 | train_loss 0.5358 | train_acc 0.9200 | test_loss 1.0068 | test_acc 0.6687 | lr 0.0010
last_idx 11
final_idx 0
task3/train/loss 0.7114684904615084 125
task3/test/loss 1.001324027285471 125
task3/test/acc 0.670125 125
task3/train/lr 0.0007678813783716699 125
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 126/256 | train_loss 0.7115 | train_acc 0.7700 | test_loss 1.0013 | test_acc 0.6701 | lr 0.0008
last_idx 11
final_idx 0
task3/train/loss 0.8992715844263633 126
task3/test/loss 0.9937957268986073 126
task3/test/acc 0.669875 126
task3/train/lr 0.0006191780148631288 126
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 127/256 | train_loss 0.8993 | train_acc 0.7740 | test_loss 0.9938 | test_acc 0.6699 | lr 0.0006
last_idx 11
final_idx 0
task3/train/loss 0.6858504917472601 127
task3/test/loss 0.9965187413515625 127
task3/test/acc 0.669625 127
task3/train/lr 0.0005298124589219829 127
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 128/256 | train_loss 0.6859 | train_acc 0.8520 | test_loss 0.9965 | test_acc 0.6696 | lr 0.0005
last_idx 11
final_idx 0
task3/train/loss 0.7743177277346452 128
task3/test/loss 1.2346420749858185 128
task3/test/acc 0.6085 128
task3/train/lr 0.05 128
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 129/256 | train_loss 0.7743 | train_acc 0.8560 | test_loss 1.2346 | test_acc 0.6085 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 0.7593459039926529 129
task3/test/loss 1.5412062467945802 129
task3/test/acc 0.54625 129
task3/train/lr 0.04999254576273106 129
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 130/256 | train_loss 0.7593 | train_acc 0.7680 | test_loss 1.5412 | test_acc 0.5463 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 0.8097358470161756 130
task3/test/loss 1.86214909858101 130
task3/test/acc 0.485375 130
task3/train/lr 0.04997018754107802 130
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 131/256 | train_loss 0.8097 | train_acc 0.7760 | test_loss 1.8621 | test_acc 0.4854 | lr 0.0500
last_idx 11
final_idx 0
task3/train/loss 0.8706337238351504 131
task3/test/loss 1.4447990339863432 131
task3/test/acc 0.5625 131
task3/train/lr 0.04993293880279759 131
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 132/256 | train_loss 0.8706 | train_acc 0.7880 | test_loss 1.4448 | test_acc 0.5625 | lr 0.0499
last_idx 11
final_idx 0
task3/train/loss 0.8565695707996687 132
task3/test/loss 1.051546424627304 132
task3/test/acc 0.653 132
task3/train/lr 0.049880821985136874 132
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 133/256 | train_loss 0.8566 | train_acc 0.6640 | test_loss 1.0515 | test_acc 0.6530 | lr 0.0499
last_idx 11
final_idx 0
task3/train/loss 1.0788083970546722 133
task3/test/loss 1.1120753567461128 133
task3/test/acc 0.629125 133
task3/train/lr 0.04981386848131808 133
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 134/256 | train_loss 1.0788 | train_acc 0.7520 | test_loss 1.1121 | test_acc 0.6291 | lr 0.0498
last_idx 11
final_idx 0
task3/train/loss 0.7618361885348955 134
task3/test/loss 1.174241943182526 134
task3/test/acc 0.631125 134
task3/train/lr 0.04973211862162834 134
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 135/256 | train_loss 0.7618 | train_acc 0.7880 | test_loss 1.1742 | test_acc 0.6311 | lr 0.0497
last_idx 11
final_idx 0
task3/train/loss 0.9873101823031902 135
task3/test/loss 1.440120891391576 135
task3/test/acc 0.558875 135
task3/train/lr 0.04963562164912629 135
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 136/256 | train_loss 0.9873 | train_acc 0.6720 | test_loss 1.4401 | test_acc 0.5589 | lr 0.0496
last_idx 11
final_idx 0
task3/train/loss 0.7417914196848869 136
task3/test/loss 1.2103573042940308 136
task3/test/acc 0.601875 136
task3/train/lr 0.049524435689979954 136
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 137/256 | train_loss 0.7418 | train_acc 0.7740 | test_loss 1.2104 | test_acc 0.6019 | lr 0.0495
last_idx 11
final_idx 0
task3/train/loss 0.7449436907966932 137
task3/test/loss 1.2713725964774143 137
task3/test/acc 0.605125 137
task3/train/lr 0.04939862771845358 137
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 138/256 | train_loss 0.7449 | train_acc 0.7780 | test_loss 1.2714 | test_acc 0.6051 | lr 0.0494
last_idx 11
final_idx 0
task3/train/loss 0.7727949594457945 138
task3/test/loss 1.1366067204501602 138
task3/test/acc 0.635 138
task3/train/lr 0.04925827351656497 138
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 139/256 | train_loss 0.7728 | train_acc 0.7920 | test_loss 1.1366 | test_acc 0.6350 | lr 0.0493
last_idx 11
final_idx 0
task3/train/loss 1.007539341847102 139
task3/test/loss 1.178962026978587 139
task3/test/acc 0.61575 139
task3/train/lr 0.04910345762843714 139
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 140/256 | train_loss 1.0075 | train_acc 0.6060 | test_loss 1.1790 | test_acc 0.6158 | lr 0.0491
last_idx 11
final_idx 0
task3/train/loss 0.700529841085275 140
task3/test/loss 1.3635529289533803 140
task3/test/acc 0.58175 140
task3/train/lr 0.048934273309372174 140
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 141/256 | train_loss 0.7005 | train_acc 0.7860 | test_loss 1.3636 | test_acc 0.5817 | lr 0.0489
last_idx 11
final_idx 0
task3/train/loss 0.9495703180631002 141
task3/test/loss 1.486625027525556 141
task3/test/acc 0.527875 141
task3/train/lr 0.04875082246967766 141
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 142/256 | train_loss 0.9496 | train_acc 0.6980 | test_loss 1.4866 | test_acc 0.5279 | lr 0.0488
last_idx 11
final_idx 0
task3/train/loss 1.0205557296673458 142
task3/test/loss 1.1714435700368095 142
task3/test/acc 0.615 142
task3/train/lr 0.048553215613279764 142
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 143/256 | train_loss 1.0206 | train_acc 0.6180 | test_loss 1.1714 | test_acc 0.6150 | lr 0.0486
last_idx 11
final_idx 0
task3/train/loss 0.9543027728796005 143
task3/test/loss 1.2636141300528914 143
task3/test/acc 0.60325 143
task3/train/lr 0.04834157177115979 143
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 144/256 | train_loss 0.9543 | train_acc 0.7000 | test_loss 1.2636 | test_acc 0.6032 | lr 0.0483
last_idx 11
final_idx 0
task3/train/loss 0.6131567060947418 144
task3/test/loss 1.1144669547185793 144
task3/test/acc 0.64 144
task3/train/lr 0.04811601842965435 144
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 145/256 | train_loss 0.6132 | train_acc 0.8440 | test_loss 1.1145 | test_acc 0.6400 | lr 0.0481
last_idx 11
final_idx 0
task3/train/loss 0.8084086552262306 145
task3/test/loss 1.4513999970404656 145
task3/test/acc 0.56 145
task3/train/lr 0.047876691453662384 145
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 146/256 | train_loss 0.8084 | train_acc 0.7760 | test_loss 1.4514 | test_acc 0.5600 | lr 0.0479
last_idx 11
final_idx 0
task3/train/loss 0.7539891451597214 146
task3/test/loss 1.1763931194340789 146
task3/test/acc 0.610625 146
task3/train/lr 0.047623735004805226 146
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 147/256 | train_loss 0.7540 | train_acc 0.7760 | test_loss 1.1764 | test_acc 0.6106 | lr 0.0476
last_idx 11
final_idx 0
task3/train/loss 0.8612813800573349 147
task3/test/loss 1.152780905365944 147
task3/test/acc 0.62825 147
task3/train/lr 0.047357301454589 147
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 148/256 | train_loss 0.8613 | train_acc 0.7360 | test_loss 1.1528 | test_acc 0.6282 | lr 0.0474
last_idx 11
final_idx 0
task3/train/loss 0.7033396201829115 148
task3/test/loss 1.136056729397931 148
task3/test/acc 0.64325 148
task3/train/lr 0.04707755129262179 148
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 149/256 | train_loss 0.7033 | train_acc 0.8020 | test_loss 1.1361 | test_acc 0.6432 | lr 0.0471
last_idx 11
final_idx 0
task3/train/loss 0.8617150324086348 149
task3/test/loss 1.1362245208941972 149
task3/test/acc 0.6335 149
task3/train/lr 0.04678465302994061 149
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 150/256 | train_loss 0.8617 | train_acc 0.7760 | test_loss 1.1362 | test_acc 0.6335 | lr 0.0468
last_idx 11
final_idx 0
task3/train/loss 0.8772306231160959 150
task3/test/loss 1.2084957310459117 150
task3/test/acc 0.6145 150
task3/train/lr 0.046478783097506735 150
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 151/256 | train_loss 0.8772 | train_acc 0.7640 | test_loss 1.2085 | test_acc 0.6145 | lr 0.0465
last_idx 11
final_idx 0
task3/train/loss 0.8660826534032822 151
task3/test/loss 1.265623319034393 151
task3/test/acc 0.58825 151
task3/train/lr 0.04616012573993025 151
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 152/256 | train_loss 0.8661 | train_acc 0.6540 | test_loss 1.2656 | test_acc 0.5883 | lr 0.0462
last_idx 11
final_idx 0
task3/train/loss 0.9679902816812197 152
task3/test/loss 1.155448102484365 152
task3/test/acc 0.625375 152
task3/train/lr 0.045828872904488 152
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 153/256 | train_loss 0.9680 | train_acc 0.7120 | test_loss 1.1554 | test_acc 0.6254 | lr 0.0458
last_idx 11
final_idx 0
task3/train/loss 0.9610984263320764 153
task3/test/loss 1.0492463907697698 153
task3/test/acc 0.6485 153
task3/train/lr 0.0454852241255017 153
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 154/256 | train_loss 0.9611 | train_acc 0.7060 | test_loss 1.0492 | test_acc 0.6485 | lr 0.0455
last_idx 11
final_idx 0
task3/train/loss 0.7627089147766432 154
task3/test/loss 1.1897253467813953 154
task3/test/acc 0.610125 154
task3/train/lr 0.04512938640414596 154
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 155/256 | train_loss 0.7627 | train_acc 0.8080 | test_loss 1.1897 | test_acc 0.6101 | lr 0.0451
last_idx 11
final_idx 0
task3/train/loss 0.8273960277438164 155
task3/test/loss 1.3317540007960664 155
task3/test/acc 0.594625 155
task3/train/lr 0.04476157408375851 155
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 156/256 | train_loss 0.8274 | train_acc 0.7180 | test_loss 1.3318 | test_acc 0.5946 | lr 0.0448
last_idx 11
final_idx 0
task3/train/loss 0.603079063196977 156
task3/test/loss 1.2166683512565855 156
task3/test/acc 0.614875 156
task3/train/lr 0.04438200872072774 156
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 157/256 | train_loss 0.6031 | train_acc 0.7780 | test_loss 1.2167 | test_acc 0.6149 | lr 0.0444
last_idx 11
final_idx 0
task3/train/loss 1.0634798904259999 157
task3/test/loss 1.235826207468143 157
task3/test/acc 0.600125 157
task3/train/lr 0.0439909189510355 157
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 158/256 | train_loss 1.0635 | train_acc 0.6140 | test_loss 1.2358 | test_acc 0.6001 | lr 0.0440
last_idx 11
final_idx 0
task3/train/loss 0.764197031656901 158
task3/test/loss 1.1893940964242915 158
task3/test/acc 0.618 158
task3/train/lr 0.043588540352535246 158
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 159/256 | train_loss 0.7642 | train_acc 0.7460 | test_loss 1.1894 | test_acc 0.6180 | lr 0.0436
last_idx 11
final_idx 0
task3/train/loss 0.5807311348617077 159
task3/test/loss 1.4123765100817105 159
task3/test/acc 0.583625 159
task3/train/lr 0.043175115303048815 159
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 160/256 | train_loss 0.5807 | train_acc 0.8240 | test_loss 1.4124 | test_acc 0.5836 | lr 0.0432
last_idx 11
final_idx 0
task3/train/loss 0.6137690829734007 160
task3/test/loss 1.4915287630571115 160
task3/test/acc 0.585125 160
task3/train/lr 0.04275089283436705 160
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 161/256 | train_loss 0.6138 | train_acc 0.8160 | test_loss 1.4915 | test_acc 0.5851 | lr 0.0428
last_idx 11
final_idx 0
task3/train/loss 0.6931087623039881 161
task3/test/loss 1.4859157679500161 161
task3/test/acc 0.574875 161
task3/train/lr 0.042316128482242414 161
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 162/256 | train_loss 0.6931 | train_acc 0.8040 | test_loss 1.4859 | test_acc 0.5749 | lr 0.0423
last_idx 11
final_idx 0
task3/train/loss 0.6876870294411978 162
task3/test/loss 1.8591282558473912 162
task3/test/acc 0.481625 162
task3/train/lr 0.04187108413246371 162
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 163/256 | train_loss 0.6877 | train_acc 0.8580 | test_loss 1.8591 | test_acc 0.4816 | lr 0.0419
last_idx 11
final_idx 0
task3/train/loss 0.6932131052017212 163
task3/test/loss 1.0614961338239712 163
task3/test/acc 0.649625 163
task3/train/lr 0.04141602786310598 163
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 164/256 | train_loss 0.6932 | train_acc 0.7680 | test_loss 1.0615 | test_acc 0.6496 | lr 0.0414
last_idx 11
final_idx 0
task3/train/loss 0.5997538641095161 164
task3/test/loss 1.2799682166877684 164
task3/test/acc 0.60025 164
task3/train/lr 0.040951233783050225 164
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 165/256 | train_loss 0.5998 | train_acc 0.7780 | test_loss 1.2800 | test_acc 0.6002 | lr 0.0410
last_idx 11
final_idx 0
task3/train/loss 0.6353731875618299 165
task3/test/loss 1.1289896543782492 165
task3/test/acc 0.630625 165
task3/train/lr 0.040476981866870515 165
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 166/256 | train_loss 0.6354 | train_acc 0.7720 | test_loss 1.1290 | test_acc 0.6306 | lr 0.0405
last_idx 11
final_idx 0
task3/train/loss 0.7132865314682325 166
task3/test/loss 1.3439051341470127 166
task3/test/acc 0.5765 166
task3/train/lr 0.03999355778618773 166
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 167/256 | train_loss 0.7133 | train_acc 0.7840 | test_loss 1.3439 | test_acc 0.5765 | lr 0.0400
last_idx 11
final_idx 0
task3/train/loss 0.9013849459588528 167
task3/test/loss 1.0742456220336012 167
task3/test/acc 0.652125 167
task3/train/lr 0.039501252737591676 167
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 168/256 | train_loss 0.9014 | train_acc 0.7020 | test_loss 1.0742 | test_acc 0.6521 | lr 0.0395
last_idx 11
final_idx 0
task3/train/loss 0.7452898348371187 168
task3/test/loss 1.0595697637770203 168
task3/test/acc 0.663375 168
task3/train/lr 0.039000363267235154 168
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 169/256 | train_loss 0.7453 | train_acc 0.7620 | test_loss 1.0596 | test_acc 0.6634 | lr 0.0390
last_idx 11
final_idx 0
task3/train/loss 0.8728839457035065 169
task3/test/loss 1.2877380362415052 169
task3/test/acc 0.5915 169
task3/train/lr 0.03849119109220566 169
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 170/256 | train_loss 0.8729 | train_acc 0.7420 | test_loss 1.2877 | test_acc 0.5915 | lr 0.0385
last_idx 11
final_idx 0
task3/train/loss 0.6220605745911598 170
task3/test/loss 1.0390393035588683 170
task3/test/acc 0.66475 170
task3/train/lr 0.03797404291878224 170
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 171/256 | train_loss 0.6221 | train_acc 0.8060 | test_loss 1.0390 | test_acc 0.6647 | lr 0.0380
last_idx 11
final_idx 0
task3/train/loss 0.7840254344046116 171
task3/test/loss 1.0967026579674783 171
task3/test/acc 0.661 171
task3/train/lr 0.03744923025768716 171
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 172/256 | train_loss 0.7840 | train_acc 0.7900 | test_loss 1.0967 | test_acc 0.6610 | lr 0.0374
last_idx 11
final_idx 0
task3/train/loss 0.8929264595111212 172
task3/test/loss 1.2776978213217232 172
task3/test/acc 0.600375 172
task3/train/lr 0.03691706923644345 172
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 173/256 | train_loss 0.8929 | train_acc 0.7760 | test_loss 1.2777 | test_acc 0.6004 | lr 0.0369
last_idx 11
final_idx 0
task3/train/loss 0.7698877267539501 173
task3/test/loss 1.2957082408797609 173
task3/test/acc 0.5865 173
task3/train/lr 0.03637788040895152 173
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 174/256 | train_loss 0.7699 | train_acc 0.7740 | test_loss 1.2957 | test_acc 0.5865 | lr 0.0364
last_idx 11
final_idx 0
task3/train/loss 0.48630887021621066 174
task3/test/loss 1.056827259342094 174
task3/test/acc 0.66275 174
task3/train/lr 0.03583198856239948 174
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 175/256 | train_loss 0.4863 | train_acc 0.8600 | test_loss 1.0568 | test_acc 0.6627 | lr 0.0358
last_idx 11
final_idx 0
task3/train/loss 0.690455611795187 175
task3/test/loss 1.0847416372744592 175
task3/test/acc 0.64675 175
task3/train/lr 0.0352797225216235 175
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 176/256 | train_loss 0.6905 | train_acc 0.8340 | test_loss 1.0847 | test_acc 0.6468 | lr 0.0353
last_idx 11
final_idx 0
task3/train/loss 0.8729465678334236 176
task3/test/loss 1.1492642390204 176
task3/test/acc 0.62975 176
task3/train/lr 0.03472141495103598 176
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 177/256 | train_loss 0.8729 | train_acc 0.6580 | test_loss 1.1493 | test_acc 0.6298 | lr 0.0347
last_idx 11
final_idx 0
task3/train/loss 0.6147136328121027 177
task3/test/loss 1.016278272474205 177
task3/test/acc 0.6685 177
task3/train/lr 0.034157402154240964 177
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 178/256 | train_loss 0.6147 | train_acc 0.8340 | test_loss 1.0163 | test_acc 0.6685 | lr 0.0342
last_idx 11
final_idx 0
task3/train/loss 0.6700560611983141 178
task3/test/loss 1.0937165048914952 178
task3/test/acc 0.650125 178
task3/train/lr 0.03358802387145745 178
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 179/256 | train_loss 0.6701 | train_acc 0.8380 | test_loss 1.0937 | test_acc 0.6501 | lr 0.0336
last_idx 11
final_idx 0
task3/train/loss 0.8265989013016224 179
task3/test/loss 1.154843423965868 179
task3/test/acc 0.638375 179
task3/train/lr 0.03301362307487257 179
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 180/256 | train_loss 0.8266 | train_acc 0.7420 | test_loss 1.1548 | test_acc 0.6384 | lr 0.0330
last_idx 11
final_idx 0
task3/train/loss 0.5399174665411314 180
task3/test/loss 0.9770754212206536 180
task3/test/acc 0.683625 180
task3/train/lr 0.03243454576204794 180
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 181/256 | train_loss 0.5399 | train_acc 0.8700 | test_loss 0.9771 | test_acc 0.6836 | lr 0.0324
last_idx 11
final_idx 0
task3/train/loss 0.8739247769117355 181
task3/test/loss 1.0946379182757913 181
task3/test/acc 0.648625 181
task3/train/lr 0.03185114074750374 181
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 182/256 | train_loss 0.8739 | train_acc 0.7700 | test_loss 1.0946 | test_acc 0.6486 | lr 0.0319
last_idx 11
final_idx 0
task3/train/loss 1.0544722601771355 182
task3/test/loss 1.0955389612814883 182
task3/test/acc 0.635 182
task3/train/lr 0.03126375945260579 182
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 183/256 | train_loss 1.0545 | train_acc 0.6240 | test_loss 1.0955 | test_acc 0.6350 | lr 0.0313
last_idx 11
final_idx 0
task3/train/loss 0.925599372635285 183
task3/test/loss 1.0282638895806375 183
task3/test/acc 0.662875 183
task3/train/lr 0.030672755693882527 183
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 184/256 | train_loss 0.9256 | train_acc 0.7160 | test_loss 1.0283 | test_acc 0.6629 | lr 0.0307
last_idx 11
final_idx 0
task3/train/loss 0.7174944709986448 184
task3/test/loss 1.260634574015717 184
task3/test/acc 0.59825 184
task3/train/lr 0.03007848546989918 184
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 185/256 | train_loss 0.7175 | train_acc 0.7800 | test_loss 1.2606 | test_acc 0.5982 | lr 0.0301
last_idx 11
final_idx 0
task3/train/loss 1.0036103688180447 185
task3/test/loss 1.1695911659971698 185
task3/test/acc 0.620875 185
task3/train/lr 0.029481306746817457 185
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 186/256 | train_loss 1.0036 | train_acc 0.6820 | test_loss 1.1696 | test_acc 0.6209 | lr 0.0295
last_idx 11
final_idx 0
task3/train/loss 0.7230975491305193 186
task3/test/loss 1.0844928066317852 186
task3/test/acc 0.6455 186
task3/train/lr 0.028881579242770204 186
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 187/256 | train_loss 0.7231 | train_acc 0.7260 | test_loss 1.0845 | test_acc 0.6455 | lr 0.0289
last_idx 11
final_idx 0
task3/train/loss 0.7255185681084791 187
task3/test/loss 1.1543558208005769 187
task3/test/acc 0.63325 187
task3/train/lr 0.028279664211180604 187
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 188/256 | train_loss 0.7255 | train_acc 0.7360 | test_loss 1.1544 | test_acc 0.6332 | lr 0.0283
last_idx 11
final_idx 0
task3/train/loss 0.8024895675480366 188
task3/test/loss 1.1101837973673265 188
task3/test/acc 0.637 188
task3/train/lr 0.027675924223156633 188
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 189/256 | train_loss 0.8025 | train_acc 0.7680 | test_loss 1.1102 | test_acc 0.6370 | lr 0.0277
last_idx 11
final_idx 0
task3/train/loss 0.5217198009292284 189
task3/test/loss 1.079408347279161 189
task3/test/acc 0.6595 189
task3/train/lr 0.027070722949091772 189
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 190/256 | train_loss 0.5217 | train_acc 0.8900 | test_loss 1.0794 | test_acc 0.6595 | lr 0.0271
last_idx 11
final_idx 0
task3/train/loss 0.4636738784611225 190
task3/test/loss 1.0537693426831738 190
task3/test/acc 0.671 190
task3/train/lr 0.0264644249396036 190
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 191/256 | train_loss 0.4637 | train_acc 0.8740 | test_loss 1.0538 | test_acc 0.6710 | lr 0.0265
last_idx 11
final_idx 0
task3/train/loss 0.7721287248035272 191
task3/test/loss 1.0775124562474399 191
task3/test/acc 0.657375 191
task3/train/lr 0.02585739540594208 191
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 192/256 | train_loss 0.7721 | train_acc 0.8220 | test_loss 1.0775 | test_acc 0.6574 | lr 0.0259
last_idx 11
final_idx 0
task3/train/loss 0.45701490963498753 192
task3/test/loss 1.0540842716019232 192
task3/test/acc 0.6665 192
task3/train/lr 0.02525 192
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 193/256 | train_loss 0.4570 | train_acc 0.8100 | test_loss 1.0541 | test_acc 0.6665 | lr 0.0253
last_idx 11
final_idx 0
task3/train/loss 0.4881454184651375 193
task3/test/loss 1.1086105745244812 193
task3/test/acc 0.653875 193
task3/train/lr 0.024642604594057926 193
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 194/256 | train_loss 0.4881 | train_acc 0.9160 | test_loss 1.1086 | test_acc 0.6539 | lr 0.0246
last_idx 11
final_idx 0
task3/train/loss 0.763850862160325 194
task3/test/loss 1.0825558949957836 194
task3/test/acc 0.6555 194
task3/train/lr 0.024035575060396407 194
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 195/256 | train_loss 0.7639 | train_acc 0.7620 | test_loss 1.0826 | test_acc 0.6555 | lr 0.0240
last_idx 11
final_idx 0
task3/train/loss 0.5672972090542316 195
task3/test/loss 1.0164629911983407 195
task3/test/acc 0.676 195
task3/train/lr 0.023429277050908234 195
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 196/256 | train_loss 0.5673 | train_acc 0.8860 | test_loss 1.0165 | test_acc 0.6760 | lr 0.0234
last_idx 11
final_idx 0
task3/train/loss 0.6056442769865195 196
task3/test/loss 1.0608524485276296 196
task3/test/acc 0.66225 196
task3/train/lr 0.022824075776843374 196
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 197/256 | train_loss 0.6056 | train_acc 0.8340 | test_loss 1.0609 | test_acc 0.6623 | lr 0.0228
last_idx 11
final_idx 0
task3/train/loss 0.46073240662614506 197
task3/test/loss 1.0281377545261121 197
task3/test/acc 0.673375 197
task3/train/lr 0.022220335788819403 197
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 198/256 | train_loss 0.4607 | train_acc 0.8780 | test_loss 1.0281 | test_acc 0.6734 | lr 0.0222
last_idx 11
final_idx 0
task3/train/loss 0.7647221417476734 198
task3/test/loss 1.0232777304046756 198
task3/test/acc 0.672625 198
task3/train/lr 0.0216184207572298 198
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 199/256 | train_loss 0.7647 | train_acc 0.7300 | test_loss 1.0233 | test_acc 0.6726 | lr 0.0216
last_idx 11
final_idx 0
task3/train/loss 0.6295597640176614 199
task3/test/loss 1.0057874327668777 199
task3/test/acc 0.67525 199
task3/train/lr 0.021018693253182546 199
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 200/256 | train_loss 0.6296 | train_acc 0.8860 | test_loss 1.0058 | test_acc 0.6753 | lr 0.0210
last_idx 11
final_idx 0
task3/train/loss 0.4509050063788891 200
task3/test/loss 0.9915870716283609 200
task3/test/acc 0.68125 200
task3/train/lr 0.02042151453010083 200
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 201/256 | train_loss 0.4509 | train_acc 0.8840 | test_loss 0.9916 | test_acc 0.6813 | lr 0.0204
last_idx 11
final_idx 0
task3/train/loss 0.5793789532035589 201
task3/test/loss 1.116376940038178 201
task3/test/acc 0.65275 201
task3/train/lr 0.019827244306117476 201
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 202/256 | train_loss 0.5794 | train_acc 0.8800 | test_loss 1.1164 | test_acc 0.6528 | lr 0.0198
last_idx 11
final_idx 0
task3/train/loss 0.8526743352413177 202
task3/test/loss 1.064044492667193 202
task3/test/acc 0.65875 202
task3/train/lr 0.019236240547394222 202
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 203/256 | train_loss 0.8527 | train_acc 0.6560 | test_loss 1.0640 | test_acc 0.6587 | lr 0.0192
last_idx 11
final_idx 0
task3/train/loss 0.8297516082723936 203
task3/test/loss 1.032272910596905 203
task3/test/acc 0.661375 203
task3/train/lr 0.018648859252496267 203
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 204/256 | train_loss 0.8298 | train_acc 0.8180 | test_loss 1.0323 | test_acc 0.6614 | lr 0.0186
last_idx 11
final_idx 0
task3/train/loss 0.7906858064234257 204
task3/test/loss 1.0171044924101986 204
task3/test/acc 0.665375 204
task3/train/lr 0.018065454237952062 204
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 205/256 | train_loss 0.7907 | train_acc 0.7140 | test_loss 1.0171 | test_acc 0.6654 | lr 0.0181
last_idx 11
final_idx 0
task3/train/loss 0.662610170741876 205
task3/test/loss 1.0042979414318944 205
task3/test/acc 0.670875 205
task3/train/lr 0.017486376925127438 205
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 206/256 | train_loss 0.6626 | train_acc 0.7760 | test_loss 1.0043 | test_acc 0.6709 | lr 0.0175
last_idx 11
final_idx 0
task3/train/loss 0.585094383607308 206
task3/test/loss 1.0253375920129346 206
task3/test/acc 0.675 206
task3/train/lr 0.016911976128542557 206
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 207/256 | train_loss 0.5851 | train_acc 0.7900 | test_loss 1.0253 | test_acc 0.6750 | lr 0.0169
last_idx 11
final_idx 0
task3/train/loss 0.6123789952447017 207
task3/test/loss 1.0789076866029383 207
task3/test/acc 0.6595 207
task3/train/lr 0.016342597845759043 207
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 208/256 | train_loss 0.6124 | train_acc 0.8420 | test_loss 1.0789 | test_acc 0.6595 | lr 0.0163
last_idx 11
final_idx 0
task3/train/loss 0.779347033550342 208
task3/test/loss 1.0115036157133814 208
task3/test/acc 0.67375 208
task3/train/lr 0.01577858504896403 208
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 209/256 | train_loss 0.7793 | train_acc 0.8400 | test_loss 1.0115 | test_acc 0.6737 | lr 0.0158
last_idx 11
final_idx 0
task3/train/loss 0.49262306342522305 209
task3/test/loss 1.0326817145386895 209
task3/test/acc 0.667 209
task3/train/lr 0.015220277478376504 209
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 210/256 | train_loss 0.4926 | train_acc 0.8180 | test_loss 1.0327 | test_acc 0.6670 | lr 0.0152
last_idx 11
final_idx 0
task3/train/loss 0.740014928082625 210
task3/test/loss 1.090796265669249 210
task3/test/acc 0.65375 210
task3/train/lr 0.014668011437600525 210
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 211/256 | train_loss 0.7400 | train_acc 0.7960 | test_loss 1.0908 | test_acc 0.6538 | lr 0.0147
last_idx 11
final_idx 0
task3/train/loss 0.5814879114429156 211
task3/test/loss 1.029433893510601 211
task3/test/acc 0.669125 211
task3/train/lr 0.014122119591048485 211
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 212/256 | train_loss 0.5815 | train_acc 0.8320 | test_loss 1.0294 | test_acc 0.6691 | lr 0.0141
last_idx 11
final_idx 0
task3/train/loss 0.6507097333669662 212
task3/test/loss 1.0104111281561328 212
task3/test/acc 0.67725 212
task3/train/lr 0.013582930763556558 212
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 213/256 | train_loss 0.6507 | train_acc 0.7960 | test_loss 1.0104 | test_acc 0.6773 | lr 0.0136
last_idx 11
final_idx 0
task3/train/loss 0.7659615383793911 213
task3/test/loss 1.0345165203888338 213
task3/test/acc 0.664 213
task3/train/lr 0.013050769742312849 213
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 214/256 | train_loss 0.7660 | train_acc 0.7200 | test_loss 1.0345 | test_acc 0.6640 | lr 0.0131
last_idx 11
final_idx 0
task3/train/loss 0.7035354655236006 214
task3/test/loss 1.0132787126924965 214
task3/test/acc 0.67025 214
task3/train/lr 0.012525957081217764 214
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 215/256 | train_loss 0.7035 | train_acc 0.7920 | test_loss 1.0133 | test_acc 0.6703 | lr 0.0125
last_idx 11
final_idx 0
task3/train/loss 0.6083594430238008 215
task3/test/loss 0.9942204230925539 215
task3/test/acc 0.6795 215
task3/train/lr 0.01200880890779435 215
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 216/256 | train_loss 0.6084 | train_acc 0.8300 | test_loss 0.9942 | test_acc 0.6795 | lr 0.0120
last_idx 11
final_idx 0
task3/train/loss 0.42609814119835693 216
task3/test/loss 0.9907891775716792 216
task3/test/acc 0.682375 216
task3/train/lr 0.011499636732764853 216
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 217/256 | train_loss 0.4261 | train_acc 0.8700 | test_loss 0.9908 | test_acc 0.6824 | lr 0.0115
last_idx 11
final_idx 0
task3/train/loss 0.4711671931048234 217
task3/test/loss 0.9992877520866447 217
task3/test/acc 0.680125 217
task3/train/lr 0.010998747262408329 217
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 218/256 | train_loss 0.4712 | train_acc 0.8860 | test_loss 0.9993 | test_acc 0.6801 | lr 0.0110
last_idx 11
final_idx 0
task3/train/loss 0.43315247135857743 218
task3/test/loss 1.0332048311010822 218
task3/test/acc 0.670625 218
task3/train/lr 0.010506442213812275 218
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 219/256 | train_loss 0.4332 | train_acc 0.8820 | test_loss 1.0332 | test_acc 0.6706 | lr 0.0105
last_idx 11
final_idx 0
task3/train/loss 0.5570140189180771 219
task3/test/loss 1.02265501587273 219
task3/test/acc 0.67325 219
task3/train/lr 0.01002301813312949 219
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 220/256 | train_loss 0.5570 | train_acc 0.7940 | test_loss 1.0227 | test_acc 0.6733 | lr 0.0100
last_idx 11
final_idx 0
task3/train/loss 0.7606675208856662 220
task3/test/loss 1.0010374614989364 220
task3/test/acc 0.674 220
task3/train/lr 0.009548766216949778 220
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 221/256 | train_loss 0.7607 | train_acc 0.8380 | test_loss 1.0010 | test_acc 0.6740 | lr 0.0095
last_idx 11
final_idx 0
task3/train/loss 0.5076625322302183 221
task3/test/loss 1.0185954897449567 221
task3/test/acc 0.66975 221
task3/train/lr 0.009083972136894032 221
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 222/256 | train_loss 0.5077 | train_acc 0.7600 | test_loss 1.0186 | test_acc 0.6697 | lr 0.0091
last_idx 11
final_idx 0
task3/train/loss 0.41899779935677844 222
task3/test/loss 1.0095501038972492 222
task3/test/acc 0.67975 222
task3/train/lr 0.008628915867536294 222
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 223/256 | train_loss 0.4190 | train_acc 0.8960 | test_loss 1.0096 | test_acc 0.6797 | lr 0.0086
last_idx 11
final_idx 0
task3/train/loss 0.4145610338697831 223
task3/test/loss 0.9983692590024446 223
task3/test/acc 0.686 223
task3/train/lr 0.008183871517757594 223
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 224/256 | train_loss 0.4146 | train_acc 0.8300 | test_loss 0.9984 | test_acc 0.6860 | lr 0.0082
last_idx 11
final_idx 0
task3/train/loss 0.7068709066758553 224
task3/test/loss 1.0079480219673325 224
task3/test/acc 0.681375 224
task3/train/lr 0.00774910716563295 224
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 225/256 | train_loss 0.7069 | train_acc 0.7920 | test_loss 1.0079 | test_acc 0.6814 | lr 0.0077
last_idx 11
final_idx 0
task3/train/loss 0.7370804796616236 225
task3/test/loss 1.0043997030991774 225
task3/test/acc 0.670375 225
task3/train/lr 0.007324884696951197 225
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 226/256 | train_loss 0.7371 | train_acc 0.7720 | test_loss 1.0044 | test_acc 0.6704 | lr 0.0073
last_idx 11
final_idx 0
task3/train/loss 0.5079593577732643 226
task3/test/loss 1.0196411147058666 226
task3/test/acc 0.668125 226
task3/train/lr 0.006911459647464768 226
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 227/256 | train_loss 0.5080 | train_acc 0.8500 | test_loss 1.0196 | test_acc 0.6681 | lr 0.0069
last_idx 11
final_idx 0
task3/train/loss 0.855563065658013 227
task3/test/loss 1.0266155020876244 227
task3/test/acc 0.664125 227
task3/train/lr 0.006509081048964508 227
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 228/256 | train_loss 0.8556 | train_acc 0.7740 | test_loss 1.0266 | test_acc 0.6641 | lr 0.0065
last_idx 11
final_idx 0
task3/train/loss 0.7292731193204721 228
task3/test/loss 1.0035596791204515 228
task3/test/acc 0.6705 228
task3/train/lr 0.0061179912792722595 228
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 229/256 | train_loss 0.7293 | train_acc 0.7880 | test_loss 1.0036 | test_acc 0.6705 | lr 0.0061
last_idx 11
final_idx 0
task3/train/loss 0.539434819171826 229
task3/test/loss 0.9947526644874405 229
task3/test/acc 0.673125 229
task3/train/lr 0.005738425916241496 229
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 230/256 | train_loss 0.5394 | train_acc 0.8580 | test_loss 0.9948 | test_acc 0.6731 | lr 0.0057
last_idx 11
final_idx 0
task3/train/loss 0.42724959676464397 230
task3/test/loss 0.9944810521799129 230
task3/test/acc 0.677625 230
task3/train/lr 0.005370613595854041 230
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 231/256 | train_loss 0.4272 | train_acc 0.9080 | test_loss 0.9945 | test_acc 0.6776 | lr 0.0054
last_idx 11
final_idx 0
task3/train/loss 0.31519141234457493 231
task3/test/loss 0.9843624152325012 231
task3/test/acc 0.685125 231
task3/train/lr 0.005014775874498306 231
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 232/256 | train_loss 0.3152 | train_acc 0.8960 | test_loss 0.9844 | test_acc 0.6851 | lr 0.0050
last_idx 11
final_idx 0
task3/train/loss 0.7612355947494507 232
task3/test/loss 0.9536135275285322 232
task3/test/acc 0.69025 232
task3/train/lr 0.004671127095512003 232
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 233/256 | train_loss 0.7612 | train_acc 0.8040 | test_loss 0.9536 | test_acc 0.6903 | lr 0.0047
last_idx 11
final_idx 0
task3/train/loss 0.5412848396226764 233
task3/test/loss 0.9646906116670304 233
task3/test/acc 0.68825 233
task3/train/lr 0.004339874260069749 233
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 234/256 | train_loss 0.5413 | train_acc 0.8460 | test_loss 0.9647 | test_acc 0.6883 | lr 0.0043
last_idx 11
final_idx 0
task3/train/loss 0.4575113731746872 234
task3/test/loss 0.9657183014429532 234
task3/test/acc 0.691125 234
task3/train/lr 0.004021216902493268 234
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 235/256 | train_loss 0.4575 | train_acc 0.9080 | test_loss 0.9657 | test_acc 0.6911 | lr 0.0040
last_idx 11
final_idx 0
task3/train/loss 0.6643048735956351 235
task3/test/loss 0.9807392159824843 235
task3/test/acc 0.683 235
task3/train/lr 0.0037153469700593944 235
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 236/256 | train_loss 0.6643 | train_acc 0.7700 | test_loss 0.9807 | test_acc 0.6830 | lr 0.0037
last_idx 11
final_idx 0
task3/train/loss 0.33162045168379944 236
task3/test/loss 0.987840745802764 236
task3/test/acc 0.684125 236
task3/train/lr 0.0034224487073782153 236
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 237/256 | train_loss 0.3316 | train_acc 0.9240 | test_loss 0.9878 | test_acc 0.6841 | lr 0.0034
last_idx 11
final_idx 0
task3/train/loss 0.5808727418382963 237
task3/test/loss 0.9812238164819204 237
task3/test/acc 0.683375 237
task3/train/lr 0.0031426985454109987 237
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 238/256 | train_loss 0.5809 | train_acc 0.7940 | test_loss 0.9812 | test_acc 0.6834 | lr 0.0031
last_idx 11
final_idx 0
task3/train/loss 0.3848220289995273 238
task3/test/loss 0.9859852546846474 238
task3/test/acc 0.685375 238
task3/train/lr 0.0028762649951947776 238
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 239/256 | train_loss 0.3848 | train_acc 0.8840 | test_loss 0.9860 | test_acc 0.6854 | lr 0.0029
last_idx 11
final_idx 0
task3/train/loss 0.7949277112881342 239
task3/test/loss 0.9900185861266576 239
task3/test/acc 0.679125 239
task3/train/lr 0.0026233085463376153 239
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 240/256 | train_loss 0.7949 | train_acc 0.7340 | test_loss 0.9900 | test_acc 0.6791 | lr 0.0026
last_idx 11
final_idx 0
task3/train/loss 0.5408173445612192 240
task3/test/loss 0.969718547476517 240
task3/test/acc 0.6865 240
task3/train/lr 0.0023839815703456534 240
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 241/256 | train_loss 0.5408 | train_acc 0.8440 | test_loss 0.9697 | test_acc 0.6865 | lr 0.0024
last_idx 11
final_idx 0
task3/train/loss 0.7050377133612832 241
task3/test/loss 0.9621043020224833 241
task3/test/acc 0.6845 241
task3/train/lr 0.0021584282288402137 241
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 242/256 | train_loss 0.7050 | train_acc 0.7380 | test_loss 0.9621 | test_acc 0.6845 | lr 0.0022
last_idx 11
final_idx 0
task3/train/loss 0.47867345499495667 242
task3/test/loss 0.97829626254983 242
task3/test/acc 0.682625 242
task3/train/lr 0.0019467843867202379 242
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 243/256 | train_loss 0.4787 | train_acc 0.8480 | test_loss 0.9783 | test_acc 0.6826 | lr 0.0019
last_idx 11
final_idx 0
task3/train/loss 0.5172925436248382 243
task3/test/loss 0.968014446916161 243
task3/test/acc 0.684375 243
task3/train/lr 0.0017491775303223424 243
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 244/256 | train_loss 0.5173 | train_acc 0.7640 | test_loss 0.9680 | test_acc 0.6844 | lr 0.0017
last_idx 11
final_idx 0
task3/train/loss 0.6450031759838263 244
task3/test/loss 0.9582413518494302 244
task3/test/acc 0.68725 244
task3/train/lr 0.0015657266906278318 244
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 245/256 | train_loss 0.6450 | train_acc 0.8360 | test_loss 0.9582 | test_acc 0.6873 | lr 0.0016
last_idx 11
final_idx 0
task3/train/loss 0.7430501580238342 245
task3/test/loss 0.9574969408440066 245
task3/test/acc 0.685875 245
task3/train/lr 0.001396542371562864 245
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 246/256 | train_loss 0.7431 | train_acc 0.8620 | test_loss 0.9575 | test_acc 0.6859 | lr 0.0014
last_idx 11
final_idx 0
task3/train/loss 0.5107309129089117 246
task3/test/loss 0.9595135361119941 246
task3/test/acc 0.6865 246
task3/train/lr 0.0012417264834350366 246
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 247/256 | train_loss 0.5107 | train_acc 0.8580 | test_loss 0.9595 | test_acc 0.6865 | lr 0.0012
last_idx 11
final_idx 0
task3/train/loss 0.39341529334584874 247
task3/test/loss 0.9646202182540526 247
task3/test/acc 0.686125 247
task3/train/lr 0.0011013722815464207 247
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 248/256 | train_loss 0.3934 | train_acc 0.8980 | test_loss 0.9646 | test_acc 0.6861 | lr 0.0011
last_idx 11
final_idx 0
task3/train/loss 0.7012083971252044 248
task3/test/loss 0.9623202674991482 248
task3/test/acc 0.6855 248
task3/train/lr 0.0009755643100200469 248
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 249/256 | train_loss 0.7012 | train_acc 0.7540 | test_loss 0.9623 | test_acc 0.6855 | lr 0.0010
last_idx 11
final_idx 0
task3/train/loss 0.9831011705100536 249
task3/test/loss 0.9527230608266789 249
task3/test/acc 0.688625 249
task3/train/lr 0.0008643783508737047 249
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 250/256 | train_loss 0.9831 | train_acc 0.6760 | test_loss 0.9527 | test_acc 0.6886 | lr 0.0009
last_idx 11
final_idx 0
task3/train/loss 0.7332928062727054 250
task3/test/loss 0.9599631663698417 250
task3/test/acc 0.687125 250
task3/train/lr 0.0007678813783716699 250
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 251/256 | train_loss 0.7333 | train_acc 0.7780 | test_loss 0.9600 | test_acc 0.6871 | lr 0.0008
last_idx 11
final_idx 0
task3/train/loss 0.729645907257994 251
task3/test/loss 0.9558981606266 251
task3/test/acc 0.6875 251
task3/train/lr 0.0006861315186819283 251
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 252/256 | train_loss 0.7296 | train_acc 0.8300 | test_loss 0.9559 | test_acc 0.6875 | lr 0.0007
last_idx 11
final_idx 0
task3/train/loss 0.47722005657851696 252
task3/test/loss 0.9599827937372438 252
task3/test/acc 0.688125 252
task3/train/lr 0.0006191780148631288 252
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 253/256 | train_loss 0.4772 | train_acc 0.8680 | test_loss 0.9600 | test_acc 0.6881 | lr 0.0006
last_idx 11
final_idx 0
task3/train/loss 0.30983102321624756 253
task3/test/loss 0.9594761021025888 253
task3/test/acc 0.690875 253
task3/train/lr 0.0005670611972024174 253
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 254/256 | train_loss 0.3098 | train_acc 0.9380 | test_loss 0.9595 | test_acc 0.6909 | lr 0.0006
last_idx 11
final_idx 0
task3/train/loss 0.5714314058423042 254
task3/test/loss 0.9538324211652462 254
task3/test/acc 0.688875 254
task3/train/lr 0.0005298124589219829 254
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 255/256 | train_loss 0.5714 | train_acc 0.9020 | test_loss 0.9538 | test_acc 0.6889 | lr 0.0005
last_idx 11
final_idx 0
task3/train/loss 0.524912233153979 255
task3/test/loss 0.9638786243049653 255
task3/test/acc 0.686125 255
task3/train/lr 0.0005074542372689448 255
[INFO] rainbow_memory.py:184 > Task 3 | Epoch 256/256 | train_loss 0.5249 | train_acc 0.8900 | test_loss 0.9639 | test_acc 0.6861 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (3)
[INFO] main.py:398 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (3)
[INFO] main.py:405 > [2-5] Report task result
Metrics/TaskAcc 0.691125 3

##################################################
# Task 4 iteration
##################################################

[INFO] main.py:316 > [2-1] Prepare a datalist for the current task
total : 1000  current step :  0
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.88s. Batch: 1.13s. S_Loss: 2.3992. T_Loss: 2.1726. Mask: 0.0078. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter:   1/1000. LR: 0.0000. Data: 0.88s. Batch: 1.13s. S_Loss: 2.3992. T_Loss: 2.1726. Mask: 0.0078. :   2%|▏         | 1/50 [00:01<00:55,  1.13s/it]Train Iter:   2/1000. LR: 0.0000. Data: 0.65s. Batch: 0.89s. S_Loss: 2.3817. T_Loss: 2.2235. Mask: 0.0059. :   2%|▏         | 1/50 [00:01<00:55,  1.13s/it]Train Iter:   2/1000. LR: 0.0000. Data: 0.65s. Batch: 0.89s. S_Loss: 2.3817. T_Loss: 2.2235. Mask: 0.0059. :   4%|▍         | 2/50 [00:01<00:40,  1.18it/s]Train Iter:   3/1000. LR: 0.0000. Data: 0.59s. Batch: 0.84s. S_Loss: 2.4054. T_Loss: 2.2425. Mask: 0.0052. :   4%|▍         | 2/50 [00:02<00:40,  1.18it/s]Train Iter:   3/1000. LR: 0.0000. Data: 0.59s. Batch: 0.84s. S_Loss: 2.4054. T_Loss: 2.2425. Mask: 0.0052. :   6%|▌         | 3/50 [00:02<00:37,  1.25it/s]total : 1000  current step :  1
total : 1000  current step :  2
total : 1000  current step :  3
Train Iter:   4/1000. LR: 0.0000. Data: 0.70s. Batch: 0.95s. S_Loss: 2.4096. T_Loss: 2.2469. Mask: 0.0049. :   6%|▌         | 3/50 [00:03<00:37,  1.25it/s]Train Iter:   4/1000. LR: 0.0000. Data: 0.70s. Batch: 0.95s. S_Loss: 2.4096. T_Loss: 2.2469. Mask: 0.0049. :   8%|▊         | 4/50 [00:03<00:45,  1.02it/s]Train Iter:   5/1000. LR: 0.0000. Data: 0.59s. Batch: 0.84s. S_Loss: 2.4085. T_Loss: 2.2485. Mask: 0.0039. :   8%|▊         | 4/50 [00:04<00:45,  1.02it/s]Train Iter:   5/1000. LR: 0.0000. Data: 0.59s. Batch: 0.84s. S_Loss: 2.4085. T_Loss: 2.2485. Mask: 0.0039. :  10%|█         | 5/50 [00:04<00:34,  1.29it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.54s. Batch: 0.79s. S_Loss: 2.4064. T_Loss: 2.2418. Mask: 0.0052. :  10%|█         | 5/50 [00:04<00:34,  1.29it/s]Train Iter:   6/1000. LR: 0.0000. Data: 0.54s. Batch: 0.79s. S_Loss: 2.4064. T_Loss: 2.2418. Mask: 0.0052. :  12%|█▏        | 6/50 [00:04<00:30,  1.44it/s]total : 1000  current step :  4
total : 1000  current step :  5
total : 1000  current step :  6
Train Iter:   7/1000. LR: 0.0000. Data: 0.64s. Batch: 0.88s. S_Loss: 2.4128. T_Loss: 2.2388. Mask: 0.0050. :  12%|█▏        | 6/50 [00:06<00:30,  1.44it/s]Train Iter:   7/1000. LR: 0.0000. Data: 0.64s. Batch: 0.88s. S_Loss: 2.4128. T_Loss: 2.2388. Mask: 0.0050. :  14%|█▍        | 7/50 [00:06<00:40,  1.07it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.58s. Batch: 0.82s. S_Loss: 2.4214. T_Loss: 2.2250. Mask: 0.0049. :  14%|█▍        | 7/50 [00:06<00:40,  1.07it/s]Train Iter:   8/1000. LR: 0.0000. Data: 0.58s. Batch: 0.82s. S_Loss: 2.4214. T_Loss: 2.2250. Mask: 0.0049. :  16%|█▌        | 8/50 [00:06<00:31,  1.32it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.53s. Batch: 0.77s. S_Loss: 2.4199. T_Loss: 2.2190. Mask: 0.0056. :  16%|█▌        | 8/50 [00:06<00:31,  1.32it/s]Train Iter:   9/1000. LR: 0.0000. Data: 0.53s. Batch: 0.77s. S_Loss: 2.4199. T_Loss: 2.2190. Mask: 0.0056. :  18%|█▊        | 9/50 [00:06<00:26,  1.52it/s]total : 1000  current step :  7
total : 1000  current step :  8
total : 1000  current step :  9
Train Iter:  10/1000. LR: 0.0000. Data: 0.58s. Batch: 0.83s. S_Loss: 2.4245. T_Loss: 2.2039. Mask: 0.0074. :  18%|█▊        | 9/50 [00:08<00:26,  1.52it/s]Train Iter:  10/1000. LR: 0.0000. Data: 0.58s. Batch: 0.83s. S_Loss: 2.4245. T_Loss: 2.2039. Mask: 0.0074. :  20%|██        | 10/50 [00:08<00:34,  1.17it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.55s. Batch: 0.79s. S_Loss: 2.4294. T_Loss: 2.1905. Mask: 0.0078. :  20%|██        | 10/50 [00:08<00:34,  1.17it/s]Train Iter:  11/1000. LR: 0.0000. Data: 0.55s. Batch: 0.79s. S_Loss: 2.4294. T_Loss: 2.1905. Mask: 0.0078. :  22%|██▏       | 11/50 [00:08<00:28,  1.37it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.51s. Batch: 0.75s. S_Loss: 2.4309. T_Loss: 2.1687. Mask: 0.0094. :  22%|██▏       | 11/50 [00:09<00:28,  1.37it/s]Train Iter:  12/1000. LR: 0.0000. Data: 0.51s. Batch: 0.75s. S_Loss: 2.4309. T_Loss: 2.1687. Mask: 0.0094. :  24%|██▍       | 12/50 [00:09<00:23,  1.65it/s]total : 1000  current step :  10
total : 1000  current step :  11
total : 1000  current step :  12
Train Iter:  13/1000. LR: 0.0000. Data: 0.55s. Batch: 0.79s. S_Loss: 2.4365. T_Loss: 2.1541. Mask: 0.0129. :  24%|██▍       | 12/50 [00:10<00:23,  1.65it/s]Train Iter:  13/1000. LR: 0.0000. Data: 0.55s. Batch: 0.79s. S_Loss: 2.4365. T_Loss: 2.1541. Mask: 0.0129. :  26%|██▌       | 13/50 [00:10<00:29,  1.24it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.52s. Batch: 0.76s. S_Loss: 2.4479. T_Loss: 2.1382. Mask: 0.0167. :  26%|██▌       | 13/50 [00:10<00:29,  1.24it/s]Train Iter:  14/1000. LR: 0.0000. Data: 0.52s. Batch: 0.76s. S_Loss: 2.4479. T_Loss: 2.1382. Mask: 0.0167. :  28%|██▊       | 14/50 [00:10<00:24,  1.47it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.50s. Batch: 0.73s. S_Loss: 2.4567. T_Loss: 2.1234. Mask: 0.0234. :  28%|██▊       | 14/50 [00:11<00:24,  1.47it/s]Train Iter:  15/1000. LR: 0.0000. Data: 0.50s. Batch: 0.73s. S_Loss: 2.4567. T_Loss: 2.1234. Mask: 0.0234. :  30%|███       | 15/50 [00:11<00:19,  1.76it/s]total : 1000  current step :  13
total : 1000  current step :  14
total : 1000  current step :  15
Train Iter:  16/1000. LR: 0.0000. Data: 0.52s. Batch: 0.75s. S_Loss: 2.4641. T_Loss: 2.0976. Mask: 0.0291. :  30%|███       | 15/50 [00:12<00:19,  1.76it/s]Train Iter:  16/1000. LR: 0.0000. Data: 0.52s. Batch: 0.75s. S_Loss: 2.4641. T_Loss: 2.0976. Mask: 0.0291. :  32%|███▏      | 16/50 [00:12<00:24,  1.38it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.49s. Batch: 0.73s. S_Loss: 2.4717. T_Loss: 2.0809. Mask: 0.0381. :  32%|███▏      | 16/50 [00:12<00:24,  1.38it/s]Train Iter:  17/1000. LR: 0.0000. Data: 0.49s. Batch: 0.73s. S_Loss: 2.4717. T_Loss: 2.0809. Mask: 0.0381. :  34%|███▍      | 17/50 [00:12<00:19,  1.67it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.4784. T_Loss: 2.0665. Mask: 0.0488. :  34%|███▍      | 17/50 [00:12<00:19,  1.67it/s]Train Iter:  18/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.4784. T_Loss: 2.0665. Mask: 0.0488. :  36%|███▌      | 18/50 [00:12<00:16,  1.89it/s]total : 1000  current step :  16
total : 1000  current step :  17
total : 1000  current step :  18
Train Iter:  19/1000. LR: 0.0000. Data: 0.50s. Batch: 0.73s. S_Loss: 2.4853. T_Loss: 2.0481. Mask: 0.0580. :  36%|███▌      | 18/50 [00:13<00:16,  1.89it/s]Train Iter:  19/1000. LR: 0.0000. Data: 0.50s. Batch: 0.73s. S_Loss: 2.4853. T_Loss: 2.0481. Mask: 0.0580. :  38%|███▊      | 19/50 [00:13<00:22,  1.38it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.4953. T_Loss: 2.0348. Mask: 0.0730. :  38%|███▊      | 19/50 [00:14<00:22,  1.38it/s]Train Iter:  20/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.4953. T_Loss: 2.0348. Mask: 0.0730. :  40%|████      | 20/50 [00:14<00:18,  1.62it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.47s. Batch: 0.70s. S_Loss: 2.5081. T_Loss: 2.0212. Mask: 0.0908. :  40%|████      | 20/50 [00:14<00:18,  1.62it/s]Train Iter:  21/1000. LR: 0.0000. Data: 0.47s. Batch: 0.70s. S_Loss: 2.5081. T_Loss: 2.0212. Mask: 0.0908. :  42%|████▏     | 21/50 [00:14<00:16,  1.78it/s]total : 1000  current step :  19
total : 1000  current step :  20
total : 1000  current step :  21
Train Iter:  22/1000. LR: 0.0000. Data: 0.49s. Batch: 0.72s. S_Loss: 2.5181. T_Loss: 2.0066. Mask: 0.1078. :  42%|████▏     | 21/50 [00:15<00:16,  1.78it/s]Train Iter:  22/1000. LR: 0.0000. Data: 0.49s. Batch: 0.72s. S_Loss: 2.5181. T_Loss: 2.0066. Mask: 0.1078. :  44%|████▍     | 22/50 [00:15<00:21,  1.32it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.5263. T_Loss: 1.9955. Mask: 0.1248. :  44%|████▍     | 22/50 [00:16<00:21,  1.32it/s]Train Iter:  23/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.5263. T_Loss: 1.9955. Mask: 0.1248. :  46%|████▌     | 23/50 [00:16<00:17,  1.53it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.47s. Batch: 0.70s. S_Loss: 2.5338. T_Loss: 1.9808. Mask: 0.1437. :  46%|████▌     | 23/50 [00:16<00:17,  1.53it/s]Train Iter:  24/1000. LR: 0.0000. Data: 0.47s. Batch: 0.70s. S_Loss: 2.5338. T_Loss: 1.9808. Mask: 0.1437. :  48%|████▊     | 24/50 [00:16<00:14,  1.74it/s]total : 1000  current step :  22
total : 1000  current step :  23
total : 1000  current step :  24
Train Iter:  25/1000. LR: 0.0000. Data: 0.49s. Batch: 0.72s. S_Loss: 2.5410. T_Loss: 1.9696. Mask: 0.1638. :  48%|████▊     | 24/50 [00:18<00:14,  1.74it/s]Train Iter:  25/1000. LR: 0.0000. Data: 0.49s. Batch: 0.72s. S_Loss: 2.5410. T_Loss: 1.9696. Mask: 0.1638. :  50%|█████     | 25/50 [00:18<00:19,  1.28it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.5485. T_Loss: 1.9562. Mask: 0.1842. :  50%|█████     | 25/50 [00:18<00:19,  1.28it/s]Train Iter:  26/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.5485. T_Loss: 1.9562. Mask: 0.1842. :  52%|█████▏    | 26/50 [00:18<00:15,  1.55it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.5543. T_Loss: 1.9443. Mask: 0.2049. :  52%|█████▏    | 26/50 [00:18<00:15,  1.55it/s]Train Iter:  27/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.5543. T_Loss: 1.9443. Mask: 0.2049. :  54%|█████▍    | 27/50 [00:18<00:12,  1.87it/s]total : 1000  current step :  25
total : 1000  current step :  26
total : 1000  current step :  27
Train Iter:  28/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.5605. T_Loss: 1.9314. Mask: 0.2243. :  54%|█████▍    | 27/50 [00:19<00:12,  1.87it/s]Train Iter:  28/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.5605. T_Loss: 1.9314. Mask: 0.2243. :  56%|█████▌    | 28/50 [00:19<00:16,  1.35it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.5674. T_Loss: 1.9200. Mask: 0.2447. :  56%|█████▌    | 28/50 [00:20<00:16,  1.35it/s]Train Iter:  29/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.5674. T_Loss: 1.9200. Mask: 0.2447. :  58%|█████▊    | 29/50 [00:20<00:13,  1.50it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5757. T_Loss: 1.9102. Mask: 0.2645. :  58%|█████▊    | 29/50 [00:20<00:13,  1.50it/s]Train Iter:  30/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5757. T_Loss: 1.9102. Mask: 0.2645. :  60%|██████    | 30/50 [00:20<00:11,  1.69it/s]total : 1000  current step :  28
total : 1000  current step :  29
total : 1000  current step :  30
Train Iter:  31/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.5838. T_Loss: 1.8965. Mask: 0.2838. :  60%|██████    | 30/50 [00:21<00:11,  1.69it/s]Train Iter:  31/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.5838. T_Loss: 1.8965. Mask: 0.2838. :  62%|██████▏   | 31/50 [00:21<00:14,  1.30it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5892. T_Loss: 1.8850. Mask: 0.3004. :  62%|██████▏   | 31/50 [00:22<00:14,  1.30it/s]Train Iter:  32/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.5892. T_Loss: 1.8850. Mask: 0.3004. :  64%|██████▍   | 32/50 [00:22<00:10,  1.64it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.5944. T_Loss: 1.8745. Mask: 0.3166. :  64%|██████▍   | 32/50 [00:22<00:10,  1.64it/s]Train Iter:  33/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.5944. T_Loss: 1.8745. Mask: 0.3166. :  66%|██████▌   | 33/50 [00:22<00:09,  1.85it/s]total : 1000  current step :  31
total : 1000  current step :  32
total : 1000  current step :  33
Train Iter:  34/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6005. T_Loss: 1.8615. Mask: 0.3334. :  66%|██████▌   | 33/50 [00:23<00:09,  1.85it/s]Train Iter:  34/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6005. T_Loss: 1.8615. Mask: 0.3334. :  68%|██████▊   | 34/50 [00:23<00:12,  1.31it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.6068. T_Loss: 1.8484. Mask: 0.3474. :  68%|██████▊   | 34/50 [00:24<00:12,  1.31it/s]Train Iter:  35/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.6068. T_Loss: 1.8484. Mask: 0.3474. :  70%|███████   | 35/50 [00:24<00:09,  1.57it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6098. T_Loss: 1.8405. Mask: 0.3617. :  70%|███████   | 35/50 [00:24<00:09,  1.57it/s]Train Iter:  36/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.6098. T_Loss: 1.8405. Mask: 0.3617. :  72%|███████▏  | 36/50 [00:24<00:07,  1.84it/s]total : 1000  current step :  34
total : 1000  current step :  35
total : 1000  current step :  36
Train Iter:  37/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.6163. T_Loss: 1.8292. Mask: 0.3760. :  72%|███████▏  | 36/50 [00:25<00:07,  1.84it/s]Train Iter:  37/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.6163. T_Loss: 1.8292. Mask: 0.3760. :  74%|███████▍  | 37/50 [00:25<00:08,  1.46it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.6213. T_Loss: 1.8202. Mask: 0.3903. :  74%|███████▍  | 37/50 [00:25<00:08,  1.46it/s]Train Iter:  38/1000. LR: 0.0000. Data: 0.47s. Batch: 0.68s. S_Loss: 2.6213. T_Loss: 1.8202. Mask: 0.3903. :  76%|███████▌  | 38/50 [00:25<00:07,  1.65it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.6288. T_Loss: 1.8123. Mask: 0.4041. :  76%|███████▌  | 38/50 [00:26<00:07,  1.65it/s]Train Iter:  39/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.6288. T_Loss: 1.8123. Mask: 0.4041. :  78%|███████▊  | 39/50 [00:26<00:06,  1.82it/s]total : 1000  current step :  37
total : 1000  current step :  38
total : 1000  current step :  39
Train Iter:  40/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6363. T_Loss: 1.8039. Mask: 0.4176. :  78%|███████▊  | 39/50 [00:27<00:06,  1.82it/s]Train Iter:  40/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6363. T_Loss: 1.8039. Mask: 0.4176. :  80%|████████  | 40/50 [00:27<00:08,  1.17it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6425. T_Loss: 1.7958. Mask: 0.4298. :  80%|████████  | 40/50 [00:28<00:08,  1.17it/s]Train Iter:  41/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6425. T_Loss: 1.7958. Mask: 0.4298. :  82%|████████▏ | 41/50 [00:28<00:07,  1.19it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.6464. T_Loss: 1.7889. Mask: 0.4408. :  82%|████████▏ | 41/50 [00:29<00:07,  1.19it/s]Train Iter:  42/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.6464. T_Loss: 1.7889. Mask: 0.4408. :  84%|████████▍ | 42/50 [00:29<00:06,  1.23it/s]total : 1000  current step :  40
total : 1000  current step :  41
total : 1000  current step :  42
Train Iter:  43/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.6502. T_Loss: 1.7823. Mask: 0.4519. :  84%|████████▍ | 42/50 [00:30<00:06,  1.23it/s]Train Iter:  43/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.6502. T_Loss: 1.7823. Mask: 0.4519. :  86%|████████▌ | 43/50 [00:30<00:06,  1.03it/s]Train Iter:  44/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.6562. T_Loss: 1.7737. Mask: 0.4618. :  86%|████████▌ | 43/50 [00:31<00:06,  1.03it/s]Train Iter:  44/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.6562. T_Loss: 1.7737. Mask: 0.4618. :  88%|████████▊ | 44/50 [00:31<00:04,  1.27it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6612. T_Loss: 1.7661. Mask: 0.4714. :  88%|████████▊ | 44/50 [00:31<00:04,  1.27it/s]Train Iter:  45/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6612. T_Loss: 1.7661. Mask: 0.4714. :  90%|█████████ | 45/50 [00:31<00:03,  1.58it/s]total : 1000  current step :  43
total : 1000  current step :  44
total : 1000  current step :  45
Train Iter:  46/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.6672. T_Loss: 1.7601. Mask: 0.4810. :  90%|█████████ | 45/50 [00:32<00:03,  1.58it/s]Train Iter:  46/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.6672. T_Loss: 1.7601. Mask: 0.4810. :  92%|█████████▏| 46/50 [00:32<00:03,  1.20it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.6697. T_Loss: 1.7530. Mask: 0.4896. :  92%|█████████▏| 46/50 [00:33<00:03,  1.20it/s]Train Iter:  47/1000. LR: 0.0000. Data: 0.49s. Batch: 0.70s. S_Loss: 2.6697. T_Loss: 1.7530. Mask: 0.4896. :  94%|█████████▍| 47/50 [00:33<00:02,  1.39it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6745. T_Loss: 1.7476. Mask: 0.4985. :  94%|█████████▍| 47/50 [00:33<00:02,  1.39it/s]Train Iter:  48/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6745. T_Loss: 1.7476. Mask: 0.4985. :  96%|█████████▌| 48/50 [00:33<00:01,  1.63it/s]total : 1000  current step :  46
total : 1000  current step :  47
total : 1000  current step :  48
Train Iter:  49/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.6784. T_Loss: 1.7419. Mask: 0.5065. :  96%|█████████▌| 48/50 [00:34<00:01,  1.63it/s]Train Iter:  49/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.6784. T_Loss: 1.7419. Mask: 0.5065. :  98%|█████████▊| 49/50 [00:34<00:00,  1.24it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6813. T_Loss: 1.7372. Mask: 0.5145. :  98%|█████████▊| 49/50 [00:35<00:00,  1.24it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6813. T_Loss: 1.7372. Mask: 0.5145. : 100%|██████████| 50/50 [00:35<00:00,  1.46it/s]Train Iter:  50/1000. LR: 0.0000. Data: 0.48s. Batch: 0.70s. S_Loss: 2.6813. T_Loss: 1.7372. Mask: 0.5145. : 100%|██████████| 50/50 [00:35<00:00,  1.42it/s]
total : 1000  current step :  49
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 11.8629. top1: 0.00. top5: 0.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 11.8629. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 12.0835. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 12.0835. top1: 0.00. top5: 0.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 12.0870. top1: 0.00. top5: 0.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 12.0870. top1: 0.00. top5: 0.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 11.9307. top1: 0.00. top5: 1.76. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 11.9307. top1: 0.00. top5: 1.76. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 11.1615. top1: 0.00. top5: 10.86. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 11.1615. top1: 0.00. top5: 10.86. :  62%|██████▎   | 5/8 [00:01<00:01,  3.00it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 10.5958. top1: 0.00. top5: 15.49. :  62%|██████▎   | 5/8 [00:02<00:01,  3.00it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 10.5958. top1: 0.00. top5: 15.49. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 10.1651. top1: 0.00. top5: 18.92. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 10.1651. top1: 0.00. top5: 18.92. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 9.9185. top1: 0.00. top5: 21.20. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s] Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 9.9185. top1: 0.00. top5: 21.20. : 100%|██████████| 8/8 [00:02<00:00,  3.76it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 9.9185. top1: 0.00. top5: 21.20. : 100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
total : 1000  current step :  50
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.01s. Batch: 0.21s. S_Loss: 2.9261. T_Loss: 1.4346. Mask: 0.9102. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter:  51/1000. LR: 0.0000. Data: 0.01s. Batch: 0.21s. S_Loss: 2.9261. T_Loss: 1.4346. Mask: 0.9102. :   2%|▏         | 1/50 [00:00<00:10,  4.68it/s]total : 1000  current step :  51
Train Iter:  52/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.9352. T_Loss: 1.4621. Mask: 0.9121. :   2%|▏         | 1/50 [00:01<00:10,  4.68it/s]Train Iter:  52/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.9352. T_Loss: 1.4621. Mask: 0.9121. :   4%|▍         | 2/50 [00:01<00:36,  1.30it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.33s. Batch: 0.57s. S_Loss: 2.9003. T_Loss: 1.4766. Mask: 0.9154. :   4%|▍         | 2/50 [00:01<00:36,  1.30it/s]Train Iter:  53/1000. LR: 0.0000. Data: 0.33s. Batch: 0.57s. S_Loss: 2.9003. T_Loss: 1.4766. Mask: 0.9154. :   6%|▌         | 3/50 [00:01<00:26,  1.74it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.27s. Batch: 0.50s. S_Loss: 2.8661. T_Loss: 1.4859. Mask: 0.9160. :   6%|▌         | 3/50 [00:02<00:26,  1.74it/s]Train Iter:  54/1000. LR: 0.0000. Data: 0.27s. Batch: 0.50s. S_Loss: 2.8661. T_Loss: 1.4859. Mask: 0.9160. :   8%|▊         | 4/50 [00:02<00:21,  2.16it/s]total : 1000  current step :  52
total : 1000  current step :  53
total : 1000  current step :  54
Train Iter:  55/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.8519. T_Loss: 1.4969. Mask: 0.9211. :   8%|▊         | 4/50 [00:03<00:21,  2.16it/s]Train Iter:  55/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.8519. T_Loss: 1.4969. Mask: 0.9211. :  10%|█         | 5/50 [00:03<00:33,  1.36it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.8482. T_Loss: 1.5042. Mask: 0.9245. :  10%|█         | 5/50 [00:03<00:33,  1.36it/s]Train Iter:  56/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.8482. T_Loss: 1.5042. Mask: 0.9245. :  12%|█▏        | 6/50 [00:03<00:25,  1.72it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.30s. Batch: 0.54s. S_Loss: 2.8306. T_Loss: 1.5021. Mask: 0.9241. :  12%|█▏        | 6/50 [00:03<00:25,  1.72it/s]Train Iter:  57/1000. LR: 0.0000. Data: 0.30s. Batch: 0.54s. S_Loss: 2.8306. T_Loss: 1.5021. Mask: 0.9241. :  14%|█▍        | 7/50 [00:03<00:20,  2.10it/s]total : 1000  current step :  55
total : 1000  current step :  56
total : 1000  current step :  57
Train Iter:  58/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.8310. T_Loss: 1.4997. Mask: 0.9263. :  14%|█▍        | 7/50 [00:04<00:20,  2.10it/s]Train Iter:  58/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.8310. T_Loss: 1.4997. Mask: 0.9263. :  16%|█▌        | 8/50 [00:04<00:28,  1.46it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.8303. T_Loss: 1.5022. Mask: 0.9280. :  16%|█▌        | 8/50 [00:05<00:28,  1.46it/s]Train Iter:  59/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.8303. T_Loss: 1.5022. Mask: 0.9280. :  18%|█▊        | 9/50 [00:05<00:24,  1.69it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.8167. T_Loss: 1.5084. Mask: 0.9262. :  18%|█▊        | 9/50 [00:05<00:24,  1.69it/s]Train Iter:  60/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.8167. T_Loss: 1.5084. Mask: 0.9262. :  20%|██        | 10/50 [00:05<00:21,  1.88it/s]total : 1000  current step :  58
total : 1000  current step :  59
total : 1000  current step :  60
Train Iter:  61/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.8100. T_Loss: 1.5117. Mask: 0.9279. :  20%|██        | 10/50 [00:06<00:21,  1.88it/s]Train Iter:  61/1000. LR: 0.0000. Data: 0.40s. Batch: 0.62s. S_Loss: 2.8100. T_Loss: 1.5117. Mask: 0.9279. :  22%|██▏       | 11/50 [00:06<00:27,  1.41it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.8057. T_Loss: 1.5113. Mask: 0.9255. :  22%|██▏       | 11/50 [00:07<00:27,  1.41it/s]Train Iter:  62/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.8057. T_Loss: 1.5113. Mask: 0.9255. :  24%|██▍       | 12/50 [00:07<00:23,  1.64it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.8085. T_Loss: 1.5157. Mask: 0.9255. :  24%|██▍       | 12/50 [00:07<00:23,  1.64it/s]Train Iter:  63/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.8085. T_Loss: 1.5157. Mask: 0.9255. :  26%|██▌       | 13/50 [00:07<00:18,  2.00it/s]total : 1000  current step :  61
total : 1000  current step :  62
total : 1000  current step :  63
Train Iter:  64/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.8006. T_Loss: 1.5146. Mask: 0.9261. :  26%|██▌       | 13/50 [00:08<00:18,  2.00it/s]Train Iter:  64/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.8006. T_Loss: 1.5146. Mask: 0.9261. :  28%|██▊       | 14/50 [00:08<00:25,  1.41it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7959. T_Loss: 1.5121. Mask: 0.9258. :  28%|██▊       | 14/50 [00:08<00:25,  1.41it/s]Train Iter:  65/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7959. T_Loss: 1.5121. Mask: 0.9258. :  30%|███       | 15/50 [00:08<00:20,  1.68it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.7919. T_Loss: 1.5096. Mask: 0.9258. :  30%|███       | 15/50 [00:09<00:20,  1.68it/s]Train Iter:  66/1000. LR: 0.0000. Data: 0.35s. Batch: 0.58s. S_Loss: 2.7919. T_Loss: 1.5096. Mask: 0.9258. :  32%|███▏      | 16/50 [00:09<00:17,  1.96it/s]total : 1000  current step :  64
total : 1000  current step :  65
total : 1000  current step :  66
Train Iter:  67/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.7867. T_Loss: 1.5124. Mask: 0.9256. :  32%|███▏      | 16/50 [00:10<00:17,  1.96it/s]Train Iter:  67/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.7867. T_Loss: 1.5124. Mask: 0.9256. :  34%|███▍      | 17/50 [00:10<00:25,  1.29it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.7848. T_Loss: 1.5117. Mask: 0.9258. :  34%|███▍      | 17/50 [00:10<00:25,  1.29it/s]Train Iter:  68/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.7848. T_Loss: 1.5117. Mask: 0.9258. :  36%|███▌      | 18/50 [00:10<00:20,  1.56it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.7879. T_Loss: 1.5145. Mask: 0.9270. :  36%|███▌      | 18/50 [00:11<00:20,  1.56it/s]Train Iter:  69/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.7879. T_Loss: 1.5145. Mask: 0.9270. :  38%|███▊      | 19/50 [00:11<00:16,  1.84it/s]total : 1000  current step :  67
total : 1000  current step :  68
total : 1000  current step :  69
Train Iter:  70/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.7882. T_Loss: 1.5129. Mask: 0.9289. :  38%|███▊      | 19/50 [00:12<00:16,  1.84it/s]Train Iter:  70/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.7882. T_Loss: 1.5129. Mask: 0.9289. :  40%|████      | 20/50 [00:12<00:22,  1.33it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.7890. T_Loss: 1.5149. Mask: 0.9302. :  40%|████      | 20/50 [00:12<00:22,  1.33it/s]Train Iter:  71/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.7890. T_Loss: 1.5149. Mask: 0.9302. :  42%|████▏     | 21/50 [00:12<00:18,  1.61it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.7850. T_Loss: 1.5168. Mask: 0.9313. :  42%|████▏     | 21/50 [00:13<00:18,  1.61it/s]Train Iter:  72/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 2.7850. T_Loss: 1.5168. Mask: 0.9313. :  44%|████▍     | 22/50 [00:13<00:14,  1.87it/s]total : 1000  current step :  70
total : 1000  current step :  71
total : 1000  current step :  72
Train Iter:  73/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.7831. T_Loss: 1.5185. Mask: 0.9321. :  44%|████▍     | 22/50 [00:14<00:14,  1.87it/s]Train Iter:  73/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.7831. T_Loss: 1.5185. Mask: 0.9321. :  46%|████▌     | 23/50 [00:14<00:20,  1.32it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.7836. T_Loss: 1.5217. Mask: 0.9321. :  46%|████▌     | 23/50 [00:14<00:20,  1.32it/s]Train Iter:  74/1000. LR: 0.0000. Data: 0.38s. Batch: 0.61s. S_Loss: 2.7836. T_Loss: 1.5217. Mask: 0.9321. :  48%|████▊     | 24/50 [00:14<00:16,  1.59it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.7851. T_Loss: 1.5265. Mask: 0.9334. :  48%|████▊     | 24/50 [00:15<00:16,  1.59it/s]Train Iter:  75/1000. LR: 0.0000. Data: 0.37s. Batch: 0.60s. S_Loss: 2.7851. T_Loss: 1.5265. Mask: 0.9334. :  50%|█████     | 25/50 [00:15<00:13,  1.82it/s]total : 1000  current step :  73
total : 1000  current step :  74
total : 1000  current step :  75
Train Iter:  76/1000. LR: 0.0000. Data: 0.40s. Batch: 0.64s. S_Loss: 2.7834. T_Loss: 1.5317. Mask: 0.9331. :  50%|█████     | 25/50 [00:16<00:13,  1.82it/s]Train Iter:  76/1000. LR: 0.0000. Data: 0.40s. Batch: 0.64s. S_Loss: 2.7834. T_Loss: 1.5317. Mask: 0.9331. :  52%|█████▏    | 26/50 [00:16<00:20,  1.18it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.7829. T_Loss: 1.5345. Mask: 0.9332. :  52%|█████▏    | 26/50 [00:17<00:20,  1.18it/s]Train Iter:  77/1000. LR: 0.0000. Data: 0.39s. Batch: 0.63s. S_Loss: 2.7829. T_Loss: 1.5345. Mask: 0.9332. :  54%|█████▍    | 27/50 [00:17<00:16,  1.41it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.37s. Batch: 0.62s. S_Loss: 2.7833. T_Loss: 1.5375. Mask: 0.9335. :  54%|█████▍    | 27/50 [00:17<00:16,  1.41it/s]Train Iter:  78/1000. LR: 0.0000. Data: 0.37s. Batch: 0.62s. S_Loss: 2.7833. T_Loss: 1.5375. Mask: 0.9335. :  56%|█████▌    | 28/50 [00:17<00:12,  1.75it/s]total : 1000  current step :  76
total : 1000  current step :  77
total : 1000  current step :  78
Train Iter:  79/1000. LR: 0.0000. Data: 0.40s. Batch: 0.64s. S_Loss: 2.7821. T_Loss: 1.5381. Mask: 0.9332. :  56%|█████▌    | 28/50 [00:18<00:12,  1.75it/s]Train Iter:  79/1000. LR: 0.0000. Data: 0.40s. Batch: 0.64s. S_Loss: 2.7821. T_Loss: 1.5381. Mask: 0.9332. :  58%|█████▊    | 29/50 [00:18<00:17,  1.23it/s]total : 1000  current step :  79
Train Iter:  80/1000. LR: 0.0000. Data: 0.40s. Batch: 0.65s. S_Loss: 2.7830. T_Loss: 1.5403. Mask: 0.9336. :  58%|█████▊    | 29/50 [00:19<00:17,  1.23it/s]Train Iter:  80/1000. LR: 0.0000. Data: 0.40s. Batch: 0.65s. S_Loss: 2.7830. T_Loss: 1.5403. Mask: 0.9336. :  60%|██████    | 30/50 [00:19<00:15,  1.26it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.40s. Batch: 0.65s. S_Loss: 2.7811. T_Loss: 1.5393. Mask: 0.9331. :  60%|██████    | 30/50 [00:20<00:15,  1.26it/s]Train Iter:  81/1000. LR: 0.0000. Data: 0.40s. Batch: 0.65s. S_Loss: 2.7811. T_Loss: 1.5393. Mask: 0.9331. :  62%|██████▏   | 31/50 [00:20<00:14,  1.31it/s]total : 1000  current step :  80
total : 1000  current step :  81
Train Iter:  82/1000. LR: 0.0000. Data: 0.42s. Batch: 0.67s. S_Loss: 2.7787. T_Loss: 1.5469. Mask: 0.9330. :  62%|██████▏   | 31/50 [00:21<00:14,  1.31it/s]Train Iter:  82/1000. LR: 0.0000. Data: 0.42s. Batch: 0.67s. S_Loss: 2.7787. T_Loss: 1.5469. Mask: 0.9330. :  64%|██████▍   | 32/50 [00:21<00:16,  1.11it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.7786. T_Loss: 1.5489. Mask: 0.9332. :  64%|██████▍   | 32/50 [00:21<00:16,  1.11it/s]Train Iter:  83/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.7786. T_Loss: 1.5489. Mask: 0.9332. :  66%|██████▌   | 33/50 [00:21<00:12,  1.40it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.7792. T_Loss: 1.5486. Mask: 0.9334. :  66%|██████▌   | 33/50 [00:22<00:12,  1.40it/s]Train Iter:  84/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.7792. T_Loss: 1.5486. Mask: 0.9334. :  68%|██████▊   | 34/50 [00:22<00:09,  1.61it/s]total : 1000  current step :  82
total : 1000  current step :  83
total : 1000  current step :  84
Train Iter:  85/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7786. T_Loss: 1.5515. Mask: 0.9335. :  68%|██████▊   | 34/50 [00:23<00:09,  1.61it/s]Train Iter:  85/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7786. T_Loss: 1.5515. Mask: 0.9335. :  70%|███████   | 35/50 [00:23<00:11,  1.26it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.42s. Batch: 0.66s. S_Loss: 2.7813. T_Loss: 1.5528. Mask: 0.9333. :  70%|███████   | 35/50 [00:23<00:11,  1.26it/s]Train Iter:  86/1000. LR: 0.0000. Data: 0.42s. Batch: 0.66s. S_Loss: 2.7813. T_Loss: 1.5528. Mask: 0.9333. :  72%|███████▏  | 36/50 [00:23<00:09,  1.44it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7814. T_Loss: 1.5528. Mask: 0.9334. :  72%|███████▏  | 36/50 [00:24<00:09,  1.44it/s]Train Iter:  87/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7814. T_Loss: 1.5528. Mask: 0.9334. :  74%|███████▍  | 37/50 [00:24<00:07,  1.69it/s]total : 1000  current step :  85
total : 1000  current step :  86
total : 1000  current step :  87
Train Iter:  88/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7825. T_Loss: 1.5559. Mask: 0.9349. :  74%|███████▍  | 37/50 [00:25<00:07,  1.69it/s]Train Iter:  88/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7825. T_Loss: 1.5559. Mask: 0.9349. :  76%|███████▌  | 38/50 [00:25<00:09,  1.31it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7831. T_Loss: 1.5576. Mask: 0.9346. :  76%|███████▌  | 38/50 [00:25<00:09,  1.31it/s]Train Iter:  89/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7831. T_Loss: 1.5576. Mask: 0.9346. :  78%|███████▊  | 39/50 [00:25<00:06,  1.61it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7857. T_Loss: 1.5591. Mask: 0.9341. :  78%|███████▊  | 39/50 [00:25<00:06,  1.61it/s]Train Iter:  90/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7857. T_Loss: 1.5591. Mask: 0.9341. :  80%|████████  | 40/50 [00:25<00:05,  1.75it/s]total : 1000  current step :  88
total : 1000  current step :  89
total : 1000  current step :  90
Train Iter:  91/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7863. T_Loss: 1.5612. Mask: 0.9341. :  80%|████████  | 40/50 [00:27<00:05,  1.75it/s]Train Iter:  91/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7863. T_Loss: 1.5612. Mask: 0.9341. :  82%|████████▏ | 41/50 [00:27<00:07,  1.27it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7860. T_Loss: 1.5640. Mask: 0.9343. :  82%|████████▏ | 41/50 [00:27<00:07,  1.27it/s]Train Iter:  92/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7860. T_Loss: 1.5640. Mask: 0.9343. :  84%|████████▍ | 42/50 [00:27<00:05,  1.52it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7877. T_Loss: 1.5659. Mask: 0.9344. :  84%|████████▍ | 42/50 [00:27<00:05,  1.52it/s]Train Iter:  93/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7877. T_Loss: 1.5659. Mask: 0.9344. :  86%|████████▌ | 43/50 [00:27<00:03,  1.85it/s]total : 1000  current step :  91
total : 1000  current step :  92
total : 1000  current step :  93
Train Iter:  94/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7872. T_Loss: 1.5686. Mask: 0.9347. :  86%|████████▌ | 43/50 [00:29<00:03,  1.85it/s]Train Iter:  94/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7872. T_Loss: 1.5686. Mask: 0.9347. :  88%|████████▊ | 44/50 [00:29<00:04,  1.32it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7881. T_Loss: 1.5713. Mask: 0.9351. :  88%|████████▊ | 44/50 [00:29<00:04,  1.32it/s]Train Iter:  95/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7881. T_Loss: 1.5713. Mask: 0.9351. :  90%|█████████ | 45/50 [00:29<00:03,  1.49it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7872. T_Loss: 1.5739. Mask: 0.9349. :  90%|█████████ | 45/50 [00:30<00:03,  1.49it/s]Train Iter:  96/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7872. T_Loss: 1.5739. Mask: 0.9349. :  92%|█████████▏| 46/50 [00:30<00:02,  1.71it/s]total : 1000  current step :  94
total : 1000  current step :  95
total : 1000  current step :  96
Train Iter:  97/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7873. T_Loss: 1.5751. Mask: 0.9349. :  92%|█████████▏| 46/50 [00:31<00:02,  1.71it/s]Train Iter:  97/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7873. T_Loss: 1.5751. Mask: 0.9349. :  94%|█████████▍| 47/50 [00:31<00:02,  1.24it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7887. T_Loss: 1.5779. Mask: 0.9348. :  94%|█████████▍| 47/50 [00:31<00:02,  1.24it/s]Train Iter:  98/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7887. T_Loss: 1.5779. Mask: 0.9348. :  96%|█████████▌| 48/50 [00:31<00:01,  1.43it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7876. T_Loss: 1.5813. Mask: 0.9354. :  96%|█████████▌| 48/50 [00:32<00:01,  1.43it/s]Train Iter:  99/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7876. T_Loss: 1.5813. Mask: 0.9354. :  98%|█████████▊| 49/50 [00:32<00:00,  1.65it/s]total : 1000  current step :  97
total : 1000  current step :  98
total : 1000  current step :  99
Train Iter: 100/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7849. T_Loss: 1.5828. Mask: 0.9357. :  98%|█████████▊| 49/50 [00:33<00:00,  1.65it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7849. T_Loss: 1.5828. Mask: 0.9357. : 100%|██████████| 50/50 [00:33<00:00,  1.21it/s]Train Iter: 100/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7849. T_Loss: 1.5828. Mask: 0.9357. : 100%|██████████| 50/50 [00:33<00:00,  1.49it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 6.5241. top1: 0.00. top5: 0.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 6.5241. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 6.6163. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 6.6163. top1: 0.00. top5: 0.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 6.6158. top1: 0.00. top5: 0.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 6.6158. top1: 0.00. top5: 0.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 6.5275. top1: 0.00. top5: 1.95. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 6.5275. top1: 0.00. top5: 1.95. :  50%|█████     | 4/8 [00:01<00:01,  3.30it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 6.0411. top1: 0.00. top5: 12.89. :  50%|█████     | 4/8 [00:01<00:01,  3.30it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 6.0411. top1: 0.00. top5: 12.89. :  62%|██████▎   | 5/8 [00:01<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 5.6929. top1: 0.00. top5: 19.86. :  62%|██████▎   | 5/8 [00:02<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 5.6929. top1: 0.00. top5: 19.86. :  75%|███████▌  | 6/8 [00:02<00:00,  3.55it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 5.4335. top1: 0.00. top5: 25.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.55it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 5.4335. top1: 0.00. top5: 25.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 5.2784. top1: 0.00. top5: 28.40. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 5.2784. top1: 0.00. top5: 28.40. : 100%|██████████| 8/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 5.2784. top1: 0.00. top5: 28.40. : 100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
total : 1000  current step :  100
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.24s. S_Loss: 2.7360. T_Loss: 1.7709. Mask: 0.9492. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 101/1000. LR: 0.0000. Data: 0.01s. Batch: 0.24s. S_Loss: 2.7360. T_Loss: 1.7709. Mask: 0.9492. :   2%|▏         | 1/50 [00:00<00:11,  4.21it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.20s. S_Loss: 2.6999. T_Loss: 1.7520. Mask: 0.9453. :   2%|▏         | 1/50 [00:00<00:11,  4.21it/s]Train Iter: 102/1000. LR: 0.0000. Data: 0.01s. Batch: 0.20s. S_Loss: 2.6999. T_Loss: 1.7520. Mask: 0.9453. :   4%|▍         | 2/50 [00:00<00:09,  5.10it/s]total : 1000  current step :  101
total : 1000  current step :  102
Train Iter: 103/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.7131. T_Loss: 1.7592. Mask: 0.9505. :   4%|▍         | 2/50 [00:01<00:09,  5.10it/s]Train Iter: 103/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.7131. T_Loss: 1.7592. Mask: 0.9505. :   6%|▌         | 3/50 [00:01<00:33,  1.42it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.28s. Batch: 0.51s. S_Loss: 2.7181. T_Loss: 1.7510. Mask: 0.9512. :   6%|▌         | 3/50 [00:02<00:33,  1.42it/s]Train Iter: 104/1000. LR: 0.0000. Data: 0.28s. Batch: 0.51s. S_Loss: 2.7181. T_Loss: 1.7510. Mask: 0.9512. :   8%|▊         | 4/50 [00:02<00:25,  1.80it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.24s. Batch: 0.47s. S_Loss: 2.7092. T_Loss: 1.7459. Mask: 0.9500. :   8%|▊         | 4/50 [00:02<00:25,  1.80it/s]Train Iter: 105/1000. LR: 0.0000. Data: 0.24s. Batch: 0.47s. S_Loss: 2.7092. T_Loss: 1.7459. Mask: 0.9500. :  10%|█         | 5/50 [00:02<00:20,  2.15it/s]total : 1000  current step :  103
total : 1000  current step :  104
total : 1000  current step :  105
Train Iter: 106/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.7289. T_Loss: 1.7728. Mask: 0.9460. :  10%|█         | 5/50 [00:03<00:20,  2.15it/s]Train Iter: 106/1000. LR: 0.0000. Data: 0.39s. Batch: 0.62s. S_Loss: 2.7289. T_Loss: 1.7728. Mask: 0.9460. :  12%|█▏        | 6/50 [00:03<00:34,  1.28it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.7338. T_Loss: 1.7646. Mask: 0.9442. :  12%|█▏        | 6/50 [00:04<00:34,  1.28it/s]Train Iter: 107/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.7338. T_Loss: 1.7646. Mask: 0.9442. :  14%|█▍        | 7/50 [00:04<00:26,  1.62it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.7350. T_Loss: 1.7624. Mask: 0.9443. :  14%|█▍        | 7/50 [00:04<00:26,  1.62it/s]Train Iter: 108/1000. LR: 0.0000. Data: 0.31s. Batch: 0.53s. S_Loss: 2.7350. T_Loss: 1.7624. Mask: 0.9443. :  16%|█▌        | 8/50 [00:04<00:20,  2.02it/s]total : 1000  current step :  106
total : 1000  current step :  107
total : 1000  current step :  108
Train Iter: 109/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7409. T_Loss: 1.7647. Mask: 0.9470. :  16%|█▌        | 8/50 [00:05<00:20,  2.02it/s]Train Iter: 109/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7409. T_Loss: 1.7647. Mask: 0.9470. :  18%|█▊        | 9/50 [00:05<00:28,  1.44it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.7406. T_Loss: 1.7555. Mask: 0.9473. :  18%|█▊        | 9/50 [00:05<00:28,  1.44it/s]Train Iter: 110/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.7406. T_Loss: 1.7555. Mask: 0.9473. :  20%|██        | 10/50 [00:05<00:23,  1.68it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.7347. T_Loss: 1.7530. Mask: 0.9489. :  20%|██        | 10/50 [00:06<00:23,  1.68it/s]Train Iter: 111/1000. LR: 0.0000. Data: 0.33s. Batch: 0.55s. S_Loss: 2.7347. T_Loss: 1.7530. Mask: 0.9489. :  22%|██▏       | 11/50 [00:06<00:19,  1.97it/s]total : 1000  current step :  109
total : 1000  current step :  110
total : 1000  current step :  111
Train Iter: 112/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7443. T_Loss: 1.7551. Mask: 0.9486. :  22%|██▏       | 11/50 [00:07<00:19,  1.97it/s]Train Iter: 112/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7443. T_Loss: 1.7551. Mask: 0.9486. :  24%|██▍       | 12/50 [00:07<00:26,  1.44it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.7463. T_Loss: 1.7619. Mask: 0.9474. :  24%|██▍       | 12/50 [00:07<00:26,  1.44it/s]Train Iter: 113/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.7463. T_Loss: 1.7619. Mask: 0.9474. :  26%|██▌       | 13/50 [00:07<00:22,  1.66it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7489. T_Loss: 1.7618. Mask: 0.9470. :  26%|██▌       | 13/50 [00:07<00:22,  1.66it/s]Train Iter: 114/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7489. T_Loss: 1.7618. Mask: 0.9470. :  28%|██▊       | 14/50 [00:07<00:18,  1.97it/s]total : 1000  current step :  112
total : 1000  current step :  113
total : 1000  current step :  114
Train Iter: 115/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7484. T_Loss: 1.7688. Mask: 0.9471. :  28%|██▊       | 14/50 [00:09<00:18,  1.97it/s]Train Iter: 115/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7484. T_Loss: 1.7688. Mask: 0.9471. :  30%|███       | 15/50 [00:09<00:25,  1.37it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.7475. T_Loss: 1.7703. Mask: 0.9456. :  30%|███       | 15/50 [00:09<00:25,  1.37it/s]Train Iter: 116/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.7475. T_Loss: 1.7703. Mask: 0.9456. :  32%|███▏      | 16/50 [00:09<00:20,  1.65it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.7423. T_Loss: 1.7753. Mask: 0.9451. :  32%|███▏      | 16/50 [00:09<00:20,  1.65it/s]Train Iter: 117/1000. LR: 0.0000. Data: 0.34s. Batch: 0.57s. S_Loss: 2.7423. T_Loss: 1.7753. Mask: 0.9451. :  34%|███▍      | 17/50 [00:09<00:16,  2.05it/s]total : 1000  current step :  115
total : 1000  current step :  116
total : 1000  current step :  117
Train Iter: 118/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7389. T_Loss: 1.7739. Mask: 0.9449. :  34%|███▍      | 17/50 [00:10<00:16,  2.05it/s]Train Iter: 118/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7389. T_Loss: 1.7739. Mask: 0.9449. :  36%|███▌      | 18/50 [00:10<00:22,  1.42it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.7416. T_Loss: 1.7752. Mask: 0.9449. :  36%|███▌      | 18/50 [00:11<00:22,  1.42it/s]Train Iter: 119/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.7416. T_Loss: 1.7752. Mask: 0.9449. :  38%|███▊      | 19/50 [00:11<00:18,  1.67it/s]total : 1000  current step :  118
total : 1000  current step :  119
Train Iter: 120/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.7408. T_Loss: 1.7761. Mask: 0.9459. :  38%|███▊      | 19/50 [00:11<00:18,  1.67it/s]Train Iter: 120/1000. LR: 0.0000. Data: 0.36s. Batch: 0.59s. S_Loss: 2.7408. T_Loss: 1.7761. Mask: 0.9459. :  40%|████      | 20/50 [00:11<00:18,  1.65it/s]total : 1000  current step :  120
Train Iter: 121/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7452. T_Loss: 1.7817. Mask: 0.9462. :  40%|████      | 20/50 [00:12<00:18,  1.65it/s]Train Iter: 121/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7452. T_Loss: 1.7817. Mask: 0.9462. :  42%|████▏     | 21/50 [00:12<00:21,  1.32it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7473. T_Loss: 1.7815. Mask: 0.9457. :  42%|████▏     | 21/50 [00:13<00:21,  1.32it/s]Train Iter: 122/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7473. T_Loss: 1.7815. Mask: 0.9457. :  44%|████▍     | 22/50 [00:13<00:17,  1.58it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7472. T_Loss: 1.7839. Mask: 0.9462. :  44%|████▍     | 22/50 [00:13<00:17,  1.58it/s]Train Iter: 123/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7472. T_Loss: 1.7839. Mask: 0.9462. :  46%|████▌     | 23/50 [00:13<00:13,  1.95it/s]total : 1000  current step :  121
total : 1000  current step :  122
total : 1000  current step :  123
Train Iter: 124/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7459. T_Loss: 1.7881. Mask: 0.9469. :  46%|████▌     | 23/50 [00:14<00:13,  1.95it/s]Train Iter: 124/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7459. T_Loss: 1.7881. Mask: 0.9469. :  48%|████▊     | 24/50 [00:14<00:17,  1.45it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7441. T_Loss: 1.7906. Mask: 0.9458. :  48%|████▊     | 24/50 [00:14<00:17,  1.45it/s]Train Iter: 125/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7441. T_Loss: 1.7906. Mask: 0.9458. :  50%|█████     | 25/50 [00:14<00:14,  1.70it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7443. T_Loss: 1.7923. Mask: 0.9461. :  50%|█████     | 25/50 [00:15<00:14,  1.70it/s]Train Iter: 126/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7443. T_Loss: 1.7923. Mask: 0.9461. :  52%|█████▏    | 26/50 [00:15<00:12,  1.86it/s]total : 1000  current step :  124
total : 1000  current step :  125
total : 1000  current step :  126
Train Iter: 127/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7431. T_Loss: 1.7972. Mask: 0.9456. :  52%|█████▏    | 26/50 [00:16<00:12,  1.86it/s]Train Iter: 127/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7431. T_Loss: 1.7972. Mask: 0.9456. :  54%|█████▍    | 27/50 [00:16<00:16,  1.38it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7434. T_Loss: 1.8042. Mask: 0.9460. :  54%|█████▍    | 27/50 [00:16<00:16,  1.38it/s]Train Iter: 128/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7434. T_Loss: 1.8042. Mask: 0.9460. :  56%|█████▌    | 28/50 [00:16<00:13,  1.58it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7436. T_Loss: 1.8096. Mask: 0.9459. :  56%|█████▌    | 28/50 [00:17<00:13,  1.58it/s]Train Iter: 129/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7436. T_Loss: 1.8096. Mask: 0.9459. :  58%|█████▊    | 29/50 [00:17<00:10,  1.94it/s]total : 1000  current step :  127
total : 1000  current step :  128
total : 1000  current step :  129
Train Iter: 130/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7423. T_Loss: 1.8126. Mask: 0.9461. :  58%|█████▊    | 29/50 [00:18<00:10,  1.94it/s]Train Iter: 130/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7423. T_Loss: 1.8126. Mask: 0.9461. :  60%|██████    | 30/50 [00:18<00:13,  1.47it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7417. T_Loss: 1.8168. Mask: 0.9456. :  60%|██████    | 30/50 [00:18<00:13,  1.47it/s]Train Iter: 131/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7417. T_Loss: 1.8168. Mask: 0.9456. :  62%|██████▏   | 31/50 [00:18<00:10,  1.79it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7421. T_Loss: 1.8158. Mask: 0.9460. :  62%|██████▏   | 31/50 [00:18<00:10,  1.79it/s]Train Iter: 132/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7421. T_Loss: 1.8158. Mask: 0.9460. :  64%|██████▍   | 32/50 [00:18<00:09,  1.99it/s]total : 1000  current step :  130
total : 1000  current step :  131
total : 1000  current step :  132
Train Iter: 133/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7424. T_Loss: 1.8214. Mask: 0.9470. :  64%|██████▍   | 32/50 [00:19<00:09,  1.99it/s]Train Iter: 133/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7424. T_Loss: 1.8214. Mask: 0.9470. :  66%|██████▌   | 33/50 [00:19<00:11,  1.49it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7442. T_Loss: 1.8217. Mask: 0.9466. :  66%|██████▌   | 33/50 [00:20<00:11,  1.49it/s]Train Iter: 134/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7442. T_Loss: 1.8217. Mask: 0.9466. :  68%|██████▊   | 34/50 [00:20<00:09,  1.70it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7436. T_Loss: 1.8240. Mask: 0.9471. :  68%|██████▊   | 34/50 [00:20<00:09,  1.70it/s]Train Iter: 135/1000. LR: 0.0000. Data: 0.37s. Batch: 0.59s. S_Loss: 2.7436. T_Loss: 1.8240. Mask: 0.9471. :  70%|███████   | 35/50 [00:20<00:07,  1.93it/s]total : 1000  current step :  133
total : 1000  current step :  134
total : 1000  current step :  135
Train Iter: 136/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7467. T_Loss: 1.8226. Mask: 0.9473. :  70%|███████   | 35/50 [00:21<00:07,  1.93it/s]Train Iter: 136/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7467. T_Loss: 1.8226. Mask: 0.9473. :  72%|███████▏  | 36/50 [00:21<00:09,  1.42it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7491. T_Loss: 1.8237. Mask: 0.9469. :  72%|███████▏  | 36/50 [00:22<00:09,  1.42it/s]Train Iter: 137/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7491. T_Loss: 1.8237. Mask: 0.9469. :  74%|███████▍  | 37/50 [00:22<00:07,  1.70it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.7495. T_Loss: 1.8254. Mask: 0.9469. :  74%|███████▍  | 37/50 [00:22<00:07,  1.70it/s]Train Iter: 138/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.7495. T_Loss: 1.8254. Mask: 0.9469. :  76%|███████▌  | 38/50 [00:22<00:06,  1.95it/s]total : 1000  current step :  136
total : 1000  current step :  137
total : 1000  current step :  138
Train Iter: 139/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7509. T_Loss: 1.8289. Mask: 0.9469. :  76%|███████▌  | 38/50 [00:23<00:06,  1.95it/s]Train Iter: 139/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7509. T_Loss: 1.8289. Mask: 0.9469. :  78%|███████▊  | 39/50 [00:23<00:07,  1.42it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7514. T_Loss: 1.8313. Mask: 0.9471. :  78%|███████▊  | 39/50 [00:24<00:07,  1.42it/s]Train Iter: 140/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7514. T_Loss: 1.8313. Mask: 0.9471. :  80%|████████  | 40/50 [00:24<00:06,  1.62it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.7530. T_Loss: 1.8346. Mask: 0.9471. :  80%|████████  | 40/50 [00:24<00:06,  1.62it/s]Train Iter: 141/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.7530. T_Loss: 1.8346. Mask: 0.9471. :  82%|████████▏ | 41/50 [00:24<00:04,  1.89it/s]total : 1000  current step :  139
total : 1000  current step :  140
total : 1000  current step :  141
Train Iter: 142/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 2.7524. T_Loss: 1.8349. Mask: 0.9475. :  82%|████████▏ | 41/50 [00:25<00:04,  1.89it/s]Train Iter: 142/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 2.7524. T_Loss: 1.8349. Mask: 0.9475. :  84%|████████▍ | 42/50 [00:25<00:06,  1.31it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7516. T_Loss: 1.8367. Mask: 0.9476. :  84%|████████▍ | 42/50 [00:26<00:06,  1.31it/s]Train Iter: 143/1000. LR: 0.0000. Data: 0.39s. Batch: 0.60s. S_Loss: 2.7516. T_Loss: 1.8367. Mask: 0.9476. :  86%|████████▌ | 43/50 [00:26<00:04,  1.56it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7507. T_Loss: 1.8388. Mask: 0.9479. :  86%|████████▌ | 43/50 [00:26<00:04,  1.56it/s]Train Iter: 144/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7507. T_Loss: 1.8388. Mask: 0.9479. :  88%|████████▊ | 44/50 [00:26<00:03,  1.83it/s]total : 1000  current step :  142
total : 1000  current step :  143
total : 1000  current step :  144
Train Iter: 145/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7510. T_Loss: 1.8414. Mask: 0.9478. :  88%|████████▊ | 44/50 [00:27<00:03,  1.83it/s]Train Iter: 145/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7510. T_Loss: 1.8414. Mask: 0.9478. :  90%|█████████ | 45/50 [00:27<00:03,  1.36it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7513. T_Loss: 1.8487. Mask: 0.9479. :  90%|█████████ | 45/50 [00:27<00:03,  1.36it/s]Train Iter: 146/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7513. T_Loss: 1.8487. Mask: 0.9479. :  92%|█████████▏| 46/50 [00:27<00:02,  1.58it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7514. T_Loss: 1.8546. Mask: 0.9479. :  92%|█████████▏| 46/50 [00:28<00:02,  1.58it/s]Train Iter: 147/1000. LR: 0.0000. Data: 0.38s. Batch: 0.60s. S_Loss: 2.7514. T_Loss: 1.8546. Mask: 0.9479. :  94%|█████████▍| 47/50 [00:28<00:01,  1.88it/s]total : 1000  current step :  145
total : 1000  current step :  146
total : 1000  current step :  147
Train Iter: 148/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7514. T_Loss: 1.8584. Mask: 0.9483. :  94%|█████████▍| 47/50 [00:29<00:01,  1.88it/s]Train Iter: 148/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7514. T_Loss: 1.8584. Mask: 0.9483. :  96%|█████████▌| 48/50 [00:29<00:01,  1.34it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7515. T_Loss: 1.8608. Mask: 0.9483. :  96%|█████████▌| 48/50 [00:29<00:01,  1.34it/s]Train Iter: 149/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7515. T_Loss: 1.8608. Mask: 0.9483. :  98%|█████████▊| 49/50 [00:29<00:00,  1.53it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7506. T_Loss: 1.8647. Mask: 0.9489. :  98%|█████████▊| 49/50 [00:30<00:00,  1.53it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7506. T_Loss: 1.8647. Mask: 0.9489. : 100%|██████████| 50/50 [00:30<00:00,  1.63it/s]Train Iter: 150/1000. LR: 0.0000. Data: 0.39s. Batch: 0.61s. S_Loss: 2.7506. T_Loss: 1.8647. Mask: 0.9489. : 100%|██████████| 50/50 [00:30<00:00,  1.64it/s]
total : 1000  current step :  148
total : 1000  current step :  149
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 4.7024. top1: 0.00. top5: 0.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 4.7024. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 4.7498. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 4.7498. top1: 0.00. top5: 0.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 4.7505. top1: 0.00. top5: 0.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.98it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 4.7505. top1: 0.00. top5: 0.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.40it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 4.6899. top1: 0.00. top5: 2.05. :  38%|███▊      | 3/8 [00:01<00:02,  2.40it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 4.6899. top1: 0.00. top5: 2.05. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 4.3354. top1: 0.00. top5: 14.30. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 4.3354. top1: 0.00. top5: 14.30. :  62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 4.0872. top1: 0.00. top5: 22.07. :  62%|██████▎   | 5/8 [00:02<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 4.0872. top1: 0.00. top5: 22.07. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 3.9053. top1: 0.00. top5: 27.68. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 3.9053. top1: 0.00. top5: 27.68. :  88%|████████▊ | 7/8 [00:02<00:00,  3.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 3.7939. top1: 0.00. top5: 31.30. :  88%|████████▊ | 7/8 [00:02<00:00,  3.05it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 3.7939. top1: 0.00. top5: 31.30. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 3.7939. top1: 0.00. top5: 31.30. : 100%|██████████| 8/8 [00:03<00:00,  2.65it/s]
total : 1000  current step :  150
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 0.94s. Batch: 1.12s. S_Loss: 2.7238. T_Loss: 2.0998. Mask: 0.9141. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 151/1000. LR: 0.0000. Data: 0.94s. Batch: 1.12s. S_Loss: 2.7238. T_Loss: 2.0998. Mask: 0.9141. :   2%|▏         | 1/50 [00:01<00:55,  1.12s/it]Train Iter: 152/1000. LR: 0.0000. Data: 0.53s. Batch: 0.72s. S_Loss: 2.7826. T_Loss: 2.1672. Mask: 0.9492. :   2%|▏         | 1/50 [00:01<00:55,  1.12s/it]Train Iter: 152/1000. LR: 0.0000. Data: 0.53s. Batch: 0.72s. S_Loss: 2.7826. T_Loss: 2.1672. Mask: 0.9492. :   4%|▍         | 2/50 [00:01<00:31,  1.54it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 2.7539. T_Loss: 2.1240. Mask: 0.9440. :   4%|▍         | 2/50 [00:01<00:31,  1.54it/s]Train Iter: 153/1000. LR: 0.0000. Data: 0.40s. Batch: 0.61s. S_Loss: 2.7539. T_Loss: 2.1240. Mask: 0.9440. :   6%|▌         | 3/50 [00:01<00:24,  1.90it/s]total : 1000  current step :  151
total : 1000  current step :  152
total : 1000  current step :  153
Train Iter: 154/1000. LR: 0.0000. Data: 0.54s. Batch: 0.75s. S_Loss: 2.7497. T_Loss: 2.1167. Mask: 0.9463. :   6%|▌         | 3/50 [00:03<00:24,  1.90it/s]Train Iter: 154/1000. LR: 0.0000. Data: 0.54s. Batch: 0.75s. S_Loss: 2.7497. T_Loss: 2.1167. Mask: 0.9463. :   8%|▊         | 4/50 [00:03<00:36,  1.26it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.7287. T_Loss: 2.1350. Mask: 0.9492. :   8%|▊         | 4/50 [00:03<00:36,  1.26it/s]Train Iter: 155/1000. LR: 0.0000. Data: 0.46s. Batch: 0.67s. S_Loss: 2.7287. T_Loss: 2.1350. Mask: 0.9492. :  10%|█         | 5/50 [00:03<00:28,  1.60it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.7324. T_Loss: 2.1442. Mask: 0.9492. :  10%|█         | 5/50 [00:03<00:28,  1.60it/s]Train Iter: 156/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.7324. T_Loss: 2.1442. Mask: 0.9492. :  12%|█▏        | 6/50 [00:03<00:24,  1.81it/s]total : 1000  current step :  154
total : 1000  current step :  155
total : 1000  current step :  156
Train Iter: 157/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.7325. T_Loss: 2.1202. Mask: 0.9492. :  12%|█▏        | 6/50 [00:05<00:24,  1.81it/s]Train Iter: 157/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.7325. T_Loss: 2.1202. Mask: 0.9492. :  14%|█▍        | 7/50 [00:05<00:33,  1.27it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.7361. T_Loss: 2.1142. Mask: 0.9468. :  14%|█▍        | 7/50 [00:05<00:33,  1.27it/s]Train Iter: 158/1000. LR: 0.0000. Data: 0.45s. Batch: 0.66s. S_Loss: 2.7361. T_Loss: 2.1142. Mask: 0.9468. :  16%|█▌        | 8/50 [00:05<00:25,  1.62it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.7276. T_Loss: 2.1038. Mask: 0.9475. :  16%|█▌        | 8/50 [00:05<00:25,  1.62it/s]Train Iter: 159/1000. LR: 0.0000. Data: 0.41s. Batch: 0.63s. S_Loss: 2.7276. T_Loss: 2.1038. Mask: 0.9475. :  18%|█▊        | 9/50 [00:05<00:23,  1.78it/s]total : 1000  current step :  157
total : 1000  current step :  158
total : 1000  current step :  159
Train Iter: 160/1000. LR: 0.0000. Data: 0.50s. Batch: 0.73s. S_Loss: 2.7233. T_Loss: 2.0888. Mask: 0.9500. :  18%|█▊        | 9/50 [00:07<00:23,  1.78it/s]Train Iter: 160/1000. LR: 0.0000. Data: 0.50s. Batch: 0.73s. S_Loss: 2.7233. T_Loss: 2.0888. Mask: 0.9500. :  20%|██        | 10/50 [00:07<00:35,  1.14it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.7265. T_Loss: 2.0836. Mask: 0.9485. :  20%|██        | 10/50 [00:07<00:35,  1.14it/s]Train Iter: 161/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.7265. T_Loss: 2.0836. Mask: 0.9485. :  22%|██▏       | 11/50 [00:07<00:31,  1.23it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.50s. Batch: 0.71s. S_Loss: 2.7254. T_Loss: 2.0867. Mask: 0.9473. :  22%|██▏       | 11/50 [00:08<00:31,  1.23it/s]Train Iter: 162/1000. LR: 0.0000. Data: 0.50s. Batch: 0.71s. S_Loss: 2.7254. T_Loss: 2.0867. Mask: 0.9473. :  24%|██▍       | 12/50 [00:08<00:28,  1.33it/s]total : 1000  current step :  160
total : 1000  current step :  161
total : 1000  current step :  162
Train Iter: 163/1000. LR: 0.0000. Data: 0.54s. Batch: 0.75s. S_Loss: 2.7282. T_Loss: 2.0855. Mask: 0.9483. :  24%|██▍       | 12/50 [00:09<00:28,  1.33it/s]Train Iter: 163/1000. LR: 0.0000. Data: 0.54s. Batch: 0.75s. S_Loss: 2.7282. T_Loss: 2.0855. Mask: 0.9483. :  26%|██▌       | 13/50 [00:09<00:33,  1.11it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.7257. T_Loss: 2.0972. Mask: 0.9506. :  26%|██▌       | 13/50 [00:10<00:33,  1.11it/s]Train Iter: 164/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.7257. T_Loss: 2.0972. Mask: 0.9506. :  28%|██▊       | 14/50 [00:10<00:25,  1.43it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.7207. T_Loss: 2.0921. Mask: 0.9521. :  28%|██▊       | 14/50 [00:10<00:25,  1.43it/s]Train Iter: 165/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.7207. T_Loss: 2.0921. Mask: 0.9521. :  30%|███       | 15/50 [00:10<00:23,  1.47it/s]total : 1000  current step :  163
total : 1000  current step :  164
total : 1000  current step :  165
Train Iter: 166/1000. LR: 0.0000. Data: 0.53s. Batch: 0.76s. S_Loss: 2.7228. T_Loss: 2.0948. Mask: 0.9514. :  30%|███       | 15/50 [00:12<00:23,  1.47it/s]Train Iter: 166/1000. LR: 0.0000. Data: 0.53s. Batch: 0.76s. S_Loss: 2.7228. T_Loss: 2.0948. Mask: 0.9514. :  32%|███▏      | 16/50 [00:12<00:31,  1.09it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.51s. Batch: 0.74s. S_Loss: 2.7237. T_Loss: 2.0940. Mask: 0.9520. :  32%|███▏      | 16/50 [00:12<00:31,  1.09it/s]Train Iter: 167/1000. LR: 0.0000. Data: 0.51s. Batch: 0.74s. S_Loss: 2.7237. T_Loss: 2.0940. Mask: 0.9520. :  34%|███▍      | 17/50 [00:12<00:25,  1.32it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.49s. Batch: 0.72s. S_Loss: 2.7204. T_Loss: 2.0867. Mask: 0.9514. :  34%|███▍      | 17/50 [00:12<00:25,  1.32it/s]Train Iter: 168/1000. LR: 0.0000. Data: 0.49s. Batch: 0.72s. S_Loss: 2.7204. T_Loss: 2.0867. Mask: 0.9514. :  36%|███▌      | 18/50 [00:12<00:20,  1.56it/s]total : 1000  current step :  166
total : 1000  current step :  167
total : 1000  current step :  168
Train Iter: 169/1000. LR: 0.0000. Data: 0.52s. Batch: 0.75s. S_Loss: 2.7185. T_Loss: 2.0781. Mask: 0.9517. :  36%|███▌      | 18/50 [00:14<00:20,  1.56it/s]Train Iter: 169/1000. LR: 0.0000. Data: 0.52s. Batch: 0.75s. S_Loss: 2.7185. T_Loss: 2.0781. Mask: 0.9517. :  38%|███▊      | 19/50 [00:14<00:26,  1.18it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.50s. Batch: 0.73s. S_Loss: 2.7220. T_Loss: 2.0917. Mask: 0.9520. :  38%|███▊      | 19/50 [00:14<00:26,  1.18it/s]Train Iter: 170/1000. LR: 0.0000. Data: 0.50s. Batch: 0.73s. S_Loss: 2.7220. T_Loss: 2.0917. Mask: 0.9520. :  40%|████      | 20/50 [00:14<00:20,  1.47it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.7206. T_Loss: 2.1010. Mask: 0.9522. :  40%|████      | 20/50 [00:14<00:20,  1.47it/s]Train Iter: 171/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.7206. T_Loss: 2.1010. Mask: 0.9522. :  42%|████▏     | 21/50 [00:14<00:16,  1.75it/s]total : 1000  current step :  169
total : 1000  current step :  170
total : 1000  current step :  171
Train Iter: 172/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.7215. T_Loss: 2.0962. Mask: 0.9524. :  42%|████▏     | 21/50 [00:15<00:16,  1.75it/s]Train Iter: 172/1000. LR: 0.0000. Data: 0.50s. Batch: 0.72s. S_Loss: 2.7215. T_Loss: 2.0962. Mask: 0.9524. :  44%|████▍     | 22/50 [00:15<00:20,  1.36it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.7213. T_Loss: 2.0976. Mask: 0.9526. :  44%|████▍     | 22/50 [00:16<00:20,  1.36it/s]Train Iter: 173/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.7213. T_Loss: 2.0976. Mask: 0.9526. :  46%|████▌     | 23/50 [00:16<00:16,  1.61it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.7237. T_Loss: 2.1000. Mask: 0.9525. :  46%|████▌     | 23/50 [00:16<00:16,  1.61it/s]Train Iter: 174/1000. LR: 0.0000. Data: 0.47s. Batch: 0.69s. S_Loss: 2.7237. T_Loss: 2.1000. Mask: 0.9525. :  48%|████▊     | 24/50 [00:16<00:13,  1.86it/s]total : 1000  current step :  172
total : 1000  current step :  173
total : 1000  current step :  174
Train Iter: 175/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.7263. T_Loss: 2.1024. Mask: 0.9527. :  48%|████▊     | 24/50 [00:17<00:13,  1.86it/s]Train Iter: 175/1000. LR: 0.0000. Data: 0.49s. Batch: 0.71s. S_Loss: 2.7263. T_Loss: 2.1024. Mask: 0.9527. :  50%|█████     | 25/50 [00:17<00:17,  1.39it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.47s. Batch: 0.70s. S_Loss: 2.7268. T_Loss: 2.1003. Mask: 0.9524. :  50%|█████     | 25/50 [00:18<00:17,  1.39it/s]Train Iter: 176/1000. LR: 0.0000. Data: 0.47s. Batch: 0.70s. S_Loss: 2.7268. T_Loss: 2.1003. Mask: 0.9524. :  52%|█████▏    | 26/50 [00:18<00:15,  1.59it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.7309. T_Loss: 2.1011. Mask: 0.9530. :  52%|█████▏    | 26/50 [00:18<00:15,  1.59it/s]Train Iter: 177/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.7309. T_Loss: 2.1011. Mask: 0.9530. :  54%|█████▍    | 27/50 [00:18<00:12,  1.79it/s]total : 1000  current step :  175
total : 1000  current step :  176
total : 1000  current step :  177
Train Iter: 178/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.7329. T_Loss: 2.1118. Mask: 0.9537. :  54%|█████▍    | 27/50 [00:19<00:12,  1.79it/s]Train Iter: 178/1000. LR: 0.0000. Data: 0.48s. Batch: 0.71s. S_Loss: 2.7329. T_Loss: 2.1118. Mask: 0.9537. :  56%|█████▌    | 28/50 [00:19<00:16,  1.30it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.7349. T_Loss: 2.1142. Mask: 0.9537. :  56%|█████▌    | 28/50 [00:20<00:16,  1.30it/s]Train Iter: 179/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.7349. T_Loss: 2.1142. Mask: 0.9537. :  58%|█████▊    | 29/50 [00:20<00:12,  1.62it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7337. T_Loss: 2.1171. Mask: 0.9529. :  58%|█████▊    | 29/50 [00:20<00:12,  1.62it/s]Train Iter: 180/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7337. T_Loss: 2.1171. Mask: 0.9529. :  60%|██████    | 30/50 [00:20<00:10,  1.87it/s]total : 1000  current step :  178
total : 1000  current step :  179
total : 1000  current step :  180
Train Iter: 181/1000. LR: 0.0000. Data: 0.47s. Batch: 0.70s. S_Loss: 2.7339. T_Loss: 2.1224. Mask: 0.9533. :  60%|██████    | 30/50 [00:21<00:10,  1.87it/s]Train Iter: 181/1000. LR: 0.0000. Data: 0.47s. Batch: 0.70s. S_Loss: 2.7339. T_Loss: 2.1224. Mask: 0.9533. :  62%|██████▏   | 31/50 [00:21<00:13,  1.40it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7322. T_Loss: 2.1275. Mask: 0.9531. :  62%|██████▏   | 31/50 [00:21<00:13,  1.40it/s]Train Iter: 182/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7322. T_Loss: 2.1275. Mask: 0.9531. :  64%|██████▍   | 32/50 [00:21<00:10,  1.75it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7333. T_Loss: 2.1288. Mask: 0.9530. :  64%|██████▍   | 32/50 [00:22<00:10,  1.75it/s]Train Iter: 183/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7333. T_Loss: 2.1288. Mask: 0.9530. :  66%|██████▌   | 33/50 [00:22<00:08,  1.99it/s]total : 1000  current step :  181
total : 1000  current step :  182
total : 1000  current step :  183
Train Iter: 184/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.7315. T_Loss: 2.1312. Mask: 0.9531. :  66%|██████▌   | 33/50 [00:23<00:08,  1.99it/s]Train Iter: 184/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.7315. T_Loss: 2.1312. Mask: 0.9531. :  68%|██████▊   | 34/50 [00:23<00:11,  1.40it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7301. T_Loss: 2.1337. Mask: 0.9537. :  68%|██████▊   | 34/50 [00:23<00:11,  1.40it/s]Train Iter: 185/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7301. T_Loss: 2.1337. Mask: 0.9537. :  70%|███████   | 35/50 [00:23<00:08,  1.68it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7287. T_Loss: 2.1336. Mask: 0.9537. :  70%|███████   | 35/50 [00:24<00:08,  1.68it/s]Train Iter: 186/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7287. T_Loss: 2.1336. Mask: 0.9537. :  72%|███████▏  | 36/50 [00:24<00:07,  2.00it/s]total : 1000  current step :  184
total : 1000  current step :  185
total : 1000  current step :  186
Train Iter: 187/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7279. T_Loss: 2.1371. Mask: 0.9540. :  72%|███████▏  | 36/50 [00:25<00:07,  2.00it/s]Train Iter: 187/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7279. T_Loss: 2.1371. Mask: 0.9540. :  74%|███████▍  | 37/50 [00:25<00:08,  1.49it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7257. T_Loss: 2.1403. Mask: 0.9529. :  74%|███████▍  | 37/50 [00:25<00:08,  1.49it/s]Train Iter: 188/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7257. T_Loss: 2.1403. Mask: 0.9529. :  76%|███████▌  | 38/50 [00:25<00:06,  1.72it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7263. T_Loss: 2.1486. Mask: 0.9528. :  76%|███████▌  | 38/50 [00:25<00:06,  1.72it/s]Train Iter: 189/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7263. T_Loss: 2.1486. Mask: 0.9528. :  78%|███████▊  | 39/50 [00:25<00:05,  1.94it/s]total : 1000  current step :  187
total : 1000  current step :  188
total : 1000  current step :  189
Train Iter: 190/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.7275. T_Loss: 2.1516. Mask: 0.9523. :  78%|███████▊  | 39/50 [00:26<00:05,  1.94it/s]Train Iter: 190/1000. LR: 0.0000. Data: 0.45s. Batch: 0.67s. S_Loss: 2.7275. T_Loss: 2.1516. Mask: 0.9523. :  80%|████████  | 40/50 [00:26<00:06,  1.45it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7258. T_Loss: 2.1561. Mask: 0.9524. :  80%|████████  | 40/50 [00:27<00:06,  1.45it/s]Train Iter: 191/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7258. T_Loss: 2.1561. Mask: 0.9524. :  82%|████████▏ | 41/50 [00:27<00:05,  1.69it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7253. T_Loss: 2.1595. Mask: 0.9528. :  82%|████████▏ | 41/50 [00:27<00:05,  1.69it/s]Train Iter: 192/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7253. T_Loss: 2.1595. Mask: 0.9528. :  84%|████████▍ | 42/50 [00:27<00:03,  2.03it/s]total : 1000  current step :  190
total : 1000  current step :  191
total : 1000  current step :  192
Train Iter: 193/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7255. T_Loss: 2.1655. Mask: 0.9519. :  84%|████████▍ | 42/50 [00:28<00:03,  2.03it/s]Train Iter: 193/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7255. T_Loss: 2.1655. Mask: 0.9519. :  86%|████████▌ | 43/50 [00:28<00:05,  1.37it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7256. T_Loss: 2.1725. Mask: 0.9521. :  86%|████████▌ | 43/50 [00:29<00:05,  1.37it/s]Train Iter: 194/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7256. T_Loss: 2.1725. Mask: 0.9521. :  88%|████████▊ | 44/50 [00:29<00:03,  1.67it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7254. T_Loss: 2.1761. Mask: 0.9526. :  88%|████████▊ | 44/50 [00:29<00:03,  1.67it/s]Train Iter: 195/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7254. T_Loss: 2.1761. Mask: 0.9526. :  90%|█████████ | 45/50 [00:29<00:02,  2.08it/s]total : 1000  current step :  193
total : 1000  current step :  194
total : 1000  current step :  195
Train Iter: 196/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7242. T_Loss: 2.1776. Mask: 0.9521. :  90%|█████████ | 45/50 [00:30<00:02,  2.08it/s]Train Iter: 196/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7242. T_Loss: 2.1776. Mask: 0.9521. :  92%|█████████▏| 46/50 [00:30<00:02,  1.45it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7257. T_Loss: 2.1828. Mask: 0.9519. :  92%|█████████▏| 46/50 [00:30<00:02,  1.45it/s]Train Iter: 197/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7257. T_Loss: 2.1828. Mask: 0.9519. :  94%|█████████▍| 47/50 [00:30<00:01,  1.74it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.7254. T_Loss: 2.1834. Mask: 0.9517. :  94%|█████████▍| 47/50 [00:31<00:01,  1.74it/s]Train Iter: 198/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.7254. T_Loss: 2.1834. Mask: 0.9517. :  96%|█████████▌| 48/50 [00:31<00:00,  2.14it/s]total : 1000  current step :  196
total : 1000  current step :  197
total : 1000  current step :  198
Train Iter: 199/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7258. T_Loss: 2.1821. Mask: 0.9519. :  96%|█████████▌| 48/50 [00:32<00:00,  2.14it/s]Train Iter: 199/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7258. T_Loss: 2.1821. Mask: 0.9519. :  98%|█████████▊| 49/50 [00:32<00:00,  1.55it/s]total : 1000  current step :  199
Train Iter: 200/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7263. T_Loss: 2.1804. Mask: 0.9520. :  98%|█████████▊| 49/50 [00:32<00:00,  1.55it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7263. T_Loss: 2.1804. Mask: 0.9520. : 100%|██████████| 50/50 [00:32<00:00,  1.48it/s]Train Iter: 200/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7263. T_Loss: 2.1804. Mask: 0.9520. : 100%|██████████| 50/50 [00:32<00:00,  1.52it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 3.9142. top1: 0.00. top5: 0.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.53s. Loss: 3.9142. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.88it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 3.9416. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.88it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 3.9416. top1: 0.00. top5: 0.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.56it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 3.9430. top1: 0.00. top5: 0.13. :  25%|██▌       | 2/8 [00:01<00:02,  2.56it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 3.9430. top1: 0.00. top5: 0.13. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 3.8973. top1: 0.00. top5: 1.86. :  38%|███▊      | 3/8 [00:01<00:01,  2.76it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 3.8973. top1: 0.00. top5: 1.86. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 3.6201. top1: 0.00. top5: 14.69. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 3.6201. top1: 0.00. top5: 14.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.42it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 3.4284. top1: 0.00. top5: 23.63. :  62%|██████▎   | 5/8 [00:01<00:00,  3.42it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 3.4284. top1: 0.00. top5: 23.63. :  75%|███████▌  | 6/8 [00:01<00:00,  3.44it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 3.2896. top1: 0.00. top5: 29.85. :  75%|███████▌  | 6/8 [00:02<00:00,  3.44it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 3.2896. top1: 0.00. top5: 29.85. :  88%|████████▊ | 7/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 3.2039. top1: 0.00. top5: 34.15. :  88%|████████▊ | 7/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 3.2039. top1: 0.00. top5: 34.15. : 100%|██████████| 8/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 3.2039. top1: 0.00. top5: 34.15. : 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]
total : 1000  current step :  200
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.00s. Batch: 0.24s. S_Loss: 2.6694. T_Loss: 2.0950. Mask: 0.9688. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 201/1000. LR: 0.0000. Data: 0.00s. Batch: 0.24s. S_Loss: 2.6694. T_Loss: 2.0950. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:11,  4.14it/s]total : 1000  current step :  201
Train Iter: 202/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.7568. T_Loss: 2.1911. Mask: 0.9590. :   2%|▏         | 1/50 [00:01<00:11,  4.14it/s]Train Iter: 202/1000. LR: 0.0000. Data: 0.43s. Batch: 0.62s. S_Loss: 2.7568. T_Loss: 2.1911. Mask: 0.9590. :   4%|▍         | 2/50 [00:01<00:33,  1.45it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.7654. T_Loss: 2.3145. Mask: 0.9570. :   4%|▍         | 2/50 [00:01<00:33,  1.45it/s]Train Iter: 203/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.7654. T_Loss: 2.3145. Mask: 0.9570. :   6%|▌         | 3/50 [00:01<00:26,  1.77it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.7408. T_Loss: 2.2803. Mask: 0.9619. :   6%|▌         | 3/50 [00:01<00:26,  1.77it/s]Train Iter: 204/1000. LR: 0.0000. Data: 0.28s. Batch: 0.48s. S_Loss: 2.7408. T_Loss: 2.2803. Mask: 0.9619. :   8%|▊         | 4/50 [00:01<00:20,  2.24it/s]total : 1000  current step :  202
total : 1000  current step :  203
total : 1000  current step :  204
Train Iter: 205/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7380. T_Loss: 2.2398. Mask: 0.9594. :   8%|▊         | 4/50 [00:02<00:20,  2.24it/s]Train Iter: 205/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7380. T_Loss: 2.2398. Mask: 0.9594. :  10%|█         | 5/50 [00:02<00:26,  1.67it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.7177. T_Loss: 2.2443. Mask: 0.9583. :  10%|█         | 5/50 [00:03<00:26,  1.67it/s]Train Iter: 206/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.7177. T_Loss: 2.2443. Mask: 0.9583. :  12%|█▏        | 6/50 [00:03<00:21,  2.03it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.29s. Batch: 0.48s. S_Loss: 2.7110. T_Loss: 2.2378. Mask: 0.9576. :  12%|█▏        | 6/50 [00:03<00:21,  2.03it/s]Train Iter: 207/1000. LR: 0.0000. Data: 0.29s. Batch: 0.48s. S_Loss: 2.7110. T_Loss: 2.2378. Mask: 0.9576. :  14%|█▍        | 7/50 [00:03<00:18,  2.29it/s]total : 1000  current step :  205
total : 1000  current step :  206
total : 1000  current step :  207
Train Iter: 208/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.7155. T_Loss: 2.2575. Mask: 0.9580. :  14%|█▍        | 7/50 [00:04<00:18,  2.29it/s]Train Iter: 208/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.7155. T_Loss: 2.2575. Mask: 0.9580. :  16%|█▌        | 8/50 [00:04<00:26,  1.59it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.7272. T_Loss: 2.2777. Mask: 0.9596. :  16%|█▌        | 8/50 [00:04<00:26,  1.59it/s]Train Iter: 209/1000. LR: 0.0000. Data: 0.31s. Batch: 0.51s. S_Loss: 2.7272. T_Loss: 2.2777. Mask: 0.9596. :  18%|█▊        | 9/50 [00:04<00:20,  2.05it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.29s. Batch: 0.49s. S_Loss: 2.7283. T_Loss: 2.3057. Mask: 0.9582. :  18%|█▊        | 9/50 [00:04<00:20,  2.05it/s]Train Iter: 210/1000. LR: 0.0000. Data: 0.29s. Batch: 0.49s. S_Loss: 2.7283. T_Loss: 2.3057. Mask: 0.9582. :  20%|██        | 10/50 [00:04<00:17,  2.31it/s]total : 1000  current step :  208
total : 1000  current step :  209
total : 1000  current step :  210
Train Iter: 211/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7359. T_Loss: 2.3180. Mask: 0.9585. :  20%|██        | 10/50 [00:06<00:17,  2.31it/s]Train Iter: 211/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7359. T_Loss: 2.3180. Mask: 0.9585. :  22%|██▏       | 11/50 [00:06<00:26,  1.47it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.7383. T_Loss: 2.3213. Mask: 0.9603. :  22%|██▏       | 11/50 [00:06<00:26,  1.47it/s]Train Iter: 212/1000. LR: 0.0000. Data: 0.33s. Batch: 0.54s. S_Loss: 2.7383. T_Loss: 2.3213. Mask: 0.9603. :  24%|██▍       | 12/50 [00:06<00:21,  1.79it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.32s. Batch: 0.52s. S_Loss: 2.7366. T_Loss: 2.3217. Mask: 0.9606. :  24%|██▍       | 12/50 [00:06<00:21,  1.79it/s]Train Iter: 213/1000. LR: 0.0000. Data: 0.32s. Batch: 0.52s. S_Loss: 2.7366. T_Loss: 2.3217. Mask: 0.9606. :  26%|██▌       | 13/50 [00:06<00:18,  2.00it/s]total : 1000  current step :  211
total : 1000  current step :  212
total : 1000  current step :  213
Train Iter: 214/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.7411. T_Loss: 2.3289. Mask: 0.9601. :  26%|██▌       | 13/50 [00:07<00:18,  2.00it/s]Train Iter: 214/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.7411. T_Loss: 2.3289. Mask: 0.9601. :  28%|██▊       | 14/50 [00:07<00:24,  1.46it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.7463. T_Loss: 2.3333. Mask: 0.9599. :  28%|██▊       | 14/50 [00:08<00:24,  1.46it/s]Train Iter: 215/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.7463. T_Loss: 2.3333. Mask: 0.9599. :  30%|███       | 15/50 [00:08<00:19,  1.81it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.33s. Batch: 0.53s. S_Loss: 2.7463. T_Loss: 2.3385. Mask: 0.9587. :  30%|███       | 15/50 [00:08<00:19,  1.81it/s]Train Iter: 216/1000. LR: 0.0000. Data: 0.33s. Batch: 0.53s. S_Loss: 2.7463. T_Loss: 2.3385. Mask: 0.9587. :  32%|███▏      | 16/50 [00:08<00:16,  2.12it/s]total : 1000  current step :  214
total : 1000  current step :  215
total : 1000  current step :  216
Train Iter: 217/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.7479. T_Loss: 2.3423. Mask: 0.9589. :  32%|███▏      | 16/50 [00:09<00:16,  2.12it/s]Train Iter: 217/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.7479. T_Loss: 2.3423. Mask: 0.9589. :  34%|███▍      | 17/50 [00:09<00:21,  1.53it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.7432. T_Loss: 2.3563. Mask: 0.9590. :  34%|███▍      | 17/50 [00:09<00:21,  1.53it/s]Train Iter: 218/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.7432. T_Loss: 2.3563. Mask: 0.9590. :  36%|███▌      | 18/50 [00:09<00:18,  1.76it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.7366. T_Loss: 2.3740. Mask: 0.9574. :  36%|███▌      | 18/50 [00:10<00:18,  1.76it/s]Train Iter: 219/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.7366. T_Loss: 2.3740. Mask: 0.9574. :  38%|███▊      | 19/50 [00:10<00:15,  1.95it/s]total : 1000  current step :  217
total : 1000  current step :  218
total : 1000  current step :  219
Train Iter: 220/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.7348. T_Loss: 2.3862. Mask: 0.9572. :  38%|███▊      | 19/50 [00:11<00:15,  1.95it/s]Train Iter: 220/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.7348. T_Loss: 2.3862. Mask: 0.9572. :  40%|████      | 20/50 [00:11<00:20,  1.48it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7344. T_Loss: 2.3957. Mask: 0.9574. :  40%|████      | 20/50 [00:11<00:20,  1.48it/s]Train Iter: 221/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7344. T_Loss: 2.3957. Mask: 0.9574. :  42%|████▏     | 21/50 [00:11<00:16,  1.71it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.7311. T_Loss: 2.4005. Mask: 0.9576. :  42%|████▏     | 21/50 [00:11<00:16,  1.71it/s]Train Iter: 222/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.7311. T_Loss: 2.4005. Mask: 0.9576. :  44%|████▍     | 22/50 [00:11<00:13,  2.11it/s]total : 1000  current step :  220
total : 1000  current step :  221
total : 1000  current step :  222
Train Iter: 223/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7285. T_Loss: 2.4061. Mask: 0.9567. :  44%|████▍     | 22/50 [00:13<00:13,  2.11it/s]Train Iter: 223/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7285. T_Loss: 2.4061. Mask: 0.9567. :  46%|████▌     | 23/50 [00:13<00:18,  1.46it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7284. T_Loss: 2.4094. Mask: 0.9556. :  46%|████▌     | 23/50 [00:13<00:18,  1.46it/s]Train Iter: 224/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7284. T_Loss: 2.4094. Mask: 0.9556. :  48%|████▊     | 24/50 [00:13<00:14,  1.73it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.7247. T_Loss: 2.4137. Mask: 0.9553. :  48%|████▊     | 24/50 [00:13<00:14,  1.73it/s]Train Iter: 225/1000. LR: 0.0000. Data: 0.34s. Batch: 0.54s. S_Loss: 2.7247. T_Loss: 2.4137. Mask: 0.9553. :  50%|█████     | 25/50 [00:13<00:11,  2.14it/s]total : 1000  current step :  223
total : 1000  current step :  224
total : 1000  current step :  225
Train Iter: 226/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7249. T_Loss: 2.4254. Mask: 0.9548. :  50%|█████     | 25/50 [00:14<00:11,  2.14it/s]Train Iter: 226/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7249. T_Loss: 2.4254. Mask: 0.9548. :  52%|█████▏    | 26/50 [00:14<00:15,  1.50it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7232. T_Loss: 2.4302. Mask: 0.9550. :  52%|█████▏    | 26/50 [00:15<00:15,  1.50it/s]Train Iter: 227/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7232. T_Loss: 2.4302. Mask: 0.9550. :  54%|█████▍    | 27/50 [00:15<00:13,  1.75it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.7244. T_Loss: 2.4282. Mask: 0.9558. :  54%|█████▍    | 27/50 [00:15<00:13,  1.75it/s]Train Iter: 228/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.7244. T_Loss: 2.4282. Mask: 0.9558. :  56%|█████▌    | 28/50 [00:15<00:11,  1.95it/s]total : 1000  current step :  226
total : 1000  current step :  227
total : 1000  current step :  228
Train Iter: 229/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7212. T_Loss: 2.4278. Mask: 0.9564. :  56%|█████▌    | 28/50 [00:16<00:11,  1.95it/s]Train Iter: 229/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7212. T_Loss: 2.4278. Mask: 0.9564. :  58%|█████▊    | 29/50 [00:16<00:14,  1.45it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.7232. T_Loss: 2.4181. Mask: 0.9564. :  58%|█████▊    | 29/50 [00:16<00:14,  1.45it/s]Train Iter: 230/1000. LR: 0.0000. Data: 0.36s. Batch: 0.56s. S_Loss: 2.7232. T_Loss: 2.4181. Mask: 0.9564. :  60%|██████    | 30/50 [00:16<00:11,  1.69it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.7233. T_Loss: 2.4212. Mask: 0.9561. :  60%|██████    | 30/50 [00:17<00:11,  1.69it/s]Train Iter: 231/1000. LR: 0.0000. Data: 0.35s. Batch: 0.55s. S_Loss: 2.7233. T_Loss: 2.4212. Mask: 0.9561. :  62%|██████▏   | 31/50 [00:17<00:09,  1.99it/s]total : 1000  current step :  229
total : 1000  current step :  230
total : 1000  current step :  231
Train Iter: 232/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7223. T_Loss: 2.4220. Mask: 0.9567. :  62%|██████▏   | 31/50 [00:18<00:09,  1.99it/s]Train Iter: 232/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7223. T_Loss: 2.4220. Mask: 0.9567. :  64%|██████▍   | 32/50 [00:18<00:11,  1.55it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7199. T_Loss: 2.4223. Mask: 0.9567. :  64%|██████▍   | 32/50 [00:18<00:11,  1.55it/s]Train Iter: 233/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7199. T_Loss: 2.4223. Mask: 0.9567. :  66%|██████▌   | 33/50 [00:18<00:09,  1.86it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.7191. T_Loss: 2.4206. Mask: 0.9569. :  66%|██████▌   | 33/50 [00:18<00:09,  1.86it/s]Train Iter: 234/1000. LR: 0.0000. Data: 0.34s. Batch: 0.55s. S_Loss: 2.7191. T_Loss: 2.4206. Mask: 0.9569. :  68%|██████▊   | 34/50 [00:18<00:07,  2.06it/s]total : 1000  current step :  232
total : 1000  current step :  233
total : 1000  current step :  234
Train Iter: 235/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.7190. T_Loss: 2.4120. Mask: 0.9566. :  68%|██████▊   | 34/50 [00:19<00:07,  2.06it/s]Train Iter: 235/1000. LR: 0.0000. Data: 0.35s. Batch: 0.57s. S_Loss: 2.7190. T_Loss: 2.4120. Mask: 0.9566. :  70%|███████   | 35/50 [00:19<00:09,  1.59it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7219. T_Loss: 2.4134. Mask: 0.9566. :  70%|███████   | 35/50 [00:20<00:09,  1.59it/s]Train Iter: 236/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7219. T_Loss: 2.4134. Mask: 0.9566. :  72%|███████▏  | 36/50 [00:20<00:08,  1.67it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7221. T_Loss: 2.4105. Mask: 0.9564. :  72%|███████▏  | 36/50 [00:20<00:08,  1.67it/s]Train Iter: 237/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7221. T_Loss: 2.4105. Mask: 0.9564. :  74%|███████▍  | 37/50 [00:20<00:06,  1.92it/s]total : 1000  current step :  235
total : 1000  current step :  236
total : 1000  current step :  237
Train Iter: 238/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7237. T_Loss: 2.4136. Mask: 0.9564. :  74%|███████▍  | 37/50 [00:21<00:06,  1.92it/s]Train Iter: 238/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7237. T_Loss: 2.4136. Mask: 0.9564. :  76%|███████▌  | 38/50 [00:21<00:07,  1.53it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7246. T_Loss: 2.4121. Mask: 0.9567. :  76%|███████▌  | 38/50 [00:21<00:07,  1.53it/s]Train Iter: 239/1000. LR: 0.0000. Data: 0.35s. Batch: 0.56s. S_Loss: 2.7246. T_Loss: 2.4121. Mask: 0.9567. :  78%|███████▊  | 39/50 [00:21<00:06,  1.83it/s]total : 1000  current step :  238
total : 1000  current step :  239
Train Iter: 240/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7257. T_Loss: 2.4066. Mask: 0.9567. :  78%|███████▊  | 39/50 [00:22<00:06,  1.83it/s]Train Iter: 240/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7257. T_Loss: 2.4066. Mask: 0.9567. :  80%|████████  | 40/50 [00:22<00:06,  1.62it/s]total : 1000  current step :  240
Train Iter: 241/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7254. T_Loss: 2.4077. Mask: 0.9569. :  80%|████████  | 40/50 [00:23<00:06,  1.62it/s]Train Iter: 241/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7254. T_Loss: 2.4077. Mask: 0.9569. :  82%|████████▏ | 41/50 [00:23<00:06,  1.38it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.7249. T_Loss: 2.4099. Mask: 0.9568. :  82%|████████▏ | 41/50 [00:24<00:06,  1.38it/s]Train Iter: 242/1000. LR: 0.0000. Data: 0.37s. Batch: 0.57s. S_Loss: 2.7249. T_Loss: 2.4099. Mask: 0.9568. :  84%|████████▍ | 42/50 [00:24<00:05,  1.56it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7246. T_Loss: 2.4154. Mask: 0.9567. :  84%|████████▍ | 42/50 [00:24<00:05,  1.56it/s]Train Iter: 243/1000. LR: 0.0000. Data: 0.36s. Batch: 0.57s. S_Loss: 2.7246. T_Loss: 2.4154. Mask: 0.9567. :  86%|████████▌ | 43/50 [00:24<00:04,  1.70it/s]total : 1000  current step :  241
total : 1000  current step :  242
total : 1000  current step :  243
Train Iter: 244/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7235. T_Loss: 2.4177. Mask: 0.9565. :  86%|████████▌ | 43/50 [00:25<00:04,  1.70it/s]Train Iter: 244/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7235. T_Loss: 2.4177. Mask: 0.9565. :  88%|████████▊ | 44/50 [00:25<00:04,  1.41it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7249. T_Loss: 2.4221. Mask: 0.9564. :  88%|████████▊ | 44/50 [00:26<00:04,  1.41it/s]Train Iter: 245/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7249. T_Loss: 2.4221. Mask: 0.9564. :  90%|█████████ | 45/50 [00:26<00:03,  1.59it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.7249. T_Loss: 2.4258. Mask: 0.9564. :  90%|█████████ | 45/50 [00:26<00:03,  1.59it/s]Train Iter: 246/1000. LR: 0.0000. Data: 0.36s. Batch: 0.58s. S_Loss: 2.7249. T_Loss: 2.4258. Mask: 0.9564. :  92%|█████████▏| 46/50 [00:26<00:02,  1.73it/s]total : 1000  current step :  244
total : 1000  current step :  245
total : 1000  current step :  246
Train Iter: 247/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.7252. T_Loss: 2.4314. Mask: 0.9566. :  92%|█████████▏| 46/50 [00:27<00:02,  1.73it/s]Train Iter: 247/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.7252. T_Loss: 2.4314. Mask: 0.9566. :  94%|█████████▍| 47/50 [00:27<00:02,  1.30it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7260. T_Loss: 2.4297. Mask: 0.9569. :  94%|█████████▍| 47/50 [00:28<00:02,  1.30it/s]Train Iter: 248/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7260. T_Loss: 2.4297. Mask: 0.9569. :  96%|█████████▌| 48/50 [00:28<00:01,  1.52it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7264. T_Loss: 2.4321. Mask: 0.9573. :  96%|█████████▌| 48/50 [00:28<00:01,  1.52it/s]Train Iter: 249/1000. LR: 0.0000. Data: 0.37s. Batch: 0.58s. S_Loss: 2.7264. T_Loss: 2.4321. Mask: 0.9573. :  98%|█████████▊| 49/50 [00:28<00:00,  1.76it/s]total : 1000  current step :  247
total : 1000  current step :  248
total : 1000  current step :  249
Train Iter: 250/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.7273. T_Loss: 2.4396. Mask: 0.9570. :  98%|█████████▊| 49/50 [00:29<00:00,  1.76it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.7273. T_Loss: 2.4396. Mask: 0.9570. : 100%|██████████| 50/50 [00:29<00:00,  1.31it/s]Train Iter: 250/1000. LR: 0.0000. Data: 0.38s. Batch: 0.59s. S_Loss: 2.7273. T_Loss: 2.4396. Mask: 0.9570. : 100%|██████████| 50/50 [00:29<00:00,  1.68it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 3.5046. top1: 0.00. top5: 0.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 3.5046. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 3.5215. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 3.5215. top1: 0.00. top5: 0.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 3.5230. top1: 0.00. top5: 0.13. :  25%|██▌       | 2/8 [00:01<00:02,  2.15it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 3.5230. top1: 0.00. top5: 0.13. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 3.4858. top1: 0.00. top5: 2.25. :  38%|███▊      | 3/8 [00:01<00:01,  2.56it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 3.4858. top1: 0.00. top5: 2.25. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 3.2576. top1: 0.00. top5: 17.19. :  50%|█████     | 4/8 [00:01<00:01,  2.77it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 3.2576. top1: 0.00. top5: 17.19. :  62%|██████▎   | 5/8 [00:01<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 3.1012. top1: 0.00. top5: 27.80. :  62%|██████▎   | 5/8 [00:02<00:01,  2.77it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 3.1012. top1: 0.00. top5: 27.80. :  75%|███████▌  | 6/8 [00:02<00:00,  2.80it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.9887. top1: 0.00. top5: 35.04. :  75%|███████▌  | 6/8 [00:02<00:00,  2.80it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 2.9887. top1: 0.00. top5: 35.04. :  88%|████████▊ | 7/8 [00:02<00:00,  2.89it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.9190. top1: 0.00. top5: 39.55. :  88%|████████▊ | 7/8 [00:02<00:00,  2.89it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.9190. top1: 0.00. top5: 39.55. : 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 2.9190. top1: 0.00. top5: 39.55. : 100%|██████████| 8/8 [00:03<00:00,  2.52it/s]
total : 1000  current step :  250
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.27s. S_Loss: 2.7356. T_Loss: 2.7363. Mask: 0.9766. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 251/1000. LR: 0.0000. Data: 0.01s. Batch: 0.27s. S_Loss: 2.7356. T_Loss: 2.7363. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:13,  3.64it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.27s. S_Loss: 2.7453. T_Loss: 2.6876. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:13,  3.64it/s]Train Iter: 252/1000. LR: 0.0000. Data: 0.01s. Batch: 0.27s. S_Loss: 2.7453. T_Loss: 2.6876. Mask: 0.9766. :   4%|▍         | 2/50 [00:00<00:13,  3.64it/s]total : 1000  current step :  251
total : 1000  current step :  252
Train Iter: 253/1000. LR: 0.0000. Data: 0.33s. Batch: 0.57s. S_Loss: 2.7425. T_Loss: 2.6846. Mask: 0.9674. :   4%|▍         | 2/50 [00:01<00:13,  3.64it/s]Train Iter: 253/1000. LR: 0.0000. Data: 0.33s. Batch: 0.57s. S_Loss: 2.7425. T_Loss: 2.6846. Mask: 0.9674. :   6%|▌         | 3/50 [00:01<00:31,  1.48it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.28s. Batch: 0.53s. S_Loss: 2.7606. T_Loss: 2.7025. Mask: 0.9688. :   6%|▌         | 3/50 [00:02<00:31,  1.48it/s]Train Iter: 254/1000. LR: 0.0000. Data: 0.28s. Batch: 0.53s. S_Loss: 2.7606. T_Loss: 2.7025. Mask: 0.9688. :   8%|▊         | 4/50 [00:02<00:26,  1.74it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.23s. Batch: 0.49s. S_Loss: 2.7727. T_Loss: 2.7278. Mask: 0.9672. :   8%|▊         | 4/50 [00:02<00:26,  1.74it/s]Train Iter: 255/1000. LR: 0.0000. Data: 0.23s. Batch: 0.49s. S_Loss: 2.7727. T_Loss: 2.7278. Mask: 0.9672. :  10%|█         | 5/50 [00:02<00:21,  2.09it/s]total : 1000  current step :  253
total : 1000  current step :  254
total : 1000  current step :  255
Train Iter: 256/1000. LR: 0.0000. Data: 0.36s. Batch: 0.61s. S_Loss: 2.7661. T_Loss: 2.7442. Mask: 0.9648. :  10%|█         | 5/50 [00:03<00:21,  2.09it/s]Train Iter: 256/1000. LR: 0.0000. Data: 0.36s. Batch: 0.61s. S_Loss: 2.7661. T_Loss: 2.7442. Mask: 0.9648. :  12%|█▏        | 6/50 [00:03<00:32,  1.35it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.7529. T_Loss: 2.7329. Mask: 0.9587. :  12%|█▏        | 6/50 [00:04<00:32,  1.35it/s]Train Iter: 257/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 2.7529. T_Loss: 2.7329. Mask: 0.9587. :  14%|█▍        | 7/50 [00:04<00:27,  1.58it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.7569. T_Loss: 2.7282. Mask: 0.9580. :  14%|█▍        | 7/50 [00:04<00:27,  1.58it/s]Train Iter: 258/1000. LR: 0.0000. Data: 0.32s. Batch: 0.55s. S_Loss: 2.7569. T_Loss: 2.7282. Mask: 0.9580. :  16%|█▌        | 8/50 [00:04<00:22,  1.91it/s]total : 1000  current step :  256
total : 1000  current step :  257
total : 1000  current step :  258
Train Iter: 259/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.7631. T_Loss: 2.7099. Mask: 0.9583. :  16%|█▌        | 8/50 [00:05<00:22,  1.91it/s]Train Iter: 259/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.7631. T_Loss: 2.7099. Mask: 0.9583. :  18%|█▊        | 9/50 [00:05<00:32,  1.26it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.7666. T_Loss: 2.7254. Mask: 0.9594. :  18%|█▊        | 9/50 [00:06<00:32,  1.26it/s]Train Iter: 260/1000. LR: 0.0000. Data: 0.38s. Batch: 0.62s. S_Loss: 2.7666. T_Loss: 2.7254. Mask: 0.9594. :  20%|██        | 10/50 [00:06<00:27,  1.48it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.7682. T_Loss: 2.7012. Mask: 0.9609. :  20%|██        | 10/50 [00:06<00:27,  1.48it/s]Train Iter: 261/1000. LR: 0.0000. Data: 0.37s. Batch: 0.61s. S_Loss: 2.7682. T_Loss: 2.7012. Mask: 0.9609. :  22%|██▏       | 11/50 [00:06<00:24,  1.59it/s]total : 1000  current step :  259
total : 1000  current step :  260
total : 1000  current step :  261
Train Iter: 262/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7653. T_Loss: 2.7156. Mask: 0.9603. :  22%|██▏       | 11/50 [00:07<00:24,  1.59it/s]Train Iter: 262/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7653. T_Loss: 2.7156. Mask: 0.9603. :  24%|██▍       | 12/50 [00:07<00:31,  1.22it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.7715. T_Loss: 2.7048. Mask: 0.9612. :  24%|██▍       | 12/50 [00:08<00:31,  1.22it/s]Train Iter: 263/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.7715. T_Loss: 2.7048. Mask: 0.9612. :  26%|██▌       | 13/50 [00:08<00:25,  1.45it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.7723. T_Loss: 2.6919. Mask: 0.9609. :  26%|██▌       | 13/50 [00:08<00:25,  1.45it/s]Train Iter: 264/1000. LR: 0.0000. Data: 0.41s. Batch: 0.64s. S_Loss: 2.7723. T_Loss: 2.6919. Mask: 0.9609. :  28%|██▊       | 14/50 [00:08<00:24,  1.50it/s]total : 1000  current step :  262
total : 1000  current step :  263
total : 1000  current step :  264
Train Iter: 265/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7736. T_Loss: 2.6964. Mask: 0.9602. :  28%|██▊       | 14/50 [00:10<00:24,  1.50it/s]Train Iter: 265/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7736. T_Loss: 2.6964. Mask: 0.9602. :  30%|███       | 15/50 [00:10<00:29,  1.19it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7703. T_Loss: 2.7076. Mask: 0.9607. :  30%|███       | 15/50 [00:10<00:29,  1.19it/s]Train Iter: 266/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7703. T_Loss: 2.7076. Mask: 0.9607. :  32%|███▏      | 16/50 [00:10<00:23,  1.47it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.7693. T_Loss: 2.7204. Mask: 0.9596. :  32%|███▏      | 16/50 [00:10<00:23,  1.47it/s]Train Iter: 267/1000. LR: 0.0000. Data: 0.42s. Batch: 0.64s. S_Loss: 2.7693. T_Loss: 2.7204. Mask: 0.9596. :  34%|███▍      | 17/50 [00:10<00:20,  1.63it/s]total : 1000  current step :  265
total : 1000  current step :  266
total : 1000  current step :  267
Train Iter: 268/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.7645. T_Loss: 2.7260. Mask: 0.9583. :  34%|███▍      | 17/50 [00:12<00:20,  1.63it/s]Train Iter: 268/1000. LR: 0.0000. Data: 0.46s. Batch: 0.68s. S_Loss: 2.7645. T_Loss: 2.7260. Mask: 0.9583. :  36%|███▌      | 18/50 [00:12<00:26,  1.23it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7602. T_Loss: 2.7424. Mask: 0.9585. :  36%|███▌      | 18/50 [00:12<00:26,  1.23it/s]Train Iter: 269/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7602. T_Loss: 2.7424. Mask: 0.9585. :  38%|███▊      | 19/50 [00:12<00:21,  1.45it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7560. T_Loss: 2.7477. Mask: 0.9580. :  38%|███▊      | 19/50 [00:13<00:21,  1.45it/s]Train Iter: 270/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7560. T_Loss: 2.7477. Mask: 0.9580. :  40%|████      | 20/50 [00:13<00:17,  1.70it/s]total : 1000  current step :  268
total : 1000  current step :  269
total : 1000  current step :  270
Train Iter: 271/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7523. T_Loss: 2.7424. Mask: 0.9570. :  40%|████      | 20/50 [00:14<00:17,  1.70it/s]Train Iter: 271/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7523. T_Loss: 2.7424. Mask: 0.9570. :  42%|████▏     | 21/50 [00:14<00:22,  1.29it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7522. T_Loss: 2.7446. Mask: 0.9574. :  42%|████▏     | 21/50 [00:14<00:22,  1.29it/s]Train Iter: 272/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7522. T_Loss: 2.7446. Mask: 0.9574. :  44%|████▍     | 22/50 [00:14<00:18,  1.53it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7493. T_Loss: 2.7538. Mask: 0.9570. :  44%|████▍     | 22/50 [00:14<00:18,  1.53it/s]Train Iter: 273/1000. LR: 0.0000. Data: 0.42s. Batch: 0.65s. S_Loss: 2.7493. T_Loss: 2.7538. Mask: 0.9570. :  46%|████▌     | 23/50 [00:14<00:15,  1.77it/s]total : 1000  current step :  271
total : 1000  current step :  272
total : 1000  current step :  273
Train Iter: 274/1000. LR: 0.0000. Data: 0.44s. Batch: 0.68s. S_Loss: 2.7502. T_Loss: 2.7559. Mask: 0.9567. :  46%|████▌     | 23/50 [00:16<00:15,  1.77it/s]Train Iter: 274/1000. LR: 0.0000. Data: 0.44s. Batch: 0.68s. S_Loss: 2.7502. T_Loss: 2.7559. Mask: 0.9567. :  48%|████▊     | 24/50 [00:16<00:20,  1.27it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7529. T_Loss: 2.7532. Mask: 0.9569. :  48%|████▊     | 24/50 [00:16<00:20,  1.27it/s]Train Iter: 275/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7529. T_Loss: 2.7532. Mask: 0.9569. :  50%|█████     | 25/50 [00:16<00:16,  1.54it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.7537. T_Loss: 2.7410. Mask: 0.9563. :  50%|█████     | 25/50 [00:16<00:16,  1.54it/s]Train Iter: 276/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.7537. T_Loss: 2.7410. Mask: 0.9563. :  52%|█████▏    | 26/50 [00:16<00:13,  1.83it/s]total : 1000  current step :  274
total : 1000  current step :  275
total : 1000  current step :  276
Train Iter: 277/1000. LR: 0.0000. Data: 0.44s. Batch: 0.68s. S_Loss: 2.7517. T_Loss: 2.7387. Mask: 0.9575. :  52%|█████▏    | 26/50 [00:18<00:13,  1.83it/s]Train Iter: 277/1000. LR: 0.0000. Data: 0.44s. Batch: 0.68s. S_Loss: 2.7517. T_Loss: 2.7387. Mask: 0.9575. :  54%|█████▍    | 27/50 [00:18<00:18,  1.23it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.42s. Batch: 0.66s. S_Loss: 2.7511. T_Loss: 2.7256. Mask: 0.9570. :  54%|█████▍    | 27/50 [00:18<00:18,  1.23it/s]Train Iter: 278/1000. LR: 0.0000. Data: 0.42s. Batch: 0.66s. S_Loss: 2.7511. T_Loss: 2.7256. Mask: 0.9570. :  56%|█████▌    | 28/50 [00:18<00:14,  1.52it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.7469. T_Loss: 2.7190. Mask: 0.9570. :  56%|█████▌    | 28/50 [00:18<00:14,  1.52it/s]Train Iter: 279/1000. LR: 0.0000. Data: 0.41s. Batch: 0.65s. S_Loss: 2.7469. T_Loss: 2.7190. Mask: 0.9570. :  58%|█████▊    | 29/50 [00:18<00:11,  1.79it/s]total : 1000  current step :  277
total : 1000  current step :  278
total : 1000  current step :  279
Train Iter: 280/1000. LR: 0.0000. Data: 0.45s. Batch: 0.69s. S_Loss: 2.7466. T_Loss: 2.7130. Mask: 0.9573. :  58%|█████▊    | 29/50 [00:20<00:11,  1.79it/s]Train Iter: 280/1000. LR: 0.0000. Data: 0.45s. Batch: 0.69s. S_Loss: 2.7466. T_Loss: 2.7130. Mask: 0.9573. :  60%|██████    | 30/50 [00:20<00:18,  1.10it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.45s. Batch: 0.69s. S_Loss: 2.7460. T_Loss: 2.7157. Mask: 0.9574. :  60%|██████    | 30/50 [00:21<00:18,  1.10it/s]Train Iter: 281/1000. LR: 0.0000. Data: 0.45s. Batch: 0.69s. S_Loss: 2.7460. T_Loss: 2.7157. Mask: 0.9574. :  62%|██████▏   | 31/50 [00:21<00:16,  1.16it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.45s. Batch: 0.69s. S_Loss: 2.7449. T_Loss: 2.7073. Mask: 0.9574. :  62%|██████▏   | 31/50 [00:22<00:16,  1.16it/s]Train Iter: 282/1000. LR: 0.0000. Data: 0.45s. Batch: 0.69s. S_Loss: 2.7449. T_Loss: 2.7073. Mask: 0.9574. :  64%|██████▍   | 32/50 [00:22<00:14,  1.26it/s]total : 1000  current step :  280
total : 1000  current step :  281
total : 1000  current step :  282
Train Iter: 283/1000. LR: 0.0000. Data: 0.46s. Batch: 0.70s. S_Loss: 2.7420. T_Loss: 2.7089. Mask: 0.9575. :  64%|██████▍   | 32/50 [00:23<00:14,  1.26it/s]Train Iter: 283/1000. LR: 0.0000. Data: 0.46s. Batch: 0.70s. S_Loss: 2.7420. T_Loss: 2.7089. Mask: 0.9575. :  66%|██████▌   | 33/50 [00:23<00:15,  1.10it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.45s. Batch: 0.69s. S_Loss: 2.7406. T_Loss: 2.7046. Mask: 0.9577. :  66%|██████▌   | 33/50 [00:23<00:15,  1.10it/s]Train Iter: 284/1000. LR: 0.0000. Data: 0.45s. Batch: 0.69s. S_Loss: 2.7406. T_Loss: 2.7046. Mask: 0.9577. :  68%|██████▊   | 34/50 [00:23<00:11,  1.40it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.44s. Batch: 0.68s. S_Loss: 2.7410. T_Loss: 2.7012. Mask: 0.9577. :  68%|██████▊   | 34/50 [00:23<00:11,  1.40it/s]Train Iter: 285/1000. LR: 0.0000. Data: 0.44s. Batch: 0.68s. S_Loss: 2.7410. T_Loss: 2.7012. Mask: 0.9577. :  70%|███████   | 35/50 [00:23<00:09,  1.65it/s]total : 1000  current step :  283
total : 1000  current step :  284
total : 1000  current step :  285
Train Iter: 286/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.7414. T_Loss: 2.6996. Mask: 0.9580. :  70%|███████   | 35/50 [00:24<00:09,  1.65it/s]Train Iter: 286/1000. LR: 0.0000. Data: 0.46s. Batch: 0.69s. S_Loss: 2.7414. T_Loss: 2.6996. Mask: 0.9580. :  72%|███████▏  | 36/50 [00:24<00:10,  1.35it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7409. T_Loss: 2.7034. Mask: 0.9582. :  72%|███████▏  | 36/50 [00:25<00:10,  1.35it/s]Train Iter: 287/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7409. T_Loss: 2.7034. Mask: 0.9582. :  74%|███████▍  | 37/50 [00:25<00:08,  1.62it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7438. T_Loss: 2.7040. Mask: 0.9582. :  74%|███████▍  | 37/50 [00:25<00:08,  1.62it/s]Train Iter: 288/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7438. T_Loss: 2.7040. Mask: 0.9582. :  76%|███████▌  | 38/50 [00:25<00:06,  1.89it/s]total : 1000  current step :  286
total : 1000  current step :  287
total : 1000  current step :  288
Train Iter: 289/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7448. T_Loss: 2.6998. Mask: 0.9579. :  76%|███████▌  | 38/50 [00:26<00:06,  1.89it/s]Train Iter: 289/1000. LR: 0.0000. Data: 0.45s. Batch: 0.68s. S_Loss: 2.7448. T_Loss: 2.6998. Mask: 0.9579. :  78%|███████▊  | 39/50 [00:26<00:07,  1.49it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7451. T_Loss: 2.7131. Mask: 0.9575. :  78%|███████▊  | 39/50 [00:26<00:07,  1.49it/s]Train Iter: 290/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7451. T_Loss: 2.7131. Mask: 0.9575. :  80%|████████  | 40/50 [00:26<00:05,  1.79it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7461. T_Loss: 2.7161. Mask: 0.9573. :  80%|████████  | 40/50 [00:27<00:05,  1.79it/s]Train Iter: 291/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7461. T_Loss: 2.7161. Mask: 0.9573. :  82%|████████▏ | 41/50 [00:27<00:04,  2.01it/s]total : 1000  current step :  289
total : 1000  current step :  290
total : 1000  current step :  291
Train Iter: 292/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7479. T_Loss: 2.7248. Mask: 0.9576. :  82%|████████▏ | 41/50 [00:28<00:04,  2.01it/s]Train Iter: 292/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7479. T_Loss: 2.7248. Mask: 0.9576. :  84%|████████▍ | 42/50 [00:28<00:05,  1.47it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7451. T_Loss: 2.7317. Mask: 0.9574. :  84%|████████▍ | 42/50 [00:28<00:05,  1.47it/s]Train Iter: 293/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7451. T_Loss: 2.7317. Mask: 0.9574. :  86%|████████▌ | 43/50 [00:28<00:03,  1.76it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7443. T_Loss: 2.7372. Mask: 0.9573. :  86%|████████▌ | 43/50 [00:29<00:03,  1.76it/s]Train Iter: 294/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7443. T_Loss: 2.7372. Mask: 0.9573. :  88%|████████▊ | 44/50 [00:29<00:03,  1.91it/s]total : 1000  current step :  292
total : 1000  current step :  293
total : 1000  current step :  294
Train Iter: 295/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7424. T_Loss: 2.7448. Mask: 0.9576. :  88%|████████▊ | 44/50 [00:30<00:03,  1.91it/s]Train Iter: 295/1000. LR: 0.0000. Data: 0.44s. Batch: 0.67s. S_Loss: 2.7424. T_Loss: 2.7448. Mask: 0.9576. :  90%|█████████ | 45/50 [00:30<00:03,  1.37it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7418. T_Loss: 2.7520. Mask: 0.9575. :  90%|█████████ | 45/50 [00:30<00:03,  1.37it/s]Train Iter: 296/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7418. T_Loss: 2.7520. Mask: 0.9575. :  92%|█████████▏| 46/50 [00:30<00:02,  1.69it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7418. T_Loss: 2.7612. Mask: 0.9574. :  92%|█████████▏| 46/50 [00:30<00:02,  1.69it/s]Train Iter: 297/1000. LR: 0.0000. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7418. T_Loss: 2.7612. Mask: 0.9574. :  94%|█████████▍| 47/50 [00:30<00:01,  2.00it/s]total : 1000  current step :  295
total : 1000  current step :  296
total : 1000  current step :  297
Train Iter: 298/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7422. T_Loss: 2.7694. Mask: 0.9574. :  94%|█████████▍| 47/50 [00:32<00:01,  2.00it/s]Train Iter: 298/1000. LR: 0.0000. Data: 0.44s. Batch: 0.66s. S_Loss: 2.7422. T_Loss: 2.7694. Mask: 0.9574. :  96%|█████████▌| 48/50 [00:32<00:01,  1.41it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7422. T_Loss: 2.7714. Mask: 0.9573. :  96%|█████████▌| 48/50 [00:32<00:01,  1.41it/s]Train Iter: 299/1000. LR: 0.0000. Data: 0.43s. Batch: 0.66s. S_Loss: 2.7422. T_Loss: 2.7714. Mask: 0.9573. :  98%|█████████▊| 49/50 [00:32<00:00,  1.69it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7417. T_Loss: 2.7748. Mask: 0.9573. :  98%|█████████▊| 49/50 [00:32<00:00,  1.69it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7417. T_Loss: 2.7748. Mask: 0.9573. : 100%|██████████| 50/50 [00:32<00:00,  2.00it/s]Train Iter: 300/1000. LR: 0.0188. Data: 0.43s. Batch: 0.65s. S_Loss: 2.7417. T_Loss: 2.7748. Mask: 0.9573. : 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]
total : 1000  current step :  298
total : 1000  current step :  299
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 3.2628. top1: 0.00. top5: 0.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 3.2628. top1: 0.00. top5: 0.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 3.2728. top1: 0.00. top5: 0.20. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 3.2728. top1: 0.00. top5: 0.20. :  25%|██▌       | 2/8 [00:00<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 3.2748. top1: 0.00. top5: 0.52. :  25%|██▌       | 2/8 [00:01<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 3.2748. top1: 0.00. top5: 0.52. :  38%|███▊      | 3/8 [00:01<00:01,  3.01it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 3.2426. top1: 0.00. top5: 2.93. :  38%|███▊      | 3/8 [00:01<00:01,  3.01it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 3.2426. top1: 0.00. top5: 2.93. :  50%|█████     | 4/8 [00:01<00:01,  3.40it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 3.0468. top1: 0.00. top5: 19.69. :  50%|█████     | 4/8 [00:01<00:01,  3.40it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 3.0468. top1: 0.00. top5: 19.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 2.9135. top1: 0.00. top5: 30.73. :  62%|██████▎   | 5/8 [00:02<00:01,  2.80it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 2.9135. top1: 0.00. top5: 30.73. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.8182. top1: 0.17. top5: 38.90. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 2.8182. top1: 0.17. top5: 38.90. :  88%|████████▊ | 7/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.7589. top1: 0.15. top5: 43.80. :  88%|████████▊ | 7/8 [00:02<00:00,  3.25it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.7589. top1: 0.15. top5: 43.80. : 100%|██████████| 8/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 2.7589. top1: 0.15. top5: 43.80. : 100%|██████████| 8/8 [00:02<00:00,  2.84it/s]
total : 1000  current step :  300
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.84s. Batch: 1.09s. S_Loss: 2.7474. T_Loss: 2.9645. Mask: 0.9531. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 301/1000. LR: 0.0188. Data: 0.84s. Batch: 1.09s. S_Loss: 2.7474. T_Loss: 2.9645. Mask: 0.9531. :   2%|▏         | 1/50 [00:01<00:53,  1.09s/it]Train Iter: 302/1000. LR: 0.0189. Data: 0.47s. Batch: 0.72s. S_Loss: 2.2234. T_Loss: 2.8785. Mask: 0.9473. :   2%|▏         | 1/50 [00:01<00:53,  1.09s/it]Train Iter: 302/1000. LR: 0.0189. Data: 0.47s. Batch: 0.72s. S_Loss: 2.2234. T_Loss: 2.8785. Mask: 0.9473. :   4%|▍         | 2/50 [00:01<00:31,  1.52it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.36s. Batch: 0.62s. S_Loss: 1.9293. T_Loss: 2.8648. Mask: 0.9518. :   4%|▍         | 2/50 [00:01<00:31,  1.52it/s]Train Iter: 303/1000. LR: 0.0189. Data: 0.36s. Batch: 0.62s. S_Loss: 1.9293. T_Loss: 2.8648. Mask: 0.9518. :   6%|▌         | 3/50 [00:01<00:25,  1.83it/s]total : 1000  current step :  301
total : 1000  current step :  302
total : 1000  current step :  303
Train Iter: 304/1000. LR: 0.0190. Data: 0.46s. Batch: 0.71s. S_Loss: 1.7776. T_Loss: 2.8456. Mask: 0.9541. :   6%|▌         | 3/50 [00:02<00:25,  1.83it/s]Train Iter: 304/1000. LR: 0.0190. Data: 0.46s. Batch: 0.71s. S_Loss: 1.7776. T_Loss: 2.8456. Mask: 0.9541. :   8%|▊         | 4/50 [00:02<00:33,  1.38it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.39s. Batch: 0.66s. S_Loss: 1.6938. T_Loss: 2.8202. Mask: 0.9547. :   8%|▊         | 4/50 [00:03<00:33,  1.38it/s]Train Iter: 305/1000. LR: 0.0191. Data: 0.39s. Batch: 0.66s. S_Loss: 1.6938. T_Loss: 2.8202. Mask: 0.9547. :  10%|█         | 5/50 [00:03<00:27,  1.61it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.33s. Batch: 0.59s. S_Loss: 1.6499. T_Loss: 2.8366. Mask: 0.9577. :  10%|█         | 5/50 [00:03<00:27,  1.61it/s]Train Iter: 306/1000. LR: 0.0191. Data: 0.33s. Batch: 0.59s. S_Loss: 1.6499. T_Loss: 2.8366. Mask: 0.9577. :  12%|█▏        | 6/50 [00:03<00:22,  2.00it/s]total : 1000  current step :  304
total : 1000  current step :  305
total : 1000  current step :  306
Train Iter: 307/1000. LR: 0.0192. Data: 0.41s. Batch: 0.68s. S_Loss: 1.6273. T_Loss: 2.8402. Mask: 0.9565. :  12%|█▏        | 6/50 [00:04<00:22,  2.00it/s]Train Iter: 307/1000. LR: 0.0192. Data: 0.41s. Batch: 0.68s. S_Loss: 1.6273. T_Loss: 2.8402. Mask: 0.9565. :  14%|█▍        | 7/50 [00:04<00:31,  1.38it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.37s. Batch: 0.64s. S_Loss: 1.6217. T_Loss: 2.8358. Mask: 0.9590. :  14%|█▍        | 7/50 [00:05<00:31,  1.38it/s]Train Iter: 308/1000. LR: 0.0193. Data: 0.37s. Batch: 0.64s. S_Loss: 1.6217. T_Loss: 2.8358. Mask: 0.9590. :  16%|█▌        | 8/50 [00:05<00:25,  1.63it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.34s. Batch: 0.60s. S_Loss: 1.6162. T_Loss: 2.8598. Mask: 0.9601. :  16%|█▌        | 8/50 [00:05<00:25,  1.63it/s]Train Iter: 309/1000. LR: 0.0193. Data: 0.34s. Batch: 0.60s. S_Loss: 1.6162. T_Loss: 2.8598. Mask: 0.9601. :  18%|█▊        | 9/50 [00:05<00:21,  1.92it/s]total : 1000  current step :  307
total : 1000  current step :  308
total : 1000  current step :  309
Train Iter: 310/1000. LR: 0.0194. Data: 0.40s. Batch: 0.66s. S_Loss: 1.6104. T_Loss: 2.8599. Mask: 0.9598. :  18%|█▊        | 9/50 [00:06<00:21,  1.92it/s]Train Iter: 310/1000. LR: 0.0194. Data: 0.40s. Batch: 0.66s. S_Loss: 1.6104. T_Loss: 2.8599. Mask: 0.9598. :  20%|██        | 10/50 [00:06<00:28,  1.38it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.37s. Batch: 0.63s. S_Loss: 1.6116. T_Loss: 2.8535. Mask: 0.9616. :  20%|██        | 10/50 [00:06<00:28,  1.38it/s]Train Iter: 311/1000. LR: 0.0194. Data: 0.37s. Batch: 0.63s. S_Loss: 1.6116. T_Loss: 2.8535. Mask: 0.9616. :  22%|██▏       | 11/50 [00:06<00:23,  1.64it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.34s. Batch: 0.60s. S_Loss: 1.6092. T_Loss: 2.8536. Mask: 0.9622. :  22%|██▏       | 11/50 [00:07<00:23,  1.64it/s]Train Iter: 312/1000. LR: 0.0195. Data: 0.34s. Batch: 0.60s. S_Loss: 1.6092. T_Loss: 2.8536. Mask: 0.9622. :  24%|██▍       | 12/50 [00:07<00:19,  1.99it/s]total : 1000  current step :  310
total : 1000  current step :  311
total : 1000  current step :  312
Train Iter: 313/1000. LR: 0.0196. Data: 0.39s. Batch: 0.64s. S_Loss: 1.6115. T_Loss: 2.8567. Mask: 0.9621. :  24%|██▍       | 12/50 [00:08<00:19,  1.99it/s]Train Iter: 313/1000. LR: 0.0196. Data: 0.39s. Batch: 0.64s. S_Loss: 1.6115. T_Loss: 2.8567. Mask: 0.9621. :  26%|██▌       | 13/50 [00:08<00:26,  1.42it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6125. T_Loss: 2.8624. Mask: 0.9637. :  26%|██▌       | 13/50 [00:08<00:26,  1.42it/s]Train Iter: 314/1000. LR: 0.0196. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6125. T_Loss: 2.8624. Mask: 0.9637. :  28%|██▊       | 14/50 [00:08<00:22,  1.57it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.36s. Batch: 0.61s. S_Loss: 1.6175. T_Loss: 2.8538. Mask: 0.9635. :  28%|██▊       | 14/50 [00:09<00:22,  1.57it/s]Train Iter: 315/1000. LR: 0.0197. Data: 0.36s. Batch: 0.61s. S_Loss: 1.6175. T_Loss: 2.8538. Mask: 0.9635. :  30%|███       | 15/50 [00:09<00:18,  1.85it/s]total : 1000  current step :  313
total : 1000  current step :  314
total : 1000  current step :  315
Train Iter: 316/1000. LR: 0.0198. Data: 0.39s. Batch: 0.63s. S_Loss: 1.6175. T_Loss: 2.8628. Mask: 0.9634. :  30%|███       | 15/50 [00:10<00:18,  1.85it/s]Train Iter: 316/1000. LR: 0.0198. Data: 0.39s. Batch: 0.63s. S_Loss: 1.6175. T_Loss: 2.8628. Mask: 0.9634. :  32%|███▏      | 16/50 [00:10<00:22,  1.49it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.37s. Batch: 0.61s. S_Loss: 1.6169. T_Loss: 2.8573. Mask: 0.9623. :  32%|███▏      | 16/50 [00:10<00:22,  1.49it/s]Train Iter: 317/1000. LR: 0.0198. Data: 0.37s. Batch: 0.61s. S_Loss: 1.6169. T_Loss: 2.8573. Mask: 0.9623. :  34%|███▍      | 17/50 [00:10<00:18,  1.76it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.35s. Batch: 0.59s. S_Loss: 1.6151. T_Loss: 2.8626. Mask: 0.9620. :  34%|███▍      | 17/50 [00:10<00:18,  1.76it/s]Train Iter: 318/1000. LR: 0.0199. Data: 0.35s. Batch: 0.59s. S_Loss: 1.6151. T_Loss: 2.8626. Mask: 0.9620. :  36%|███▌      | 18/50 [00:10<00:15,  2.12it/s]total : 1000  current step :  316
total : 1000  current step :  317
total : 1000  current step :  318
Train Iter: 319/1000. LR: 0.0199. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6148. T_Loss: 2.8957. Mask: 0.9616. :  36%|███▌      | 18/50 [00:11<00:15,  2.12it/s]Train Iter: 319/1000. LR: 0.0199. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6148. T_Loss: 2.8957. Mask: 0.9616. :  38%|███▊      | 19/50 [00:11<00:21,  1.44it/s]total : 1000  current step :  319
Train Iter: 320/1000. LR: 0.0200. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6123. T_Loss: 2.9314. Mask: 0.9602. :  38%|███▊      | 19/50 [00:12<00:21,  1.44it/s]Train Iter: 320/1000. LR: 0.0200. Data: 0.38s. Batch: 0.63s. S_Loss: 1.6123. T_Loss: 2.9314. Mask: 0.9602. :  40%|████      | 20/50 [00:12<00:21,  1.41it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.39s. Batch: 0.64s. S_Loss: 1.6080. T_Loss: 2.9386. Mask: 0.9598. :  40%|████      | 20/50 [00:13<00:21,  1.41it/s]Train Iter: 321/1000. LR: 0.0201. Data: 0.39s. Batch: 0.64s. S_Loss: 1.6080. T_Loss: 2.9386. Mask: 0.9598. :  42%|████▏     | 21/50 [00:13<00:20,  1.41it/s]total : 1000  current step :  320
total : 1000  current step :  321
Train Iter: 322/1000. LR: 0.0201. Data: 0.41s. Batch: 0.65s. S_Loss: 1.6052. T_Loss: 2.9573. Mask: 0.9597. :  42%|████▏     | 21/50 [00:14<00:20,  1.41it/s]Train Iter: 322/1000. LR: 0.0201. Data: 0.41s. Batch: 0.65s. S_Loss: 1.6052. T_Loss: 2.9573. Mask: 0.9597. :  44%|████▍     | 22/50 [00:14<00:22,  1.23it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.40s. Batch: 0.64s. S_Loss: 1.6020. T_Loss: 2.9681. Mask: 0.9591. :  44%|████▍     | 22/50 [00:14<00:22,  1.23it/s]Train Iter: 323/1000. LR: 0.0202. Data: 0.40s. Batch: 0.64s. S_Loss: 1.6020. T_Loss: 2.9681. Mask: 0.9591. :  46%|████▌     | 23/50 [00:14<00:18,  1.47it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.38s. Batch: 0.63s. S_Loss: 1.5979. T_Loss: 2.9702. Mask: 0.9583. :  46%|████▌     | 23/50 [00:15<00:18,  1.47it/s]Train Iter: 324/1000. LR: 0.0203. Data: 0.38s. Batch: 0.63s. S_Loss: 1.5979. T_Loss: 2.9702. Mask: 0.9583. :  48%|████▊     | 24/50 [00:15<00:14,  1.80it/s]total : 1000  current step :  322
total : 1000  current step :  323
total : 1000  current step :  324
Train Iter: 325/1000. LR: 0.0203. Data: 0.40s. Batch: 0.64s. S_Loss: 1.5929. T_Loss: 2.9779. Mask: 0.9583. :  48%|████▊     | 24/50 [00:16<00:14,  1.80it/s]Train Iter: 325/1000. LR: 0.0203. Data: 0.40s. Batch: 0.64s. S_Loss: 1.5929. T_Loss: 2.9779. Mask: 0.9583. :  50%|█████     | 25/50 [00:16<00:17,  1.44it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5866. T_Loss: 2.9872. Mask: 0.9569. :  50%|█████     | 25/50 [00:16<00:17,  1.44it/s]Train Iter: 326/1000. LR: 0.0204. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5866. T_Loss: 2.9872. Mask: 0.9569. :  52%|█████▏    | 26/50 [00:16<00:14,  1.67it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.38s. Batch: 0.62s. S_Loss: 1.5807. T_Loss: 2.9833. Mask: 0.9578. :  52%|█████▏    | 26/50 [00:16<00:14,  1.67it/s]Train Iter: 327/1000. LR: 0.0204. Data: 0.38s. Batch: 0.62s. S_Loss: 1.5807. T_Loss: 2.9833. Mask: 0.9578. :  54%|█████▍    | 27/50 [00:16<00:12,  1.86it/s]total : 1000  current step :  325
total : 1000  current step :  326
total : 1000  current step :  327
Train Iter: 328/1000. LR: 0.0205. Data: 0.40s. Batch: 0.64s. S_Loss: 1.5734. T_Loss: 2.9945. Mask: 0.9577. :  54%|█████▍    | 27/50 [00:18<00:12,  1.86it/s]Train Iter: 328/1000. LR: 0.0205. Data: 0.40s. Batch: 0.64s. S_Loss: 1.5734. T_Loss: 2.9945. Mask: 0.9577. :  56%|█████▌    | 28/50 [00:18<00:15,  1.40it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.40s. Batch: 0.63s. S_Loss: 1.5661. T_Loss: 2.9981. Mask: 0.9580. :  56%|█████▌    | 28/50 [00:18<00:15,  1.40it/s]Train Iter: 329/1000. LR: 0.0206. Data: 0.40s. Batch: 0.63s. S_Loss: 1.5661. T_Loss: 2.9981. Mask: 0.9580. :  58%|█████▊    | 29/50 [00:18<00:13,  1.57it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.39s. Batch: 0.62s. S_Loss: 1.5589. T_Loss: 2.9897. Mask: 0.9586. :  58%|█████▊    | 29/50 [00:18<00:13,  1.57it/s]Train Iter: 330/1000. LR: 0.0206. Data: 0.39s. Batch: 0.62s. S_Loss: 1.5589. T_Loss: 2.9897. Mask: 0.9586. :  60%|██████    | 30/50 [00:18<00:10,  1.83it/s]total : 1000  current step :  328
total : 1000  current step :  329
total : 1000  current step :  330
Train Iter: 331/1000. LR: 0.0207. Data: 0.40s. Batch: 0.63s. S_Loss: 1.5522. T_Loss: 2.9857. Mask: 0.9587. :  60%|██████    | 30/50 [00:19<00:10,  1.83it/s]Train Iter: 331/1000. LR: 0.0207. Data: 0.40s. Batch: 0.63s. S_Loss: 1.5522. T_Loss: 2.9857. Mask: 0.9587. :  62%|██████▏   | 31/50 [00:19<00:12,  1.50it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5451. T_Loss: 2.9846. Mask: 0.9585. :  62%|██████▏   | 31/50 [00:20<00:12,  1.50it/s]Train Iter: 332/1000. LR: 0.0208. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5451. T_Loss: 2.9846. Mask: 0.9585. :  64%|██████▍   | 32/50 [00:20<00:10,  1.72it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.38s. Batch: 0.62s. S_Loss: 1.5366. T_Loss: 2.9847. Mask: 0.9586. :  64%|██████▍   | 32/50 [00:20<00:10,  1.72it/s]Train Iter: 333/1000. LR: 0.0208. Data: 0.38s. Batch: 0.62s. S_Loss: 1.5366. T_Loss: 2.9847. Mask: 0.9586. :  66%|██████▌   | 33/50 [00:20<00:08,  1.90it/s]total : 1000  current step :  331
total : 1000  current step :  332
total : 1000  current step :  333
Train Iter: 334/1000. LR: 0.0209. Data: 0.40s. Batch: 0.63s. S_Loss: 1.5303. T_Loss: 2.9893. Mask: 0.9586. :  66%|██████▌   | 33/50 [00:21<00:08,  1.90it/s]Train Iter: 334/1000. LR: 0.0209. Data: 0.40s. Batch: 0.63s. S_Loss: 1.5303. T_Loss: 2.9893. Mask: 0.9586. :  68%|██████▊   | 34/50 [00:21<00:10,  1.48it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5226. T_Loss: 2.9869. Mask: 0.9594. :  68%|██████▊   | 34/50 [00:21<00:10,  1.48it/s]Train Iter: 335/1000. LR: 0.0209. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5226. T_Loss: 2.9869. Mask: 0.9594. :  70%|███████   | 35/50 [00:21<00:09,  1.66it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.38s. Batch: 0.62s. S_Loss: 1.5145. T_Loss: 2.9781. Mask: 0.9594. :  70%|███████   | 35/50 [00:22<00:09,  1.66it/s]Train Iter: 336/1000. LR: 0.0210. Data: 0.38s. Batch: 0.62s. S_Loss: 1.5145. T_Loss: 2.9781. Mask: 0.9594. :  72%|███████▏  | 36/50 [00:22<00:07,  1.93it/s]total : 1000  current step :  334
total : 1000  current step :  335
total : 1000  current step :  336
Train Iter: 337/1000. LR: 0.0211. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5069. T_Loss: 2.9751. Mask: 0.9596. :  72%|███████▏  | 36/50 [00:23<00:07,  1.93it/s]Train Iter: 337/1000. LR: 0.0211. Data: 0.39s. Batch: 0.63s. S_Loss: 1.5069. T_Loss: 2.9751. Mask: 0.9596. :  74%|███████▍  | 37/50 [00:23<00:08,  1.49it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.39s. Batch: 0.62s. S_Loss: 1.5009. T_Loss: 2.9812. Mask: 0.9593. :  74%|███████▍  | 37/50 [00:23<00:08,  1.49it/s]Train Iter: 338/1000. LR: 0.0211. Data: 0.39s. Batch: 0.62s. S_Loss: 1.5009. T_Loss: 2.9812. Mask: 0.9593. :  76%|███████▌  | 38/50 [00:23<00:06,  1.78it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4938. T_Loss: 2.9816. Mask: 0.9598. :  76%|███████▌  | 38/50 [00:23<00:06,  1.78it/s]Train Iter: 339/1000. LR: 0.0212. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4938. T_Loss: 2.9816. Mask: 0.9598. :  78%|███████▊  | 39/50 [00:23<00:05,  2.03it/s]total : 1000  current step :  337
total : 1000  current step :  338
total : 1000  current step :  339
Train Iter: 340/1000. LR: 0.0213. Data: 0.39s. Batch: 0.62s. S_Loss: 1.4862. T_Loss: 2.9848. Mask: 0.9597. :  78%|███████▊  | 39/50 [00:24<00:05,  2.03it/s]Train Iter: 340/1000. LR: 0.0213. Data: 0.39s. Batch: 0.62s. S_Loss: 1.4862. T_Loss: 2.9848. Mask: 0.9597. :  80%|████████  | 40/50 [00:24<00:06,  1.60it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4791. T_Loss: 2.9855. Mask: 0.9596. :  80%|████████  | 40/50 [00:25<00:06,  1.60it/s]Train Iter: 341/1000. LR: 0.0213. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4791. T_Loss: 2.9855. Mask: 0.9596. :  82%|████████▏ | 41/50 [00:25<00:04,  1.95it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4723. T_Loss: 2.9922. Mask: 0.9600. :  82%|████████▏ | 41/50 [00:25<00:04,  1.95it/s]Train Iter: 342/1000. LR: 0.0214. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4723. T_Loss: 2.9922. Mask: 0.9600. :  84%|████████▍ | 42/50 [00:25<00:03,  2.15it/s]total : 1000  current step :  340
total : 1000  current step :  341
total : 1000  current step :  342
Train Iter: 343/1000. LR: 0.0214. Data: 0.39s. Batch: 0.62s. S_Loss: 1.4652. T_Loss: 2.9916. Mask: 0.9603. :  84%|████████▍ | 42/50 [00:26<00:03,  2.15it/s]Train Iter: 343/1000. LR: 0.0214. Data: 0.39s. Batch: 0.62s. S_Loss: 1.4652. T_Loss: 2.9916. Mask: 0.9603. :  86%|████████▌ | 43/50 [00:26<00:04,  1.56it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4577. T_Loss: 2.9931. Mask: 0.9606. :  86%|████████▌ | 43/50 [00:26<00:04,  1.56it/s]Train Iter: 344/1000. LR: 0.0215. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4577. T_Loss: 2.9931. Mask: 0.9606. :  88%|████████▊ | 44/50 [00:26<00:03,  1.81it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.37s. Batch: 0.60s. S_Loss: 1.4507. T_Loss: 2.9960. Mask: 0.9609. :  88%|████████▊ | 44/50 [00:27<00:03,  1.81it/s]Train Iter: 345/1000. LR: 0.0216. Data: 0.37s. Batch: 0.60s. S_Loss: 1.4507. T_Loss: 2.9960. Mask: 0.9609. :  90%|█████████ | 45/50 [00:27<00:02,  2.05it/s]total : 1000  current step :  343
total : 1000  current step :  344
total : 1000  current step :  345
Train Iter: 346/1000. LR: 0.0216. Data: 0.39s. Batch: 0.62s. S_Loss: 1.4459. T_Loss: 3.0172. Mask: 0.9603. :  90%|█████████ | 45/50 [00:28<00:02,  2.05it/s]Train Iter: 346/1000. LR: 0.0216. Data: 0.39s. Batch: 0.62s. S_Loss: 1.4459. T_Loss: 3.0172. Mask: 0.9603. :  92%|█████████▏| 46/50 [00:28<00:02,  1.41it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4406. T_Loss: 3.0309. Mask: 0.9597. :  92%|█████████▏| 46/50 [00:28<00:02,  1.41it/s]Train Iter: 347/1000. LR: 0.0217. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4406. T_Loss: 3.0309. Mask: 0.9597. :  94%|█████████▍| 47/50 [00:28<00:01,  1.65it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.37s. Batch: 0.60s. S_Loss: 1.4347. T_Loss: 3.0375. Mask: 0.9600. :  94%|█████████▍| 47/50 [00:29<00:01,  1.65it/s]Train Iter: 348/1000. LR: 0.0218. Data: 0.37s. Batch: 0.60s. S_Loss: 1.4347. T_Loss: 3.0375. Mask: 0.9600. :  96%|█████████▌| 48/50 [00:29<00:00,  2.04it/s]total : 1000  current step :  346
total : 1000  current step :  347
total : 1000  current step :  348
Train Iter: 349/1000. LR: 0.0218. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4281. T_Loss: 3.0393. Mask: 0.9600. :  96%|█████████▌| 48/50 [00:30<00:00,  2.04it/s]Train Iter: 349/1000. LR: 0.0218. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4281. T_Loss: 3.0393. Mask: 0.9600. :  98%|█████████▊| 49/50 [00:30<00:00,  1.48it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4216. T_Loss: 3.0470. Mask: 0.9605. :  98%|█████████▊| 49/50 [00:30<00:00,  1.48it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4216. T_Loss: 3.0470. Mask: 0.9605. : 100%|██████████| 50/50 [00:30<00:00,  1.75it/s]Train Iter: 350/1000. LR: 0.0219. Data: 0.38s. Batch: 0.61s. S_Loss: 1.4216. T_Loss: 3.0470. Mask: 0.9605. : 100%|██████████| 50/50 [00:30<00:00,  1.64it/s]
total : 1000  current step :  349
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.7944. top1: 41.80. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 1.7944. top1: 41.80. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.40it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.7930. top1: 40.82. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.40it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 1.7930. top1: 40.82. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.7956. top1: 40.23. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 1.7956. top1: 40.23. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.92it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.7895. top1: 41.89. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.92it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.7895. top1: 41.89. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.28it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.7224. top1: 53.28. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.28it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.7224. top1: 53.28. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.6821. top1: 60.55. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.63it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.6821. top1: 60.55. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.6541. top1: 65.62. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 1.6541. top1: 65.62. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.6341. top1: 68.90. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.6341. top1: 68.90. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 1.6341. top1: 68.90. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.00it/s]
total : 1000  current step :  350
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.26s. S_Loss: 1.1447. T_Loss: 3.4876. Mask: 0.9297. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 351/1000. LR: 0.0219. Data: 0.01s. Batch: 0.26s. S_Loss: 1.1447. T_Loss: 3.4876. Mask: 0.9297. :   2%|▏         | 1/50 [00:00<00:13,  3.76it/s]total : 1000  current step :  351
Train Iter: 352/1000. LR: 0.0220. Data: 0.42s. Batch: 0.66s. S_Loss: 1.1331. T_Loss: 3.3128. Mask: 0.9531. :   2%|▏         | 1/50 [00:01<00:13,  3.76it/s]Train Iter: 352/1000. LR: 0.0220. Data: 0.42s. Batch: 0.66s. S_Loss: 1.1331. T_Loss: 3.3128. Mask: 0.9531. :   4%|▍         | 2/50 [00:01<00:35,  1.36it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.30s. Batch: 0.56s. S_Loss: 1.1301. T_Loss: 3.4383. Mask: 0.9583. :   4%|▍         | 2/50 [00:01<00:35,  1.36it/s]Train Iter: 353/1000. LR: 0.0221. Data: 0.30s. Batch: 0.56s. S_Loss: 1.1301. T_Loss: 3.4383. Mask: 0.9583. :   6%|▌         | 3/50 [00:01<00:26,  1.77it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.23s. Batch: 0.49s. S_Loss: 1.1387. T_Loss: 3.4601. Mask: 0.9561. :   6%|▌         | 3/50 [00:01<00:26,  1.77it/s]Train Iter: 354/1000. LR: 0.0221. Data: 0.23s. Batch: 0.49s. S_Loss: 1.1387. T_Loss: 3.4601. Mask: 0.9561. :   8%|▊         | 4/50 [00:01<00:20,  2.24it/s]total : 1000  current step :  352
total : 1000  current step :  353
total : 1000  current step :  354
Train Iter: 355/1000. LR: 0.0222. Data: 0.34s. Batch: 0.59s. S_Loss: 1.1322. T_Loss: 3.4927. Mask: 0.9578. :   8%|▊         | 4/50 [00:02<00:20,  2.24it/s]Train Iter: 355/1000. LR: 0.0222. Data: 0.34s. Batch: 0.59s. S_Loss: 1.1322. T_Loss: 3.4927. Mask: 0.9578. :  10%|█         | 5/50 [00:02<00:29,  1.55it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.32s. Batch: 0.58s. S_Loss: 1.1272. T_Loss: 3.4411. Mask: 0.9557. :  10%|█         | 5/50 [00:03<00:29,  1.55it/s]Train Iter: 356/1000. LR: 0.0223. Data: 0.32s. Batch: 0.58s. S_Loss: 1.1272. T_Loss: 3.4411. Mask: 0.9557. :  12%|█▏        | 6/50 [00:03<00:26,  1.66it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.28s. Batch: 0.54s. S_Loss: 1.1184. T_Loss: 3.3908. Mask: 0.9576. :  12%|█▏        | 6/50 [00:03<00:26,  1.66it/s]Train Iter: 357/1000. LR: 0.0223. Data: 0.28s. Batch: 0.54s. S_Loss: 1.1184. T_Loss: 3.3908. Mask: 0.9576. :  14%|█▍        | 7/50 [00:03<00:22,  1.93it/s]total : 1000  current step :  355
total : 1000  current step :  356
total : 1000  current step :  357
Train Iter: 358/1000. LR: 0.0224. Data: 0.36s. Batch: 0.62s. S_Loss: 1.1095. T_Loss: 3.3532. Mask: 0.9595. :  14%|█▍        | 7/50 [00:05<00:22,  1.93it/s]Train Iter: 358/1000. LR: 0.0224. Data: 0.36s. Batch: 0.62s. S_Loss: 1.1095. T_Loss: 3.3532. Mask: 0.9595. :  16%|█▌        | 8/50 [00:05<00:30,  1.36it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.33s. Batch: 0.60s. S_Loss: 1.1116. T_Loss: 3.3605. Mask: 0.9596. :  16%|█▌        | 8/50 [00:05<00:30,  1.36it/s]Train Iter: 359/1000. LR: 0.0224. Data: 0.33s. Batch: 0.60s. S_Loss: 1.1116. T_Loss: 3.3605. Mask: 0.9596. :  18%|█▊        | 9/50 [00:05<00:25,  1.62it/s]total : 1000  current step :  358
total : 1000  current step :  359
Train Iter: 360/1000. LR: 0.0225. Data: 0.35s. Batch: 0.61s. S_Loss: 1.1114. T_Loss: 3.3538. Mask: 0.9586. :  18%|█▊        | 9/50 [00:06<00:25,  1.62it/s]Train Iter: 360/1000. LR: 0.0225. Data: 0.35s. Batch: 0.61s. S_Loss: 1.1114. T_Loss: 3.3538. Mask: 0.9586. :  20%|██        | 10/50 [00:06<00:25,  1.55it/s]total : 1000  current step :  360
Train Iter: 361/1000. LR: 0.0226. Data: 0.41s. Batch: 0.67s. S_Loss: 1.1089. T_Loss: 3.2960. Mask: 0.9602. :  20%|██        | 10/50 [00:07<00:25,  1.55it/s]Train Iter: 361/1000. LR: 0.0226. Data: 0.41s. Batch: 0.67s. S_Loss: 1.1089. T_Loss: 3.2960. Mask: 0.9602. :  22%|██▏       | 11/50 [00:07<00:32,  1.18it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.37s. Batch: 0.63s. S_Loss: 1.1054. T_Loss: 3.2352. Mask: 0.9600. :  22%|██▏       | 11/50 [00:07<00:32,  1.18it/s]Train Iter: 362/1000. LR: 0.0226. Data: 0.37s. Batch: 0.63s. S_Loss: 1.1054. T_Loss: 3.2352. Mask: 0.9600. :  24%|██▍       | 12/50 [00:07<00:24,  1.53it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.36s. Batch: 0.62s. S_Loss: 1.1032. T_Loss: 3.2111. Mask: 0.9612. :  24%|██▍       | 12/50 [00:08<00:24,  1.53it/s]Train Iter: 363/1000. LR: 0.0227. Data: 0.36s. Batch: 0.62s. S_Loss: 1.1032. T_Loss: 3.2111. Mask: 0.9612. :  26%|██▌       | 13/50 [00:08<00:21,  1.70it/s]total : 1000  current step :  361
total : 1000  current step :  362
total : 1000  current step :  363
Train Iter: 364/1000. LR: 0.0228. Data: 0.41s. Batch: 0.66s. S_Loss: 1.1023. T_Loss: 3.2263. Mask: 0.9623. :  26%|██▌       | 13/50 [00:09<00:21,  1.70it/s]Train Iter: 364/1000. LR: 0.0228. Data: 0.41s. Batch: 0.66s. S_Loss: 1.1023. T_Loss: 3.2263. Mask: 0.9623. :  28%|██▊       | 14/50 [00:09<00:27,  1.30it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.39s. Batch: 0.63s. S_Loss: 1.0994. T_Loss: 3.2074. Mask: 0.9630. :  28%|██▊       | 14/50 [00:09<00:27,  1.30it/s]Train Iter: 365/1000. LR: 0.0228. Data: 0.39s. Batch: 0.63s. S_Loss: 1.0994. T_Loss: 3.2074. Mask: 0.9630. :  30%|███       | 15/50 [00:09<00:22,  1.59it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.38s. Batch: 0.63s. S_Loss: 1.0952. T_Loss: 3.1898. Mask: 0.9631. :  30%|███       | 15/50 [00:10<00:22,  1.59it/s]Train Iter: 366/1000. LR: 0.0229. Data: 0.38s. Batch: 0.63s. S_Loss: 1.0952. T_Loss: 3.1898. Mask: 0.9631. :  32%|███▏      | 16/50 [00:10<00:20,  1.66it/s]total : 1000  current step :  364
total : 1000  current step :  365
total : 1000  current step :  366
Train Iter: 367/1000. LR: 0.0229. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0922. T_Loss: 3.1926. Mask: 0.9644. :  32%|███▏      | 16/50 [00:11<00:20,  1.66it/s]Train Iter: 367/1000. LR: 0.0229. Data: 0.42s. Batch: 0.66s. S_Loss: 1.0922. T_Loss: 3.1926. Mask: 0.9644. :  34%|███▍      | 17/50 [00:11<00:26,  1.25it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0880. T_Loss: 3.2148. Mask: 0.9644. :  34%|███▍      | 17/50 [00:11<00:26,  1.25it/s]Train Iter: 368/1000. LR: 0.0230. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0880. T_Loss: 3.2148. Mask: 0.9644. :  36%|███▌      | 18/50 [00:11<00:21,  1.48it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.39s. Batch: 0.64s. S_Loss: 1.0879. T_Loss: 3.2463. Mask: 0.9650. :  36%|███▌      | 18/50 [00:12<00:21,  1.48it/s]Train Iter: 369/1000. LR: 0.0231. Data: 0.39s. Batch: 0.64s. S_Loss: 1.0879. T_Loss: 3.2463. Mask: 0.9650. :  38%|███▊      | 19/50 [00:12<00:18,  1.69it/s]total : 1000  current step :  367
total : 1000  current step :  368
total : 1000  current step :  369
Train Iter: 370/1000. LR: 0.0231. Data: 0.43s. Batch: 0.67s. S_Loss: 1.0882. T_Loss: 3.2874. Mask: 0.9633. :  38%|███▊      | 19/50 [00:13<00:18,  1.69it/s]Train Iter: 370/1000. LR: 0.0231. Data: 0.43s. Batch: 0.67s. S_Loss: 1.0882. T_Loss: 3.2874. Mask: 0.9633. :  40%|████      | 20/50 [00:13<00:23,  1.27it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0854. T_Loss: 3.3072. Mask: 0.9630. :  40%|████      | 20/50 [00:13<00:23,  1.27it/s]Train Iter: 371/1000. LR: 0.0232. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0854. T_Loss: 3.3072. Mask: 0.9630. :  42%|████▏     | 21/50 [00:13<00:18,  1.53it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0847. T_Loss: 3.3153. Mask: 0.9620. :  42%|████▏     | 21/50 [00:14<00:18,  1.53it/s]Train Iter: 372/1000. LR: 0.0233. Data: 0.40s. Batch: 0.64s. S_Loss: 1.0847. T_Loss: 3.3153. Mask: 0.9620. :  44%|████▍     | 22/50 [00:14<00:16,  1.73it/s]total : 1000  current step :  370
total : 1000  current step :  371
total : 1000  current step :  372
Train Iter: 373/1000. LR: 0.0233. Data: 0.43s. Batch: 0.67s. S_Loss: 1.0845. T_Loss: 3.3222. Mask: 0.9616. :  44%|████▍     | 22/50 [00:15<00:16,  1.73it/s]Train Iter: 373/1000. LR: 0.0233. Data: 0.43s. Batch: 0.67s. S_Loss: 1.0845. T_Loss: 3.3222. Mask: 0.9616. :  46%|████▌     | 23/50 [00:15<00:21,  1.28it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0838. T_Loss: 3.3188. Mask: 0.9609. :  46%|████▌     | 23/50 [00:15<00:21,  1.28it/s]Train Iter: 374/1000. LR: 0.0234. Data: 0.41s. Batch: 0.65s. S_Loss: 1.0838. T_Loss: 3.3188. Mask: 0.9609. :  48%|████▊     | 24/50 [00:15<00:16,  1.59it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0819. T_Loss: 3.3113. Mask: 0.9609. :  48%|████▊     | 24/50 [00:16<00:16,  1.59it/s]Train Iter: 375/1000. LR: 0.0234. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0819. T_Loss: 3.3113. Mask: 0.9609. :  50%|█████     | 25/50 [00:16<00:14,  1.67it/s]total : 1000  current step :  373
total : 1000  current step :  374
total : 1000  current step :  375
Train Iter: 376/1000. LR: 0.0235. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0805. T_Loss: 3.2844. Mask: 0.9606. :  50%|█████     | 25/50 [00:17<00:14,  1.67it/s]Train Iter: 376/1000. LR: 0.0235. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0805. T_Loss: 3.2844. Mask: 0.9606. :  52%|█████▏    | 26/50 [00:17<00:18,  1.30it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0776. T_Loss: 3.2691. Mask: 0.9605. :  52%|█████▏    | 26/50 [00:17<00:18,  1.30it/s]Train Iter: 377/1000. LR: 0.0236. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0776. T_Loss: 3.2691. Mask: 0.9605. :  54%|█████▍    | 27/50 [00:17<00:14,  1.55it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0750. T_Loss: 3.2443. Mask: 0.9608. :  54%|█████▍    | 27/50 [00:18<00:14,  1.55it/s]Train Iter: 378/1000. LR: 0.0236. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0750. T_Loss: 3.2443. Mask: 0.9608. :  56%|█████▌    | 28/50 [00:18<00:12,  1.75it/s]total : 1000  current step :  376
total : 1000  current step :  377
total : 1000  current step :  378
Train Iter: 379/1000. LR: 0.0237. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0732. T_Loss: 3.2285. Mask: 0.9603. :  56%|█████▌    | 28/50 [00:19<00:12,  1.75it/s]Train Iter: 379/1000. LR: 0.0237. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0732. T_Loss: 3.2285. Mask: 0.9603. :  58%|█████▊    | 29/50 [00:19<00:15,  1.35it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0715. T_Loss: 3.2154. Mask: 0.9602. :  58%|█████▊    | 29/50 [00:19<00:15,  1.35it/s]Train Iter: 380/1000. LR: 0.0238. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0715. T_Loss: 3.2154. Mask: 0.9602. :  60%|██████    | 30/50 [00:19<00:12,  1.60it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0692. T_Loss: 3.2030. Mask: 0.9609. :  60%|██████    | 30/50 [00:19<00:12,  1.60it/s]Train Iter: 381/1000. LR: 0.0238. Data: 0.41s. Batch: 0.64s. S_Loss: 1.0692. T_Loss: 3.2030. Mask: 0.9609. :  62%|██████▏   | 31/50 [00:19<00:10,  1.83it/s]total : 1000  current step :  379
total : 1000  current step :  380
total : 1000  current step :  381
Train Iter: 382/1000. LR: 0.0239. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0678. T_Loss: 3.2044. Mask: 0.9615. :  62%|██████▏   | 31/50 [00:21<00:10,  1.83it/s]Train Iter: 382/1000. LR: 0.0239. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0678. T_Loss: 3.2044. Mask: 0.9615. :  64%|██████▍   | 32/50 [00:21<00:14,  1.23it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0652. T_Loss: 3.1977. Mask: 0.9613. :  64%|██████▍   | 32/50 [00:21<00:14,  1.23it/s]Train Iter: 383/1000. LR: 0.0239. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0652. T_Loss: 3.1977. Mask: 0.9613. :  66%|██████▌   | 33/50 [00:21<00:11,  1.44it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0658. T_Loss: 3.2025. Mask: 0.9605. :  66%|██████▌   | 33/50 [00:22<00:11,  1.44it/s]Train Iter: 384/1000. LR: 0.0240. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0658. T_Loss: 3.2025. Mask: 0.9605. :  68%|██████▊   | 34/50 [00:22<00:09,  1.65it/s]total : 1000  current step :  382
total : 1000  current step :  383
total : 1000  current step :  384
Train Iter: 385/1000. LR: 0.0241. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0653. T_Loss: 3.1997. Mask: 0.9603. :  68%|██████▊   | 34/50 [00:23<00:09,  1.65it/s]Train Iter: 385/1000. LR: 0.0241. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0653. T_Loss: 3.1997. Mask: 0.9603. :  70%|███████   | 35/50 [00:23<00:12,  1.21it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0646. T_Loss: 3.2154. Mask: 0.9602. :  70%|███████   | 35/50 [00:23<00:12,  1.21it/s]Train Iter: 386/1000. LR: 0.0241. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0646. T_Loss: 3.2154. Mask: 0.9602. :  72%|███████▏  | 36/50 [00:23<00:09,  1.43it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0643. T_Loss: 3.2294. Mask: 0.9598. :  72%|███████▏  | 36/50 [00:24<00:09,  1.43it/s]Train Iter: 387/1000. LR: 0.0242. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0643. T_Loss: 3.2294. Mask: 0.9598. :  74%|███████▍  | 37/50 [00:24<00:07,  1.76it/s]total : 1000  current step :  385
total : 1000  current step :  386
total : 1000  current step :  387
Train Iter: 388/1000. LR: 0.0243. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0639. T_Loss: 3.2469. Mask: 0.9599. :  74%|███████▍  | 37/50 [00:25<00:07,  1.76it/s]Train Iter: 388/1000. LR: 0.0243. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0639. T_Loss: 3.2469. Mask: 0.9599. :  76%|███████▌  | 38/50 [00:25<00:09,  1.26it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.44s. Batch: 0.66s. S_Loss: 1.0631. T_Loss: 3.2531. Mask: 0.9599. :  76%|███████▌  | 38/50 [00:25<00:09,  1.26it/s]Train Iter: 389/1000. LR: 0.0243. Data: 0.44s. Batch: 0.66s. S_Loss: 1.0631. T_Loss: 3.2531. Mask: 0.9599. :  78%|███████▊  | 39/50 [00:25<00:07,  1.53it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0626. T_Loss: 3.2628. Mask: 0.9601. :  78%|███████▊  | 39/50 [00:26<00:07,  1.53it/s]Train Iter: 390/1000. LR: 0.0244. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0626. T_Loss: 3.2628. Mask: 0.9601. :  80%|████████  | 40/50 [00:26<00:05,  1.77it/s]total : 1000  current step :  388
total : 1000  current step :  389
total : 1000  current step :  390
Train Iter: 391/1000. LR: 0.0244. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0614. T_Loss: 3.2638. Mask: 0.9606. :  80%|████████  | 40/50 [00:27<00:05,  1.77it/s]Train Iter: 391/1000. LR: 0.0244. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0614. T_Loss: 3.2638. Mask: 0.9606. :  82%|████████▏ | 41/50 [00:27<00:06,  1.34it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.44s. Batch: 0.66s. S_Loss: 1.0606. T_Loss: 3.2710. Mask: 0.9606. :  82%|████████▏ | 41/50 [00:27<00:06,  1.34it/s]Train Iter: 392/1000. LR: 0.0245. Data: 0.44s. Batch: 0.66s. S_Loss: 1.0606. T_Loss: 3.2710. Mask: 0.9606. :  84%|████████▍ | 42/50 [00:27<00:05,  1.51it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0603. T_Loss: 3.2724. Mask: 0.9604. :  84%|████████▍ | 42/50 [00:28<00:05,  1.51it/s]Train Iter: 393/1000. LR: 0.0246. Data: 0.43s. Batch: 0.65s. S_Loss: 1.0603. T_Loss: 3.2724. Mask: 0.9604. :  86%|████████▌ | 43/50 [00:28<00:03,  1.85it/s]total : 1000  current step :  391
total : 1000  current step :  392
total : 1000  current step :  393
Train Iter: 394/1000. LR: 0.0246. Data: 0.44s. Batch: 0.66s. S_Loss: 1.0605. T_Loss: 3.2778. Mask: 0.9602. :  86%|████████▌ | 43/50 [00:29<00:03,  1.85it/s]Train Iter: 394/1000. LR: 0.0246. Data: 0.44s. Batch: 0.66s. S_Loss: 1.0605. T_Loss: 3.2778. Mask: 0.9602. :  88%|████████▊ | 44/50 [00:29<00:04,  1.33it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0592. T_Loss: 3.2732. Mask: 0.9604. :  88%|████████▊ | 44/50 [00:29<00:04,  1.33it/s]Train Iter: 395/1000. LR: 0.0247. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0592. T_Loss: 3.2732. Mask: 0.9604. :  90%|█████████ | 45/50 [00:29<00:03,  1.54it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0588. T_Loss: 3.2754. Mask: 0.9604. :  90%|█████████ | 45/50 [00:30<00:03,  1.54it/s]Train Iter: 396/1000. LR: 0.0248. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0588. T_Loss: 3.2754. Mask: 0.9604. :  92%|█████████▏| 46/50 [00:30<00:02,  1.81it/s]total : 1000  current step :  394
total : 1000  current step :  395
total : 1000  current step :  396
Train Iter: 397/1000. LR: 0.0248. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0577. T_Loss: 3.2787. Mask: 0.9605. :  92%|█████████▏| 46/50 [00:31<00:02,  1.81it/s]Train Iter: 397/1000. LR: 0.0248. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0577. T_Loss: 3.2787. Mask: 0.9605. :  94%|█████████▍| 47/50 [00:31<00:02,  1.26it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0569. T_Loss: 3.2858. Mask: 0.9609. :  94%|█████████▍| 47/50 [00:31<00:02,  1.26it/s]Train Iter: 398/1000. LR: 0.0249. Data: 0.43s. Batch: 0.66s. S_Loss: 1.0569. T_Loss: 3.2858. Mask: 0.9609. :  96%|█████████▌| 48/50 [00:31<00:01,  1.51it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0563. T_Loss: 3.2889. Mask: 0.9606. :  96%|█████████▌| 48/50 [00:32<00:01,  1.51it/s]Train Iter: 399/1000. LR: 0.0249. Data: 0.42s. Batch: 0.65s. S_Loss: 1.0563. T_Loss: 3.2889. Mask: 0.9606. :  98%|█████████▊| 49/50 [00:32<00:00,  1.75it/s]total : 1000  current step :  397
total : 1000  current step :  398
total : 1000  current step :  399
Train Iter: 400/1000. LR: 0.0250. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0551. T_Loss: 3.2806. Mask: 0.9603. :  98%|█████████▊| 49/50 [00:33<00:00,  1.75it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0551. T_Loss: 3.2806. Mask: 0.9603. : 100%|██████████| 50/50 [00:33<00:00,  1.22it/s]Train Iter: 400/1000. LR: 0.0250. Data: 0.44s. Batch: 0.67s. S_Loss: 1.0551. T_Loss: 3.2806. Mask: 0.9603. : 100%|██████████| 50/50 [00:33<00:00,  1.49it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 1.2774. top1: 83.98. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 1.2774. top1: 83.98. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.2741. top1: 85.16. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.30it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 1.2741. top1: 85.16. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.2798. top1: 84.24. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.2798. top1: 84.24. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.2807. top1: 85.35. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 1.2807. top1: 85.35. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.2887. top1: 85.78. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.2887. top1: 85.78. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.18it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.2970. top1: 85.68. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.18it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.2970. top1: 85.68. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.3057. top1: 84.99. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.3057. top1: 84.99. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.19it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.3084. top1: 84.60. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.19it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.3084. top1: 84.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.3084. top1: 84.60. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
total : 1000  current step :  400
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.22s. S_Loss: 0.9721. T_Loss: 3.1516. Mask: 0.9688. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 401/1000. LR: 0.0251. Data: 0.01s. Batch: 0.22s. S_Loss: 0.9721. T_Loss: 3.1516. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:11,  4.42it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.25s. S_Loss: 1.0006. T_Loss: 3.1413. Mask: 0.9707. :   2%|▏         | 1/50 [00:00<00:11,  4.42it/s]Train Iter: 402/1000. LR: 0.0251. Data: 0.01s. Batch: 0.25s. S_Loss: 1.0006. T_Loss: 3.1413. Mask: 0.9707. :   4%|▍         | 2/50 [00:00<00:12,  3.87it/s]total : 1000  current step :  401
total : 1000  current step :  402
Train Iter: 403/1000. LR: 0.0252. Data: 0.30s. Batch: 0.53s. S_Loss: 1.0161. T_Loss: 3.2839. Mask: 0.9688. :   4%|▍         | 2/50 [00:01<00:12,  3.87it/s]Train Iter: 403/1000. LR: 0.0252. Data: 0.30s. Batch: 0.53s. S_Loss: 1.0161. T_Loss: 3.2839. Mask: 0.9688. :   6%|▌         | 3/50 [00:01<00:30,  1.57it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.27s. Batch: 0.50s. S_Loss: 1.0166. T_Loss: 3.3374. Mask: 0.9658. :   6%|▌         | 3/50 [00:02<00:30,  1.57it/s]Train Iter: 404/1000. LR: 0.0253. Data: 0.27s. Batch: 0.50s. S_Loss: 1.0166. T_Loss: 3.3374. Mask: 0.9658. :   8%|▊         | 4/50 [00:02<00:25,  1.82it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.23s. Batch: 0.47s. S_Loss: 1.0118. T_Loss: 3.3973. Mask: 0.9680. :   8%|▊         | 4/50 [00:02<00:25,  1.82it/s]Train Iter: 405/1000. LR: 0.0253. Data: 0.23s. Batch: 0.47s. S_Loss: 1.0118. T_Loss: 3.3973. Mask: 0.9680. :  10%|█         | 5/50 [00:02<00:21,  2.07it/s]total : 1000  current step :  403
total : 1000  current step :  404
total : 1000  current step :  405
Train Iter: 406/1000. LR: 0.0254. Data: 0.34s. Batch: 0.57s. S_Loss: 1.0071. T_Loss: 3.3983. Mask: 0.9661. :  10%|█         | 5/50 [00:03<00:21,  2.07it/s]Train Iter: 406/1000. LR: 0.0254. Data: 0.34s. Batch: 0.57s. S_Loss: 1.0071. T_Loss: 3.3983. Mask: 0.9661. :  12%|█▏        | 6/50 [00:03<00:30,  1.46it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.31s. Batch: 0.54s. S_Loss: 1.0113. T_Loss: 3.4135. Mask: 0.9632. :  12%|█▏        | 6/50 [00:03<00:30,  1.46it/s]Train Iter: 407/1000. LR: 0.0254. Data: 0.31s. Batch: 0.54s. S_Loss: 1.0113. T_Loss: 3.4135. Mask: 0.9632. :  14%|█▍        | 7/50 [00:03<00:24,  1.73it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.30s. Batch: 0.53s. S_Loss: 1.0148. T_Loss: 3.4261. Mask: 0.9644. :  14%|█▍        | 7/50 [00:04<00:24,  1.73it/s]Train Iter: 408/1000. LR: 0.0255. Data: 0.30s. Batch: 0.53s. S_Loss: 1.0148. T_Loss: 3.4261. Mask: 0.9644. :  16%|█▌        | 8/50 [00:04<00:22,  1.89it/s]total : 1000  current step :  406
total : 1000  current step :  407
total : 1000  current step :  408
Train Iter: 409/1000. LR: 0.0256. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0161. T_Loss: 3.4763. Mask: 0.9644. :  16%|█▌        | 8/50 [00:05<00:22,  1.89it/s]Train Iter: 409/1000. LR: 0.0256. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0161. T_Loss: 3.4763. Mask: 0.9644. :  18%|█▊        | 9/50 [00:05<00:27,  1.47it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.33s. Batch: 0.56s. S_Loss: 1.0150. T_Loss: 3.4563. Mask: 0.9602. :  18%|█▊        | 9/50 [00:05<00:27,  1.47it/s]Train Iter: 410/1000. LR: 0.0256. Data: 0.33s. Batch: 0.56s. S_Loss: 1.0150. T_Loss: 3.4563. Mask: 0.9602. :  20%|██        | 10/50 [00:05<00:23,  1.71it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.31s. Batch: 0.53s. S_Loss: 1.0143. T_Loss: 3.4754. Mask: 0.9599. :  20%|██        | 10/50 [00:05<00:23,  1.71it/s]Train Iter: 411/1000. LR: 0.0257. Data: 0.31s. Batch: 0.53s. S_Loss: 1.0143. T_Loss: 3.4754. Mask: 0.9599. :  22%|██▏       | 11/50 [00:05<00:18,  2.15it/s]total : 1000  current step :  409
total : 1000  current step :  410
total : 1000  current step :  411
Train Iter: 412/1000. LR: 0.0258. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0135. T_Loss: 3.4629. Mask: 0.9583. :  22%|██▏       | 11/50 [00:06<00:18,  2.15it/s]Train Iter: 412/1000. LR: 0.0258. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0135. T_Loss: 3.4629. Mask: 0.9583. :  24%|██▍       | 12/50 [00:06<00:24,  1.52it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.34s. Batch: 0.56s. S_Loss: 1.0168. T_Loss: 3.4840. Mask: 0.9588. :  24%|██▍       | 12/50 [00:07<00:24,  1.52it/s]Train Iter: 413/1000. LR: 0.0258. Data: 0.34s. Batch: 0.56s. S_Loss: 1.0168. T_Loss: 3.4840. Mask: 0.9588. :  26%|██▌       | 13/50 [00:07<00:20,  1.76it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0159. T_Loss: 3.4878. Mask: 0.9590. :  26%|██▌       | 13/50 [00:07<00:20,  1.76it/s]Train Iter: 414/1000. LR: 0.0259. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0159. T_Loss: 3.4878. Mask: 0.9590. :  28%|██▊       | 14/50 [00:07<00:18,  1.97it/s]total : 1000  current step :  412
total : 1000  current step :  413
total : 1000  current step :  414
Train Iter: 415/1000. LR: 0.0259. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0173. T_Loss: 3.4863. Mask: 0.9599. :  28%|██▊       | 14/50 [00:08<00:18,  1.97it/s]Train Iter: 415/1000. LR: 0.0259. Data: 0.37s. Batch: 0.58s. S_Loss: 1.0173. T_Loss: 3.4863. Mask: 0.9599. :  30%|███       | 15/50 [00:08<00:23,  1.48it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.34s. Batch: 0.56s. S_Loss: 1.0176. T_Loss: 3.4649. Mask: 0.9592. :  30%|███       | 15/50 [00:08<00:23,  1.48it/s]Train Iter: 416/1000. LR: 0.0260. Data: 0.34s. Batch: 0.56s. S_Loss: 1.0176. T_Loss: 3.4649. Mask: 0.9592. :  32%|███▏      | 16/50 [00:08<00:18,  1.86it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0159. T_Loss: 3.4565. Mask: 0.9600. :  32%|███▏      | 16/50 [00:09<00:18,  1.86it/s]Train Iter: 417/1000. LR: 0.0261. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0159. T_Loss: 3.4565. Mask: 0.9600. :  34%|███▍      | 17/50 [00:09<00:15,  2.07it/s]total : 1000  current step :  415
total : 1000  current step :  416
total : 1000  current step :  417
Train Iter: 418/1000. LR: 0.0261. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0142. T_Loss: 3.4432. Mask: 0.9599. :  34%|███▍      | 17/50 [00:10<00:15,  2.07it/s]Train Iter: 418/1000. LR: 0.0261. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0142. T_Loss: 3.4432. Mask: 0.9599. :  36%|███▌      | 18/50 [00:10<00:20,  1.55it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0125. T_Loss: 3.4438. Mask: 0.9605. :  36%|███▌      | 18/50 [00:10<00:20,  1.55it/s]Train Iter: 419/1000. LR: 0.0262. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0125. T_Loss: 3.4438. Mask: 0.9605. :  38%|███▊      | 19/50 [00:10<00:16,  1.93it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0109. T_Loss: 3.4390. Mask: 0.9605. :  38%|███▊      | 19/50 [00:10<00:16,  1.93it/s]Train Iter: 420/1000. LR: 0.0263. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0109. T_Loss: 3.4390. Mask: 0.9605. :  40%|████      | 20/50 [00:10<00:13,  2.17it/s]total : 1000  current step :  418
total : 1000  current step :  419
total : 1000  current step :  420
Train Iter: 421/1000. LR: 0.0263. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0102. T_Loss: 3.4439. Mask: 0.9608. :  40%|████      | 20/50 [00:11<00:13,  2.17it/s]Train Iter: 421/1000. LR: 0.0263. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0102. T_Loss: 3.4439. Mask: 0.9608. :  42%|████▏     | 21/50 [00:11<00:18,  1.58it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0106. T_Loss: 3.4671. Mask: 0.9611. :  42%|████▏     | 21/50 [00:12<00:18,  1.58it/s]Train Iter: 422/1000. LR: 0.0264. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0106. T_Loss: 3.4671. Mask: 0.9611. :  44%|████▍     | 22/50 [00:12<00:14,  1.90it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0080. T_Loss: 3.4576. Mask: 0.9613. :  44%|████▍     | 22/50 [00:12<00:14,  1.90it/s]Train Iter: 423/1000. LR: 0.0264. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0080. T_Loss: 3.4576. Mask: 0.9613. :  46%|████▌     | 23/50 [00:12<00:12,  2.14it/s]total : 1000  current step :  421
total : 1000  current step :  422
total : 1000  current step :  423
Train Iter: 424/1000. LR: 0.0265. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0083. T_Loss: 3.4645. Mask: 0.9608. :  46%|████▌     | 23/50 [00:13<00:12,  2.14it/s]Train Iter: 424/1000. LR: 0.0265. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0083. T_Loss: 3.4645. Mask: 0.9608. :  48%|████▊     | 24/50 [00:13<00:17,  1.47it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0063. T_Loss: 3.4556. Mask: 0.9619. :  48%|████▊     | 24/50 [00:13<00:17,  1.47it/s]Train Iter: 425/1000. LR: 0.0266. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0063. T_Loss: 3.4556. Mask: 0.9619. :  50%|█████     | 25/50 [00:13<00:13,  1.87it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0068. T_Loss: 3.4810. Mask: 0.9620. :  50%|█████     | 25/50 [00:14<00:13,  1.87it/s]Train Iter: 426/1000. LR: 0.0266. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0068. T_Loss: 3.4810. Mask: 0.9620. :  52%|█████▏    | 26/50 [00:14<00:11,  2.17it/s]total : 1000  current step :  424
total : 1000  current step :  425
total : 1000  current step :  426
Train Iter: 427/1000. LR: 0.0267. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0071. T_Loss: 3.4899. Mask: 0.9624. :  52%|█████▏    | 26/50 [00:15<00:11,  2.17it/s]Train Iter: 427/1000. LR: 0.0267. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0071. T_Loss: 3.4899. Mask: 0.9624. :  54%|█████▍    | 27/50 [00:15<00:14,  1.57it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0084. T_Loss: 3.5021. Mask: 0.9619. :  54%|█████▍    | 27/50 [00:15<00:14,  1.57it/s]Train Iter: 428/1000. LR: 0.0268. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0084. T_Loss: 3.5021. Mask: 0.9619. :  56%|█████▌    | 28/50 [00:15<00:11,  1.88it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0085. T_Loss: 3.5138. Mask: 0.9621. :  56%|█████▌    | 28/50 [00:15<00:11,  1.88it/s]Train Iter: 429/1000. LR: 0.0268. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0085. T_Loss: 3.5138. Mask: 0.9621. :  58%|█████▊    | 29/50 [00:15<00:09,  2.17it/s]total : 1000  current step :  427
total : 1000  current step :  428
total : 1000  current step :  429
Train Iter: 430/1000. LR: 0.0269. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0080. T_Loss: 3.5260. Mask: 0.9621. :  58%|█████▊    | 29/50 [00:16<00:09,  2.17it/s]Train Iter: 430/1000. LR: 0.0269. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0080. T_Loss: 3.5260. Mask: 0.9621. :  60%|██████    | 30/50 [00:16<00:13,  1.54it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0075. T_Loss: 3.5283. Mask: 0.9618. :  60%|██████    | 30/50 [00:17<00:13,  1.54it/s]Train Iter: 431/1000. LR: 0.0269. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0075. T_Loss: 3.5283. Mask: 0.9618. :  62%|██████▏   | 31/50 [00:17<00:10,  1.83it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0079. T_Loss: 3.5308. Mask: 0.9618. :  62%|██████▏   | 31/50 [00:17<00:10,  1.83it/s]Train Iter: 432/1000. LR: 0.0270. Data: 0.33s. Batch: 0.54s. S_Loss: 1.0079. T_Loss: 3.5308. Mask: 0.9618. :  64%|██████▍   | 32/50 [00:17<00:07,  2.30it/s]total : 1000  current step :  430
total : 1000  current step :  431
total : 1000  current step :  432
Train Iter: 433/1000. LR: 0.0271. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0075. T_Loss: 3.5278. Mask: 0.9618. :  64%|██████▍   | 32/50 [00:18<00:07,  2.30it/s]Train Iter: 433/1000. LR: 0.0271. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0075. T_Loss: 3.5278. Mask: 0.9618. :  66%|██████▌   | 33/50 [00:18<00:11,  1.52it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0071. T_Loss: 3.5258. Mask: 0.9619. :  66%|██████▌   | 33/50 [00:18<00:11,  1.52it/s]Train Iter: 434/1000. LR: 0.0271. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0071. T_Loss: 3.5258. Mask: 0.9619. :  68%|██████▊   | 34/50 [00:18<00:08,  1.79it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0072. T_Loss: 3.5261. Mask: 0.9616. :  68%|██████▊   | 34/50 [00:19<00:08,  1.79it/s]Train Iter: 435/1000. LR: 0.0272. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0072. T_Loss: 3.5261. Mask: 0.9616. :  70%|███████   | 35/50 [00:19<00:07,  2.09it/s]total : 1000  current step :  433
total : 1000  current step :  434
total : 1000  current step :  435
Train Iter: 436/1000. LR: 0.0273. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0067. T_Loss: 3.5178. Mask: 0.9617. :  70%|███████   | 35/50 [00:20<00:07,  2.09it/s]Train Iter: 436/1000. LR: 0.0273. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0067. T_Loss: 3.5178. Mask: 0.9617. :  72%|███████▏  | 36/50 [00:20<00:09,  1.44it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.34s. Batch: 0.56s. S_Loss: 1.0063. T_Loss: 3.5153. Mask: 0.9618. :  72%|███████▏  | 36/50 [00:20<00:09,  1.44it/s]Train Iter: 437/1000. LR: 0.0273. Data: 0.34s. Batch: 0.56s. S_Loss: 1.0063. T_Loss: 3.5153. Mask: 0.9618. :  74%|███████▍  | 37/50 [00:20<00:07,  1.75it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0062. T_Loss: 3.5190. Mask: 0.9618. :  74%|███████▍  | 37/50 [00:20<00:07,  1.75it/s]Train Iter: 438/1000. LR: 0.0274. Data: 0.34s. Batch: 0.55s. S_Loss: 1.0062. T_Loss: 3.5190. Mask: 0.9618. :  76%|███████▌  | 38/50 [00:20<00:05,  2.05it/s]total : 1000  current step :  436
total : 1000  current step :  437
total : 1000  current step :  438
Train Iter: 439/1000. LR: 0.0274. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0063. T_Loss: 3.5206. Mask: 0.9614. :  76%|███████▌  | 38/50 [00:22<00:05,  2.05it/s]Train Iter: 439/1000. LR: 0.0274. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0063. T_Loss: 3.5206. Mask: 0.9614. :  78%|███████▊  | 39/50 [00:22<00:07,  1.49it/s]total : 1000  current step :  439
Train Iter: 440/1000. LR: 0.0275. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0061. T_Loss: 3.5164. Mask: 0.9612. :  78%|███████▊  | 39/50 [00:22<00:07,  1.49it/s]Train Iter: 440/1000. LR: 0.0275. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0061. T_Loss: 3.5164. Mask: 0.9612. :  80%|████████  | 40/50 [00:22<00:06,  1.62it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0065. T_Loss: 3.5206. Mask: 0.9615. :  80%|████████  | 40/50 [00:23<00:06,  1.62it/s]Train Iter: 441/1000. LR: 0.0276. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0065. T_Loss: 3.5206. Mask: 0.9615. :  82%|████████▏ | 41/50 [00:23<00:05,  1.56it/s]total : 1000  current step :  440
total : 1000  current step :  441
Train Iter: 442/1000. LR: 0.0276. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0061. T_Loss: 3.5248. Mask: 0.9608. :  82%|████████▏ | 41/50 [00:24<00:05,  1.56it/s]Train Iter: 442/1000. LR: 0.0276. Data: 0.36s. Batch: 0.58s. S_Loss: 1.0061. T_Loss: 3.5248. Mask: 0.9608. :  84%|████████▍ | 42/50 [00:24<00:06,  1.33it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0057. T_Loss: 3.5199. Mask: 0.9609. :  84%|████████▍ | 42/50 [00:24<00:06,  1.33it/s]Train Iter: 443/1000. LR: 0.0277. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0057. T_Loss: 3.5199. Mask: 0.9609. :  86%|████████▌ | 43/50 [00:24<00:04,  1.59it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0050. T_Loss: 3.5263. Mask: 0.9613. :  86%|████████▌ | 43/50 [00:24<00:04,  1.59it/s]Train Iter: 444/1000. LR: 0.0278. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0050. T_Loss: 3.5263. Mask: 0.9613. :  88%|████████▊ | 44/50 [00:24<00:03,  1.88it/s]total : 1000  current step :  442
total : 1000  current step :  443
total : 1000  current step :  444
Train Iter: 445/1000. LR: 0.0278. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0046. T_Loss: 3.5306. Mask: 0.9614. :  88%|████████▊ | 44/50 [00:25<00:03,  1.88it/s]Train Iter: 445/1000. LR: 0.0278. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0046. T_Loss: 3.5306. Mask: 0.9614. :  90%|█████████ | 45/50 [00:25<00:03,  1.44it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0036. T_Loss: 3.5294. Mask: 0.9615. :  90%|█████████ | 45/50 [00:26<00:03,  1.44it/s]Train Iter: 446/1000. LR: 0.0279. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0036. T_Loss: 3.5294. Mask: 0.9615. :  92%|█████████▏| 46/50 [00:26<00:02,  1.69it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0029. T_Loss: 3.5277. Mask: 0.9615. :  92%|█████████▏| 46/50 [00:26<00:02,  1.69it/s]Train Iter: 447/1000. LR: 0.0279. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0029. T_Loss: 3.5277. Mask: 0.9615. :  94%|█████████▍| 47/50 [00:26<00:01,  2.10it/s]total : 1000  current step :  445
total : 1000  current step :  446
total : 1000  current step :  447
Train Iter: 448/1000. LR: 0.0280. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0031. T_Loss: 3.5323. Mask: 0.9616. :  94%|█████████▍| 47/50 [00:27<00:01,  2.10it/s]Train Iter: 448/1000. LR: 0.0280. Data: 0.36s. Batch: 0.57s. S_Loss: 1.0031. T_Loss: 3.5323. Mask: 0.9616. :  96%|█████████▌| 48/50 [00:27<00:01,  1.54it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0025. T_Loss: 3.5285. Mask: 0.9620. :  96%|█████████▌| 48/50 [00:27<00:01,  1.54it/s]Train Iter: 449/1000. LR: 0.0281. Data: 0.35s. Batch: 0.57s. S_Loss: 1.0025. T_Loss: 3.5285. Mask: 0.9620. :  98%|█████████▊| 49/50 [00:27<00:00,  1.89it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0025. T_Loss: 3.5287. Mask: 0.9623. :  98%|█████████▊| 49/50 [00:28<00:00,  1.89it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0025. T_Loss: 3.5287. Mask: 0.9623. : 100%|██████████| 50/50 [00:28<00:00,  2.12it/s]Train Iter: 450/1000. LR: 0.0281. Data: 0.35s. Batch: 0.56s. S_Loss: 1.0025. T_Loss: 3.5287. Mask: 0.9623. : 100%|██████████| 50/50 [00:28<00:00,  1.78it/s]
total : 1000  current step :  448
total : 1000  current step :  449
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.0401. top1: 95.70. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 1.0401. top1: 95.70. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0350. top1: 94.92. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 1.0350. top1: 94.92. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0426. top1: 94.27. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 1.0426. top1: 94.27. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0470. top1: 94.24. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0470. top1: 94.24. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0938. top1: 91.64. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0938. top1: 91.64. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.1274. top1: 89.45. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.1274. top1: 89.45. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1557. top1: 87.22. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1557. top1: 87.22. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1708. top1: 85.85. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1708. top1: 85.85. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 1.1708. top1: 85.85. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
total : 1000  current step :  450
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.97s. Batch: 1.13s. S_Loss: 0.9822. T_Loss: 3.2460. Mask: 0.9258. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 451/1000. LR: 0.0282. Data: 0.97s. Batch: 1.13s. S_Loss: 0.9822. T_Loss: 3.2460. Mask: 0.9258. :   2%|▏         | 1/50 [00:01<00:55,  1.13s/it]Train Iter: 452/1000. LR: 0.0282. Data: 0.63s. Batch: 0.84s. S_Loss: 0.9911. T_Loss: 3.5094. Mask: 0.9473. :   2%|▏         | 1/50 [00:01<00:55,  1.13s/it]Train Iter: 452/1000. LR: 0.0282. Data: 0.63s. Batch: 0.84s. S_Loss: 0.9911. T_Loss: 3.5094. Mask: 0.9473. :   4%|▍         | 2/50 [00:01<00:38,  1.26it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.46s. Batch: 0.69s. S_Loss: 1.0023. T_Loss: 3.6621. Mask: 0.9544. :   4%|▍         | 2/50 [00:02<00:38,  1.26it/s]Train Iter: 453/1000. LR: 0.0283. Data: 0.46s. Batch: 0.69s. S_Loss: 1.0023. T_Loss: 3.6621. Mask: 0.9544. :   6%|▌         | 3/50 [00:02<00:28,  1.64it/s]total : 1000  current step :  451
total : 1000  current step :  452
total : 1000  current step :  453
Train Iter: 454/1000. LR: 0.0284. Data: 0.58s. Batch: 0.81s. S_Loss: 0.9986. T_Loss: 3.6553. Mask: 0.9541. :   6%|▌         | 3/50 [00:03<00:28,  1.64it/s]Train Iter: 454/1000. LR: 0.0284. Data: 0.58s. Batch: 0.81s. S_Loss: 0.9986. T_Loss: 3.6553. Mask: 0.9541. :   8%|▊         | 4/50 [00:03<00:38,  1.20it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.49s. Batch: 0.72s. S_Loss: 0.9920. T_Loss: 3.6603. Mask: 0.9570. :   8%|▊         | 4/50 [00:03<00:38,  1.20it/s]Train Iter: 455/1000. LR: 0.0284. Data: 0.49s. Batch: 0.72s. S_Loss: 0.9920. T_Loss: 3.6603. Mask: 0.9570. :  10%|█         | 5/50 [00:03<00:29,  1.52it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9924. T_Loss: 3.6762. Mask: 0.9577. :  10%|█         | 5/50 [00:03<00:29,  1.52it/s]Train Iter: 456/1000. LR: 0.0285. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9924. T_Loss: 3.6762. Mask: 0.9577. :  12%|█▏        | 6/50 [00:03<00:24,  1.80it/s]total : 1000  current step :  454
total : 1000  current step :  455
total : 1000  current step :  456
Train Iter: 457/1000. LR: 0.0286. Data: 0.51s. Batch: 0.75s. S_Loss: 0.9897. T_Loss: 3.7005. Mask: 0.9604. :  12%|█▏        | 6/50 [00:05<00:24,  1.80it/s]Train Iter: 457/1000. LR: 0.0286. Data: 0.51s. Batch: 0.75s. S_Loss: 0.9897. T_Loss: 3.7005. Mask: 0.9604. :  14%|█▍        | 7/50 [00:05<00:33,  1.27it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9917. T_Loss: 3.6913. Mask: 0.9570. :  14%|█▍        | 7/50 [00:05<00:33,  1.27it/s]Train Iter: 458/1000. LR: 0.0286. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9917. T_Loss: 3.6913. Mask: 0.9570. :  16%|█▌        | 8/50 [00:05<00:27,  1.55it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9888. T_Loss: 3.6509. Mask: 0.9566. :  16%|█▌        | 8/50 [00:05<00:27,  1.55it/s]Train Iter: 459/1000. LR: 0.0287. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9888. T_Loss: 3.6509. Mask: 0.9566. :  18%|█▊        | 9/50 [00:05<00:23,  1.77it/s]total : 1000  current step :  457
total : 1000  current step :  458
total : 1000  current step :  459
Train Iter: 460/1000. LR: 0.0287. Data: 0.48s. Batch: 0.71s. S_Loss: 0.9850. T_Loss: 3.6149. Mask: 0.9602. :  18%|█▊        | 9/50 [00:07<00:23,  1.77it/s]Train Iter: 460/1000. LR: 0.0287. Data: 0.48s. Batch: 0.71s. S_Loss: 0.9850. T_Loss: 3.6149. Mask: 0.9602. :  20%|██        | 10/50 [00:07<00:29,  1.36it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9810. T_Loss: 3.5862. Mask: 0.9627. :  20%|██        | 10/50 [00:07<00:29,  1.36it/s]Train Iter: 461/1000. LR: 0.0288. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9810. T_Loss: 3.5862. Mask: 0.9627. :  22%|██▏       | 11/50 [00:07<00:23,  1.65it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9809. T_Loss: 3.5696. Mask: 0.9642. :  22%|██▏       | 11/50 [00:07<00:23,  1.65it/s]Train Iter: 462/1000. LR: 0.0289. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9809. T_Loss: 3.5696. Mask: 0.9642. :  24%|██▍       | 12/50 [00:07<00:22,  1.72it/s]total : 1000  current step :  460
total : 1000  current step :  461
total : 1000  current step :  462
Train Iter: 463/1000. LR: 0.0289. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9786. T_Loss: 3.5440. Mask: 0.9654. :  24%|██▍       | 12/50 [00:08<00:22,  1.72it/s]Train Iter: 463/1000. LR: 0.0289. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9786. T_Loss: 3.5440. Mask: 0.9654. :  26%|██▌       | 13/50 [00:08<00:27,  1.36it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9792. T_Loss: 3.5370. Mask: 0.9665. :  26%|██▌       | 13/50 [00:09<00:27,  1.36it/s]Train Iter: 464/1000. LR: 0.0290. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9792. T_Loss: 3.5370. Mask: 0.9665. :  28%|██▊       | 14/50 [00:09<00:23,  1.54it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9793. T_Loss: 3.5795. Mask: 0.9677. :  28%|██▊       | 14/50 [00:09<00:23,  1.54it/s]Train Iter: 465/1000. LR: 0.0291. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9793. T_Loss: 3.5795. Mask: 0.9677. :  30%|███       | 15/50 [00:09<00:19,  1.77it/s]total : 1000  current step :  463
total : 1000  current step :  464
total : 1000  current step :  465
Train Iter: 466/1000. LR: 0.0291. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9801. T_Loss: 3.6080. Mask: 0.9678. :  30%|███       | 15/50 [00:10<00:19,  1.77it/s]Train Iter: 466/1000. LR: 0.0291. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9801. T_Loss: 3.6080. Mask: 0.9678. :  32%|███▏      | 16/50 [00:10<00:24,  1.37it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9816. T_Loss: 3.6135. Mask: 0.9665. :  32%|███▏      | 16/50 [00:11<00:24,  1.37it/s]Train Iter: 467/1000. LR: 0.0292. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9816. T_Loss: 3.6135. Mask: 0.9665. :  34%|███▍      | 17/50 [00:11<00:20,  1.65it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9811. T_Loss: 3.6312. Mask: 0.9661. :  34%|███▍      | 17/50 [00:11<00:20,  1.65it/s]Train Iter: 468/1000. LR: 0.0292. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9811. T_Loss: 3.6312. Mask: 0.9661. :  36%|███▌      | 18/50 [00:11<00:17,  1.87it/s]total : 1000  current step :  466
total : 1000  current step :  467
total : 1000  current step :  468
Train Iter: 469/1000. LR: 0.0293. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9848. T_Loss: 3.6892. Mask: 0.9665. :  36%|███▌      | 18/50 [00:12<00:17,  1.87it/s]Train Iter: 469/1000. LR: 0.0293. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9848. T_Loss: 3.6892. Mask: 0.9665. :  38%|███▊      | 19/50 [00:12<00:22,  1.41it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9847. T_Loss: 3.7217. Mask: 0.9654. :  38%|███▊      | 19/50 [00:13<00:22,  1.41it/s]Train Iter: 470/1000. LR: 0.0294. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9847. T_Loss: 3.7217. Mask: 0.9654. :  40%|████      | 20/50 [00:13<00:17,  1.69it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9850. T_Loss: 3.7462. Mask: 0.9652. :  40%|████      | 20/50 [00:13<00:17,  1.69it/s]Train Iter: 471/1000. LR: 0.0294. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9850. T_Loss: 3.7462. Mask: 0.9652. :  42%|████▏     | 21/50 [00:13<00:14,  1.98it/s]total : 1000  current step :  469
total : 1000  current step :  470
total : 1000  current step :  471
Train Iter: 472/1000. LR: 0.0295. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9851. T_Loss: 3.7479. Mask: 0.9648. :  42%|████▏     | 21/50 [00:14<00:14,  1.98it/s]Train Iter: 472/1000. LR: 0.0295. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9851. T_Loss: 3.7479. Mask: 0.9648. :  44%|████▍     | 22/50 [00:14<00:20,  1.39it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9848. T_Loss: 3.7626. Mask: 0.9652. :  44%|████▍     | 22/50 [00:14<00:20,  1.39it/s]Train Iter: 473/1000. LR: 0.0296. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9848. T_Loss: 3.7626. Mask: 0.9652. :  46%|████▌     | 23/50 [00:14<00:16,  1.64it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9840. T_Loss: 3.7611. Mask: 0.9655. :  46%|████▌     | 23/50 [00:15<00:16,  1.64it/s]Train Iter: 474/1000. LR: 0.0296. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9840. T_Loss: 3.7611. Mask: 0.9655. :  48%|████▊     | 24/50 [00:15<00:13,  1.94it/s]total : 1000  current step :  472
total : 1000  current step :  473
total : 1000  current step :  474
Train Iter: 475/1000. LR: 0.0297. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9831. T_Loss: 3.7447. Mask: 0.9655. :  48%|████▊     | 24/50 [00:16<00:13,  1.94it/s]Train Iter: 475/1000. LR: 0.0297. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9831. T_Loss: 3.7447. Mask: 0.9655. :  50%|█████     | 25/50 [00:16<00:18,  1.35it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9829. T_Loss: 3.7448. Mask: 0.9662. :  50%|█████     | 25/50 [00:16<00:18,  1.35it/s]Train Iter: 476/1000. LR: 0.0297. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9829. T_Loss: 3.7448. Mask: 0.9662. :  52%|█████▏    | 26/50 [00:16<00:15,  1.56it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9832. T_Loss: 3.7391. Mask: 0.9659. :  52%|█████▏    | 26/50 [00:17<00:15,  1.56it/s]Train Iter: 477/1000. LR: 0.0298. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9832. T_Loss: 3.7391. Mask: 0.9659. :  54%|█████▍    | 27/50 [00:17<00:13,  1.77it/s]total : 1000  current step :  475
total : 1000  current step :  476
total : 1000  current step :  477
Train Iter: 478/1000. LR: 0.0299. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9820. T_Loss: 3.7224. Mask: 0.9660. :  54%|█████▍    | 27/50 [00:18<00:13,  1.77it/s]Train Iter: 478/1000. LR: 0.0299. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9820. T_Loss: 3.7224. Mask: 0.9660. :  56%|█████▌    | 28/50 [00:18<00:16,  1.31it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9830. T_Loss: 3.7242. Mask: 0.9655. :  56%|█████▌    | 28/50 [00:18<00:16,  1.31it/s]Train Iter: 479/1000. LR: 0.0299. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9830. T_Loss: 3.7242. Mask: 0.9655. :  58%|█████▊    | 29/50 [00:18<00:13,  1.57it/s]total : 1000  current step :  478
total : 1000  current step :  479
Train Iter: 480/1000. LR: 0.0300. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9835. T_Loss: 3.7212. Mask: 0.9654. :  58%|█████▊    | 29/50 [00:19<00:13,  1.57it/s]Train Iter: 480/1000. LR: 0.0300. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9835. T_Loss: 3.7212. Mask: 0.9654. :  60%|██████    | 30/50 [00:19<00:12,  1.57it/s]total : 1000  current step :  480
Train Iter: 481/1000. LR: 0.0301. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9839. T_Loss: 3.7255. Mask: 0.9657. :  60%|██████    | 30/50 [00:20<00:12,  1.57it/s]Train Iter: 481/1000. LR: 0.0301. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9839. T_Loss: 3.7255. Mask: 0.9657. :  62%|██████▏   | 31/50 [00:20<00:15,  1.23it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9819. T_Loss: 3.7184. Mask: 0.9666. :  62%|██████▏   | 31/50 [00:20<00:15,  1.23it/s]Train Iter: 482/1000. LR: 0.0301. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9819. T_Loss: 3.7184. Mask: 0.9666. :  64%|██████▍   | 32/50 [00:20<00:11,  1.53it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9810. T_Loss: 3.7075. Mask: 0.9664. :  64%|██████▍   | 32/50 [00:21<00:11,  1.53it/s]Train Iter: 483/1000. LR: 0.0302. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9810. T_Loss: 3.7075. Mask: 0.9664. :  66%|██████▌   | 33/50 [00:21<00:08,  1.91it/s]total : 1000  current step :  481
total : 1000  current step :  482
total : 1000  current step :  483
Train Iter: 484/1000. LR: 0.0302. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9802. T_Loss: 3.7058. Mask: 0.9668. :  66%|██████▌   | 33/50 [00:22<00:08,  1.91it/s]Train Iter: 484/1000. LR: 0.0302. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9802. T_Loss: 3.7058. Mask: 0.9668. :  68%|██████▊   | 34/50 [00:22<00:12,  1.31it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9800. T_Loss: 3.7093. Mask: 0.9665. :  68%|██████▊   | 34/50 [00:22<00:12,  1.31it/s]Train Iter: 485/1000. LR: 0.0303. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9800. T_Loss: 3.7093. Mask: 0.9665. :  70%|███████   | 35/50 [00:22<00:09,  1.57it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9793. T_Loss: 3.7116. Mask: 0.9666. :  70%|███████   | 35/50 [00:23<00:09,  1.57it/s]Train Iter: 486/1000. LR: 0.0304. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9793. T_Loss: 3.7116. Mask: 0.9666. :  72%|███████▏  | 36/50 [00:23<00:07,  1.95it/s]total : 1000  current step :  484
total : 1000  current step :  485
total : 1000  current step :  486
Train Iter: 487/1000. LR: 0.0304. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9793. T_Loss: 3.7135. Mask: 0.9672. :  72%|███████▏  | 36/50 [00:24<00:07,  1.95it/s]Train Iter: 487/1000. LR: 0.0304. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9793. T_Loss: 3.7135. Mask: 0.9672. :  74%|███████▍  | 37/50 [00:24<00:09,  1.38it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9789. T_Loss: 3.7214. Mask: 0.9672. :  74%|███████▍  | 37/50 [00:24<00:09,  1.38it/s]Train Iter: 488/1000. LR: 0.0305. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9789. T_Loss: 3.7214. Mask: 0.9672. :  76%|███████▌  | 38/50 [00:24<00:07,  1.63it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9791. T_Loss: 3.7292. Mask: 0.9674. :  76%|███████▌  | 38/50 [00:25<00:07,  1.63it/s]Train Iter: 489/1000. LR: 0.0306. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9791. T_Loss: 3.7292. Mask: 0.9674. :  78%|███████▊  | 39/50 [00:25<00:05,  1.87it/s]total : 1000  current step :  487
total : 1000  current step :  488
total : 1000  current step :  489
Train Iter: 490/1000. LR: 0.0306. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9786. T_Loss: 3.7345. Mask: 0.9672. :  78%|███████▊  | 39/50 [00:26<00:05,  1.87it/s]Train Iter: 490/1000. LR: 0.0306. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9786. T_Loss: 3.7345. Mask: 0.9672. :  80%|████████  | 40/50 [00:26<00:06,  1.44it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9782. T_Loss: 3.7378. Mask: 0.9670. :  80%|████████  | 40/50 [00:26<00:06,  1.44it/s]Train Iter: 491/1000. LR: 0.0307. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9782. T_Loss: 3.7378. Mask: 0.9670. :  82%|████████▏ | 41/50 [00:26<00:05,  1.58it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9772. T_Loss: 3.7307. Mask: 0.9673. :  82%|████████▏ | 41/50 [00:26<00:05,  1.58it/s]Train Iter: 492/1000. LR: 0.0307. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9772. T_Loss: 3.7307. Mask: 0.9673. :  84%|████████▍ | 42/50 [00:26<00:04,  1.86it/s]total : 1000  current step :  490
total : 1000  current step :  491
total : 1000  current step :  492
Train Iter: 493/1000. LR: 0.0308. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9761. T_Loss: 3.7239. Mask: 0.9671. :  84%|████████▍ | 42/50 [00:27<00:04,  1.86it/s]Train Iter: 493/1000. LR: 0.0308. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9761. T_Loss: 3.7239. Mask: 0.9671. :  86%|████████▌ | 43/50 [00:27<00:04,  1.57it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9756. T_Loss: 3.7173. Mask: 0.9671. :  86%|████████▌ | 43/50 [00:28<00:04,  1.57it/s]Train Iter: 494/1000. LR: 0.0309. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9756. T_Loss: 3.7173. Mask: 0.9671. :  88%|████████▊ | 44/50 [00:28<00:03,  1.71it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9757. T_Loss: 3.7209. Mask: 0.9670. :  88%|████████▊ | 44/50 [00:28<00:03,  1.71it/s]Train Iter: 495/1000. LR: 0.0309. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9757. T_Loss: 3.7209. Mask: 0.9670. :  90%|█████████ | 45/50 [00:28<00:02,  2.07it/s]total : 1000  current step :  493
total : 1000  current step :  494
total : 1000  current step :  495
Train Iter: 496/1000. LR: 0.0310. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9748. T_Loss: 3.7109. Mask: 0.9671. :  90%|█████████ | 45/50 [00:29<00:02,  2.07it/s]Train Iter: 496/1000. LR: 0.0310. Data: 0.42s. Batch: 0.64s. S_Loss: 0.9748. T_Loss: 3.7109. Mask: 0.9671. :  92%|█████████▏| 46/50 [00:29<00:02,  1.46it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9746. T_Loss: 3.7056. Mask: 0.9668. :  92%|█████████▏| 46/50 [00:29<00:02,  1.46it/s]Train Iter: 497/1000. LR: 0.0311. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9746. T_Loss: 3.7056. Mask: 0.9668. :  94%|█████████▍| 47/50 [00:29<00:01,  1.69it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9740. T_Loss: 3.7077. Mask: 0.9673. :  94%|█████████▍| 47/50 [00:30<00:01,  1.69it/s]Train Iter: 498/1000. LR: 0.0311. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9740. T_Loss: 3.7077. Mask: 0.9673. :  96%|█████████▌| 48/50 [00:30<00:00,  2.06it/s]total : 1000  current step :  496
total : 1000  current step :  497
total : 1000  current step :  498
Train Iter: 499/1000. LR: 0.0312. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9752. T_Loss: 3.7279. Mask: 0.9671. :  96%|█████████▌| 48/50 [00:31<00:00,  2.06it/s]Train Iter: 499/1000. LR: 0.0312. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9752. T_Loss: 3.7279. Mask: 0.9671. :  98%|█████████▊| 49/50 [00:31<00:00,  1.41it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9759. T_Loss: 3.7439. Mask: 0.9668. :  98%|█████████▊| 49/50 [00:31<00:00,  1.41it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9759. T_Loss: 3.7439. Mask: 0.9668. : 100%|██████████| 50/50 [00:31<00:00,  1.67it/s]Train Iter: 500/1000. LR: 0.0312. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9759. T_Loss: 3.7439. Mask: 0.9668. : 100%|██████████| 50/50 [00:31<00:00,  1.57it/s]
total : 1000  current step :  499
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.9453. top1: 97.27. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.9453. top1: 97.27. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.9409. top1: 97.07. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.9409. top1: 97.07. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9496. top1: 96.74. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.16it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.9496. top1: 96.74. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9556. top1: 96.29. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9556. top1: 96.29. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0108. top1: 92.81. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 1.0108. top1: 92.81. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0498. top1: 90.43. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 1.0498. top1: 90.43. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0828. top1: 88.00. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.17it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0828. top1: 88.00. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1010. top1: 86.35. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1010. top1: 86.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.1010. top1: 86.35. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
total : 1000  current step :  500
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.28s. S_Loss: 0.9841. T_Loss: 3.8123. Mask: 0.9570. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 501/1000. LR: 0.0313. Data: 0.01s. Batch: 0.28s. S_Loss: 0.9841. T_Loss: 3.8123. Mask: 0.9570. :   2%|▏         | 1/50 [00:00<00:13,  3.61it/s]total : 1000  current step :  501
Train Iter: 502/1000. LR: 0.0314. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9863. T_Loss: 4.0494. Mask: 0.9609. :   2%|▏         | 1/50 [00:01<00:13,  3.61it/s]Train Iter: 502/1000. LR: 0.0314. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9863. T_Loss: 4.0494. Mask: 0.9609. :   4%|▍         | 2/50 [00:01<00:34,  1.39it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.33s. Batch: 0.59s. S_Loss: 0.9812. T_Loss: 4.0097. Mask: 0.9609. :   4%|▍         | 2/50 [00:01<00:34,  1.39it/s]Train Iter: 503/1000. LR: 0.0314. Data: 0.33s. Batch: 0.59s. S_Loss: 0.9812. T_Loss: 4.0097. Mask: 0.9609. :   6%|▌         | 3/50 [00:01<00:28,  1.66it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9733. T_Loss: 3.8649. Mask: 0.9619. :   6%|▌         | 3/50 [00:02<00:28,  1.66it/s]Train Iter: 504/1000. LR: 0.0315. Data: 0.26s. Batch: 0.52s. S_Loss: 0.9733. T_Loss: 3.8649. Mask: 0.9619. :   8%|▊         | 4/50 [00:02<00:22,  2.05it/s]total : 1000  current step :  502
total : 1000  current step :  503
total : 1000  current step :  504
Train Iter: 505/1000. LR: 0.0316. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9729. T_Loss: 3.8735. Mask: 0.9602. :   8%|▊         | 4/50 [00:03<00:22,  2.05it/s]Train Iter: 505/1000. LR: 0.0316. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9729. T_Loss: 3.8735. Mask: 0.9602. :  10%|█         | 5/50 [00:03<00:34,  1.31it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.35s. Batch: 0.62s. S_Loss: 0.9691. T_Loss: 3.8386. Mask: 0.9655. :  10%|█         | 5/50 [00:03<00:34,  1.31it/s]Train Iter: 506/1000. LR: 0.0316. Data: 0.35s. Batch: 0.62s. S_Loss: 0.9691. T_Loss: 3.8386. Mask: 0.9655. :  12%|█▏        | 6/50 [00:03<00:27,  1.59it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9605. T_Loss: 3.8158. Mask: 0.9688. :  12%|█▏        | 6/50 [00:04<00:27,  1.59it/s]Train Iter: 507/1000. LR: 0.0317. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9605. T_Loss: 3.8158. Mask: 0.9688. :  14%|█▍        | 7/50 [00:04<00:22,  1.88it/s]total : 1000  current step :  505
total : 1000  current step :  506
total : 1000  current step :  507
Train Iter: 508/1000. LR: 0.0318. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9610. T_Loss: 3.8029. Mask: 0.9683. :  14%|█▍        | 7/50 [00:05<00:22,  1.88it/s]Train Iter: 508/1000. LR: 0.0318. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9610. T_Loss: 3.8029. Mask: 0.9683. :  16%|█▌        | 8/50 [00:05<00:32,  1.29it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.36s. Batch: 0.63s. S_Loss: 0.9584. T_Loss: 3.7936. Mask: 0.9688. :  16%|█▌        | 8/50 [00:05<00:32,  1.29it/s]Train Iter: 509/1000. LR: 0.0318. Data: 0.36s. Batch: 0.63s. S_Loss: 0.9584. T_Loss: 3.7936. Mask: 0.9688. :  18%|█▊        | 9/50 [00:05<00:26,  1.57it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.33s. Batch: 0.60s. S_Loss: 0.9586. T_Loss: 3.7965. Mask: 0.9691. :  18%|█▊        | 9/50 [00:05<00:26,  1.57it/s]Train Iter: 510/1000. LR: 0.0319. Data: 0.33s. Batch: 0.60s. S_Loss: 0.9586. T_Loss: 3.7965. Mask: 0.9691. :  20%|██        | 10/50 [00:05<00:21,  1.86it/s]total : 1000  current step :  508
total : 1000  current step :  509
total : 1000  current step :  510
Train Iter: 511/1000. LR: 0.0319. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9549. T_Loss: 3.7630. Mask: 0.9684. :  20%|██        | 10/50 [00:07<00:21,  1.86it/s]Train Iter: 511/1000. LR: 0.0319. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9549. T_Loss: 3.7630. Mask: 0.9684. :  22%|██▏       | 11/50 [00:07<00:28,  1.35it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9581. T_Loss: 3.7693. Mask: 0.9688. :  22%|██▏       | 11/50 [00:07<00:28,  1.35it/s]Train Iter: 512/1000. LR: 0.0320. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9581. T_Loss: 3.7693. Mask: 0.9688. :  24%|██▍       | 12/50 [00:07<00:23,  1.59it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.35s. Batch: 0.61s. S_Loss: 0.9607. T_Loss: 3.7987. Mask: 0.9684. :  24%|██▍       | 12/50 [00:07<00:23,  1.59it/s]Train Iter: 513/1000. LR: 0.0321. Data: 0.35s. Batch: 0.61s. S_Loss: 0.9607. T_Loss: 3.7987. Mask: 0.9684. :  26%|██▌       | 13/50 [00:07<00:20,  1.77it/s]total : 1000  current step :  511
total : 1000  current step :  512
total : 1000  current step :  513
Train Iter: 514/1000. LR: 0.0321. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9604. T_Loss: 3.7839. Mask: 0.9679. :  26%|██▌       | 13/50 [00:09<00:20,  1.77it/s]Train Iter: 514/1000. LR: 0.0321. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9604. T_Loss: 3.7839. Mask: 0.9679. :  28%|██▊       | 14/50 [00:09<00:25,  1.40it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9610. T_Loss: 3.7923. Mask: 0.9680. :  28%|██▊       | 14/50 [00:09<00:25,  1.40it/s]Train Iter: 515/1000. LR: 0.0322. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9610. T_Loss: 3.7923. Mask: 0.9680. :  30%|███       | 15/50 [00:09<00:21,  1.65it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.34s. Batch: 0.60s. S_Loss: 0.9585. T_Loss: 3.7726. Mask: 0.9675. :  30%|███       | 15/50 [00:09<00:21,  1.65it/s]Train Iter: 516/1000. LR: 0.0323. Data: 0.34s. Batch: 0.60s. S_Loss: 0.9585. T_Loss: 3.7726. Mask: 0.9675. :  32%|███▏      | 16/50 [00:09<00:17,  1.92it/s]total : 1000  current step :  514
total : 1000  current step :  515
total : 1000  current step :  516
Train Iter: 517/1000. LR: 0.0323. Data: 0.37s. Batch: 0.64s. S_Loss: 0.9575. T_Loss: 3.7873. Mask: 0.9674. :  32%|███▏      | 16/50 [00:10<00:17,  1.92it/s]Train Iter: 517/1000. LR: 0.0323. Data: 0.37s. Batch: 0.64s. S_Loss: 0.9575. T_Loss: 3.7873. Mask: 0.9674. :  34%|███▍      | 17/50 [00:10<00:23,  1.42it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.35s. Batch: 0.62s. S_Loss: 0.9571. T_Loss: 3.7735. Mask: 0.9674. :  34%|███▍      | 17/50 [00:11<00:23,  1.42it/s]Train Iter: 518/1000. LR: 0.0324. Data: 0.35s. Batch: 0.62s. S_Loss: 0.9571. T_Loss: 3.7735. Mask: 0.9674. :  36%|███▌      | 18/50 [00:11<00:19,  1.67it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.34s. Batch: 0.60s. S_Loss: 0.9588. T_Loss: 3.7686. Mask: 0.9677. :  36%|███▌      | 18/50 [00:11<00:19,  1.67it/s]Train Iter: 519/1000. LR: 0.0324. Data: 0.34s. Batch: 0.60s. S_Loss: 0.9588. T_Loss: 3.7686. Mask: 0.9677. :  38%|███▊      | 19/50 [00:11<00:16,  1.92it/s]total : 1000  current step :  517
total : 1000  current step :  518
total : 1000  current step :  519
Train Iter: 520/1000. LR: 0.0325. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9590. T_Loss: 3.7475. Mask: 0.9674. :  38%|███▊      | 19/50 [00:12<00:16,  1.92it/s]Train Iter: 520/1000. LR: 0.0325. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9590. T_Loss: 3.7475. Mask: 0.9674. :  40%|████      | 20/50 [00:12<00:23,  1.27it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9589. T_Loss: 3.7508. Mask: 0.9678. :  40%|████      | 20/50 [00:13<00:23,  1.27it/s]Train Iter: 521/1000. LR: 0.0326. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9589. T_Loss: 3.7508. Mask: 0.9678. :  42%|████▏     | 21/50 [00:13<00:20,  1.38it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9594. T_Loss: 3.7580. Mask: 0.9664. :  42%|████▏     | 21/50 [00:13<00:20,  1.38it/s]Train Iter: 522/1000. LR: 0.0326. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9594. T_Loss: 3.7580. Mask: 0.9664. :  44%|████▍     | 22/50 [00:13<00:18,  1.55it/s]total : 1000  current step :  520
total : 1000  current step :  521
total : 1000  current step :  522
Train Iter: 523/1000. LR: 0.0327. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9601. T_Loss: 3.7726. Mask: 0.9667. :  44%|████▍     | 22/50 [00:14<00:18,  1.55it/s]Train Iter: 523/1000. LR: 0.0327. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9601. T_Loss: 3.7726. Mask: 0.9667. :  46%|████▌     | 23/50 [00:14<00:20,  1.34it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9588. T_Loss: 3.7755. Mask: 0.9674. :  46%|████▌     | 23/50 [00:15<00:20,  1.34it/s]Train Iter: 524/1000. LR: 0.0328. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9588. T_Loss: 3.7755. Mask: 0.9674. :  48%|████▊     | 24/50 [00:15<00:15,  1.66it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9576. T_Loss: 3.7764. Mask: 0.9680. :  48%|████▊     | 24/50 [00:15<00:15,  1.66it/s]Train Iter: 525/1000. LR: 0.0328. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9576. T_Loss: 3.7764. Mask: 0.9680. :  50%|█████     | 25/50 [00:15<00:12,  2.02it/s]total : 1000  current step :  523
total : 1000  current step :  524
total : 1000  current step :  525
Train Iter: 526/1000. LR: 0.0329. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9577. T_Loss: 3.7868. Mask: 0.9688. :  50%|█████     | 25/50 [00:16<00:12,  2.02it/s]Train Iter: 526/1000. LR: 0.0329. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9577. T_Loss: 3.7868. Mask: 0.9688. :  52%|█████▏    | 26/50 [00:16<00:15,  1.52it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9578. T_Loss: 3.7831. Mask: 0.9692. :  52%|█████▏    | 26/50 [00:16<00:15,  1.52it/s]Train Iter: 527/1000. LR: 0.0329. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9578. T_Loss: 3.7831. Mask: 0.9692. :  54%|█████▍    | 27/50 [00:16<00:12,  1.80it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9575. T_Loss: 3.7772. Mask: 0.9689. :  54%|█████▍    | 27/50 [00:17<00:12,  1.80it/s]Train Iter: 528/1000. LR: 0.0330. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9575. T_Loss: 3.7772. Mask: 0.9689. :  56%|█████▌    | 28/50 [00:17<00:10,  2.12it/s]total : 1000  current step :  526
total : 1000  current step :  527
total : 1000  current step :  528
Train Iter: 529/1000. LR: 0.0331. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9573. T_Loss: 3.7720. Mask: 0.9692. :  56%|█████▌    | 28/50 [00:18<00:10,  2.12it/s]Train Iter: 529/1000. LR: 0.0331. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9573. T_Loss: 3.7720. Mask: 0.9692. :  58%|█████▊    | 29/50 [00:18<00:14,  1.47it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9568. T_Loss: 3.7691. Mask: 0.9695. :  58%|█████▊    | 29/50 [00:18<00:14,  1.47it/s]Train Iter: 530/1000. LR: 0.0331. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9568. T_Loss: 3.7691. Mask: 0.9695. :  60%|██████    | 30/50 [00:18<00:11,  1.74it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9561. T_Loss: 3.7618. Mask: 0.9696. :  60%|██████    | 30/50 [00:18<00:11,  1.74it/s]Train Iter: 531/1000. LR: 0.0332. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9561. T_Loss: 3.7618. Mask: 0.9696. :  62%|██████▏   | 31/50 [00:18<00:09,  2.09it/s]total : 1000  current step :  529
total : 1000  current step :  530
total : 1000  current step :  531
Train Iter: 532/1000. LR: 0.0333. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9564. T_Loss: 3.7702. Mask: 0.9701. :  62%|██████▏   | 31/50 [00:19<00:09,  2.09it/s]Train Iter: 532/1000. LR: 0.0333. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9564. T_Loss: 3.7702. Mask: 0.9701. :  64%|██████▍   | 32/50 [00:19<00:11,  1.58it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9564. T_Loss: 3.7678. Mask: 0.9703. :  64%|██████▍   | 32/50 [00:20<00:11,  1.58it/s]Train Iter: 533/1000. LR: 0.0333. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9564. T_Loss: 3.7678. Mask: 0.9703. :  66%|██████▌   | 33/50 [00:20<00:09,  1.75it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9561. T_Loss: 3.7552. Mask: 0.9708. :  66%|██████▌   | 33/50 [00:20<00:09,  1.75it/s]Train Iter: 534/1000. LR: 0.0334. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9561. T_Loss: 3.7552. Mask: 0.9708. :  68%|██████▊   | 34/50 [00:20<00:07,  2.12it/s]total : 1000  current step :  532
total : 1000  current step :  533
total : 1000  current step :  534
Train Iter: 535/1000. LR: 0.0334. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9546. T_Loss: 3.7379. Mask: 0.9709. :  68%|██████▊   | 34/50 [00:21<00:07,  2.12it/s]Train Iter: 535/1000. LR: 0.0334. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9546. T_Loss: 3.7379. Mask: 0.9709. :  70%|███████   | 35/50 [00:21<00:10,  1.50it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9539. T_Loss: 3.7347. Mask: 0.9708. :  70%|███████   | 35/50 [00:21<00:10,  1.50it/s]Train Iter: 536/1000. LR: 0.0335. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9539. T_Loss: 3.7347. Mask: 0.9708. :  72%|███████▏  | 36/50 [00:21<00:07,  1.81it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9546. T_Loss: 3.7359. Mask: 0.9705. :  72%|███████▏  | 36/50 [00:22<00:07,  1.81it/s]Train Iter: 537/1000. LR: 0.0336. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9546. T_Loss: 3.7359. Mask: 0.9705. :  74%|███████▍  | 37/50 [00:22<00:05,  2.20it/s]total : 1000  current step :  535
total : 1000  current step :  536
total : 1000  current step :  537
Train Iter: 538/1000. LR: 0.0336. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9548. T_Loss: 3.7414. Mask: 0.9704. :  74%|███████▍  | 37/50 [00:23<00:05,  2.20it/s]Train Iter: 538/1000. LR: 0.0336. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9548. T_Loss: 3.7414. Mask: 0.9704. :  76%|███████▌  | 38/50 [00:23<00:07,  1.64it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9545. T_Loss: 3.7394. Mask: 0.9703. :  76%|███████▌  | 38/50 [00:23<00:07,  1.64it/s]Train Iter: 539/1000. LR: 0.0337. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9545. T_Loss: 3.7394. Mask: 0.9703. :  78%|███████▊  | 39/50 [00:23<00:05,  1.92it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9542. T_Loss: 3.7380. Mask: 0.9700. :  78%|███████▊  | 39/50 [00:23<00:05,  1.92it/s]Train Iter: 540/1000. LR: 0.0338. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9542. T_Loss: 3.7380. Mask: 0.9700. :  80%|████████  | 40/50 [00:23<00:04,  2.22it/s]total : 1000  current step :  538
total : 1000  current step :  539
total : 1000  current step :  540
Train Iter: 541/1000. LR: 0.0338. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9532. T_Loss: 3.7372. Mask: 0.9704. :  80%|████████  | 40/50 [00:24<00:04,  2.22it/s]Train Iter: 541/1000. LR: 0.0338. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9532. T_Loss: 3.7372. Mask: 0.9704. :  82%|████████▏ | 41/50 [00:24<00:05,  1.69it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9533. T_Loss: 3.7377. Mask: 0.9707. :  82%|████████▏ | 41/50 [00:24<00:05,  1.69it/s]Train Iter: 542/1000. LR: 0.0339. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9533. T_Loss: 3.7377. Mask: 0.9707. :  84%|████████▍ | 42/50 [00:24<00:04,  1.98it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9528. T_Loss: 3.7422. Mask: 0.9709. :  84%|████████▍ | 42/50 [00:25<00:04,  1.98it/s]Train Iter: 543/1000. LR: 0.0339. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9528. T_Loss: 3.7422. Mask: 0.9709. :  86%|████████▌ | 43/50 [00:25<00:03,  2.21it/s]total : 1000  current step :  541
total : 1000  current step :  542
total : 1000  current step :  543
Train Iter: 544/1000. LR: 0.0340. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9527. T_Loss: 3.7486. Mask: 0.9710. :  86%|████████▌ | 43/50 [00:26<00:03,  2.21it/s]Train Iter: 544/1000. LR: 0.0340. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9527. T_Loss: 3.7486. Mask: 0.9710. :  88%|████████▊ | 44/50 [00:26<00:03,  1.68it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9536. T_Loss: 3.7573. Mask: 0.9706. :  88%|████████▊ | 44/50 [00:26<00:03,  1.68it/s]Train Iter: 545/1000. LR: 0.0341. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9536. T_Loss: 3.7573. Mask: 0.9706. :  90%|█████████ | 45/50 [00:26<00:02,  2.07it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9536. T_Loss: 3.7567. Mask: 0.9703. :  90%|█████████ | 45/50 [00:26<00:02,  2.07it/s]Train Iter: 546/1000. LR: 0.0341. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9536. T_Loss: 3.7567. Mask: 0.9703. :  92%|█████████▏| 46/50 [00:26<00:01,  2.25it/s]total : 1000  current step :  544
total : 1000  current step :  545
total : 1000  current step :  546
Train Iter: 547/1000. LR: 0.0342. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9541. T_Loss: 3.7650. Mask: 0.9704. :  92%|█████████▏| 46/50 [00:27<00:01,  2.25it/s]Train Iter: 547/1000. LR: 0.0342. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9541. T_Loss: 3.7650. Mask: 0.9704. :  94%|█████████▍| 47/50 [00:27<00:01,  1.57it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9536. T_Loss: 3.7755. Mask: 0.9707. :  94%|█████████▍| 47/50 [00:28<00:01,  1.57it/s]Train Iter: 548/1000. LR: 0.0343. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9536. T_Loss: 3.7755. Mask: 0.9707. :  96%|█████████▌| 48/50 [00:28<00:01,  1.97it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9530. T_Loss: 3.7714. Mask: 0.9704. :  96%|█████████▌| 48/50 [00:28<00:01,  1.97it/s]Train Iter: 549/1000. LR: 0.0343. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9530. T_Loss: 3.7714. Mask: 0.9704. :  98%|█████████▊| 49/50 [00:28<00:00,  2.23it/s]total : 1000  current step :  547
total : 1000  current step :  548
total : 1000  current step :  549
Train Iter: 550/1000. LR: 0.0344. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9530. T_Loss: 3.7694. Mask: 0.9705. :  98%|█████████▊| 49/50 [00:29<00:00,  2.23it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9530. T_Loss: 3.7694. Mask: 0.9705. : 100%|██████████| 50/50 [00:29<00:00,  1.57it/s]Train Iter: 550/1000. LR: 0.0344. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9530. T_Loss: 3.7694. Mask: 0.9705. : 100%|██████████| 50/50 [00:29<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8934. top1: 97.66. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8934. top1: 97.66. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8894. top1: 97.27. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8894. top1: 97.27. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.65it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8986. top1: 97.01. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.65it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8986. top1: 97.01. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9062. top1: 96.39. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.95it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9062. top1: 96.39. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.15it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9653. top1: 92.73. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.15it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9653. top1: 92.73. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.39it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0072. top1: 89.78. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.39it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0072. top1: 89.78. top5: 100.00. :  75%|███████▌  | 6/8 [00:01<00:00,  3.70it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0421. top1: 87.67. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.70it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 1.0421. top1: 87.67. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0621. top1: 86.20. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.71it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0621. top1: 86.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 1.0621. top1: 86.20. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]
total : 1000  current step :  550
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9441. T_Loss: 3.4844. Mask: 0.9688. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 551/1000. LR: 0.0344. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9441. T_Loss: 3.4844. Mask: 0.9688. :   2%|▏         | 1/50 [00:00<00:14,  3.35it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9560. T_Loss: 3.4118. Mask: 0.9629. :   2%|▏         | 1/50 [00:00<00:14,  3.35it/s]Train Iter: 552/1000. LR: 0.0345. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9560. T_Loss: 3.4118. Mask: 0.9629. :   4%|▍         | 2/50 [00:00<00:14,  3.33it/s]total : 1000  current step :  551
total : 1000  current step :  552
Train Iter: 553/1000. LR: 0.0346. Data: 0.32s. Batch: 0.62s. S_Loss: 0.9454. T_Loss: 3.4331. Mask: 0.9661. :   4%|▍         | 2/50 [00:01<00:14,  3.33it/s]Train Iter: 553/1000. LR: 0.0346. Data: 0.32s. Batch: 0.62s. S_Loss: 0.9454. T_Loss: 3.4331. Mask: 0.9661. :   6%|▌         | 3/50 [00:01<00:34,  1.36it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.24s. Batch: 0.53s. S_Loss: 0.9392. T_Loss: 3.4417. Mask: 0.9697. :   6%|▌         | 3/50 [00:02<00:34,  1.36it/s]Train Iter: 554/1000. LR: 0.0346. Data: 0.24s. Batch: 0.53s. S_Loss: 0.9392. T_Loss: 3.4417. Mask: 0.9697. :   8%|▊         | 4/50 [00:02<00:25,  1.81it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9426. T_Loss: 3.4336. Mask: 0.9703. :   8%|▊         | 4/50 [00:02<00:25,  1.81it/s]Train Iter: 555/1000. LR: 0.0347. Data: 0.20s. Batch: 0.48s. S_Loss: 0.9426. T_Loss: 3.4336. Mask: 0.9703. :  10%|█         | 5/50 [00:02<00:20,  2.20it/s]total : 1000  current step :  553
total : 1000  current step :  554
total : 1000  current step :  555
Train Iter: 556/1000. LR: 0.0347. Data: 0.29s. Batch: 0.56s. S_Loss: 0.9399. T_Loss: 3.4225. Mask: 0.9714. :  10%|█         | 5/50 [00:03<00:20,  2.20it/s]Train Iter: 556/1000. LR: 0.0347. Data: 0.29s. Batch: 0.56s. S_Loss: 0.9399. T_Loss: 3.4225. Mask: 0.9714. :  12%|█▏        | 6/50 [00:03<00:27,  1.59it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9422. T_Loss: 3.4079. Mask: 0.9727. :  12%|█▏        | 6/50 [00:03<00:27,  1.59it/s]Train Iter: 557/1000. LR: 0.0348. Data: 0.27s. Batch: 0.53s. S_Loss: 0.9422. T_Loss: 3.4079. Mask: 0.9727. :  14%|█▍        | 7/50 [00:03<00:23,  1.87it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9426. T_Loss: 3.4459. Mask: 0.9717. :  14%|█▍        | 7/50 [00:04<00:23,  1.87it/s]Train Iter: 558/1000. LR: 0.0349. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9426. T_Loss: 3.4459. Mask: 0.9717. :  16%|█▌        | 8/50 [00:04<00:19,  2.14it/s]total : 1000  current step :  556
total : 1000  current step :  557
total : 1000  current step :  558
Train Iter: 559/1000. LR: 0.0349. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9444. T_Loss: 3.5105. Mask: 0.9709. :  16%|█▌        | 8/50 [00:05<00:19,  2.14it/s]Train Iter: 559/1000. LR: 0.0349. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9444. T_Loss: 3.5105. Mask: 0.9709. :  18%|█▊        | 9/50 [00:05<00:27,  1.50it/s]total : 1000  current step :  559
Train Iter: 560/1000. LR: 0.0350. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9441. T_Loss: 3.5413. Mask: 0.9727. :  18%|█▊        | 9/50 [00:05<00:27,  1.50it/s]Train Iter: 560/1000. LR: 0.0350. Data: 0.32s. Batch: 0.57s. S_Loss: 0.9441. T_Loss: 3.5413. Mask: 0.9727. :  20%|██        | 10/50 [00:05<00:25,  1.59it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.33s. Batch: 0.57s. S_Loss: 0.9474. T_Loss: 3.6008. Mask: 0.9727. :  20%|██        | 10/50 [00:06<00:25,  1.59it/s]Train Iter: 561/1000. LR: 0.0351. Data: 0.33s. Batch: 0.57s. S_Loss: 0.9474. T_Loss: 3.6008. Mask: 0.9727. :  22%|██▏       | 11/50 [00:06<00:24,  1.57it/s]total : 1000  current step :  560
total : 1000  current step :  561
Train Iter: 562/1000. LR: 0.0351. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9451. T_Loss: 3.6344. Mask: 0.9730. :  22%|██▏       | 11/50 [00:07<00:24,  1.57it/s]Train Iter: 562/1000. LR: 0.0351. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9451. T_Loss: 3.6344. Mask: 0.9730. :  24%|██▍       | 12/50 [00:07<00:29,  1.27it/s]Train Iter: 563/1000. LR: 0.0352. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9490. T_Loss: 3.6689. Mask: 0.9727. :  24%|██▍       | 12/50 [00:07<00:29,  1.27it/s]Train Iter: 563/1000. LR: 0.0352. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9490. T_Loss: 3.6689. Mask: 0.9727. :  26%|██▌       | 13/50 [00:07<00:23,  1.59it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9496. T_Loss: 3.7212. Mask: 0.9721. :  26%|██▌       | 13/50 [00:08<00:23,  1.59it/s]Train Iter: 564/1000. LR: 0.0352. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9496. T_Loss: 3.7212. Mask: 0.9721. :  28%|██▊       | 14/50 [00:08<00:19,  1.81it/s]total : 1000  current step :  562
total : 1000  current step :  563
total : 1000  current step :  564
Train Iter: 565/1000. LR: 0.0353. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9504. T_Loss: 3.7506. Mask: 0.9727. :  28%|██▊       | 14/50 [00:09<00:19,  1.81it/s]Train Iter: 565/1000. LR: 0.0353. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9504. T_Loss: 3.7506. Mask: 0.9727. :  30%|███       | 15/50 [00:09<00:24,  1.45it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9501. T_Loss: 3.7308. Mask: 0.9705. :  30%|███       | 15/50 [00:09<00:24,  1.45it/s]Train Iter: 566/1000. LR: 0.0354. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9501. T_Loss: 3.7308. Mask: 0.9705. :  32%|███▏      | 16/50 [00:09<00:20,  1.64it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9521. T_Loss: 3.7378. Mask: 0.9699. :  32%|███▏      | 16/50 [00:09<00:20,  1.64it/s]Train Iter: 567/1000. LR: 0.0354. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9521. T_Loss: 3.7378. Mask: 0.9699. :  34%|███▍      | 17/50 [00:09<00:17,  1.85it/s]total : 1000  current step :  565
total : 1000  current step :  566
total : 1000  current step :  567
Train Iter: 568/1000. LR: 0.0355. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9532. T_Loss: 3.7437. Mask: 0.9694. :  34%|███▍      | 17/50 [00:10<00:17,  1.85it/s]Train Iter: 568/1000. LR: 0.0355. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9532. T_Loss: 3.7437. Mask: 0.9694. :  36%|███▌      | 18/50 [00:10<00:22,  1.43it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9521. T_Loss: 3.7390. Mask: 0.9696. :  36%|███▌      | 18/50 [00:11<00:22,  1.43it/s]Train Iter: 569/1000. LR: 0.0356. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9521. T_Loss: 3.7390. Mask: 0.9696. :  38%|███▊      | 19/50 [00:11<00:17,  1.73it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9525. T_Loss: 3.7242. Mask: 0.9693. :  38%|███▊      | 19/50 [00:11<00:17,  1.73it/s]Train Iter: 570/1000. LR: 0.0356. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9525. T_Loss: 3.7242. Mask: 0.9693. :  40%|████      | 20/50 [00:11<00:15,  1.90it/s]total : 1000  current step :  568
total : 1000  current step :  569
total : 1000  current step :  570
Train Iter: 571/1000. LR: 0.0357. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9526. T_Loss: 3.7171. Mask: 0.9693. :  40%|████      | 20/50 [00:12<00:15,  1.90it/s]Train Iter: 571/1000. LR: 0.0357. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9526. T_Loss: 3.7171. Mask: 0.9693. :  42%|████▏     | 21/50 [00:12<00:20,  1.40it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9525. T_Loss: 3.7123. Mask: 0.9698. :  42%|████▏     | 21/50 [00:13<00:20,  1.40it/s]Train Iter: 572/1000. LR: 0.0357. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9525. T_Loss: 3.7123. Mask: 0.9698. :  44%|████▍     | 22/50 [00:13<00:16,  1.65it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9540. T_Loss: 3.7041. Mask: 0.9704. :  44%|████▍     | 22/50 [00:13<00:16,  1.65it/s]Train Iter: 573/1000. LR: 0.0358. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9540. T_Loss: 3.7041. Mask: 0.9704. :  46%|████▌     | 23/50 [00:13<00:13,  1.97it/s]total : 1000  current step :  571
total : 1000  current step :  572
total : 1000  current step :  573
Train Iter: 574/1000. LR: 0.0359. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9534. T_Loss: 3.6851. Mask: 0.9710. :  46%|████▌     | 23/50 [00:14<00:13,  1.97it/s]Train Iter: 574/1000. LR: 0.0359. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9534. T_Loss: 3.6851. Mask: 0.9710. :  48%|████▊     | 24/50 [00:14<00:17,  1.45it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9529. T_Loss: 3.6721. Mask: 0.9719. :  48%|████▊     | 24/50 [00:14<00:17,  1.45it/s]Train Iter: 575/1000. LR: 0.0359. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9529. T_Loss: 3.6721. Mask: 0.9719. :  50%|█████     | 25/50 [00:14<00:14,  1.74it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9522. T_Loss: 3.6507. Mask: 0.9728. :  50%|█████     | 25/50 [00:15<00:14,  1.74it/s]Train Iter: 576/1000. LR: 0.0360. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9522. T_Loss: 3.6507. Mask: 0.9728. :  52%|█████▏    | 26/50 [00:15<00:12,  1.91it/s]total : 1000  current step :  574
total : 1000  current step :  575
total : 1000  current step :  576
Train Iter: 577/1000. LR: 0.0361. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9517. T_Loss: 3.6437. Mask: 0.9721. :  52%|█████▏    | 26/50 [00:16<00:12,  1.91it/s]Train Iter: 577/1000. LR: 0.0361. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9517. T_Loss: 3.6437. Mask: 0.9721. :  54%|█████▍    | 27/50 [00:16<00:16,  1.43it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9496. T_Loss: 3.6332. Mask: 0.9724. :  54%|█████▍    | 27/50 [00:16<00:16,  1.43it/s]Train Iter: 578/1000. LR: 0.0361. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9496. T_Loss: 3.6332. Mask: 0.9724. :  56%|█████▌    | 28/50 [00:16<00:12,  1.74it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9490. T_Loss: 3.6280. Mask: 0.9723. :  56%|█████▌    | 28/50 [00:17<00:12,  1.74it/s]Train Iter: 579/1000. LR: 0.0362. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9490. T_Loss: 3.6280. Mask: 0.9723. :  58%|█████▊    | 29/50 [00:17<00:10,  1.99it/s]total : 1000  current step :  577
total : 1000  current step :  578
total : 1000  current step :  579
Train Iter: 580/1000. LR: 0.0362. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9488. T_Loss: 3.6331. Mask: 0.9719. :  58%|█████▊    | 29/50 [00:18<00:10,  1.99it/s]Train Iter: 580/1000. LR: 0.0362. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9488. T_Loss: 3.6331. Mask: 0.9719. :  60%|██████    | 30/50 [00:18<00:13,  1.44it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9501. T_Loss: 3.6520. Mask: 0.9711. :  60%|██████    | 30/50 [00:18<00:13,  1.44it/s]Train Iter: 581/1000. LR: 0.0363. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9501. T_Loss: 3.6520. Mask: 0.9711. :  62%|██████▏   | 31/50 [00:18<00:11,  1.69it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9498. T_Loss: 3.6632. Mask: 0.9711. :  62%|██████▏   | 31/50 [00:18<00:11,  1.69it/s]Train Iter: 582/1000. LR: 0.0364. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9498. T_Loss: 3.6632. Mask: 0.9711. :  64%|██████▍   | 32/50 [00:18<00:09,  1.98it/s]total : 1000  current step :  580
total : 1000  current step :  581
total : 1000  current step :  582
Train Iter: 583/1000. LR: 0.0364. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9479. T_Loss: 3.6630. Mask: 0.9710. :  64%|██████▍   | 32/50 [00:19<00:09,  1.98it/s]Train Iter: 583/1000. LR: 0.0364. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9479. T_Loss: 3.6630. Mask: 0.9710. :  66%|██████▌   | 33/50 [00:19<00:10,  1.55it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9472. T_Loss: 3.6715. Mask: 0.9710. :  66%|██████▌   | 33/50 [00:20<00:10,  1.55it/s]Train Iter: 584/1000. LR: 0.0365. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9472. T_Loss: 3.6715. Mask: 0.9710. :  68%|██████▊   | 34/50 [00:20<00:09,  1.77it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9473. T_Loss: 3.6862. Mask: 0.9706. :  68%|██████▊   | 34/50 [00:20<00:09,  1.77it/s]Train Iter: 585/1000. LR: 0.0366. Data: 0.35s. Batch: 0.58s. S_Loss: 0.9473. T_Loss: 3.6862. Mask: 0.9706. :  70%|███████   | 35/50 [00:20<00:06,  2.16it/s]total : 1000  current step :  583
total : 1000  current step :  584
total : 1000  current step :  585
Train Iter: 586/1000. LR: 0.0366. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9487. T_Loss: 3.6926. Mask: 0.9704. :  70%|███████   | 35/50 [00:21<00:06,  2.16it/s]Train Iter: 586/1000. LR: 0.0366. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9487. T_Loss: 3.6926. Mask: 0.9704. :  72%|███████▏  | 36/50 [00:21<00:09,  1.44it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9476. T_Loss: 3.6905. Mask: 0.9703. :  72%|███████▏  | 36/50 [00:21<00:09,  1.44it/s]Train Iter: 587/1000. LR: 0.0367. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9476. T_Loss: 3.6905. Mask: 0.9703. :  74%|███████▍  | 37/50 [00:21<00:07,  1.81it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9481. T_Loss: 3.6980. Mask: 0.9700. :  74%|███████▍  | 37/50 [00:22<00:07,  1.81it/s]Train Iter: 588/1000. LR: 0.0367. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9481. T_Loss: 3.6980. Mask: 0.9700. :  76%|███████▌  | 38/50 [00:22<00:05,  2.00it/s]total : 1000  current step :  586
total : 1000  current step :  587
total : 1000  current step :  588
Train Iter: 589/1000. LR: 0.0368. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9477. T_Loss: 3.6991. Mask: 0.9702. :  76%|███████▌  | 38/50 [00:23<00:05,  2.00it/s]Train Iter: 589/1000. LR: 0.0368. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9477. T_Loss: 3.6991. Mask: 0.9702. :  78%|███████▊  | 39/50 [00:23<00:06,  1.57it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9477. T_Loss: 3.7021. Mask: 0.9703. :  78%|███████▊  | 39/50 [00:23<00:06,  1.57it/s]Train Iter: 590/1000. LR: 0.0369. Data: 0.36s. Batch: 0.59s. S_Loss: 0.9477. T_Loss: 3.7021. Mask: 0.9703. :  80%|████████  | 40/50 [00:23<00:05,  1.87it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9482. T_Loss: 3.7073. Mask: 0.9705. :  80%|████████  | 40/50 [00:23<00:05,  1.87it/s]Train Iter: 591/1000. LR: 0.0369. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9482. T_Loss: 3.7073. Mask: 0.9705. :  82%|████████▏ | 41/50 [00:23<00:04,  1.90it/s]total : 1000  current step :  589
total : 1000  current step :  590
total : 1000  current step :  591
Train Iter: 592/1000. LR: 0.0370. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9488. T_Loss: 3.7165. Mask: 0.9704. :  82%|████████▏ | 41/50 [00:24<00:04,  1.90it/s]Train Iter: 592/1000. LR: 0.0370. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9488. T_Loss: 3.7165. Mask: 0.9704. :  84%|████████▍ | 42/50 [00:24<00:05,  1.52it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9477. T_Loss: 3.7127. Mask: 0.9703. :  84%|████████▍ | 42/50 [00:25<00:05,  1.52it/s]Train Iter: 593/1000. LR: 0.0371. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9477. T_Loss: 3.7127. Mask: 0.9703. :  86%|████████▌ | 43/50 [00:25<00:03,  1.88it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9480. T_Loss: 3.7124. Mask: 0.9703. :  86%|████████▌ | 43/50 [00:25<00:03,  1.88it/s]Train Iter: 594/1000. LR: 0.0371. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9480. T_Loss: 3.7124. Mask: 0.9703. :  88%|████████▊ | 44/50 [00:25<00:02,  2.23it/s]total : 1000  current step :  592
total : 1000  current step :  593
total : 1000  current step :  594
Train Iter: 595/1000. LR: 0.0372. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9478. T_Loss: 3.7133. Mask: 0.9702. :  88%|████████▊ | 44/50 [00:26<00:02,  2.23it/s]Train Iter: 595/1000. LR: 0.0372. Data: 0.37s. Batch: 0.59s. S_Loss: 0.9478. T_Loss: 3.7133. Mask: 0.9702. :  90%|█████████ | 45/50 [00:26<00:03,  1.64it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9491. T_Loss: 3.7227. Mask: 0.9704. :  90%|█████████ | 45/50 [00:26<00:03,  1.64it/s]Train Iter: 596/1000. LR: 0.0372. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9491. T_Loss: 3.7227. Mask: 0.9704. :  92%|█████████▏| 46/50 [00:26<00:02,  1.92it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.36s. Batch: 0.57s. S_Loss: 0.9494. T_Loss: 3.7320. Mask: 0.9706. :  92%|█████████▏| 46/50 [00:27<00:02,  1.92it/s]Train Iter: 597/1000. LR: 0.0373. Data: 0.36s. Batch: 0.57s. S_Loss: 0.9494. T_Loss: 3.7320. Mask: 0.9706. :  94%|█████████▍| 47/50 [00:27<00:01,  2.20it/s]total : 1000  current step :  595
total : 1000  current step :  596
total : 1000  current step :  597
Train Iter: 598/1000. LR: 0.0374. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9492. T_Loss: 3.7249. Mask: 0.9701. :  94%|█████████▍| 47/50 [00:28<00:01,  2.20it/s]Train Iter: 598/1000. LR: 0.0374. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9492. T_Loss: 3.7249. Mask: 0.9701. :  96%|█████████▌| 48/50 [00:28<00:01,  1.58it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9488. T_Loss: 3.7233. Mask: 0.9703. :  96%|█████████▌| 48/50 [00:28<00:01,  1.58it/s]Train Iter: 599/1000. LR: 0.0374. Data: 0.36s. Batch: 0.58s. S_Loss: 0.9488. T_Loss: 3.7233. Mask: 0.9703. :  98%|█████████▊| 49/50 [00:28<00:00,  1.83it/s]total : 1000  current step :  598
total : 1000  current step :  599
Train Iter: 600/1000. LR: 0.0375. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9478. T_Loss: 3.7078. Mask: 0.9703. :  98%|█████████▊| 49/50 [00:29<00:00,  1.83it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9478. T_Loss: 3.7078. Mask: 0.9703. : 100%|██████████| 50/50 [00:29<00:00,  1.63it/s]Train Iter: 600/1000. LR: 0.0375. Data: 0.37s. Batch: 0.58s. S_Loss: 0.9478. T_Loss: 3.7078. Mask: 0.9703. : 100%|██████████| 50/50 [00:29<00:00,  1.71it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8644. top1: 97.66. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8644. top1: 97.66. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8596. top1: 97.66. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8596. top1: 97.66. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8680. top1: 97.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8680. top1: 97.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8764. top1: 97.07. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8764. top1: 97.07. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9377. top1: 93.67. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9377. top1: 93.67. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9817. top1: 90.49. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.9817. top1: 90.49. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0176. top1: 88.11. top5: 100.00. :  75%|███████▌  | 6/8 [00:02<00:00,  3.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0176. top1: 88.11. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0391. top1: 86.55. top5: 100.00. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0391. top1: 86.55. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 1.0391. top1: 86.55. top5: 100.00. : 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
total : 1000  current step :  600
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.87s. Batch: 1.14s. S_Loss: 0.9726. T_Loss: 3.7215. Mask: 0.9688. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 601/1000. LR: 0.0376. Data: 0.87s. Batch: 1.14s. S_Loss: 0.9726. T_Loss: 3.7215. Mask: 0.9688. :   2%|▏         | 1/50 [00:01<00:55,  1.14s/it]Train Iter: 602/1000. LR: 0.0376. Data: 0.49s. Batch: 0.75s. S_Loss: 0.9762. T_Loss: 3.8223. Mask: 0.9688. :   2%|▏         | 1/50 [00:01<00:55,  1.14s/it]Train Iter: 602/1000. LR: 0.0376. Data: 0.49s. Batch: 0.75s. S_Loss: 0.9762. T_Loss: 3.8223. Mask: 0.9688. :   4%|▍         | 2/50 [00:01<00:33,  1.45it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9521. T_Loss: 3.7647. Mask: 0.9740. :   4%|▍         | 2/50 [00:01<00:33,  1.45it/s]Train Iter: 603/1000. LR: 0.0377. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9521. T_Loss: 3.7647. Mask: 0.9740. :   6%|▌         | 3/50 [00:01<00:22,  2.09it/s]total : 1000  current step :  601
total : 1000  current step :  602
total : 1000  current step :  603
Train Iter: 604/1000. LR: 0.0378. Data: 0.47s. Batch: 0.71s. S_Loss: 0.9488. T_Loss: 3.6718. Mask: 0.9756. :   6%|▌         | 3/50 [00:02<00:22,  2.09it/s]Train Iter: 604/1000. LR: 0.0378. Data: 0.47s. Batch: 0.71s. S_Loss: 0.9488. T_Loss: 3.6718. Mask: 0.9756. :   8%|▊         | 4/50 [00:02<00:33,  1.37it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9362. T_Loss: 3.6071. Mask: 0.9758. :   8%|▊         | 4/50 [00:03<00:33,  1.37it/s]Train Iter: 605/1000. LR: 0.0378. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9362. T_Loss: 3.6071. Mask: 0.9758. :  10%|█         | 5/50 [00:03<00:26,  1.72it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.33s. Batch: 0.56s. S_Loss: 0.9393. T_Loss: 3.6190. Mask: 0.9720. :  10%|█         | 5/50 [00:03<00:26,  1.72it/s]Train Iter: 606/1000. LR: 0.0379. Data: 0.33s. Batch: 0.56s. S_Loss: 0.9393. T_Loss: 3.6190. Mask: 0.9720. :  12%|█▏        | 6/50 [00:03<00:19,  2.21it/s]total : 1000  current step :  604
total : 1000  current step :  605
total : 1000  current step :  606
Train Iter: 607/1000. LR: 0.0379. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9359. T_Loss: 3.6031. Mask: 0.9727. :  12%|█▏        | 6/50 [00:04<00:19,  2.21it/s]Train Iter: 607/1000. LR: 0.0379. Data: 0.43s. Batch: 0.65s. S_Loss: 0.9359. T_Loss: 3.6031. Mask: 0.9727. :  14%|█▍        | 7/50 [00:04<00:29,  1.44it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9437. T_Loss: 3.6974. Mask: 0.9736. :  14%|█▍        | 7/50 [00:05<00:29,  1.44it/s]Train Iter: 608/1000. LR: 0.0380. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9437. T_Loss: 3.6974. Mask: 0.9736. :  16%|█▌        | 8/50 [00:05<00:26,  1.60it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9428. T_Loss: 3.6744. Mask: 0.9714. :  16%|█▌        | 8/50 [00:05<00:26,  1.60it/s]Train Iter: 609/1000. LR: 0.0381. Data: 0.37s. Batch: 0.60s. S_Loss: 0.9428. T_Loss: 3.6744. Mask: 0.9714. :  18%|█▊        | 9/50 [00:05<00:21,  1.87it/s]total : 1000  current step :  607
total : 1000  current step :  608
total : 1000  current step :  609
Train Iter: 610/1000. LR: 0.0381. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9439. T_Loss: 3.6782. Mask: 0.9723. :  18%|█▊        | 9/50 [00:06<00:21,  1.87it/s]Train Iter: 610/1000. LR: 0.0381. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9439. T_Loss: 3.6782. Mask: 0.9723. :  20%|██        | 10/50 [00:06<00:28,  1.41it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9437. T_Loss: 3.6996. Mask: 0.9719. :  20%|██        | 10/50 [00:06<00:28,  1.41it/s]Train Iter: 611/1000. LR: 0.0382. Data: 0.41s. Batch: 0.63s. S_Loss: 0.9437. T_Loss: 3.6996. Mask: 0.9719. :  22%|██▏       | 11/50 [00:06<00:25,  1.55it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9444. T_Loss: 3.7133. Mask: 0.9730. :  22%|██▏       | 11/50 [00:07<00:25,  1.55it/s]Train Iter: 612/1000. LR: 0.0383. Data: 0.38s. Batch: 0.61s. S_Loss: 0.9444. T_Loss: 3.7133. Mask: 0.9730. :  24%|██▍       | 12/50 [00:07<00:21,  1.79it/s]total : 1000  current step :  610
total : 1000  current step :  611
total : 1000  current step :  612
Train Iter: 613/1000. LR: 0.0383. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9471. T_Loss: 3.7317. Mask: 0.9736. :  24%|██▍       | 12/50 [00:08<00:21,  1.79it/s]Train Iter: 613/1000. LR: 0.0383. Data: 0.44s. Batch: 0.66s. S_Loss: 0.9471. T_Loss: 3.7317. Mask: 0.9736. :  26%|██▌       | 13/50 [00:08<00:28,  1.29it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9476. T_Loss: 3.7514. Mask: 0.9738. :  26%|██▌       | 13/50 [00:09<00:28,  1.29it/s]Train Iter: 614/1000. LR: 0.0384. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9476. T_Loss: 3.7514. Mask: 0.9738. :  28%|██▊       | 14/50 [00:09<00:24,  1.47it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9474. T_Loss: 3.7635. Mask: 0.9729. :  28%|██▊       | 14/50 [00:09<00:24,  1.47it/s]Train Iter: 615/1000. LR: 0.0384. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9474. T_Loss: 3.7635. Mask: 0.9729. :  30%|███       | 15/50 [00:09<00:20,  1.71it/s]total : 1000  current step :  613
total : 1000  current step :  614
total : 1000  current step :  615
Train Iter: 616/1000. LR: 0.0385. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9446. T_Loss: 3.7581. Mask: 0.9719. :  30%|███       | 15/50 [00:10<00:20,  1.71it/s]Train Iter: 616/1000. LR: 0.0385. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9446. T_Loss: 3.7581. Mask: 0.9719. :  32%|███▏      | 16/50 [00:10<00:26,  1.31it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9430. T_Loss: 3.7680. Mask: 0.9722. :  32%|███▏      | 16/50 [00:11<00:26,  1.31it/s]Train Iter: 617/1000. LR: 0.0386. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9430. T_Loss: 3.7680. Mask: 0.9722. :  34%|███▍      | 17/50 [00:11<00:22,  1.50it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9421. T_Loss: 3.7512. Mask: 0.9714. :  34%|███▍      | 17/50 [00:11<00:22,  1.50it/s]Train Iter: 618/1000. LR: 0.0386. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9421. T_Loss: 3.7512. Mask: 0.9714. :  36%|███▌      | 18/50 [00:11<00:18,  1.71it/s]total : 1000  current step :  616
total : 1000  current step :  617
total : 1000  current step :  618
Train Iter: 619/1000. LR: 0.0387. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9415. T_Loss: 3.7462. Mask: 0.9718. :  36%|███▌      | 18/50 [00:12<00:18,  1.71it/s]Train Iter: 619/1000. LR: 0.0387. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9415. T_Loss: 3.7462. Mask: 0.9718. :  38%|███▊      | 19/50 [00:12<00:25,  1.24it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9415. T_Loss: 3.7412. Mask: 0.9713. :  38%|███▊      | 19/50 [00:13<00:25,  1.24it/s]Train Iter: 620/1000. LR: 0.0388. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9415. T_Loss: 3.7412. Mask: 0.9713. :  40%|████      | 20/50 [00:13<00:20,  1.49it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9423. T_Loss: 3.7507. Mask: 0.9710. :  40%|████      | 20/50 [00:13<00:20,  1.49it/s]Train Iter: 621/1000. LR: 0.0388. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9423. T_Loss: 3.7507. Mask: 0.9710. :  42%|████▏     | 21/50 [00:13<00:16,  1.81it/s]total : 1000  current step :  619
total : 1000  current step :  620
total : 1000  current step :  621
Train Iter: 622/1000. LR: 0.0389. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9421. T_Loss: 3.7421. Mask: 0.9714. :  42%|████▏     | 21/50 [00:14<00:16,  1.81it/s]Train Iter: 622/1000. LR: 0.0389. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9421. T_Loss: 3.7421. Mask: 0.9714. :  44%|████▍     | 22/50 [00:14<00:20,  1.38it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9435. T_Loss: 3.7403. Mask: 0.9706. :  44%|████▍     | 22/50 [00:14<00:20,  1.38it/s]Train Iter: 623/1000. LR: 0.0389. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9435. T_Loss: 3.7403. Mask: 0.9706. :  46%|████▌     | 23/50 [00:14<00:16,  1.61it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9432. T_Loss: 3.7460. Mask: 0.9702. :  46%|████▌     | 23/50 [00:15<00:16,  1.61it/s]Train Iter: 624/1000. LR: 0.0390. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9432. T_Loss: 3.7460. Mask: 0.9702. :  48%|████▊     | 24/50 [00:15<00:14,  1.82it/s]total : 1000  current step :  622
total : 1000  current step :  623
total : 1000  current step :  624
Train Iter: 625/1000. LR: 0.0391. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9425. T_Loss: 3.7536. Mask: 0.9705. :  48%|████▊     | 24/50 [00:16<00:14,  1.82it/s]Train Iter: 625/1000. LR: 0.0391. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9425. T_Loss: 3.7536. Mask: 0.9705. :  50%|█████     | 25/50 [00:16<00:18,  1.38it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9435. T_Loss: 3.7535. Mask: 0.9706. :  50%|█████     | 25/50 [00:16<00:18,  1.38it/s]Train Iter: 626/1000. LR: 0.0391. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9435. T_Loss: 3.7535. Mask: 0.9706. :  52%|█████▏    | 26/50 [00:16<00:14,  1.62it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9428. T_Loss: 3.7416. Mask: 0.9706. :  52%|█████▏    | 26/50 [00:17<00:14,  1.62it/s]Train Iter: 627/1000. LR: 0.0392. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9428. T_Loss: 3.7416. Mask: 0.9706. :  54%|█████▍    | 27/50 [00:17<00:13,  1.77it/s]total : 1000  current step :  625
total : 1000  current step :  626
total : 1000  current step :  627
Train Iter: 628/1000. LR: 0.0393. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9431. T_Loss: 3.7419. Mask: 0.9710. :  54%|█████▍    | 27/50 [00:18<00:13,  1.77it/s]Train Iter: 628/1000. LR: 0.0393. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9431. T_Loss: 3.7419. Mask: 0.9710. :  56%|█████▌    | 28/50 [00:18<00:15,  1.43it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9441. T_Loss: 3.7451. Mask: 0.9713. :  56%|█████▌    | 28/50 [00:18<00:15,  1.43it/s]Train Iter: 629/1000. LR: 0.0393. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9441. T_Loss: 3.7451. Mask: 0.9713. :  58%|█████▊    | 29/50 [00:18<00:12,  1.62it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9437. T_Loss: 3.7378. Mask: 0.9712. :  58%|█████▊    | 29/50 [00:19<00:12,  1.62it/s]Train Iter: 630/1000. LR: 0.0394. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9437. T_Loss: 3.7378. Mask: 0.9712. :  60%|██████    | 30/50 [00:19<00:10,  1.86it/s]total : 1000  current step :  628
total : 1000  current step :  629
total : 1000  current step :  630
Train Iter: 631/1000. LR: 0.0394. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9431. T_Loss: 3.7289. Mask: 0.9709. :  60%|██████    | 30/50 [00:20<00:10,  1.86it/s]Train Iter: 631/1000. LR: 0.0394. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9431. T_Loss: 3.7289. Mask: 0.9709. :  62%|██████▏   | 31/50 [00:20<00:13,  1.42it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9422. T_Loss: 3.7206. Mask: 0.9708. :  62%|██████▏   | 31/50 [00:20<00:13,  1.42it/s]Train Iter: 632/1000. LR: 0.0395. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9422. T_Loss: 3.7206. Mask: 0.9708. :  64%|██████▍   | 32/50 [00:20<00:11,  1.62it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9428. T_Loss: 3.7241. Mask: 0.9712. :  64%|██████▍   | 32/50 [00:20<00:11,  1.62it/s]Train Iter: 633/1000. LR: 0.0396. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9428. T_Loss: 3.7241. Mask: 0.9712. :  66%|██████▌   | 33/50 [00:20<00:09,  1.87it/s]total : 1000  current step :  631
total : 1000  current step :  632
total : 1000  current step :  633
Train Iter: 634/1000. LR: 0.0396. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9440. T_Loss: 3.7242. Mask: 0.9710. :  66%|██████▌   | 33/50 [00:22<00:09,  1.87it/s]Train Iter: 634/1000. LR: 0.0396. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9440. T_Loss: 3.7242. Mask: 0.9710. :  68%|██████▊   | 34/50 [00:22<00:12,  1.30it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9428. T_Loss: 3.7130. Mask: 0.9714. :  68%|██████▊   | 34/50 [00:22<00:12,  1.30it/s]Train Iter: 635/1000. LR: 0.0397. Data: 0.41s. Batch: 0.64s. S_Loss: 0.9428. T_Loss: 3.7130. Mask: 0.9714. :  70%|███████   | 35/50 [00:22<00:09,  1.55it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9414. T_Loss: 3.7158. Mask: 0.9718. :  70%|███████   | 35/50 [00:22<00:09,  1.55it/s]Train Iter: 636/1000. LR: 0.0398. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9414. T_Loss: 3.7158. Mask: 0.9718. :  72%|███████▏  | 36/50 [00:22<00:07,  1.79it/s]total : 1000  current step :  634
total : 1000  current step :  635
total : 1000  current step :  636
Train Iter: 637/1000. LR: 0.0398. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9411. T_Loss: 3.7205. Mask: 0.9717. :  72%|███████▏  | 36/50 [00:24<00:07,  1.79it/s]Train Iter: 637/1000. LR: 0.0398. Data: 0.42s. Batch: 0.65s. S_Loss: 0.9411. T_Loss: 3.7205. Mask: 0.9717. :  74%|███████▍  | 37/50 [00:24<00:10,  1.26it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9404. T_Loss: 3.7114. Mask: 0.9716. :  74%|███████▍  | 37/50 [00:24<00:10,  1.26it/s]Train Iter: 638/1000. LR: 0.0399. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9404. T_Loss: 3.7114. Mask: 0.9716. :  76%|███████▌  | 38/50 [00:24<00:07,  1.50it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9401. T_Loss: 3.7046. Mask: 0.9718. :  76%|███████▌  | 38/50 [00:24<00:07,  1.50it/s]Train Iter: 639/1000. LR: 0.0399. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9401. T_Loss: 3.7046. Mask: 0.9718. :  78%|███████▊  | 39/50 [00:24<00:06,  1.79it/s]total : 1000  current step :  637
total : 1000  current step :  638
total : 1000  current step :  639
Train Iter: 640/1000. LR: 0.0400. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9396. T_Loss: 3.6923. Mask: 0.9721. :  78%|███████▊  | 39/50 [00:26<00:06,  1.79it/s]Train Iter: 640/1000. LR: 0.0400. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9396. T_Loss: 3.6923. Mask: 0.9721. :  80%|████████  | 40/50 [00:26<00:09,  1.09it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9397. T_Loss: 3.6882. Mask: 0.9720. :  80%|████████  | 40/50 [00:27<00:09,  1.09it/s]Train Iter: 641/1000. LR: 0.0401. Data: 0.43s. Batch: 0.67s. S_Loss: 0.9397. T_Loss: 3.6882. Mask: 0.9720. :  82%|████████▏ | 41/50 [00:27<00:07,  1.19it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9397. T_Loss: 3.6894. Mask: 0.9720. :  82%|████████▏ | 41/50 [00:27<00:07,  1.19it/s]Train Iter: 642/1000. LR: 0.0401. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9397. T_Loss: 3.6894. Mask: 0.9720. :  84%|████████▍ | 42/50 [00:27<00:06,  1.32it/s]total : 1000  current step :  640
total : 1000  current step :  641
total : 1000  current step :  642
Train Iter: 643/1000. LR: 0.0402. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9386. T_Loss: 3.6830. Mask: 0.9718. :  84%|████████▍ | 42/50 [00:28<00:06,  1.32it/s]Train Iter: 643/1000. LR: 0.0402. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9386. T_Loss: 3.6830. Mask: 0.9718. :  86%|████████▌ | 43/50 [00:28<00:05,  1.17it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9401. T_Loss: 3.6917. Mask: 0.9715. :  86%|████████▌ | 43/50 [00:29<00:05,  1.17it/s]Train Iter: 644/1000. LR: 0.0403. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9401. T_Loss: 3.6917. Mask: 0.9715. :  88%|████████▊ | 44/50 [00:29<00:04,  1.39it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9394. T_Loss: 3.6921. Mask: 0.9717. :  88%|████████▊ | 44/50 [00:29<00:04,  1.39it/s]Train Iter: 645/1000. LR: 0.0403. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9394. T_Loss: 3.6921. Mask: 0.9717. :  90%|█████████ | 45/50 [00:29<00:03,  1.64it/s]total : 1000  current step :  643
total : 1000  current step :  644
total : 1000  current step :  645
Train Iter: 646/1000. LR: 0.0404. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9395. T_Loss: 3.6896. Mask: 0.9715. :  90%|█████████ | 45/50 [00:30<00:03,  1.64it/s]Train Iter: 646/1000. LR: 0.0404. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9395. T_Loss: 3.6896. Mask: 0.9715. :  92%|█████████▏| 46/50 [00:30<00:03,  1.29it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9399. T_Loss: 3.6990. Mask: 0.9717. :  92%|█████████▏| 46/50 [00:31<00:03,  1.29it/s]Train Iter: 647/1000. LR: 0.0404. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9399. T_Loss: 3.6990. Mask: 0.9717. :  94%|█████████▍| 47/50 [00:31<00:02,  1.46it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9402. T_Loss: 3.6995. Mask: 0.9711. :  94%|█████████▍| 47/50 [00:31<00:02,  1.46it/s]Train Iter: 648/1000. LR: 0.0405. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9402. T_Loss: 3.6995. Mask: 0.9711. :  96%|█████████▌| 48/50 [00:31<00:01,  1.73it/s]total : 1000  current step :  646
total : 1000  current step :  647
total : 1000  current step :  648
Train Iter: 649/1000. LR: 0.0406. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9397. T_Loss: 3.6995. Mask: 0.9715. :  96%|█████████▌| 48/50 [00:32<00:01,  1.73it/s]Train Iter: 649/1000. LR: 0.0406. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9397. T_Loss: 3.6995. Mask: 0.9715. :  98%|█████████▊| 49/50 [00:32<00:00,  1.30it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9393. T_Loss: 3.6982. Mask: 0.9716. :  98%|█████████▊| 49/50 [00:33<00:00,  1.30it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9393. T_Loss: 3.6982. Mask: 0.9716. : 100%|██████████| 50/50 [00:33<00:00,  1.52it/s]Train Iter: 650/1000. LR: 0.0406. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9393. T_Loss: 3.6982. Mask: 0.9716. : 100%|██████████| 50/50 [00:33<00:00,  1.50it/s]
total : 1000  current step :  649
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 0.8424. top1: 98.05. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.75s. Loss: 0.8424. top1: 98.05. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8376. top1: 97.85. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.34it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8376. top1: 97.85. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8448. top1: 97.53. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8448. top1: 97.53. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8529. top1: 97.36. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.69it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8529. top1: 97.36. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.91it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9155. top1: 93.91. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.91it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.9155. top1: 93.91. top5: 100.00. :  62%|██████▎   | 5/8 [00:01<00:01,  2.83it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9615. top1: 90.89. top5: 99.93. :  62%|██████▎   | 5/8 [00:02<00:01,  2.83it/s] Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9615. top1: 90.89. top5: 99.93. :  75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9988. top1: 88.67. top5: 99.94. :  75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.9988. top1: 88.67. top5: 99.94. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0215. top1: 86.95. top5: 99.90. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0215. top1: 86.95. top5: 99.90. : 100%|██████████| 8/8 [00:02<00:00,  3.45it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 1.0215. top1: 86.95. top5: 99.90. : 100%|██████████| 8/8 [00:03<00:00,  2.65it/s]
total : 1000  current step :  650
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.02s. Batch: 0.21s. S_Loss: 0.8972. T_Loss: 3.5620. Mask: 0.9805. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 651/1000. LR: 0.0407. Data: 0.02s. Batch: 0.21s. S_Loss: 0.8972. T_Loss: 3.5620. Mask: 0.9805. :   2%|▏         | 1/50 [00:00<00:10,  4.77it/s]total : 1000  current step :  651
Train Iter: 652/1000. LR: 0.0408. Data: 0.54s. Batch: 0.73s. S_Loss: 0.9261. T_Loss: 3.6983. Mask: 0.9785. :   2%|▏         | 1/50 [00:01<00:10,  4.77it/s]Train Iter: 652/1000. LR: 0.0408. Data: 0.54s. Batch: 0.73s. S_Loss: 0.9261. T_Loss: 3.6983. Mask: 0.9785. :   4%|▍         | 2/50 [00:01<00:39,  1.22it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9406. T_Loss: 3.7228. Mask: 0.9779. :   4%|▍         | 2/50 [00:01<00:39,  1.22it/s]Train Iter: 653/1000. LR: 0.0408. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9406. T_Loss: 3.7228. Mask: 0.9779. :   6%|▌         | 3/50 [00:01<00:28,  1.63it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.37s. Batch: 0.56s. S_Loss: 0.9342. T_Loss: 3.7151. Mask: 0.9766. :   6%|▌         | 3/50 [00:02<00:28,  1.63it/s]Train Iter: 654/1000. LR: 0.0409. Data: 0.37s. Batch: 0.56s. S_Loss: 0.9342. T_Loss: 3.7151. Mask: 0.9766. :   8%|▊         | 4/50 [00:02<00:25,  1.84it/s]total : 1000  current step :  652
total : 1000  current step :  653
total : 1000  current step :  654
Train Iter: 655/1000. LR: 0.0409. Data: 0.50s. Batch: 0.69s. S_Loss: 0.9262. T_Loss: 3.6417. Mask: 0.9773. :   8%|▊         | 4/50 [00:03<00:25,  1.84it/s]Train Iter: 655/1000. LR: 0.0409. Data: 0.50s. Batch: 0.69s. S_Loss: 0.9262. T_Loss: 3.6417. Mask: 0.9773. :  10%|█         | 5/50 [00:03<00:35,  1.28it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9173. T_Loss: 3.5494. Mask: 0.9766. :  10%|█         | 5/50 [00:03<00:35,  1.28it/s]Train Iter: 656/1000. LR: 0.0410. Data: 0.44s. Batch: 0.63s. S_Loss: 0.9173. T_Loss: 3.5494. Mask: 0.9766. :  12%|█▏        | 6/50 [00:03<00:27,  1.59it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9223. T_Loss: 3.5820. Mask: 0.9754. :  12%|█▏        | 6/50 [00:04<00:27,  1.59it/s]Train Iter: 657/1000. LR: 0.0411. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9223. T_Loss: 3.5820. Mask: 0.9754. :  14%|█▍        | 7/50 [00:04<00:23,  1.80it/s]total : 1000  current step :  655
total : 1000  current step :  656
total : 1000  current step :  657
Train Iter: 658/1000. LR: 0.0411. Data: 0.47s. Batch: 0.66s. S_Loss: 0.9190. T_Loss: 3.5324. Mask: 0.9766. :  14%|█▍        | 7/50 [00:05<00:23,  1.80it/s]Train Iter: 658/1000. LR: 0.0411. Data: 0.47s. Batch: 0.66s. S_Loss: 0.9190. T_Loss: 3.5324. Mask: 0.9766. :  16%|█▌        | 8/50 [00:05<00:30,  1.36it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9181. T_Loss: 3.5098. Mask: 0.9770. :  16%|█▌        | 8/50 [00:05<00:30,  1.36it/s]Train Iter: 659/1000. LR: 0.0412. Data: 0.43s. Batch: 0.62s. S_Loss: 0.9181. T_Loss: 3.5098. Mask: 0.9770. :  18%|█▊        | 9/50 [00:05<00:24,  1.66it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9191. T_Loss: 3.5063. Mask: 0.9766. :  18%|█▊        | 9/50 [00:05<00:24,  1.66it/s]Train Iter: 660/1000. LR: 0.0413. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9191. T_Loss: 3.5063. Mask: 0.9766. :  20%|██        | 10/50 [00:05<00:20,  1.91it/s]total : 1000  current step :  658
total : 1000  current step :  659
total : 1000  current step :  660
Train Iter: 661/1000. LR: 0.0413. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9186. T_Loss: 3.5099. Mask: 0.9766. :  20%|██        | 10/50 [00:07<00:20,  1.91it/s]Train Iter: 661/1000. LR: 0.0413. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9186. T_Loss: 3.5099. Mask: 0.9766. :  22%|██▏       | 11/50 [00:07<00:27,  1.43it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9187. T_Loss: 3.5126. Mask: 0.9775. :  22%|██▏       | 11/50 [00:07<00:27,  1.43it/s]Train Iter: 662/1000. LR: 0.0414. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9187. T_Loss: 3.5126. Mask: 0.9775. :  24%|██▍       | 12/50 [00:07<00:22,  1.66it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9193. T_Loss: 3.5495. Mask: 0.9772. :  24%|██▍       | 12/50 [00:07<00:22,  1.66it/s]Train Iter: 663/1000. LR: 0.0414. Data: 0.41s. Batch: 0.60s. S_Loss: 0.9193. T_Loss: 3.5495. Mask: 0.9772. :  26%|██▌       | 13/50 [00:07<00:19,  1.87it/s]total : 1000  current step :  661
total : 1000  current step :  662
total : 1000  current step :  663
Train Iter: 664/1000. LR: 0.0415. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9215. T_Loss: 3.5839. Mask: 0.9763. :  26%|██▌       | 13/50 [00:08<00:19,  1.87it/s]Train Iter: 664/1000. LR: 0.0415. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9215. T_Loss: 3.5839. Mask: 0.9763. :  28%|██▊       | 14/50 [00:08<00:25,  1.43it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9234. T_Loss: 3.5950. Mask: 0.9758. :  28%|██▊       | 14/50 [00:09<00:25,  1.43it/s]Train Iter: 665/1000. LR: 0.0416. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9234. T_Loss: 3.5950. Mask: 0.9758. :  30%|███       | 15/50 [00:09<00:21,  1.65it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9250. T_Loss: 3.6109. Mask: 0.9749. :  30%|███       | 15/50 [00:09<00:21,  1.65it/s]Train Iter: 666/1000. LR: 0.0416. Data: 0.40s. Batch: 0.60s. S_Loss: 0.9250. T_Loss: 3.6109. Mask: 0.9749. :  32%|███▏      | 16/50 [00:09<00:17,  1.93it/s]total : 1000  current step :  664
total : 1000  current step :  665
total : 1000  current step :  666
Train Iter: 667/1000. LR: 0.0417. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9249. T_Loss: 3.6032. Mask: 0.9733. :  32%|███▏      | 16/50 [00:10<00:17,  1.93it/s]Train Iter: 667/1000. LR: 0.0417. Data: 0.45s. Batch: 0.64s. S_Loss: 0.9249. T_Loss: 3.6032. Mask: 0.9733. :  34%|███▍      | 17/50 [00:10<00:25,  1.30it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9251. T_Loss: 3.6151. Mask: 0.9729. :  34%|███▍      | 17/50 [00:11<00:25,  1.30it/s]Train Iter: 668/1000. LR: 0.0418. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9251. T_Loss: 3.6151. Mask: 0.9729. :  36%|███▌      | 18/50 [00:11<00:20,  1.57it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9232. T_Loss: 3.6015. Mask: 0.9735. :  36%|███▌      | 18/50 [00:11<00:20,  1.57it/s]Train Iter: 669/1000. LR: 0.0418. Data: 0.42s. Batch: 0.61s. S_Loss: 0.9232. T_Loss: 3.6015. Mask: 0.9735. :  38%|███▊      | 19/50 [00:11<00:17,  1.79it/s]total : 1000  current step :  667
total : 1000  current step :  668
total : 1000  current step :  669
Train Iter: 670/1000. LR: 0.0419. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9218. T_Loss: 3.5985. Mask: 0.9736. :  38%|███▊      | 19/50 [00:12<00:17,  1.79it/s]Train Iter: 670/1000. LR: 0.0419. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9218. T_Loss: 3.5985. Mask: 0.9736. :  40%|████      | 20/50 [00:12<00:22,  1.34it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9221. T_Loss: 3.5794. Mask: 0.9734. :  40%|████      | 20/50 [00:13<00:22,  1.34it/s]Train Iter: 671/1000. LR: 0.0419. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9221. T_Loss: 3.5794. Mask: 0.9734. :  42%|████▏     | 21/50 [00:13<00:17,  1.62it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9219. T_Loss: 3.5689. Mask: 0.9730. :  42%|████▏     | 21/50 [00:13<00:17,  1.62it/s]Train Iter: 672/1000. LR: 0.0420. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9219. T_Loss: 3.5689. Mask: 0.9730. :  44%|████▍     | 22/50 [00:13<00:16,  1.75it/s]total : 1000  current step :  670
total : 1000  current step :  671
total : 1000  current step :  672
Train Iter: 673/1000. LR: 0.0421. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9226. T_Loss: 3.5744. Mask: 0.9728. :  44%|████▍     | 22/50 [00:14<00:16,  1.75it/s]Train Iter: 673/1000. LR: 0.0421. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9226. T_Loss: 3.5744. Mask: 0.9728. :  46%|████▌     | 23/50 [00:14<00:20,  1.34it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9230. T_Loss: 3.5881. Mask: 0.9731. :  46%|████▌     | 23/50 [00:15<00:20,  1.34it/s]Train Iter: 674/1000. LR: 0.0421. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9230. T_Loss: 3.5881. Mask: 0.9731. :  48%|████▊     | 24/50 [00:15<00:16,  1.62it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9246. T_Loss: 3.5827. Mask: 0.9720. :  48%|████▊     | 24/50 [00:15<00:16,  1.62it/s]Train Iter: 675/1000. LR: 0.0422. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9246. T_Loss: 3.5827. Mask: 0.9720. :  50%|█████     | 25/50 [00:15<00:13,  1.81it/s]total : 1000  current step :  673
total : 1000  current step :  674
total : 1000  current step :  675
Train Iter: 676/1000. LR: 0.0423. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9232. T_Loss: 3.5949. Mask: 0.9728. :  50%|█████     | 25/50 [00:16<00:13,  1.81it/s]Train Iter: 676/1000. LR: 0.0423. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9232. T_Loss: 3.5949. Mask: 0.9728. :  52%|█████▏    | 26/50 [00:16<00:17,  1.41it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9238. T_Loss: 3.5953. Mask: 0.9727. :  52%|█████▏    | 26/50 [00:17<00:17,  1.41it/s]Train Iter: 677/1000. LR: 0.0423. Data: 0.43s. Batch: 0.63s. S_Loss: 0.9238. T_Loss: 3.5953. Mask: 0.9727. :  54%|█████▍    | 27/50 [00:17<00:14,  1.57it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9249. T_Loss: 3.5984. Mask: 0.9729. :  54%|█████▍    | 27/50 [00:17<00:14,  1.57it/s]Train Iter: 678/1000. LR: 0.0424. Data: 0.42s. Batch: 0.62s. S_Loss: 0.9249. T_Loss: 3.5984. Mask: 0.9729. :  56%|█████▌    | 28/50 [00:17<00:12,  1.83it/s]total : 1000  current step :  676
total : 1000  current step :  677
total : 1000  current step :  678
Train Iter: 679/1000. LR: 0.0424. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9257. T_Loss: 3.6075. Mask: 0.9731. :  56%|█████▌    | 28/50 [00:18<00:12,  1.83it/s]Train Iter: 679/1000. LR: 0.0424. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9257. T_Loss: 3.6075. Mask: 0.9731. :  58%|█████▊    | 29/50 [00:18<00:15,  1.39it/s]total : 1000  current step :  679
Train Iter: 680/1000. LR: 0.0425. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9255. T_Loss: 3.6068. Mask: 0.9729. :  58%|█████▊    | 29/50 [00:19<00:15,  1.39it/s]Train Iter: 680/1000. LR: 0.0425. Data: 0.44s. Batch: 0.64s. S_Loss: 0.9255. T_Loss: 3.6068. Mask: 0.9729. :  60%|██████    | 30/50 [00:19<00:15,  1.32it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9255. T_Loss: 3.6044. Mask: 0.9727. :  60%|██████    | 30/50 [00:20<00:15,  1.32it/s]Train Iter: 681/1000. LR: 0.0426. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9255. T_Loss: 3.6044. Mask: 0.9727. :  62%|██████▏   | 31/50 [00:20<00:15,  1.26it/s]total : 1000  current step :  680
total : 1000  current step :  681
Train Iter: 682/1000. LR: 0.0426. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9263. T_Loss: 3.6210. Mask: 0.9730. :  62%|██████▏   | 31/50 [00:21<00:15,  1.26it/s]Train Iter: 682/1000. LR: 0.0426. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9263. T_Loss: 3.6210. Mask: 0.9730. :  64%|██████▍   | 32/50 [00:21<00:16,  1.10it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9269. T_Loss: 3.6248. Mask: 0.9732. :  64%|██████▍   | 32/50 [00:21<00:16,  1.10it/s]Train Iter: 683/1000. LR: 0.0427. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9269. T_Loss: 3.6248. Mask: 0.9732. :  66%|██████▌   | 33/50 [00:21<00:12,  1.34it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9286. T_Loss: 3.6513. Mask: 0.9729. :  66%|██████▌   | 33/50 [00:22<00:12,  1.34it/s]Train Iter: 684/1000. LR: 0.0428. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9286. T_Loss: 3.6513. Mask: 0.9729. :  68%|██████▊   | 34/50 [00:22<00:09,  1.64it/s]total : 1000  current step :  682
total : 1000  current step :  683
total : 1000  current step :  684
Train Iter: 685/1000. LR: 0.0428. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9288. T_Loss: 3.6500. Mask: 0.9727. :  68%|██████▊   | 34/50 [00:23<00:09,  1.64it/s]Train Iter: 685/1000. LR: 0.0428. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9288. T_Loss: 3.6500. Mask: 0.9727. :  70%|███████   | 35/50 [00:23<00:12,  1.18it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9291. T_Loss: 3.6551. Mask: 0.9724. :  70%|███████   | 35/50 [00:23<00:12,  1.18it/s]Train Iter: 686/1000. LR: 0.0429. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9291. T_Loss: 3.6551. Mask: 0.9724. :  72%|███████▏  | 36/50 [00:23<00:09,  1.44it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9285. T_Loss: 3.6600. Mask: 0.9726. :  72%|███████▏  | 36/50 [00:24<00:09,  1.44it/s]Train Iter: 687/1000. LR: 0.0429. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9285. T_Loss: 3.6600. Mask: 0.9726. :  74%|███████▍  | 37/50 [00:24<00:07,  1.71it/s]total : 1000  current step :  685
total : 1000  current step :  686
total : 1000  current step :  687
Train Iter: 688/1000. LR: 0.0430. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9283. T_Loss: 3.6618. Mask: 0.9728. :  74%|███████▍  | 37/50 [00:25<00:07,  1.71it/s]Train Iter: 688/1000. LR: 0.0430. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9283. T_Loss: 3.6618. Mask: 0.9728. :  76%|███████▌  | 38/50 [00:25<00:09,  1.25it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9292. T_Loss: 3.6708. Mask: 0.9728. :  76%|███████▌  | 38/50 [00:25<00:09,  1.25it/s]Train Iter: 689/1000. LR: 0.0431. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9292. T_Loss: 3.6708. Mask: 0.9728. :  78%|███████▊  | 39/50 [00:25<00:07,  1.50it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9290. T_Loss: 3.6790. Mask: 0.9730. :  78%|███████▊  | 39/50 [00:26<00:07,  1.50it/s]Train Iter: 690/1000. LR: 0.0431. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9290. T_Loss: 3.6790. Mask: 0.9730. :  80%|████████  | 40/50 [00:26<00:05,  1.71it/s]total : 1000  current step :  688
total : 1000  current step :  689
total : 1000  current step :  690
Train Iter: 691/1000. LR: 0.0432. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9285. T_Loss: 3.6728. Mask: 0.9727. :  80%|████████  | 40/50 [00:27<00:05,  1.71it/s]Train Iter: 691/1000. LR: 0.0432. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9285. T_Loss: 3.6728. Mask: 0.9727. :  82%|████████▏ | 41/50 [00:27<00:07,  1.24it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9292. T_Loss: 3.6773. Mask: 0.9727. :  82%|████████▏ | 41/50 [00:27<00:07,  1.24it/s]Train Iter: 692/1000. LR: 0.0433. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9292. T_Loss: 3.6773. Mask: 0.9727. :  84%|████████▍ | 42/50 [00:27<00:05,  1.56it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9291. T_Loss: 3.6728. Mask: 0.9724. :  84%|████████▍ | 42/50 [00:28<00:05,  1.56it/s]Train Iter: 693/1000. LR: 0.0433. Data: 0.44s. Batch: 0.65s. S_Loss: 0.9291. T_Loss: 3.6728. Mask: 0.9724. :  86%|████████▌ | 43/50 [00:28<00:03,  1.88it/s]total : 1000  current step :  691
total : 1000  current step :  692
total : 1000  current step :  693
Train Iter: 694/1000. LR: 0.0434. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9278. T_Loss: 3.6608. Mask: 0.9726. :  86%|████████▌ | 43/50 [00:29<00:03,  1.88it/s]Train Iter: 694/1000. LR: 0.0434. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9278. T_Loss: 3.6608. Mask: 0.9726. :  88%|████████▊ | 44/50 [00:29<00:04,  1.30it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9278. T_Loss: 3.6644. Mask: 0.9729. :  88%|████████▊ | 44/50 [00:29<00:04,  1.30it/s]Train Iter: 695/1000. LR: 0.0434. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9278. T_Loss: 3.6644. Mask: 0.9729. :  90%|█████████ | 45/50 [00:29<00:03,  1.53it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9283. T_Loss: 3.6537. Mask: 0.9730. :  90%|█████████ | 45/50 [00:30<00:03,  1.53it/s]Train Iter: 696/1000. LR: 0.0435. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9283. T_Loss: 3.6537. Mask: 0.9730. :  92%|█████████▏| 46/50 [00:30<00:02,  1.78it/s]total : 1000  current step :  694
total : 1000  current step :  695
total : 1000  current step :  696
Train Iter: 697/1000. LR: 0.0436. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9285. T_Loss: 3.6432. Mask: 0.9729. :  92%|█████████▏| 46/50 [00:31<00:02,  1.78it/s]Train Iter: 697/1000. LR: 0.0436. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9285. T_Loss: 3.6432. Mask: 0.9729. :  94%|█████████▍| 47/50 [00:31<00:02,  1.30it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9287. T_Loss: 3.6371. Mask: 0.9728. :  94%|█████████▍| 47/50 [00:31<00:02,  1.30it/s]Train Iter: 698/1000. LR: 0.0436. Data: 0.45s. Batch: 0.66s. S_Loss: 0.9287. T_Loss: 3.6371. Mask: 0.9728. :  96%|█████████▌| 48/50 [00:31<00:01,  1.50it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9286. T_Loss: 3.6327. Mask: 0.9728. :  96%|█████████▌| 48/50 [00:32<00:01,  1.50it/s]Train Iter: 699/1000. LR: 0.0437. Data: 0.45s. Batch: 0.65s. S_Loss: 0.9286. T_Loss: 3.6327. Mask: 0.9728. :  98%|█████████▊| 49/50 [00:32<00:00,  1.78it/s]total : 1000  current step :  697
total : 1000  current step :  698
total : 1000  current step :  699
Train Iter: 700/1000. LR: 0.0438. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9284. T_Loss: 3.6248. Mask: 0.9732. :  98%|█████████▊| 49/50 [00:33<00:00,  1.78it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9284. T_Loss: 3.6248. Mask: 0.9732. : 100%|██████████| 50/50 [00:33<00:00,  1.28it/s]Train Iter: 700/1000. LR: 0.0438. Data: 0.46s. Batch: 0.67s. S_Loss: 0.9284. T_Loss: 3.6248. Mask: 0.9732. : 100%|██████████| 50/50 [00:33<00:00,  1.50it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.94s. Loss: 0.8302. top1: 98.44. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.94s. Loss: 0.8302. top1: 98.44. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:06,  1.07it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8253. top1: 98.05. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:06,  1.07it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8253. top1: 98.05. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.58it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8315. top1: 98.05. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.58it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8315. top1: 98.05. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.15it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8392. top1: 97.85. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.15it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8392. top1: 97.85. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.9017. top1: 94.06. top5: 100.00. :  50%|█████     | 4/8 [00:02<00:01,  2.39it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.9017. top1: 94.06. top5: 100.00. :  62%|██████▎   | 5/8 [00:02<00:01,  2.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.9482. top1: 91.08. top5: 99.93. :  62%|██████▎   | 5/8 [00:02<00:01,  2.38it/s] Test Iter:   6/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.9482. top1: 91.08. top5: 99.93. :  75%|███████▌  | 6/8 [00:02<00:00,  2.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.9856. top1: 89.12. top5: 99.94. :  75%|███████▌  | 6/8 [00:03<00:00,  2.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.9856. top1: 89.12. top5: 99.94. :  88%|████████▊ | 7/8 [00:03<00:00,  2.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0091. top1: 87.55. top5: 99.85. :  88%|████████▊ | 7/8 [00:03<00:00,  2.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0091. top1: 87.55. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.41s. Loss: 1.0091. top1: 87.55. top5: 99.85. : 100%|██████████| 8/8 [00:03<00:00,  2.28it/s]
total : 1000  current step :  700
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.21s. S_Loss: 0.9468. T_Loss: 3.3969. Mask: 0.9766. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 701/1000. LR: 0.0438. Data: 0.01s. Batch: 0.21s. S_Loss: 0.9468. T_Loss: 3.3969. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:10,  4.76it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.21s. S_Loss: 0.9273. T_Loss: 3.2850. Mask: 0.9844. :   2%|▏         | 1/50 [00:00<00:10,  4.76it/s]Train Iter: 702/1000. LR: 0.0439. Data: 0.01s. Batch: 0.21s. S_Loss: 0.9273. T_Loss: 3.2850. Mask: 0.9844. :   4%|▍         | 2/50 [00:00<00:10,  4.61it/s]total : 1000  current step :  701
total : 1000  current step :  702
Train Iter: 703/1000. LR: 0.0439. Data: 0.38s. Batch: 0.60s. S_Loss: 0.9407. T_Loss: 3.2925. Mask: 0.9818. :   4%|▍         | 2/50 [00:01<00:10,  4.61it/s]Train Iter: 703/1000. LR: 0.0439. Data: 0.38s. Batch: 0.60s. S_Loss: 0.9407. T_Loss: 3.2925. Mask: 0.9818. :   6%|▌         | 3/50 [00:01<00:34,  1.34it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9310. T_Loss: 3.2360. Mask: 0.9785. :   6%|▌         | 3/50 [00:02<00:34,  1.34it/s]Train Iter: 704/1000. LR: 0.0440. Data: 0.31s. Batch: 0.54s. S_Loss: 0.9310. T_Loss: 3.2360. Mask: 0.9785. :   8%|▊         | 4/50 [00:02<00:27,  1.67it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9324. T_Loss: 3.2328. Mask: 0.9773. :   8%|▊         | 4/50 [00:02<00:27,  1.67it/s]Train Iter: 705/1000. LR: 0.0441. Data: 0.28s. Batch: 0.53s. S_Loss: 0.9324. T_Loss: 3.2328. Mask: 0.9773. :  10%|█         | 5/50 [00:02<00:24,  1.80it/s]total : 1000  current step :  703
total : 1000  current step :  704
total : 1000  current step :  705
Train Iter: 706/1000. LR: 0.0441. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9266. T_Loss: 3.2791. Mask: 0.9759. :  10%|█         | 5/50 [00:03<00:24,  1.80it/s]Train Iter: 706/1000. LR: 0.0441. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9266. T_Loss: 3.2791. Mask: 0.9759. :  12%|█▏        | 6/50 [00:03<00:34,  1.27it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9311. T_Loss: 3.2587. Mask: 0.9766. :  12%|█▏        | 6/50 [00:04<00:34,  1.27it/s]Train Iter: 707/1000. LR: 0.0442. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9311. T_Loss: 3.2587. Mask: 0.9766. :  14%|█▍        | 7/50 [00:04<00:27,  1.57it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9298. T_Loss: 3.2985. Mask: 0.9756. :  14%|█▍        | 7/50 [00:04<00:27,  1.57it/s]Train Iter: 708/1000. LR: 0.0443. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9298. T_Loss: 3.2985. Mask: 0.9756. :  16%|█▌        | 8/50 [00:04<00:24,  1.69it/s]total : 1000  current step :  706
total : 1000  current step :  707
total : 1000  current step :  708
Train Iter: 709/1000. LR: 0.0443. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9280. T_Loss: 3.3123. Mask: 0.9757. :  16%|█▌        | 8/50 [00:06<00:24,  1.69it/s]Train Iter: 709/1000. LR: 0.0443. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9280. T_Loss: 3.3123. Mask: 0.9757. :  18%|█▊        | 9/50 [00:06<00:35,  1.17it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9315. T_Loss: 3.3273. Mask: 0.9770. :  18%|█▊        | 9/50 [00:06<00:35,  1.17it/s]Train Iter: 710/1000. LR: 0.0444. Data: 0.40s. Batch: 0.64s. S_Loss: 0.9315. T_Loss: 3.3273. Mask: 0.9770. :  20%|██        | 10/50 [00:06<00:27,  1.46it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9292. T_Loss: 3.3915. Mask: 0.9762. :  20%|██        | 10/50 [00:06<00:27,  1.46it/s]Train Iter: 711/1000. LR: 0.0444. Data: 0.40s. Batch: 0.63s. S_Loss: 0.9292. T_Loss: 3.3915. Mask: 0.9762. :  22%|██▏       | 11/50 [00:06<00:24,  1.57it/s]total : 1000  current step :  709
total : 1000  current step :  710
total : 1000  current step :  711
Train Iter: 712/1000. LR: 0.0445. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9304. T_Loss: 3.4141. Mask: 0.9775. :  22%|██▏       | 11/50 [00:08<00:24,  1.57it/s]Train Iter: 712/1000. LR: 0.0445. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9304. T_Loss: 3.4141. Mask: 0.9775. :  24%|██▍       | 12/50 [00:08<00:30,  1.24it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9318. T_Loss: 3.4209. Mask: 0.9772. :  24%|██▍       | 12/50 [00:08<00:30,  1.24it/s]Train Iter: 713/1000. LR: 0.0446. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9318. T_Loss: 3.4209. Mask: 0.9772. :  26%|██▌       | 13/50 [00:08<00:25,  1.43it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9291. T_Loss: 3.4343. Mask: 0.9760. :  26%|██▌       | 13/50 [00:09<00:25,  1.43it/s]Train Iter: 714/1000. LR: 0.0446. Data: 0.43s. Batch: 0.66s. S_Loss: 0.9291. T_Loss: 3.4343. Mask: 0.9760. :  28%|██▊       | 14/50 [00:09<00:25,  1.43it/s]total : 1000  current step :  712
total : 1000  current step :  713
total : 1000  current step :  714
Train Iter: 715/1000. LR: 0.0447. Data: 0.47s. Batch: 0.71s. S_Loss: 0.9294. T_Loss: 3.4360. Mask: 0.9755. :  28%|██▊       | 14/50 [00:10<00:25,  1.43it/s]Train Iter: 715/1000. LR: 0.0447. Data: 0.47s. Batch: 0.71s. S_Loss: 0.9294. T_Loss: 3.4360. Mask: 0.9755. :  30%|███       | 15/50 [00:10<00:31,  1.12it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9289. T_Loss: 3.4620. Mask: 0.9744. :  30%|███       | 15/50 [00:10<00:31,  1.12it/s]Train Iter: 716/1000. LR: 0.0448. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9289. T_Loss: 3.4620. Mask: 0.9744. :  32%|███▏      | 16/50 [00:10<00:24,  1.39it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9261. T_Loss: 3.4639. Mask: 0.9740. :  32%|███▏      | 16/50 [00:11<00:24,  1.39it/s]Train Iter: 717/1000. LR: 0.0448. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9261. T_Loss: 3.4639. Mask: 0.9740. :  34%|███▍      | 17/50 [00:11<00:20,  1.62it/s]total : 1000  current step :  715
total : 1000  current step :  716
total : 1000  current step :  717
Train Iter: 718/1000. LR: 0.0449. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9227. T_Loss: 3.4496. Mask: 0.9735. :  34%|███▍      | 17/50 [00:12<00:20,  1.62it/s]Train Iter: 718/1000. LR: 0.0449. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9227. T_Loss: 3.4496. Mask: 0.9735. :  36%|███▌      | 18/50 [00:12<00:26,  1.21it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9202. T_Loss: 3.4520. Mask: 0.9743. :  36%|███▌      | 18/50 [00:12<00:26,  1.21it/s]Train Iter: 719/1000. LR: 0.0449. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9202. T_Loss: 3.4520. Mask: 0.9743. :  38%|███▊      | 19/50 [00:12<00:20,  1.51it/s]total : 1000  current step :  718
total : 1000  current step :  719
Train Iter: 720/1000. LR: 0.0450. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9181. T_Loss: 3.4420. Mask: 0.9748. :  38%|███▊      | 19/50 [00:13<00:20,  1.51it/s]Train Iter: 720/1000. LR: 0.0450. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9181. T_Loss: 3.4420. Mask: 0.9748. :  40%|████      | 20/50 [00:13<00:19,  1.51it/s]total : 1000  current step :  720
Train Iter: 721/1000. LR: 0.0451. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9190. T_Loss: 3.4571. Mask: 0.9751. :  40%|████      | 20/50 [00:14<00:19,  1.51it/s]Train Iter: 721/1000. LR: 0.0451. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9190. T_Loss: 3.4571. Mask: 0.9751. :  42%|████▏     | 21/50 [00:14<00:24,  1.21it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9204. T_Loss: 3.4696. Mask: 0.9748. :  42%|████▏     | 21/50 [00:15<00:24,  1.21it/s]Train Iter: 722/1000. LR: 0.0451. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9204. T_Loss: 3.4696. Mask: 0.9748. :  44%|████▍     | 22/50 [00:15<00:19,  1.47it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9210. T_Loss: 3.4918. Mask: 0.9752. :  44%|████▍     | 22/50 [00:15<00:19,  1.47it/s]Train Iter: 723/1000. LR: 0.0452. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9210. T_Loss: 3.4918. Mask: 0.9752. :  46%|████▌     | 23/50 [00:15<00:15,  1.76it/s]total : 1000  current step :  721
total : 1000  current step :  722
total : 1000  current step :  723
Train Iter: 724/1000. LR: 0.0453. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9200. T_Loss: 3.4879. Mask: 0.9753. :  46%|████▌     | 23/50 [00:16<00:15,  1.76it/s]Train Iter: 724/1000. LR: 0.0453. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9200. T_Loss: 3.4879. Mask: 0.9753. :  48%|████▊     | 24/50 [00:16<00:20,  1.25it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9208. T_Loss: 3.4845. Mask: 0.9755. :  48%|████▊     | 24/50 [00:17<00:20,  1.25it/s]Train Iter: 725/1000. LR: 0.0453. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9208. T_Loss: 3.4845. Mask: 0.9755. :  50%|█████     | 25/50 [00:17<00:16,  1.49it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9204. T_Loss: 3.4831. Mask: 0.9761. :  50%|█████     | 25/50 [00:17<00:16,  1.49it/s]Train Iter: 726/1000. LR: 0.0454. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9204. T_Loss: 3.4831. Mask: 0.9761. :  52%|█████▏    | 26/50 [00:17<00:13,  1.80it/s]total : 1000  current step :  724
total : 1000  current step :  725
total : 1000  current step :  726
Train Iter: 727/1000. LR: 0.0454. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9203. T_Loss: 3.4867. Mask: 0.9763. :  52%|█████▏    | 26/50 [00:18<00:13,  1.80it/s]Train Iter: 727/1000. LR: 0.0454. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9203. T_Loss: 3.4867. Mask: 0.9763. :  54%|█████▍    | 27/50 [00:18<00:17,  1.31it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9214. T_Loss: 3.4999. Mask: 0.9757. :  54%|█████▍    | 27/50 [00:19<00:17,  1.31it/s]Train Iter: 728/1000. LR: 0.0455. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9214. T_Loss: 3.4999. Mask: 0.9757. :  56%|█████▌    | 28/50 [00:19<00:14,  1.53it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9221. T_Loss: 3.5025. Mask: 0.9760. :  56%|█████▌    | 28/50 [00:19<00:14,  1.53it/s]Train Iter: 729/1000. LR: 0.0456. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9221. T_Loss: 3.5025. Mask: 0.9760. :  58%|█████▊    | 29/50 [00:19<00:11,  1.82it/s]total : 1000  current step :  727
total : 1000  current step :  728
total : 1000  current step :  729
Train Iter: 730/1000. LR: 0.0456. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9214. T_Loss: 3.5079. Mask: 0.9766. :  58%|█████▊    | 29/50 [00:20<00:11,  1.82it/s]Train Iter: 730/1000. LR: 0.0456. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9214. T_Loss: 3.5079. Mask: 0.9766. :  60%|██████    | 30/50 [00:20<00:15,  1.26it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9229. T_Loss: 3.5204. Mask: 0.9766. :  60%|██████    | 30/50 [00:21<00:15,  1.26it/s]Train Iter: 731/1000. LR: 0.0457. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9229. T_Loss: 3.5204. Mask: 0.9766. :  62%|██████▏   | 31/50 [00:21<00:12,  1.49it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9229. T_Loss: 3.5157. Mask: 0.9760. :  62%|██████▏   | 31/50 [00:21<00:12,  1.49it/s]Train Iter: 732/1000. LR: 0.0458. Data: 0.44s. Batch: 0.67s. S_Loss: 0.9229. T_Loss: 3.5157. Mask: 0.9760. :  64%|██████▍   | 32/50 [00:21<00:10,  1.71it/s]total : 1000  current step :  730
total : 1000  current step :  731
total : 1000  current step :  732
Train Iter: 733/1000. LR: 0.0458. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9220. T_Loss: 3.5044. Mask: 0.9759. :  64%|██████▍   | 32/50 [00:22<00:10,  1.71it/s]Train Iter: 733/1000. LR: 0.0458. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9220. T_Loss: 3.5044. Mask: 0.9759. :  66%|██████▌   | 33/50 [00:22<00:13,  1.22it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9218. T_Loss: 3.5005. Mask: 0.9759. :  66%|██████▌   | 33/50 [00:23<00:13,  1.22it/s]Train Iter: 734/1000. LR: 0.0459. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9218. T_Loss: 3.5005. Mask: 0.9759. :  68%|██████▊   | 34/50 [00:23<00:10,  1.48it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9213. T_Loss: 3.5039. Mask: 0.9760. :  68%|██████▊   | 34/50 [00:23<00:10,  1.48it/s]Train Iter: 735/1000. LR: 0.0459. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9213. T_Loss: 3.5039. Mask: 0.9760. :  70%|███████   | 35/50 [00:23<00:10,  1.48it/s]total : 1000  current step :  733
total : 1000  current step :  734
total : 1000  current step :  735
Train Iter: 736/1000. LR: 0.0460. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9213. T_Loss: 3.5038. Mask: 0.9759. :  70%|███████   | 35/50 [00:25<00:10,  1.48it/s]Train Iter: 736/1000. LR: 0.0460. Data: 0.47s. Batch: 0.70s. S_Loss: 0.9213. T_Loss: 3.5038. Mask: 0.9759. :  72%|███████▏  | 36/50 [00:25<00:11,  1.20it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9213. T_Loss: 3.5043. Mask: 0.9757. :  72%|███████▏  | 36/50 [00:25<00:11,  1.20it/s]Train Iter: 737/1000. LR: 0.0461. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9213. T_Loss: 3.5043. Mask: 0.9757. :  74%|███████▍  | 37/50 [00:25<00:09,  1.39it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9217. T_Loss: 3.5126. Mask: 0.9756. :  74%|███████▍  | 37/50 [00:25<00:09,  1.39it/s]Train Iter: 738/1000. LR: 0.0461. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9217. T_Loss: 3.5126. Mask: 0.9756. :  76%|███████▌  | 38/50 [00:25<00:06,  1.73it/s]total : 1000  current step :  736
total : 1000  current step :  737
total : 1000  current step :  738
Train Iter: 739/1000. LR: 0.0462. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9198. T_Loss: 3.5025. Mask: 0.9759. :  76%|███████▌  | 38/50 [00:26<00:06,  1.73it/s]Train Iter: 739/1000. LR: 0.0462. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9198. T_Loss: 3.5025. Mask: 0.9759. :  78%|███████▊  | 39/50 [00:26<00:08,  1.36it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9195. T_Loss: 3.4954. Mask: 0.9756. :  78%|███████▊  | 39/50 [00:27<00:08,  1.36it/s]Train Iter: 740/1000. LR: 0.0463. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9195. T_Loss: 3.4954. Mask: 0.9756. :  80%|████████  | 40/50 [00:27<00:06,  1.58it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9198. T_Loss: 3.4916. Mask: 0.9759. :  80%|████████  | 40/50 [00:27<00:06,  1.58it/s]Train Iter: 741/1000. LR: 0.0463. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9198. T_Loss: 3.4916. Mask: 0.9759. :  82%|████████▏ | 41/50 [00:27<00:04,  1.81it/s]total : 1000  current step :  739
total : 1000  current step :  740
total : 1000  current step :  741
Train Iter: 742/1000. LR: 0.0464. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9194. T_Loss: 3.4860. Mask: 0.9759. :  82%|████████▏ | 41/50 [00:29<00:04,  1.81it/s]Train Iter: 742/1000. LR: 0.0464. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9194. T_Loss: 3.4860. Mask: 0.9759. :  84%|████████▍ | 42/50 [00:29<00:06,  1.22it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9187. T_Loss: 3.4740. Mask: 0.9759. :  84%|████████▍ | 42/50 [00:29<00:06,  1.22it/s]Train Iter: 743/1000. LR: 0.0464. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9187. T_Loss: 3.4740. Mask: 0.9759. :  86%|████████▌ | 43/50 [00:29<00:04,  1.47it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9186. T_Loss: 3.4723. Mask: 0.9757. :  86%|████████▌ | 43/50 [00:29<00:04,  1.47it/s]Train Iter: 744/1000. LR: 0.0465. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9186. T_Loss: 3.4723. Mask: 0.9757. :  88%|████████▊ | 44/50 [00:29<00:03,  1.71it/s]total : 1000  current step :  742
total : 1000  current step :  743
total : 1000  current step :  744
Train Iter: 745/1000. LR: 0.0466. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9180. T_Loss: 3.4720. Mask: 0.9760. :  88%|████████▊ | 44/50 [00:31<00:03,  1.71it/s]Train Iter: 745/1000. LR: 0.0466. Data: 0.47s. Batch: 0.69s. S_Loss: 0.9180. T_Loss: 3.4720. Mask: 0.9760. :  90%|█████████ | 45/50 [00:31<00:04,  1.23it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9185. T_Loss: 3.4741. Mask: 0.9759. :  90%|█████████ | 45/50 [00:31<00:04,  1.23it/s]Train Iter: 746/1000. LR: 0.0466. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9185. T_Loss: 3.4741. Mask: 0.9759. :  92%|█████████▏| 46/50 [00:31<00:02,  1.47it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9185. T_Loss: 3.4740. Mask: 0.9756. :  92%|█████████▏| 46/50 [00:31<00:02,  1.47it/s]Train Iter: 747/1000. LR: 0.0467. Data: 0.45s. Batch: 0.68s. S_Loss: 0.9185. T_Loss: 3.4740. Mask: 0.9756. :  94%|█████████▍| 47/50 [00:31<00:01,  1.79it/s]total : 1000  current step :  745
total : 1000  current step :  746
total : 1000  current step :  747
Train Iter: 748/1000. LR: 0.0468. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9189. T_Loss: 3.4662. Mask: 0.9755. :  94%|█████████▍| 47/50 [00:33<00:01,  1.79it/s]Train Iter: 748/1000. LR: 0.0468. Data: 0.46s. Batch: 0.69s. S_Loss: 0.9189. T_Loss: 3.4662. Mask: 0.9755. :  96%|█████████▌| 48/50 [00:33<00:01,  1.29it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9184. T_Loss: 3.4618. Mask: 0.9754. :  96%|█████████▌| 48/50 [00:33<00:01,  1.29it/s]Train Iter: 749/1000. LR: 0.0468. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9184. T_Loss: 3.4618. Mask: 0.9754. :  98%|█████████▊| 49/50 [00:33<00:00,  1.54it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9178. T_Loss: 3.4568. Mask: 0.9758. :  98%|█████████▊| 49/50 [00:33<00:00,  1.54it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9178. T_Loss: 3.4568. Mask: 0.9758. : 100%|██████████| 50/50 [00:33<00:00,  1.82it/s]Train Iter: 750/1000. LR: 0.0469. Data: 0.45s. Batch: 0.67s. S_Loss: 0.9178. T_Loss: 3.4568. Mask: 0.9758. : 100%|██████████| 50/50 [00:33<00:00,  1.48it/s]
total : 1000  current step :  748
total : 1000  current step :  749
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8240. top1: 98.83. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8240. top1: 98.83. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.28it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8193. top1: 98.24. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.28it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8193. top1: 98.24. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8250. top1: 98.18. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8250. top1: 98.18. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8317. top1: 97.95. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8317. top1: 97.95. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8905. top1: 94.22. top5: 99.92. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8905. top1: 94.22. top5: 99.92. :  62%|██████▎   | 5/8 [00:01<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9351. top1: 91.21. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9351. top1: 91.21. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9708. top1: 89.45. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9708. top1: 89.45. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.47it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9936. top1: 87.95. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.47it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9936. top1: 87.95. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.61it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9936. top1: 87.95. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
total : 1000  current step :  750
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 0.93s. Batch: 1.10s. S_Loss: 0.9382. T_Loss: 3.7891. Mask: 0.9844. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 751/1000. LR: 0.0469. Data: 0.93s. Batch: 1.10s. S_Loss: 0.9382. T_Loss: 3.7891. Mask: 0.9844. :   2%|▏         | 1/50 [00:01<00:53,  1.10s/it]Train Iter: 752/1000. LR: 0.0470. Data: 0.55s. Batch: 0.75s. S_Loss: 0.9312. T_Loss: 3.6844. Mask: 0.9805. :   2%|▏         | 1/50 [00:01<00:53,  1.10s/it]Train Iter: 752/1000. LR: 0.0470. Data: 0.55s. Batch: 0.75s. S_Loss: 0.9312. T_Loss: 3.6844. Mask: 0.9805. :   4%|▍         | 2/50 [00:01<00:33,  1.44it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9330. T_Loss: 3.5411. Mask: 0.9766. :   4%|▍         | 2/50 [00:01<00:33,  1.44it/s]Train Iter: 753/1000. LR: 0.0471. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9330. T_Loss: 3.5411. Mask: 0.9766. :   6%|▌         | 3/50 [00:01<00:27,  1.68it/s]total : 1000  current step :  751
total : 1000  current step :  752
total : 1000  current step :  753
Train Iter: 754/1000. LR: 0.0471. Data: 0.54s. Batch: 0.77s. S_Loss: 0.9250. T_Loss: 3.4762. Mask: 0.9766. :   6%|▌         | 3/50 [00:03<00:27,  1.68it/s]Train Iter: 754/1000. LR: 0.0471. Data: 0.54s. Batch: 0.77s. S_Loss: 0.9250. T_Loss: 3.4762. Mask: 0.9766. :   8%|▊         | 4/50 [00:03<00:36,  1.26it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9197. T_Loss: 3.4416. Mask: 0.9758. :   8%|▊         | 4/50 [00:03<00:36,  1.26it/s]Train Iter: 755/1000. LR: 0.0472. Data: 0.46s. Batch: 0.68s. S_Loss: 0.9197. T_Loss: 3.4416. Mask: 0.9758. :  10%|█         | 5/50 [00:03<00:28,  1.59it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9269. T_Loss: 3.4990. Mask: 0.9759. :  10%|█         | 5/50 [00:03<00:28,  1.59it/s]Train Iter: 756/1000. LR: 0.0473. Data: 0.41s. Batch: 0.65s. S_Loss: 0.9269. T_Loss: 3.4990. Mask: 0.9759. :  12%|█▏        | 6/50 [00:03<00:25,  1.71it/s]total : 1000  current step :  754
total : 1000  current step :  755
total : 1000  current step :  756
Train Iter: 757/1000. LR: 0.0473. Data: 0.50s. Batch: 0.73s. S_Loss: 0.9226. T_Loss: 3.5187. Mask: 0.9754. :  12%|█▏        | 6/50 [00:05<00:25,  1.71it/s]Train Iter: 757/1000. LR: 0.0473. Data: 0.50s. Batch: 0.73s. S_Loss: 0.9226. T_Loss: 3.5187. Mask: 0.9754. :  14%|█▍        | 7/50 [00:05<00:33,  1.27it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9215. T_Loss: 3.5221. Mask: 0.9756. :  14%|█▍        | 7/50 [00:05<00:33,  1.27it/s]Train Iter: 758/1000. LR: 0.0474. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9215. T_Loss: 3.5221. Mask: 0.9756. :  16%|█▌        | 8/50 [00:05<00:28,  1.50it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9192. T_Loss: 3.5032. Mask: 0.9753. :  16%|█▌        | 8/50 [00:06<00:28,  1.50it/s]Train Iter: 759/1000. LR: 0.0474. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9192. T_Loss: 3.5032. Mask: 0.9753. :  18%|█▊        | 9/50 [00:06<00:25,  1.64it/s]total : 1000  current step :  757
total : 1000  current step :  758
total : 1000  current step :  759
Train Iter: 760/1000. LR: 0.0475. Data: 0.51s. Batch: 0.75s. S_Loss: 0.9172. T_Loss: 3.4803. Mask: 0.9742. :  18%|█▊        | 9/50 [00:07<00:25,  1.64it/s]Train Iter: 760/1000. LR: 0.0475. Data: 0.51s. Batch: 0.75s. S_Loss: 0.9172. T_Loss: 3.4803. Mask: 0.9742. :  20%|██        | 10/50 [00:07<00:35,  1.12it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.51s. Batch: 0.75s. S_Loss: 0.9148. T_Loss: 3.4688. Mask: 0.9748. :  20%|██        | 10/50 [00:08<00:35,  1.12it/s]Train Iter: 761/1000. LR: 0.0476. Data: 0.51s. Batch: 0.75s. S_Loss: 0.9148. T_Loss: 3.4688. Mask: 0.9748. :  22%|██▏       | 11/50 [00:08<00:32,  1.19it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.50s. Batch: 0.74s. S_Loss: 0.9149. T_Loss: 3.4753. Mask: 0.9756. :  22%|██▏       | 11/50 [00:08<00:32,  1.19it/s]Train Iter: 762/1000. LR: 0.0476. Data: 0.50s. Batch: 0.74s. S_Loss: 0.9149. T_Loss: 3.4753. Mask: 0.9756. :  24%|██▍       | 12/50 [00:08<00:29,  1.30it/s]total : 1000  current step :  760
total : 1000  current step :  761
total : 1000  current step :  762
Train Iter: 763/1000. LR: 0.0477. Data: 0.51s. Batch: 0.76s. S_Loss: 0.9111. T_Loss: 3.4652. Mask: 0.9760. :  24%|██▍       | 12/50 [00:09<00:29,  1.30it/s]Train Iter: 763/1000. LR: 0.0477. Data: 0.51s. Batch: 0.76s. S_Loss: 0.9111. T_Loss: 3.4652. Mask: 0.9760. :  26%|██▌       | 13/50 [00:09<00:30,  1.21it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.48s. Batch: 0.73s. S_Loss: 0.9084. T_Loss: 3.4309. Mask: 0.9749. :  26%|██▌       | 13/50 [00:10<00:30,  1.21it/s]Train Iter: 764/1000. LR: 0.0478. Data: 0.48s. Batch: 0.73s. S_Loss: 0.9084. T_Loss: 3.4309. Mask: 0.9749. :  28%|██▊       | 14/50 [00:10<00:25,  1.40it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.45s. Batch: 0.70s. S_Loss: 0.9087. T_Loss: 3.4205. Mask: 0.9742. :  28%|██▊       | 14/50 [00:10<00:25,  1.40it/s]Train Iter: 765/1000. LR: 0.0478. Data: 0.45s. Batch: 0.70s. S_Loss: 0.9087. T_Loss: 3.4205. Mask: 0.9742. :  30%|███       | 15/50 [00:10<00:20,  1.74it/s]total : 1000  current step :  763
total : 1000  current step :  764
total : 1000  current step :  765
Train Iter: 766/1000. LR: 0.0479. Data: 0.48s. Batch: 0.74s. S_Loss: 0.9092. T_Loss: 3.4349. Mask: 0.9746. :  30%|███       | 15/50 [00:11<00:20,  1.74it/s]Train Iter: 766/1000. LR: 0.0479. Data: 0.48s. Batch: 0.74s. S_Loss: 0.9092. T_Loss: 3.4349. Mask: 0.9746. :  32%|███▏      | 16/50 [00:11<00:26,  1.29it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9063. T_Loss: 3.4190. Mask: 0.9745. :  32%|███▏      | 16/50 [00:12<00:26,  1.29it/s]Train Iter: 767/1000. LR: 0.0479. Data: 0.46s. Batch: 0.71s. S_Loss: 0.9063. T_Loss: 3.4190. Mask: 0.9745. :  34%|███▍      | 17/50 [00:12<00:21,  1.54it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9073. T_Loss: 3.4403. Mask: 0.9750. :  34%|███▍      | 17/50 [00:12<00:21,  1.54it/s]Train Iter: 768/1000. LR: 0.0480. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9073. T_Loss: 3.4403. Mask: 0.9750. :  36%|███▌      | 18/50 [00:12<00:16,  1.89it/s]total : 1000  current step :  766
total : 1000  current step :  767
total : 1000  current step :  768
Train Iter: 769/1000. LR: 0.0481. Data: 0.47s. Batch: 0.71s. S_Loss: 0.9088. T_Loss: 3.4519. Mask: 0.9745. :  36%|███▌      | 18/50 [00:13<00:16,  1.89it/s]Train Iter: 769/1000. LR: 0.0481. Data: 0.47s. Batch: 0.71s. S_Loss: 0.9088. T_Loss: 3.4519. Mask: 0.9745. :  38%|███▊      | 19/50 [00:13<00:22,  1.39it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9095. T_Loss: 3.4612. Mask: 0.9748. :  38%|███▊      | 19/50 [00:13<00:22,  1.39it/s]Train Iter: 770/1000. LR: 0.0481. Data: 0.45s. Batch: 0.69s. S_Loss: 0.9095. T_Loss: 3.4612. Mask: 0.9748. :  40%|████      | 20/50 [00:13<00:18,  1.63it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9107. T_Loss: 3.4813. Mask: 0.9751. :  40%|████      | 20/50 [00:14<00:18,  1.63it/s]Train Iter: 771/1000. LR: 0.0482. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9107. T_Loss: 3.4813. Mask: 0.9751. :  42%|████▏     | 21/50 [00:14<00:15,  1.90it/s]total : 1000  current step :  769
total : 1000  current step :  770
total : 1000  current step :  771
Train Iter: 772/1000. LR: 0.0483. Data: 0.45s. Batch: 0.70s. S_Loss: 0.9089. T_Loss: 3.4738. Mask: 0.9755. :  42%|████▏     | 21/50 [00:15<00:15,  1.90it/s]Train Iter: 772/1000. LR: 0.0483. Data: 0.45s. Batch: 0.70s. S_Loss: 0.9089. T_Loss: 3.4738. Mask: 0.9755. :  44%|████▍     | 22/50 [00:15<00:20,  1.37it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9083. T_Loss: 3.4676. Mask: 0.9757. :  44%|████▍     | 22/50 [00:15<00:20,  1.37it/s]Train Iter: 773/1000. LR: 0.0483. Data: 0.44s. Batch: 0.68s. S_Loss: 0.9083. T_Loss: 3.4676. Mask: 0.9757. :  46%|████▌     | 23/50 [00:15<00:16,  1.65it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9084. T_Loss: 3.4624. Mask: 0.9759. :  46%|████▌     | 23/50 [00:15<00:16,  1.65it/s]Train Iter: 774/1000. LR: 0.0484. Data: 0.42s. Batch: 0.66s. S_Loss: 0.9084. T_Loss: 3.4624. Mask: 0.9759. :  48%|████▊     | 24/50 [00:15<00:12,  2.08it/s]total : 1000  current step :  772
total : 1000  current step :  773
total : 1000  current step :  774
Train Iter: 775/1000. LR: 0.0484. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9077. T_Loss: 3.4658. Mask: 0.9758. :  48%|████▊     | 24/50 [00:17<00:12,  2.08it/s]Train Iter: 775/1000. LR: 0.0484. Data: 0.44s. Batch: 0.69s. S_Loss: 0.9077. T_Loss: 3.4658. Mask: 0.9758. :  50%|█████     | 25/50 [00:17<00:17,  1.40it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9064. T_Loss: 3.4552. Mask: 0.9760. :  50%|█████     | 25/50 [00:17<00:17,  1.40it/s]Train Iter: 776/1000. LR: 0.0485. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9064. T_Loss: 3.4552. Mask: 0.9760. :  52%|█████▏    | 26/50 [00:17<00:14,  1.68it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9055. T_Loss: 3.4453. Mask: 0.9764. :  52%|█████▏    | 26/50 [00:17<00:14,  1.68it/s]Train Iter: 777/1000. LR: 0.0486. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9055. T_Loss: 3.4453. Mask: 0.9764. :  54%|█████▍    | 27/50 [00:17<00:11,  2.05it/s]total : 1000  current step :  775
total : 1000  current step :  776
total : 1000  current step :  777
Train Iter: 778/1000. LR: 0.0486. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9051. T_Loss: 3.4333. Mask: 0.9764. :  54%|█████▍    | 27/50 [00:18<00:11,  2.05it/s]Train Iter: 778/1000. LR: 0.0486. Data: 0.42s. Batch: 0.67s. S_Loss: 0.9051. T_Loss: 3.4333. Mask: 0.9764. :  56%|█████▌    | 28/50 [00:18<00:15,  1.44it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9048. T_Loss: 3.4235. Mask: 0.9760. :  56%|█████▌    | 28/50 [00:19<00:15,  1.44it/s]Train Iter: 779/1000. LR: 0.0487. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9048. T_Loss: 3.4235. Mask: 0.9760. :  58%|█████▊    | 29/50 [00:19<00:12,  1.64it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9041. T_Loss: 3.4167. Mask: 0.9760. :  58%|█████▊    | 29/50 [00:19<00:12,  1.64it/s]Train Iter: 780/1000. LR: 0.0488. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9041. T_Loss: 3.4167. Mask: 0.9760. :  60%|██████    | 30/50 [00:19<00:10,  1.98it/s]total : 1000  current step :  778
total : 1000  current step :  779
total : 1000  current step :  780
Train Iter: 781/1000. LR: 0.0488. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9035. T_Loss: 3.4073. Mask: 0.9762. :  60%|██████    | 30/50 [00:20<00:10,  1.98it/s]Train Iter: 781/1000. LR: 0.0488. Data: 0.41s. Batch: 0.67s. S_Loss: 0.9035. T_Loss: 3.4073. Mask: 0.9762. :  62%|██████▏   | 31/50 [00:20<00:12,  1.49it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9038. T_Loss: 3.4135. Mask: 0.9766. :  62%|██████▏   | 31/50 [00:21<00:12,  1.49it/s]Train Iter: 782/1000. LR: 0.0489. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9038. T_Loss: 3.4135. Mask: 0.9766. :  64%|██████▍   | 32/50 [00:21<00:10,  1.73it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9056. T_Loss: 3.4096. Mask: 0.9760. :  64%|██████▍   | 32/50 [00:21<00:10,  1.73it/s]Train Iter: 783/1000. LR: 0.0489. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9056. T_Loss: 3.4096. Mask: 0.9760. :  66%|██████▌   | 33/50 [00:21<00:08,  2.06it/s]total : 1000  current step :  781
total : 1000  current step :  782
total : 1000  current step :  783
Train Iter: 784/1000. LR: 0.0490. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9048. T_Loss: 3.3927. Mask: 0.9762. :  66%|██████▌   | 33/50 [00:22<00:08,  2.06it/s]Train Iter: 784/1000. LR: 0.0490. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9048. T_Loss: 3.3927. Mask: 0.9762. :  68%|██████▊   | 34/50 [00:22<00:11,  1.39it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9052. T_Loss: 3.3932. Mask: 0.9760. :  68%|██████▊   | 34/50 [00:22<00:11,  1.39it/s]Train Iter: 785/1000. LR: 0.0491. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9052. T_Loss: 3.3932. Mask: 0.9760. :  70%|███████   | 35/50 [00:22<00:08,  1.69it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9035. T_Loss: 3.3732. Mask: 0.9759. :  70%|███████   | 35/50 [00:23<00:08,  1.69it/s]Train Iter: 786/1000. LR: 0.0491. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9035. T_Loss: 3.3732. Mask: 0.9759. :  72%|███████▏  | 36/50 [00:23<00:06,  2.05it/s]total : 1000  current step :  784
total : 1000  current step :  785
total : 1000  current step :  786
Train Iter: 787/1000. LR: 0.0492. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9026. T_Loss: 3.3664. Mask: 0.9764. :  72%|███████▏  | 36/50 [00:24<00:06,  2.05it/s]Train Iter: 787/1000. LR: 0.0492. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9026. T_Loss: 3.3664. Mask: 0.9764. :  74%|███████▍  | 37/50 [00:24<00:08,  1.60it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9030. T_Loss: 3.3649. Mask: 0.9765. :  74%|███████▍  | 37/50 [00:24<00:08,  1.60it/s]Train Iter: 788/1000. LR: 0.0493. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9030. T_Loss: 3.3649. Mask: 0.9765. :  76%|███████▌  | 38/50 [00:24<00:06,  1.81it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9024. T_Loss: 3.3589. Mask: 0.9767. :  76%|███████▌  | 38/50 [00:24<00:06,  1.81it/s]Train Iter: 789/1000. LR: 0.0493. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9024. T_Loss: 3.3589. Mask: 0.9767. :  78%|███████▊  | 39/50 [00:24<00:05,  2.00it/s]total : 1000  current step :  787
total : 1000  current step :  788
total : 1000  current step :  789
Train Iter: 790/1000. LR: 0.0494. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9036. T_Loss: 3.3645. Mask: 0.9766. :  78%|███████▊  | 39/50 [00:25<00:05,  2.00it/s]Train Iter: 790/1000. LR: 0.0494. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9036. T_Loss: 3.3645. Mask: 0.9766. :  80%|████████  | 40/50 [00:25<00:06,  1.56it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9041. T_Loss: 3.3725. Mask: 0.9767. :  80%|████████  | 40/50 [00:26<00:06,  1.56it/s]Train Iter: 791/1000. LR: 0.0494. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9041. T_Loss: 3.3725. Mask: 0.9767. :  82%|████████▏ | 41/50 [00:26<00:05,  1.77it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9045. T_Loss: 3.3822. Mask: 0.9763. :  82%|████████▏ | 41/50 [00:26<00:05,  1.77it/s]Train Iter: 792/1000. LR: 0.0495. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9045. T_Loss: 3.3822. Mask: 0.9763. :  84%|████████▍ | 42/50 [00:26<00:03,  2.14it/s]total : 1000  current step :  790
total : 1000  current step :  791
total : 1000  current step :  792
Train Iter: 793/1000. LR: 0.0496. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9047. T_Loss: 3.3820. Mask: 0.9764. :  84%|████████▍ | 42/50 [00:27<00:03,  2.14it/s]Train Iter: 793/1000. LR: 0.0496. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9047. T_Loss: 3.3820. Mask: 0.9764. :  86%|████████▌ | 43/50 [00:27<00:04,  1.41it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9051. T_Loss: 3.3837. Mask: 0.9765. :  86%|████████▌ | 43/50 [00:28<00:04,  1.41it/s]Train Iter: 794/1000. LR: 0.0496. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9051. T_Loss: 3.3837. Mask: 0.9765. :  88%|████████▊ | 44/50 [00:28<00:03,  1.67it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9045. T_Loss: 3.3806. Mask: 0.9765. :  88%|████████▊ | 44/50 [00:28<00:03,  1.67it/s]Train Iter: 795/1000. LR: 0.0497. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9045. T_Loss: 3.3806. Mask: 0.9765. :  90%|█████████ | 45/50 [00:28<00:02,  2.05it/s]total : 1000  current step :  793
total : 1000  current step :  794
total : 1000  current step :  795
Train Iter: 796/1000. LR: 0.0498. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9040. T_Loss: 3.3802. Mask: 0.9763. :  90%|█████████ | 45/50 [00:29<00:02,  2.05it/s]Train Iter: 796/1000. LR: 0.0498. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9040. T_Loss: 3.3802. Mask: 0.9763. :  92%|█████████▏| 46/50 [00:29<00:02,  1.47it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9045. T_Loss: 3.3812. Mask: 0.9763. :  92%|█████████▏| 46/50 [00:29<00:02,  1.47it/s]Train Iter: 797/1000. LR: 0.0498. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9045. T_Loss: 3.3812. Mask: 0.9763. :  94%|█████████▍| 47/50 [00:29<00:01,  1.68it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9049. T_Loss: 3.3865. Mask: 0.9763. :  94%|█████████▍| 47/50 [00:30<00:01,  1.68it/s]Train Iter: 798/1000. LR: 0.0499. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9049. T_Loss: 3.3865. Mask: 0.9763. :  96%|█████████▌| 48/50 [00:30<00:01,  1.98it/s]total : 1000  current step :  796
total : 1000  current step :  797
total : 1000  current step :  798
Train Iter: 799/1000. LR: 0.0499. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9050. T_Loss: 3.3873. Mask: 0.9765. :  96%|█████████▌| 48/50 [00:31<00:01,  1.98it/s]Train Iter: 799/1000. LR: 0.0499. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9050. T_Loss: 3.3873. Mask: 0.9765. :  98%|█████████▊| 49/50 [00:31<00:00,  1.37it/s]total : 1000  current step :  799
Train Iter: 800/1000. LR: 0.0500. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9047. T_Loss: 3.3817. Mask: 0.9765. :  98%|█████████▊| 49/50 [00:31<00:00,  1.37it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9047. T_Loss: 3.3817. Mask: 0.9765. : 100%|██████████| 50/50 [00:31<00:00,  1.49it/s]Train Iter: 800/1000. LR: 0.0500. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9047. T_Loss: 3.3817. Mask: 0.9765. : 100%|██████████| 50/50 [00:31<00:00,  1.57it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8244. top1: 98.44. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8244. top1: 98.44. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8203. top1: 98.05. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.77it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8203. top1: 98.05. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8257. top1: 98.05. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8257. top1: 98.05. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.26it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8309. top1: 98.05. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  3.26it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8309. top1: 98.05. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  3.54it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8826. top1: 94.61. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  3.54it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8826. top1: 94.61. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.68it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9224. top1: 91.86. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.68it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9224. top1: 91.86. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.86it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9542. top1: 90.35. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.86it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9542. top1: 90.35. top5: 99.61. :  88%|████████▊ | 7/8 [00:02<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9755. top1: 88.90. top5: 99.50. :  88%|████████▊ | 7/8 [00:02<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9755. top1: 88.90. top5: 99.50. : 100%|██████████| 8/8 [00:02<00:00,  4.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.9755. top1: 88.90. top5: 99.50. : 100%|██████████| 8/8 [00:02<00:00,  3.30it/s]
total : 1000  current step :  800
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.01s. Batch: 0.31s. S_Loss: 0.9032. T_Loss: 3.4737. Mask: 0.9844. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 801/1000. LR: 0.0500. Data: 0.01s. Batch: 0.31s. S_Loss: 0.9032. T_Loss: 3.4737. Mask: 0.9844. :   2%|▏         | 1/50 [00:00<00:15,  3.17it/s]total : 1000  current step :  801
Train Iter: 802/1000. LR: 0.0500. Data: 0.50s. Batch: 0.79s. S_Loss: 0.9047. T_Loss: 3.3811. Mask: 0.9824. :   2%|▏         | 1/50 [00:01<00:15,  3.17it/s]Train Iter: 802/1000. LR: 0.0500. Data: 0.50s. Batch: 0.79s. S_Loss: 0.9047. T_Loss: 3.3811. Mask: 0.9824. :   4%|▍         | 2/50 [00:01<00:42,  1.14it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.38s. Batch: 0.65s. S_Loss: 0.8988. T_Loss: 3.3957. Mask: 0.9831. :   4%|▍         | 2/50 [00:01<00:42,  1.14it/s]Train Iter: 803/1000. LR: 0.0500. Data: 0.38s. Batch: 0.65s. S_Loss: 0.8988. T_Loss: 3.3957. Mask: 0.9831. :   6%|▌         | 3/50 [00:01<00:30,  1.54it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.32s. Batch: 0.59s. S_Loss: 0.8986. T_Loss: 3.4419. Mask: 0.9844. :   6%|▌         | 3/50 [00:02<00:30,  1.54it/s]Train Iter: 804/1000. LR: 0.0500. Data: 0.32s. Batch: 0.59s. S_Loss: 0.8986. T_Loss: 3.4419. Mask: 0.9844. :   8%|▊         | 4/50 [00:02<00:25,  1.81it/s]total : 1000  current step :  802
total : 1000  current step :  803
total : 1000  current step :  804
Train Iter: 805/1000. LR: 0.0499. Data: 0.41s. Batch: 0.68s. S_Loss: 0.9076. T_Loss: 3.4605. Mask: 0.9781. :   8%|▊         | 4/50 [00:03<00:25,  1.81it/s]Train Iter: 805/1000. LR: 0.0499. Data: 0.41s. Batch: 0.68s. S_Loss: 0.9076. T_Loss: 3.4605. Mask: 0.9781. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9079. T_Loss: 3.4401. Mask: 0.9785. :  10%|█         | 5/50 [00:03<00:32,  1.39it/s]Train Iter: 806/1000. LR: 0.0499. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9079. T_Loss: 3.4401. Mask: 0.9785. :  12%|█▏        | 6/50 [00:03<00:26,  1.69it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9092. T_Loss: 3.4393. Mask: 0.9799. :  12%|█▏        | 6/50 [00:04<00:26,  1.69it/s]Train Iter: 807/1000. LR: 0.0498. Data: 0.32s. Batch: 0.58s. S_Loss: 0.9092. T_Loss: 3.4393. Mask: 0.9799. :  14%|█▍        | 7/50 [00:04<00:22,  1.92it/s]total : 1000  current step :  805
total : 1000  current step :  806
total : 1000  current step :  807
Train Iter: 808/1000. LR: 0.0498. Data: 0.37s. Batch: 0.64s. S_Loss: 0.9102. T_Loss: 3.4353. Mask: 0.9785. :  14%|█▍        | 7/50 [00:05<00:22,  1.92it/s]Train Iter: 808/1000. LR: 0.0498. Data: 0.37s. Batch: 0.64s. S_Loss: 0.9102. T_Loss: 3.4353. Mask: 0.9785. :  16%|█▌        | 8/50 [00:05<00:28,  1.46it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.34s. Batch: 0.60s. S_Loss: 0.9129. T_Loss: 3.4183. Mask: 0.9770. :  16%|█▌        | 8/50 [00:05<00:28,  1.46it/s]Train Iter: 809/1000. LR: 0.0498. Data: 0.34s. Batch: 0.60s. S_Loss: 0.9129. T_Loss: 3.4183. Mask: 0.9770. :  18%|█▊        | 9/50 [00:05<00:23,  1.75it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9151. T_Loss: 3.4334. Mask: 0.9781. :  18%|█▊        | 9/50 [00:05<00:23,  1.75it/s]Train Iter: 810/1000. LR: 0.0497. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9151. T_Loss: 3.4334. Mask: 0.9781. :  20%|██        | 10/50 [00:05<00:18,  2.12it/s]total : 1000  current step :  808
total : 1000  current step :  809
total : 1000  current step :  810
Train Iter: 811/1000. LR: 0.0496. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9143. T_Loss: 3.4017. Mask: 0.9780. :  20%|██        | 10/50 [00:06<00:18,  2.12it/s]Train Iter: 811/1000. LR: 0.0496. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9143. T_Loss: 3.4017. Mask: 0.9780. :  22%|██▏       | 11/50 [00:06<00:25,  1.51it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9153. T_Loss: 3.4134. Mask: 0.9788. :  22%|██▏       | 11/50 [00:07<00:25,  1.51it/s]Train Iter: 812/1000. LR: 0.0496. Data: 0.34s. Batch: 0.59s. S_Loss: 0.9153. T_Loss: 3.4134. Mask: 0.9788. :  24%|██▍       | 12/50 [00:07<00:21,  1.76it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9135. T_Loss: 3.3894. Mask: 0.9799. :  24%|██▍       | 12/50 [00:07<00:21,  1.76it/s]Train Iter: 813/1000. LR: 0.0495. Data: 0.33s. Batch: 0.58s. S_Loss: 0.9135. T_Loss: 3.3894. Mask: 0.9799. :  26%|██▌       | 13/50 [00:07<00:18,  1.98it/s]total : 1000  current step :  811
total : 1000  current step :  812
total : 1000  current step :  813
Train Iter: 814/1000. LR: 0.0494. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9150. T_Loss: 3.4108. Mask: 0.9805. :  26%|██▌       | 13/50 [00:08<00:18,  1.98it/s]Train Iter: 814/1000. LR: 0.0494. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9150. T_Loss: 3.4108. Mask: 0.9805. :  28%|██▊       | 14/50 [00:08<00:26,  1.38it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9167. T_Loss: 3.4080. Mask: 0.9807. :  28%|██▊       | 14/50 [00:09<00:26,  1.38it/s]Train Iter: 815/1000. LR: 0.0493. Data: 0.36s. Batch: 0.60s. S_Loss: 0.9167. T_Loss: 3.4080. Mask: 0.9807. :  30%|███       | 15/50 [00:09<00:21,  1.64it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9180. T_Loss: 3.4172. Mask: 0.9814. :  30%|███       | 15/50 [00:09<00:21,  1.64it/s]Train Iter: 816/1000. LR: 0.0492. Data: 0.34s. Batch: 0.58s. S_Loss: 0.9180. T_Loss: 3.4172. Mask: 0.9814. :  32%|███▏      | 16/50 [00:09<00:17,  1.93it/s]total : 1000  current step :  814
total : 1000  current step :  815
total : 1000  current step :  816
Train Iter: 817/1000. LR: 0.0491. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9166. T_Loss: 3.4035. Mask: 0.9818. :  32%|███▏      | 16/50 [00:10<00:17,  1.93it/s]Train Iter: 817/1000. LR: 0.0491. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9166. T_Loss: 3.4035. Mask: 0.9818. :  34%|███▍      | 17/50 [00:10<00:24,  1.35it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9143. T_Loss: 3.3730. Mask: 0.9816. :  34%|███▍      | 17/50 [00:11<00:24,  1.35it/s]Train Iter: 818/1000. LR: 0.0490. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9143. T_Loss: 3.3730. Mask: 0.9816. :  36%|███▌      | 18/50 [00:11<00:19,  1.61it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9127. T_Loss: 3.3591. Mask: 0.9813. :  36%|███▌      | 18/50 [00:11<00:19,  1.61it/s]Train Iter: 819/1000. LR: 0.0489. Data: 0.35s. Batch: 0.59s. S_Loss: 0.9127. T_Loss: 3.3591. Mask: 0.9813. :  38%|███▊      | 19/50 [00:11<00:15,  1.97it/s]total : 1000  current step :  817
total : 1000  current step :  818
total : 1000  current step :  819
Train Iter: 820/1000. LR: 0.0488. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9100. T_Loss: 3.3372. Mask: 0.9814. :  38%|███▊      | 19/50 [00:12<00:15,  1.97it/s]Train Iter: 820/1000. LR: 0.0488. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9100. T_Loss: 3.3372. Mask: 0.9814. :  40%|████      | 20/50 [00:12<00:21,  1.37it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9082. T_Loss: 3.3258. Mask: 0.9814. :  40%|████      | 20/50 [00:12<00:21,  1.37it/s]Train Iter: 821/1000. LR: 0.0487. Data: 0.37s. Batch: 0.61s. S_Loss: 0.9082. T_Loss: 3.3258. Mask: 0.9814. :  42%|████▏     | 21/50 [00:12<00:18,  1.58it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.35s. Batch: 0.60s. S_Loss: 0.9061. T_Loss: 3.3140. Mask: 0.9808. :  42%|████▏     | 21/50 [00:13<00:18,  1.58it/s]Train Iter: 822/1000. LR: 0.0485. Data: 0.35s. Batch: 0.60s. S_Loss: 0.9061. T_Loss: 3.3140. Mask: 0.9808. :  44%|████▍     | 22/50 [00:13<00:14,  1.88it/s]total : 1000  current step :  820
total : 1000  current step :  821
total : 1000  current step :  822
Train Iter: 823/1000. LR: 0.0484. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9060. T_Loss: 3.3174. Mask: 0.9810. :  44%|████▍     | 22/50 [00:14<00:14,  1.88it/s]Train Iter: 823/1000. LR: 0.0484. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9060. T_Loss: 3.3174. Mask: 0.9810. :  46%|████▌     | 23/50 [00:14<00:20,  1.31it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9050. T_Loss: 3.3123. Mask: 0.9806. :  46%|████▌     | 23/50 [00:14<00:20,  1.31it/s]Train Iter: 824/1000. LR: 0.0482. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9050. T_Loss: 3.3123. Mask: 0.9806. :  48%|████▊     | 24/50 [00:14<00:16,  1.56it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.35s. Batch: 0.61s. S_Loss: 0.9048. T_Loss: 3.3017. Mask: 0.9805. :  48%|████▊     | 24/50 [00:15<00:16,  1.56it/s]Train Iter: 825/1000. LR: 0.0481. Data: 0.35s. Batch: 0.61s. S_Loss: 0.9048. T_Loss: 3.3017. Mask: 0.9805. :  50%|█████     | 25/50 [00:15<00:13,  1.82it/s]total : 1000  current step :  823
total : 1000  current step :  824
total : 1000  current step :  825
Train Iter: 826/1000. LR: 0.0479. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9044. T_Loss: 3.2974. Mask: 0.9811. :  50%|█████     | 25/50 [00:16<00:13,  1.82it/s]Train Iter: 826/1000. LR: 0.0479. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9044. T_Loss: 3.2974. Mask: 0.9811. :  52%|█████▏    | 26/50 [00:16<00:17,  1.37it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9041. T_Loss: 3.2954. Mask: 0.9808. :  52%|█████▏    | 26/50 [00:16<00:17,  1.37it/s]Train Iter: 827/1000. LR: 0.0478. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9041. T_Loss: 3.2954. Mask: 0.9808. :  54%|█████▍    | 27/50 [00:16<00:14,  1.56it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9053. T_Loss: 3.2927. Mask: 0.9806. :  54%|█████▍    | 27/50 [00:17<00:14,  1.56it/s]Train Iter: 828/1000. LR: 0.0476. Data: 0.36s. Batch: 0.61s. S_Loss: 0.9053. T_Loss: 3.2927. Mask: 0.9806. :  56%|█████▌    | 28/50 [00:17<00:12,  1.75it/s]total : 1000  current step :  826
total : 1000  current step :  827
total : 1000  current step :  828
Train Iter: 829/1000. LR: 0.0475. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9051. T_Loss: 3.2925. Mask: 0.9802. :  56%|█████▌    | 28/50 [00:18<00:12,  1.75it/s]Train Iter: 829/1000. LR: 0.0475. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9051. T_Loss: 3.2925. Mask: 0.9802. :  58%|█████▊    | 29/50 [00:18<00:16,  1.31it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9045. T_Loss: 3.2924. Mask: 0.9805. :  58%|█████▊    | 29/50 [00:18<00:16,  1.31it/s]Train Iter: 830/1000. LR: 0.0473. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9045. T_Loss: 3.2924. Mask: 0.9805. :  60%|██████    | 30/50 [00:18<00:13,  1.53it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9037. T_Loss: 3.2878. Mask: 0.9806. :  60%|██████    | 30/50 [00:19<00:13,  1.53it/s]Train Iter: 831/1000. LR: 0.0471. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9037. T_Loss: 3.2878. Mask: 0.9806. :  62%|██████▏   | 31/50 [00:19<00:10,  1.74it/s]total : 1000  current step :  829
total : 1000  current step :  830
total : 1000  current step :  831
Train Iter: 832/1000. LR: 0.0469. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9034. T_Loss: 3.2779. Mask: 0.9806. :  62%|██████▏   | 31/50 [00:20<00:10,  1.74it/s]Train Iter: 832/1000. LR: 0.0469. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9034. T_Loss: 3.2779. Mask: 0.9806. :  64%|██████▍   | 32/50 [00:20<00:14,  1.29it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9030. T_Loss: 3.2752. Mask: 0.9804. :  64%|██████▍   | 32/50 [00:20<00:14,  1.29it/s]Train Iter: 833/1000. LR: 0.0467. Data: 0.38s. Batch: 0.63s. S_Loss: 0.9030. T_Loss: 3.2752. Mask: 0.9804. :  66%|██████▌   | 33/50 [00:20<00:10,  1.56it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9021. T_Loss: 3.2641. Mask: 0.9800. :  66%|██████▌   | 33/50 [00:21<00:10,  1.56it/s]Train Iter: 834/1000. LR: 0.0465. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9021. T_Loss: 3.2641. Mask: 0.9800. :  68%|██████▊   | 34/50 [00:21<00:08,  1.82it/s]total : 1000  current step :  832
total : 1000  current step :  833
total : 1000  current step :  834
Train Iter: 835/1000. LR: 0.0463. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9025. T_Loss: 3.2633. Mask: 0.9799. :  68%|██████▊   | 34/50 [00:22<00:08,  1.82it/s]Train Iter: 835/1000. LR: 0.0463. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9025. T_Loss: 3.2633. Mask: 0.9799. :  70%|███████   | 35/50 [00:22<00:11,  1.26it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9007. T_Loss: 3.2518. Mask: 0.9796. :  70%|███████   | 35/50 [00:22<00:11,  1.26it/s]Train Iter: 836/1000. LR: 0.0461. Data: 0.39s. Batch: 0.63s. S_Loss: 0.9007. T_Loss: 3.2518. Mask: 0.9796. :  72%|███████▏  | 36/50 [00:22<00:09,  1.50it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9000. T_Loss: 3.2458. Mask: 0.9796. :  72%|███████▏  | 36/50 [00:23<00:09,  1.50it/s]Train Iter: 837/1000. LR: 0.0459. Data: 0.38s. Batch: 0.62s. S_Loss: 0.9000. T_Loss: 3.2458. Mask: 0.9796. :  74%|███████▍  | 37/50 [00:23<00:07,  1.78it/s]total : 1000  current step :  835
total : 1000  current step :  836
total : 1000  current step :  837
Train Iter: 838/1000. LR: 0.0457. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8988. T_Loss: 3.2378. Mask: 0.9795. :  74%|███████▍  | 37/50 [00:24<00:07,  1.78it/s]Train Iter: 838/1000. LR: 0.0457. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8988. T_Loss: 3.2378. Mask: 0.9795. :  76%|███████▌  | 38/50 [00:24<00:09,  1.27it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.39s. Batch: 0.63s. S_Loss: 0.8980. T_Loss: 3.2363. Mask: 0.9795. :  76%|███████▌  | 38/50 [00:24<00:09,  1.27it/s]Train Iter: 839/1000. LR: 0.0455. Data: 0.39s. Batch: 0.63s. S_Loss: 0.8980. T_Loss: 3.2363. Mask: 0.9795. :  78%|███████▊  | 39/50 [00:24<00:06,  1.57it/s]total : 1000  current step :  838
total : 1000  current step :  839
Train Iter: 840/1000. LR: 0.0452. Data: 0.39s. Batch: 0.64s. S_Loss: 0.8967. T_Loss: 3.2331. Mask: 0.9794. :  78%|███████▊  | 39/50 [00:25<00:06,  1.57it/s]Train Iter: 840/1000. LR: 0.0452. Data: 0.39s. Batch: 0.64s. S_Loss: 0.8967. T_Loss: 3.2331. Mask: 0.9794. :  80%|████████  | 40/50 [00:25<00:06,  1.48it/s]total : 1000  current step :  840
Train Iter: 841/1000. LR: 0.0450. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8972. T_Loss: 3.2405. Mask: 0.9792. :  80%|████████  | 40/50 [00:26<00:06,  1.48it/s]Train Iter: 841/1000. LR: 0.0450. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8972. T_Loss: 3.2405. Mask: 0.9792. :  82%|████████▏ | 41/50 [00:26<00:07,  1.19it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8980. T_Loss: 3.2383. Mask: 0.9794. :  82%|████████▏ | 41/50 [00:27<00:07,  1.19it/s]Train Iter: 842/1000. LR: 0.0448. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8980. T_Loss: 3.2383. Mask: 0.9794. :  84%|████████▍ | 42/50 [00:27<00:05,  1.45it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.39s. Batch: 0.64s. S_Loss: 0.8996. T_Loss: 3.2421. Mask: 0.9795. :  84%|████████▍ | 42/50 [00:27<00:05,  1.45it/s]Train Iter: 843/1000. LR: 0.0445. Data: 0.39s. Batch: 0.64s. S_Loss: 0.8996. T_Loss: 3.2421. Mask: 0.9795. :  86%|████████▌ | 43/50 [00:27<00:04,  1.62it/s]total : 1000  current step :  841
total : 1000  current step :  842
total : 1000  current step :  843
Train Iter: 844/1000. LR: 0.0443. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8997. T_Loss: 3.2460. Mask: 0.9794. :  86%|████████▌ | 43/50 [00:28<00:04,  1.62it/s]Train Iter: 844/1000. LR: 0.0443. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8997. T_Loss: 3.2460. Mask: 0.9794. :  88%|████████▊ | 44/50 [00:28<00:04,  1.23it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9000. T_Loss: 3.2458. Mask: 0.9792. :  88%|████████▊ | 44/50 [00:29<00:04,  1.23it/s]Train Iter: 845/1000. LR: 0.0440. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9000. T_Loss: 3.2458. Mask: 0.9792. :  90%|█████████ | 45/50 [00:29<00:03,  1.41it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9010. T_Loss: 3.2594. Mask: 0.9792. :  90%|█████████ | 45/50 [00:29<00:03,  1.41it/s]Train Iter: 846/1000. LR: 0.0438. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9010. T_Loss: 3.2594. Mask: 0.9792. :  92%|█████████▏| 46/50 [00:29<00:02,  1.64it/s]total : 1000  current step :  844
total : 1000  current step :  845
total : 1000  current step :  846
Train Iter: 847/1000. LR: 0.0435. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9014. T_Loss: 3.2527. Mask: 0.9793. :  92%|█████████▏| 46/50 [00:30<00:02,  1.64it/s]Train Iter: 847/1000. LR: 0.0435. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9014. T_Loss: 3.2527. Mask: 0.9793. :  94%|█████████▍| 47/50 [00:30<00:02,  1.34it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9010. T_Loss: 3.2466. Mask: 0.9792. :  94%|█████████▍| 47/50 [00:31<00:02,  1.34it/s]Train Iter: 848/1000. LR: 0.0432. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9010. T_Loss: 3.2466. Mask: 0.9792. :  96%|█████████▌| 48/50 [00:31<00:01,  1.51it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9008. T_Loss: 3.2401. Mask: 0.9793. :  96%|█████████▌| 48/50 [00:31<00:01,  1.51it/s]Train Iter: 849/1000. LR: 0.0430. Data: 0.39s. Batch: 0.64s. S_Loss: 0.9008. T_Loss: 3.2401. Mask: 0.9793. :  98%|█████████▊| 49/50 [00:31<00:00,  1.80it/s]total : 1000  current step :  847
total : 1000  current step :  848
total : 1000  current step :  849
Train Iter: 850/1000. LR: 0.0427. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9004. T_Loss: 3.2420. Mask: 0.9796. :  98%|█████████▊| 49/50 [00:32<00:00,  1.80it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9004. T_Loss: 3.2420. Mask: 0.9796. : 100%|██████████| 50/50 [00:32<00:00,  1.42it/s]Train Iter: 850/1000. LR: 0.0427. Data: 0.40s. Batch: 0.65s. S_Loss: 0.9004. T_Loss: 3.2420. Mask: 0.9796. : 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8263. top1: 98.05. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8263. top1: 98.05. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.60it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8241. top1: 97.85. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.60it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8241. top1: 97.85. top5: 100.00. :  25%|██▌       | 2/8 [00:00<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8292. top1: 97.79. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8292. top1: 97.79. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8327. top1: 97.85. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8327. top1: 97.85. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8755. top1: 95.00. top5: 99.84. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8755. top1: 95.00. top5: 99.84. :  62%|██████▎   | 5/8 [00:01<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9084. top1: 92.84. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.9084. top1: 92.84. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9348. top1: 91.46. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.9348. top1: 91.46. top5: 99.61. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9534. top1: 90.20. top5: 99.45. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9534. top1: 90.20. top5: 99.45. : 100%|██████████| 8/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9534. top1: 90.20. top5: 99.45. : 100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
total : 1000  current step :  850
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9385. T_Loss: 3.3325. Mask: 0.9766. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 851/1000. LR: 0.0424. Data: 0.01s. Batch: 0.30s. S_Loss: 0.9385. T_Loss: 3.3325. Mask: 0.9766. :   2%|▏         | 1/50 [00:00<00:14,  3.34it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.28s. S_Loss: 0.9434. T_Loss: 3.5492. Mask: 0.9844. :   2%|▏         | 1/50 [00:00<00:14,  3.34it/s]Train Iter: 852/1000. LR: 0.0421. Data: 0.01s. Batch: 0.28s. S_Loss: 0.9434. T_Loss: 3.5492. Mask: 0.9844. :   4%|▍         | 2/50 [00:00<00:13,  3.52it/s]total : 1000  current step :  851
total : 1000  current step :  852
Train Iter: 853/1000. LR: 0.0418. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9337. T_Loss: 3.4979. Mask: 0.9844. :   4%|▍         | 2/50 [00:01<00:13,  3.52it/s]Train Iter: 853/1000. LR: 0.0418. Data: 0.30s. Batch: 0.56s. S_Loss: 0.9337. T_Loss: 3.4979. Mask: 0.9844. :   6%|▌         | 3/50 [00:01<00:31,  1.52it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9398. T_Loss: 3.6050. Mask: 0.9854. :   6%|▌         | 3/50 [00:02<00:31,  1.52it/s]Train Iter: 854/1000. LR: 0.0415. Data: 0.25s. Batch: 0.50s. S_Loss: 0.9398. T_Loss: 3.6050. Mask: 0.9854. :   8%|▊         | 4/50 [00:02<00:24,  1.88it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9388. T_Loss: 3.6527. Mask: 0.9805. :   8%|▊         | 4/50 [00:02<00:24,  1.88it/s]Train Iter: 855/1000. LR: 0.0412. Data: 0.24s. Batch: 0.50s. S_Loss: 0.9388. T_Loss: 3.6527. Mask: 0.9805. :  10%|█         | 5/50 [00:02<00:23,  1.95it/s]total : 1000  current step :  853
total : 1000  current step :  854
total : 1000  current step :  855
Train Iter: 856/1000. LR: 0.0409. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9287. T_Loss: 3.5837. Mask: 0.9792. :  10%|█         | 5/50 [00:03<00:23,  1.95it/s]Train Iter: 856/1000. LR: 0.0409. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9287. T_Loss: 3.5837. Mask: 0.9792. :  12%|█▏        | 6/50 [00:03<00:35,  1.25it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.34s. Batch: 0.60s. S_Loss: 0.9231. T_Loss: 3.5527. Mask: 0.9794. :  12%|█▏        | 6/50 [00:04<00:35,  1.25it/s]Train Iter: 857/1000. LR: 0.0406. Data: 0.34s. Batch: 0.60s. S_Loss: 0.9231. T_Loss: 3.5527. Mask: 0.9794. :  14%|█▍        | 7/50 [00:04<00:28,  1.51it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9214. T_Loss: 3.5300. Mask: 0.9800. :  14%|█▍        | 7/50 [00:04<00:28,  1.51it/s]Train Iter: 858/1000. LR: 0.0403. Data: 0.31s. Batch: 0.57s. S_Loss: 0.9214. T_Loss: 3.5300. Mask: 0.9800. :  16%|█▌        | 8/50 [00:04<00:23,  1.81it/s]total : 1000  current step :  856
total : 1000  current step :  857
total : 1000  current step :  858
Train Iter: 859/1000. LR: 0.0400. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9242. T_Loss: 3.5654. Mask: 0.9792. :  16%|█▌        | 8/50 [00:05<00:23,  1.81it/s]Train Iter: 859/1000. LR: 0.0400. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9242. T_Loss: 3.5654. Mask: 0.9792. :  18%|█▊        | 9/50 [00:05<00:33,  1.24it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9247. T_Loss: 3.5772. Mask: 0.9789. :  18%|█▊        | 9/50 [00:06<00:33,  1.24it/s]Train Iter: 860/1000. LR: 0.0397. Data: 0.37s. Batch: 0.62s. S_Loss: 0.9247. T_Loss: 3.5772. Mask: 0.9789. :  20%|██        | 10/50 [00:06<00:26,  1.53it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.35s. Batch: 0.60s. S_Loss: 0.9279. T_Loss: 3.5862. Mask: 0.9794. :  20%|██        | 10/50 [00:06<00:26,  1.53it/s]Train Iter: 861/1000. LR: 0.0394. Data: 0.35s. Batch: 0.60s. S_Loss: 0.9279. T_Loss: 3.5862. Mask: 0.9794. :  22%|██▏       | 11/50 [00:06<00:22,  1.75it/s]total : 1000  current step :  859
total : 1000  current step :  860
total : 1000  current step :  861
Train Iter: 862/1000. LR: 0.0391. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9242. T_Loss: 3.5607. Mask: 0.9808. :  22%|██▏       | 11/50 [00:07<00:22,  1.75it/s]Train Iter: 862/1000. LR: 0.0391. Data: 0.40s. Batch: 0.66s. S_Loss: 0.9242. T_Loss: 3.5607. Mask: 0.9808. :  24%|██▍       | 12/50 [00:07<00:30,  1.25it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9223. T_Loss: 3.5591. Mask: 0.9814. :  24%|██▍       | 12/50 [00:08<00:30,  1.25it/s]Train Iter: 863/1000. LR: 0.0387. Data: 0.37s. Batch: 0.63s. S_Loss: 0.9223. T_Loss: 3.5591. Mask: 0.9814. :  26%|██▌       | 13/50 [00:08<00:23,  1.56it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.35s. Batch: 0.60s. S_Loss: 0.9191. T_Loss: 3.5456. Mask: 0.9813. :  26%|██▌       | 13/50 [00:08<00:23,  1.56it/s]Train Iter: 864/1000. LR: 0.0384. Data: 0.35s. Batch: 0.60s. S_Loss: 0.9191. T_Loss: 3.5456. Mask: 0.9813. :  28%|██▊       | 14/50 [00:08<00:19,  1.87it/s]total : 1000  current step :  862
total : 1000  current step :  863
total : 1000  current step :  864
Train Iter: 865/1000. LR: 0.0381. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9187. T_Loss: 3.5370. Mask: 0.9802. :  28%|██▊       | 14/50 [00:09<00:19,  1.87it/s]Train Iter: 865/1000. LR: 0.0381. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9187. T_Loss: 3.5370. Mask: 0.9802. :  30%|███       | 15/50 [00:09<00:26,  1.33it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9166. T_Loss: 3.5231. Mask: 0.9797. :  30%|███       | 15/50 [00:10<00:26,  1.33it/s]Train Iter: 866/1000. LR: 0.0377. Data: 0.38s. Batch: 0.64s. S_Loss: 0.9166. T_Loss: 3.5231. Mask: 0.9797. :  32%|███▏      | 16/50 [00:10<00:22,  1.51it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9151. T_Loss: 3.5138. Mask: 0.9793. :  32%|███▏      | 16/50 [00:10<00:22,  1.51it/s]Train Iter: 867/1000. LR: 0.0374. Data: 0.36s. Batch: 0.62s. S_Loss: 0.9151. T_Loss: 3.5138. Mask: 0.9793. :  34%|███▍      | 17/50 [00:10<00:19,  1.72it/s]total : 1000  current step :  865
total : 1000  current step :  866
total : 1000  current step :  867
Train Iter: 868/1000. LR: 0.0370. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9143. T_Loss: 3.5164. Mask: 0.9796. :  34%|███▍      | 17/50 [00:12<00:19,  1.72it/s]Train Iter: 868/1000. LR: 0.0370. Data: 0.41s. Batch: 0.66s. S_Loss: 0.9143. T_Loss: 3.5164. Mask: 0.9796. :  36%|███▌      | 18/50 [00:12<00:26,  1.21it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9134. T_Loss: 3.5023. Mask: 0.9790. :  36%|███▌      | 18/50 [00:12<00:26,  1.21it/s]Train Iter: 869/1000. LR: 0.0367. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9134. T_Loss: 3.5023. Mask: 0.9790. :  38%|███▊      | 19/50 [00:12<00:21,  1.41it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.37s. Batch: 0.64s. S_Loss: 0.9120. T_Loss: 3.4903. Mask: 0.9787. :  38%|███▊      | 19/50 [00:12<00:21,  1.41it/s]Train Iter: 870/1000. LR: 0.0363. Data: 0.37s. Batch: 0.64s. S_Loss: 0.9120. T_Loss: 3.4903. Mask: 0.9787. :  40%|████      | 20/50 [00:12<00:17,  1.67it/s]total : 1000  current step :  868
total : 1000  current step :  869
total : 1000  current step :  870
Train Iter: 871/1000. LR: 0.0360. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9088. T_Loss: 3.4680. Mask: 0.9786. :  40%|████      | 20/50 [00:14<00:17,  1.67it/s]Train Iter: 871/1000. LR: 0.0360. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9088. T_Loss: 3.4680. Mask: 0.9786. :  42%|████▏     | 21/50 [00:14<00:23,  1.24it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9077. T_Loss: 3.4519. Mask: 0.9782. :  42%|████▏     | 21/50 [00:14<00:23,  1.24it/s]Train Iter: 872/1000. LR: 0.0356. Data: 0.39s. Batch: 0.65s. S_Loss: 0.9077. T_Loss: 3.4519. Mask: 0.9782. :  44%|████▍     | 22/50 [00:14<00:19,  1.47it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.37s. Batch: 0.64s. S_Loss: 0.9049. T_Loss: 3.4280. Mask: 0.9784. :  44%|████▍     | 22/50 [00:14<00:19,  1.47it/s]Train Iter: 873/1000. LR: 0.0353. Data: 0.37s. Batch: 0.64s. S_Loss: 0.9049. T_Loss: 3.4280. Mask: 0.9784. :  46%|████▌     | 23/50 [00:14<00:16,  1.65it/s]total : 1000  current step :  871
total : 1000  current step :  872
total : 1000  current step :  873
Train Iter: 874/1000. LR: 0.0349. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9035. T_Loss: 3.4023. Mask: 0.9784. :  46%|████▌     | 23/50 [00:16<00:16,  1.65it/s]Train Iter: 874/1000. LR: 0.0349. Data: 0.40s. Batch: 0.67s. S_Loss: 0.9035. T_Loss: 3.4023. Mask: 0.9784. :  48%|████▊     | 24/50 [00:16<00:21,  1.23it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.39s. Batch: 0.66s. S_Loss: 0.9020. T_Loss: 3.3962. Mask: 0.9784. :  48%|████▊     | 24/50 [00:16<00:21,  1.23it/s]Train Iter: 875/1000. LR: 0.0346. Data: 0.39s. Batch: 0.66s. S_Loss: 0.9020. T_Loss: 3.3962. Mask: 0.9784. :  50%|█████     | 25/50 [00:16<00:17,  1.43it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9021. T_Loss: 3.4015. Mask: 0.9785. :  50%|█████     | 25/50 [00:16<00:17,  1.43it/s]Train Iter: 876/1000. LR: 0.0342. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9021. T_Loss: 3.4015. Mask: 0.9785. :  52%|█████▏    | 26/50 [00:16<00:14,  1.67it/s]total : 1000  current step :  874
total : 1000  current step :  875
total : 1000  current step :  876
Train Iter: 877/1000. LR: 0.0338. Data: 0.40s. Batch: 0.68s. S_Loss: 0.9035. T_Loss: 3.3948. Mask: 0.9784. :  52%|█████▏    | 26/50 [00:18<00:14,  1.67it/s]Train Iter: 877/1000. LR: 0.0338. Data: 0.40s. Batch: 0.68s. S_Loss: 0.9035. T_Loss: 3.3948. Mask: 0.9784. :  54%|█████▍    | 27/50 [00:18<00:18,  1.23it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.39s. Batch: 0.67s. S_Loss: 0.9025. T_Loss: 3.3856. Mask: 0.9788. :  54%|█████▍    | 27/50 [00:18<00:18,  1.23it/s]Train Iter: 878/1000. LR: 0.0335. Data: 0.39s. Batch: 0.67s. S_Loss: 0.9025. T_Loss: 3.3856. Mask: 0.9788. :  56%|█████▌    | 28/50 [00:18<00:15,  1.42it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9033. T_Loss: 3.3820. Mask: 0.9783. :  56%|█████▌    | 28/50 [00:19<00:15,  1.42it/s]Train Iter: 879/1000. LR: 0.0331. Data: 0.38s. Batch: 0.65s. S_Loss: 0.9033. T_Loss: 3.3820. Mask: 0.9783. :  58%|█████▊    | 29/50 [00:19<00:12,  1.72it/s]total : 1000  current step :  877
total : 1000  current step :  878
total : 1000  current step :  879
Train Iter: 880/1000. LR: 0.0327. Data: 0.41s. Batch: 0.69s. S_Loss: 0.9025. T_Loss: 3.3811. Mask: 0.9785. :  58%|█████▊    | 29/50 [00:20<00:12,  1.72it/s]Train Iter: 880/1000. LR: 0.0327. Data: 0.41s. Batch: 0.69s. S_Loss: 0.9025. T_Loss: 3.3811. Mask: 0.9785. :  60%|██████    | 30/50 [00:20<00:18,  1.08it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9031. T_Loss: 3.3723. Mask: 0.9786. :  60%|██████    | 30/50 [00:21<00:18,  1.08it/s]Train Iter: 881/1000. LR: 0.0324. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9031. T_Loss: 3.3723. Mask: 0.9786. :  62%|██████▏   | 31/50 [00:21<00:16,  1.16it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.41s. Batch: 0.69s. S_Loss: 0.9029. T_Loss: 3.3649. Mask: 0.9779. :  62%|██████▏   | 31/50 [00:22<00:16,  1.16it/s]Train Iter: 882/1000. LR: 0.0320. Data: 0.41s. Batch: 0.69s. S_Loss: 0.9029. T_Loss: 3.3649. Mask: 0.9779. :  64%|██████▍   | 32/50 [00:22<00:14,  1.25it/s]total : 1000  current step :  880
total : 1000  current step :  881
total : 1000  current step :  882
Train Iter: 883/1000. LR: 0.0316. Data: 0.43s. Batch: 0.71s. S_Loss: 0.9028. T_Loss: 3.3634. Mask: 0.9780. :  64%|██████▍   | 32/50 [00:23<00:14,  1.25it/s]Train Iter: 883/1000. LR: 0.0316. Data: 0.43s. Batch: 0.71s. S_Loss: 0.9028. T_Loss: 3.3634. Mask: 0.9780. :  66%|██████▌   | 33/50 [00:23<00:16,  1.02it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.42s. Batch: 0.70s. S_Loss: 0.9033. T_Loss: 3.3691. Mask: 0.9782. :  66%|██████▌   | 33/50 [00:23<00:16,  1.02it/s]Train Iter: 884/1000. LR: 0.0312. Data: 0.42s. Batch: 0.70s. S_Loss: 0.9033. T_Loss: 3.3691. Mask: 0.9782. :  68%|██████▊   | 34/50 [00:23<00:12,  1.31it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.41s. Batch: 0.69s. S_Loss: 0.9031. T_Loss: 3.3704. Mask: 0.9779. :  68%|██████▊   | 34/50 [00:24<00:12,  1.31it/s]Train Iter: 885/1000. LR: 0.0308. Data: 0.41s. Batch: 0.69s. S_Loss: 0.9031. T_Loss: 3.3704. Mask: 0.9779. :  70%|███████   | 35/50 [00:24<00:10,  1.49it/s]total : 1000  current step :  883
total : 1000  current step :  884
total : 1000  current step :  885
Train Iter: 886/1000. LR: 0.0305. Data: 0.43s. Batch: 0.70s. S_Loss: 0.9021. T_Loss: 3.3666. Mask: 0.9779. :  70%|███████   | 35/50 [00:25<00:10,  1.49it/s]Train Iter: 886/1000. LR: 0.0305. Data: 0.43s. Batch: 0.70s. S_Loss: 0.9021. T_Loss: 3.3666. Mask: 0.9779. :  72%|███████▏  | 36/50 [00:25<00:11,  1.24it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.43s. Batch: 0.70s. S_Loss: 0.9013. T_Loss: 3.3565. Mask: 0.9780. :  72%|███████▏  | 36/50 [00:25<00:11,  1.24it/s]Train Iter: 887/1000. LR: 0.0301. Data: 0.43s. Batch: 0.70s. S_Loss: 0.9013. T_Loss: 3.3565. Mask: 0.9780. :  74%|███████▍  | 37/50 [00:25<00:09,  1.41it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9021. T_Loss: 3.3596. Mask: 0.9782. :  74%|███████▍  | 37/50 [00:26<00:09,  1.41it/s]Train Iter: 888/1000. LR: 0.0297. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9021. T_Loss: 3.3596. Mask: 0.9782. :  76%|███████▌  | 38/50 [00:26<00:07,  1.66it/s]total : 1000  current step :  886
total : 1000  current step :  887
total : 1000  current step :  888
Train Iter: 889/1000. LR: 0.0293. Data: 0.43s. Batch: 0.70s. S_Loss: 0.9015. T_Loss: 3.3524. Mask: 0.9784. :  76%|███████▌  | 38/50 [00:27<00:07,  1.66it/s]Train Iter: 889/1000. LR: 0.0293. Data: 0.43s. Batch: 0.70s. S_Loss: 0.9015. T_Loss: 3.3524. Mask: 0.9784. :  78%|███████▊  | 39/50 [00:27<00:08,  1.27it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9017. T_Loss: 3.3476. Mask: 0.9781. :  78%|███████▊  | 39/50 [00:27<00:08,  1.27it/s]Train Iter: 890/1000. LR: 0.0289. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9017. T_Loss: 3.3476. Mask: 0.9781. :  80%|████████  | 40/50 [00:27<00:06,  1.51it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9016. T_Loss: 3.3459. Mask: 0.9779. :  80%|████████  | 40/50 [00:28<00:06,  1.51it/s]Train Iter: 891/1000. LR: 0.0285. Data: 0.42s. Batch: 0.68s. S_Loss: 0.9016. T_Loss: 3.3459. Mask: 0.9779. :  82%|████████▏ | 41/50 [00:28<00:05,  1.75it/s]total : 1000  current step :  889
total : 1000  current step :  890
total : 1000  current step :  891
Train Iter: 892/1000. LR: 0.0281. Data: 0.43s. Batch: 0.70s. S_Loss: 0.9005. T_Loss: 3.3431. Mask: 0.9783. :  82%|████████▏ | 41/50 [00:29<00:05,  1.75it/s]Train Iter: 892/1000. LR: 0.0281. Data: 0.43s. Batch: 0.70s. S_Loss: 0.9005. T_Loss: 3.3431. Mask: 0.9783. :  84%|████████▍ | 42/50 [00:29<00:06,  1.33it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9016. T_Loss: 3.3537. Mask: 0.9784. :  84%|████████▍ | 42/50 [00:29<00:06,  1.33it/s]Train Iter: 893/1000. LR: 0.0277. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9016. T_Loss: 3.3537. Mask: 0.9784. :  86%|████████▌ | 43/50 [00:29<00:04,  1.55it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9012. T_Loss: 3.3518. Mask: 0.9787. :  86%|████████▌ | 43/50 [00:30<00:04,  1.55it/s]Train Iter: 894/1000. LR: 0.0274. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9012. T_Loss: 3.3518. Mask: 0.9787. :  88%|████████▊ | 44/50 [00:30<00:03,  1.62it/s]total : 1000  current step :  892
total : 1000  current step :  893
total : 1000  current step :  894
Train Iter: 895/1000. LR: 0.0270. Data: 0.44s. Batch: 0.70s. S_Loss: 0.9009. T_Loss: 3.3509. Mask: 0.9791. :  88%|████████▊ | 44/50 [00:31<00:03,  1.62it/s]Train Iter: 895/1000. LR: 0.0270. Data: 0.44s. Batch: 0.70s. S_Loss: 0.9009. T_Loss: 3.3509. Mask: 0.9791. :  90%|█████████ | 45/50 [00:31<00:04,  1.18it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9012. T_Loss: 3.3528. Mask: 0.9787. :  90%|█████████ | 45/50 [00:32<00:04,  1.18it/s]Train Iter: 896/1000. LR: 0.0266. Data: 0.43s. Batch: 0.69s. S_Loss: 0.9012. T_Loss: 3.3528. Mask: 0.9787. :  92%|█████████▏| 46/50 [00:32<00:02,  1.42it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9008. T_Loss: 3.3457. Mask: 0.9788. :  92%|█████████▏| 46/50 [00:32<00:02,  1.42it/s]Train Iter: 897/1000. LR: 0.0262. Data: 0.42s. Batch: 0.69s. S_Loss: 0.9008. T_Loss: 3.3457. Mask: 0.9788. :  94%|█████████▍| 47/50 [00:32<00:01,  1.59it/s]total : 1000  current step :  895
total : 1000  current step :  896
total : 1000  current step :  897
Train Iter: 898/1000. LR: 0.0258. Data: 0.44s. Batch: 0.70s. S_Loss: 0.9001. T_Loss: 3.3402. Mask: 0.9789. :  94%|█████████▍| 47/50 [00:33<00:01,  1.59it/s]Train Iter: 898/1000. LR: 0.0258. Data: 0.44s. Batch: 0.70s. S_Loss: 0.9001. T_Loss: 3.3402. Mask: 0.9789. :  96%|█████████▌| 48/50 [00:33<00:01,  1.20it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.43s. Batch: 0.69s. S_Loss: 0.8994. T_Loss: 3.3386. Mask: 0.9792. :  96%|█████████▌| 48/50 [00:34<00:01,  1.20it/s]Train Iter: 899/1000. LR: 0.0254. Data: 0.43s. Batch: 0.69s. S_Loss: 0.8994. T_Loss: 3.3386. Mask: 0.9792. :  98%|█████████▊| 49/50 [00:34<00:00,  1.44it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.42s. Batch: 0.69s. S_Loss: 0.8988. T_Loss: 3.3352. Mask: 0.9793. :  98%|█████████▊| 49/50 [00:34<00:00,  1.44it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.42s. Batch: 0.69s. S_Loss: 0.8988. T_Loss: 3.3352. Mask: 0.9793. : 100%|██████████| 50/50 [00:34<00:00,  1.70it/s]Train Iter: 900/1000. LR: 0.0250. Data: 0.42s. Batch: 0.69s. S_Loss: 0.8988. T_Loss: 3.3352. Mask: 0.9793. : 100%|██████████| 50/50 [00:34<00:00,  1.45it/s]
total : 1000  current step :  898
total : 1000  current step :  899
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8339. top1: 97.27. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8339. top1: 97.27. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.28it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8325. top1: 97.27. top5: 100.00. :  12%|█▎        | 1/8 [00:01<00:05,  1.28it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8325. top1: 97.27. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.93it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8372. top1: 97.40. top5: 100.00. :  25%|██▌       | 2/8 [00:01<00:03,  1.93it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8372. top1: 97.40. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.21it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8395. top1: 97.46. top5: 100.00. :  38%|███▊      | 3/8 [00:01<00:02,  2.21it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8395. top1: 97.46. top5: 100.00. :  50%|█████     | 4/8 [00:01<00:01,  2.44it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8724. top1: 95.00. top5: 99.77. :  50%|█████     | 4/8 [00:02<00:01,  2.44it/s] Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8724. top1: 95.00. top5: 99.77. :  62%|██████▎   | 5/8 [00:02<00:01,  2.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8972. top1: 93.42. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.64it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8972. top1: 93.42. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  2.78it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9179. top1: 92.19. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  2.78it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.9179. top1: 92.19. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  2.85it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9331. top1: 91.25. top5: 99.55. :  88%|████████▊ | 7/8 [00:03<00:00,  2.85it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9331. top1: 91.25. top5: 99.55. : 100%|██████████| 8/8 [00:03<00:00,  3.13it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.9331. top1: 91.25. top5: 99.55. : 100%|██████████| 8/8 [00:03<00:00,  2.43it/s]
total : 1000  current step :  900
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 1.01s. Batch: 1.19s. S_Loss: 0.8985. T_Loss: 3.1181. Mask: 0.9805. :   0%|          | 0/50 [00:01<?, ?it/s]Train Iter: 901/1000. LR: 0.0246. Data: 1.01s. Batch: 1.19s. S_Loss: 0.8985. T_Loss: 3.1181. Mask: 0.9805. :   2%|▏         | 1/50 [00:01<00:58,  1.19s/it]Train Iter: 902/1000. LR: 0.0242. Data: 0.72s. Batch: 0.90s. S_Loss: 0.9145. T_Loss: 3.3891. Mask: 0.9844. :   2%|▏         | 1/50 [00:01<00:58,  1.19s/it]Train Iter: 902/1000. LR: 0.0242. Data: 0.72s. Batch: 0.90s. S_Loss: 0.9145. T_Loss: 3.3891. Mask: 0.9844. :   4%|▍         | 2/50 [00:01<00:40,  1.18it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.54s. Batch: 0.76s. S_Loss: 0.9069. T_Loss: 3.3112. Mask: 0.9818. :   4%|▍         | 2/50 [00:02<00:40,  1.18it/s]Train Iter: 903/1000. LR: 0.0238. Data: 0.54s. Batch: 0.76s. S_Loss: 0.9069. T_Loss: 3.3112. Mask: 0.9818. :   6%|▌         | 3/50 [00:02<00:32,  1.47it/s]total : 1000  current step :  901
total : 1000  current step :  902
total : 1000  current step :  903
Train Iter: 904/1000. LR: 0.0234. Data: 0.65s. Batch: 0.88s. S_Loss: 0.9007. T_Loss: 3.2742. Mask: 0.9775. :   6%|▌         | 3/50 [00:03<00:32,  1.47it/s]Train Iter: 904/1000. LR: 0.0234. Data: 0.65s. Batch: 0.88s. S_Loss: 0.9007. T_Loss: 3.2742. Mask: 0.9775. :   8%|▊         | 4/50 [00:03<00:41,  1.11it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.54s. Batch: 0.76s. S_Loss: 0.8953. T_Loss: 3.2466. Mask: 0.9797. :   8%|▊         | 4/50 [00:03<00:41,  1.11it/s]Train Iter: 905/1000. LR: 0.0230. Data: 0.54s. Batch: 0.76s. S_Loss: 0.8953. T_Loss: 3.2466. Mask: 0.9797. :  10%|█         | 5/50 [00:03<00:30,  1.47it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.48s. Batch: 0.70s. S_Loss: 0.8939. T_Loss: 3.1729. Mask: 0.9785. :  10%|█         | 5/50 [00:04<00:30,  1.47it/s]Train Iter: 906/1000. LR: 0.0226. Data: 0.48s. Batch: 0.70s. S_Loss: 0.8939. T_Loss: 3.1729. Mask: 0.9785. :  12%|█▏        | 6/50 [00:04<00:25,  1.69it/s]total : 1000  current step :  904
total : 1000  current step :  905
total : 1000  current step :  906
Train Iter: 907/1000. LR: 0.0223. Data: 0.58s. Batch: 0.80s. S_Loss: 0.8910. T_Loss: 3.1572. Mask: 0.9771. :  12%|█▏        | 6/50 [00:05<00:25,  1.69it/s]Train Iter: 907/1000. LR: 0.0223. Data: 0.58s. Batch: 0.80s. S_Loss: 0.8910. T_Loss: 3.1572. Mask: 0.9771. :  14%|█▍        | 7/50 [00:05<00:36,  1.16it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.52s. Batch: 0.74s. S_Loss: 0.8929. T_Loss: 3.1600. Mask: 0.9780. :  14%|█▍        | 7/50 [00:05<00:36,  1.16it/s]Train Iter: 908/1000. LR: 0.0219. Data: 0.52s. Batch: 0.74s. S_Loss: 0.8929. T_Loss: 3.1600. Mask: 0.9780. :  16%|█▌        | 8/50 [00:05<00:28,  1.45it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.48s. Batch: 0.70s. S_Loss: 0.8891. T_Loss: 3.1235. Mask: 0.9783. :  16%|█▌        | 8/50 [00:06<00:28,  1.45it/s]Train Iter: 909/1000. LR: 0.0215. Data: 0.48s. Batch: 0.70s. S_Loss: 0.8891. T_Loss: 3.1235. Mask: 0.9783. :  18%|█▊        | 9/50 [00:06<00:24,  1.68it/s]total : 1000  current step :  907
total : 1000  current step :  908
total : 1000  current step :  909
Train Iter: 910/1000. LR: 0.0211. Data: 0.54s. Batch: 0.77s. S_Loss: 0.8901. T_Loss: 3.1020. Mask: 0.9797. :  18%|█▊        | 9/50 [00:07<00:24,  1.68it/s]Train Iter: 910/1000. LR: 0.0211. Data: 0.54s. Batch: 0.77s. S_Loss: 0.8901. T_Loss: 3.1020. Mask: 0.9797. :  20%|██        | 10/50 [00:07<00:33,  1.21it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.51s. Batch: 0.73s. S_Loss: 0.8866. T_Loss: 3.0723. Mask: 0.9801. :  20%|██        | 10/50 [00:08<00:33,  1.21it/s]Train Iter: 911/1000. LR: 0.0207. Data: 0.51s. Batch: 0.73s. S_Loss: 0.8866. T_Loss: 3.0723. Mask: 0.9801. :  22%|██▏       | 11/50 [00:08<00:27,  1.44it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8870. T_Loss: 3.0817. Mask: 0.9818. :  22%|██▏       | 11/50 [00:08<00:27,  1.44it/s]Train Iter: 912/1000. LR: 0.0203. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8870. T_Loss: 3.0817. Mask: 0.9818. :  24%|██▍       | 12/50 [00:08<00:22,  1.68it/s]total : 1000  current step :  910
total : 1000  current step :  911
total : 1000  current step :  912
Train Iter: 913/1000. LR: 0.0199. Data: 0.52s. Batch: 0.74s. S_Loss: 0.8857. T_Loss: 3.0767. Mask: 0.9814. :  24%|██▍       | 12/50 [00:09<00:22,  1.68it/s]Train Iter: 913/1000. LR: 0.0199. Data: 0.52s. Batch: 0.74s. S_Loss: 0.8857. T_Loss: 3.0767. Mask: 0.9814. :  26%|██▌       | 13/50 [00:09<00:29,  1.27it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.49s. Batch: 0.72s. S_Loss: 0.8850. T_Loss: 3.0811. Mask: 0.9824. :  26%|██▌       | 13/50 [00:10<00:29,  1.27it/s]Train Iter: 914/1000. LR: 0.0195. Data: 0.49s. Batch: 0.72s. S_Loss: 0.8850. T_Loss: 3.0811. Mask: 0.9824. :  28%|██▊       | 14/50 [00:10<00:23,  1.50it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.46s. Batch: 0.70s. S_Loss: 0.8836. T_Loss: 3.0619. Mask: 0.9823. :  28%|██▊       | 14/50 [00:10<00:23,  1.50it/s]Train Iter: 915/1000. LR: 0.0192. Data: 0.46s. Batch: 0.70s. S_Loss: 0.8836. T_Loss: 3.0619. Mask: 0.9823. :  30%|███       | 15/50 [00:10<00:20,  1.72it/s]total : 1000  current step :  913
total : 1000  current step :  914
total : 1000  current step :  915
Train Iter: 916/1000. LR: 0.0188. Data: 0.49s. Batch: 0.72s. S_Loss: 0.8831. T_Loss: 3.0566. Mask: 0.9819. :  30%|███       | 15/50 [00:11<00:20,  1.72it/s]Train Iter: 916/1000. LR: 0.0188. Data: 0.49s. Batch: 0.72s. S_Loss: 0.8831. T_Loss: 3.0566. Mask: 0.9819. :  32%|███▏      | 16/50 [00:11<00:25,  1.35it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8833. T_Loss: 3.0430. Mask: 0.9821. :  32%|███▏      | 16/50 [00:11<00:25,  1.35it/s]Train Iter: 917/1000. LR: 0.0184. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8833. T_Loss: 3.0430. Mask: 0.9821. :  34%|███▍      | 17/50 [00:11<00:20,  1.61it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8826. T_Loss: 3.0373. Mask: 0.9822. :  34%|███▍      | 17/50 [00:12<00:20,  1.61it/s]Train Iter: 918/1000. LR: 0.0180. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8826. T_Loss: 3.0373. Mask: 0.9822. :  36%|███▌      | 18/50 [00:12<00:17,  1.86it/s]total : 1000  current step :  916
total : 1000  current step :  917
total : 1000  current step :  918
Train Iter: 919/1000. LR: 0.0176. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8818. T_Loss: 3.0294. Mask: 0.9817. :  36%|███▌      | 18/50 [00:13<00:17,  1.86it/s]Train Iter: 919/1000. LR: 0.0176. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8818. T_Loss: 3.0294. Mask: 0.9817. :  38%|███▊      | 19/50 [00:13<00:22,  1.38it/s]total : 1000  current step :  919
Train Iter: 920/1000. LR: 0.0173. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8809. T_Loss: 3.0211. Mask: 0.9814. :  38%|███▊      | 19/50 [00:14<00:22,  1.38it/s]Train Iter: 920/1000. LR: 0.0173. Data: 0.47s. Batch: 0.71s. S_Loss: 0.8809. T_Loss: 3.0211. Mask: 0.9814. :  40%|████      | 20/50 [00:14<00:22,  1.36it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8792. T_Loss: 3.0131. Mask: 0.9818. :  40%|████      | 20/50 [00:14<00:22,  1.36it/s]Train Iter: 921/1000. LR: 0.0169. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8792. T_Loss: 3.0131. Mask: 0.9818. :  42%|████▏     | 21/50 [00:14<00:20,  1.44it/s]total : 1000  current step :  920
total : 1000  current step :  921
Train Iter: 922/1000. LR: 0.0165. Data: 0.49s. Batch: 0.72s. S_Loss: 0.8799. T_Loss: 3.0175. Mask: 0.9817. :  42%|████▏     | 21/50 [00:15<00:20,  1.44it/s]Train Iter: 922/1000. LR: 0.0165. Data: 0.49s. Batch: 0.72s. S_Loss: 0.8799. T_Loss: 3.0175. Mask: 0.9817. :  44%|████▍     | 22/50 [00:15<00:23,  1.21it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8794. T_Loss: 3.0066. Mask: 0.9817. :  44%|████▍     | 22/50 [00:16<00:23,  1.21it/s]Train Iter: 923/1000. LR: 0.0162. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8794. T_Loss: 3.0066. Mask: 0.9817. :  46%|████▌     | 23/50 [00:16<00:17,  1.55it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8805. T_Loss: 3.0197. Mask: 0.9818. :  46%|████▌     | 23/50 [00:16<00:17,  1.55it/s]Train Iter: 924/1000. LR: 0.0158. Data: 0.46s. Batch: 0.69s. S_Loss: 0.8805. T_Loss: 3.0197. Mask: 0.9818. :  48%|████▊     | 24/50 [00:16<00:14,  1.76it/s]total : 1000  current step :  922
total : 1000  current step :  923
total : 1000  current step :  924
Train Iter: 925/1000. LR: 0.0154. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8814. T_Loss: 3.0316. Mask: 0.9817. :  48%|████▊     | 24/50 [00:17<00:14,  1.76it/s]Train Iter: 925/1000. LR: 0.0154. Data: 0.47s. Batch: 0.70s. S_Loss: 0.8814. T_Loss: 3.0316. Mask: 0.9817. :  50%|█████     | 25/50 [00:17<00:17,  1.40it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8830. T_Loss: 3.0316. Mask: 0.9815. :  50%|█████     | 25/50 [00:17<00:17,  1.40it/s]Train Iter: 926/1000. LR: 0.0151. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8830. T_Loss: 3.0316. Mask: 0.9815. :  52%|█████▏    | 26/50 [00:17<00:13,  1.74it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8843. T_Loss: 3.0421. Mask: 0.9816. :  52%|█████▏    | 26/50 [00:18<00:13,  1.74it/s]Train Iter: 927/1000. LR: 0.0147. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8843. T_Loss: 3.0421. Mask: 0.9816. :  54%|█████▍    | 27/50 [00:18<00:11,  1.97it/s]total : 1000  current step :  925
total : 1000  current step :  926
total : 1000  current step :  927
Train Iter: 928/1000. LR: 0.0144. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8837. T_Loss: 3.0368. Mask: 0.9817. :  54%|█████▍    | 27/50 [00:19<00:11,  1.97it/s]Train Iter: 928/1000. LR: 0.0144. Data: 0.46s. Batch: 0.68s. S_Loss: 0.8837. T_Loss: 3.0368. Mask: 0.9817. :  56%|█████▌    | 28/50 [00:19<00:14,  1.52it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8835. T_Loss: 3.0322. Mask: 0.9818. :  56%|█████▌    | 28/50 [00:19<00:14,  1.52it/s]Train Iter: 929/1000. LR: 0.0140. Data: 0.45s. Batch: 0.67s. S_Loss: 0.8835. T_Loss: 3.0322. Mask: 0.9818. :  58%|█████▊    | 29/50 [00:19<00:12,  1.71it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.44s. Batch: 0.66s. S_Loss: 0.8836. T_Loss: 3.0370. Mask: 0.9819. :  58%|█████▊    | 29/50 [00:19<00:12,  1.71it/s]Train Iter: 930/1000. LR: 0.0137. Data: 0.44s. Batch: 0.66s. S_Loss: 0.8836. T_Loss: 3.0370. Mask: 0.9819. :  60%|██████    | 30/50 [00:19<00:10,  1.94it/s]total : 1000  current step :  928
total : 1000  current step :  929
total : 1000  current step :  930
Train Iter: 931/1000. LR: 0.0133. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8821. T_Loss: 3.0327. Mask: 0.9819. :  60%|██████    | 30/50 [00:21<00:10,  1.94it/s]Train Iter: 931/1000. LR: 0.0133. Data: 0.45s. Batch: 0.68s. S_Loss: 0.8821. T_Loss: 3.0327. Mask: 0.9819. :  62%|██████▏   | 31/50 [00:21<00:14,  1.36it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8821. T_Loss: 3.0327. Mask: 0.9814. :  62%|██████▏   | 31/50 [00:21<00:14,  1.36it/s]Train Iter: 932/1000. LR: 0.0130. Data: 0.44s. Batch: 0.67s. S_Loss: 0.8821. T_Loss: 3.0327. Mask: 0.9814. :  64%|██████▍   | 32/50 [00:21<00:11,  1.60it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.43s. Batch: 0.66s. S_Loss: 0.8813. T_Loss: 3.0272. Mask: 0.9815. :  64%|██████▍   | 32/50 [00:21<00:11,  1.60it/s]Train Iter: 933/1000. LR: 0.0126. Data: 0.43s. Batch: 0.66s. S_Loss: 0.8813. T_Loss: 3.0272. Mask: 0.9815. :  66%|██████▌   | 33/50 [00:21<00:09,  1.84it/s]total : 1000  current step :  931
total : 1000  current step :  932
total : 1000  current step :  933
Train Iter: 934/1000. LR: 0.0123. Data: 0.44s. Batch: 0.68s. S_Loss: 0.8824. T_Loss: 3.0318. Mask: 0.9812. :  66%|██████▌   | 33/50 [00:23<00:09,  1.84it/s]Train Iter: 934/1000. LR: 0.0123. Data: 0.44s. Batch: 0.68s. S_Loss: 0.8824. T_Loss: 3.0318. Mask: 0.9812. :  68%|██████▊   | 34/50 [00:23<00:11,  1.36it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.43s. Batch: 0.67s. S_Loss: 0.8827. T_Loss: 3.0354. Mask: 0.9812. :  68%|██████▊   | 34/50 [00:23<00:11,  1.36it/s]Train Iter: 935/1000. LR: 0.0119. Data: 0.43s. Batch: 0.67s. S_Loss: 0.8827. T_Loss: 3.0354. Mask: 0.9812. :  70%|███████   | 35/50 [00:23<00:09,  1.59it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.41s. Batch: 0.66s. S_Loss: 0.8830. T_Loss: 3.0407. Mask: 0.9816. :  70%|███████   | 35/50 [00:23<00:09,  1.59it/s]Train Iter: 936/1000. LR: 0.0116. Data: 0.41s. Batch: 0.66s. S_Loss: 0.8830. T_Loss: 3.0407. Mask: 0.9816. :  72%|███████▏  | 36/50 [00:23<00:07,  1.93it/s]total : 1000  current step :  934
total : 1000  current step :  935
total : 1000  current step :  936
Train Iter: 937/1000. LR: 0.0113. Data: 0.42s. Batch: 0.67s. S_Loss: 0.8836. T_Loss: 3.0387. Mask: 0.9816. :  72%|███████▏  | 36/50 [00:24<00:07,  1.93it/s]Train Iter: 937/1000. LR: 0.0113. Data: 0.42s. Batch: 0.67s. S_Loss: 0.8836. T_Loss: 3.0387. Mask: 0.9816. :  74%|███████▍  | 37/50 [00:24<00:08,  1.52it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.41s. Batch: 0.66s. S_Loss: 0.8829. T_Loss: 3.0369. Mask: 0.9818. :  74%|███████▍  | 37/50 [00:25<00:08,  1.52it/s]Train Iter: 938/1000. LR: 0.0109. Data: 0.41s. Batch: 0.66s. S_Loss: 0.8829. T_Loss: 3.0369. Mask: 0.9818. :  76%|███████▌  | 38/50 [00:25<00:06,  1.83it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8827. T_Loss: 3.0447. Mask: 0.9815. :  76%|███████▌  | 38/50 [00:25<00:06,  1.83it/s]Train Iter: 939/1000. LR: 0.0106. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8827. T_Loss: 3.0447. Mask: 0.9815. :  78%|███████▊  | 39/50 [00:25<00:05,  2.13it/s]total : 1000  current step :  937
total : 1000  current step :  938
total : 1000  current step :  939
Train Iter: 940/1000. LR: 0.0103. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8830. T_Loss: 3.0457. Mask: 0.9817. :  78%|███████▊  | 39/50 [00:26<00:05,  2.13it/s]Train Iter: 940/1000. LR: 0.0103. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8830. T_Loss: 3.0457. Mask: 0.9817. :  80%|████████  | 40/50 [00:26<00:06,  1.64it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8826. T_Loss: 3.0405. Mask: 0.9819. :  80%|████████  | 40/50 [00:26<00:06,  1.64it/s]Train Iter: 941/1000. LR: 0.0100. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8826. T_Loss: 3.0405. Mask: 0.9819. :  82%|████████▏ | 41/50 [00:26<00:04,  1.93it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8829. T_Loss: 3.0469. Mask: 0.9816. :  82%|████████▏ | 41/50 [00:26<00:04,  1.93it/s]Train Iter: 942/1000. LR: 0.0097. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8829. T_Loss: 3.0469. Mask: 0.9816. :  84%|████████▍ | 42/50 [00:26<00:03,  2.11it/s]total : 1000  current step :  940
total : 1000  current step :  941
total : 1000  current step :  942
Train Iter: 943/1000. LR: 0.0094. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8837. T_Loss: 3.0634. Mask: 0.9817. :  84%|████████▍ | 42/50 [00:28<00:03,  2.11it/s]Train Iter: 943/1000. LR: 0.0094. Data: 0.41s. Batch: 0.65s. S_Loss: 0.8837. T_Loss: 3.0634. Mask: 0.9817. :  86%|████████▌ | 43/50 [00:28<00:04,  1.46it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8843. T_Loss: 3.0699. Mask: 0.9818. :  86%|████████▌ | 43/50 [00:28<00:04,  1.46it/s]Train Iter: 944/1000. LR: 0.0091. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8843. T_Loss: 3.0699. Mask: 0.9818. :  88%|████████▊ | 44/50 [00:28<00:03,  1.79it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8846. T_Loss: 3.0696. Mask: 0.9820. :  88%|████████▊ | 44/50 [00:28<00:03,  1.79it/s]Train Iter: 945/1000. LR: 0.0088. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8846. T_Loss: 3.0696. Mask: 0.9820. :  90%|█████████ | 45/50 [00:28<00:02,  2.03it/s]total : 1000  current step :  943
total : 1000  current step :  944
total : 1000  current step :  945
Train Iter: 946/1000. LR: 0.0085. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8843. T_Loss: 3.0703. Mask: 0.9817. :  90%|█████████ | 45/50 [00:29<00:02,  2.03it/s]Train Iter: 946/1000. LR: 0.0085. Data: 0.41s. Batch: 0.64s. S_Loss: 0.8843. T_Loss: 3.0703. Mask: 0.9817. :  92%|█████████▏| 46/50 [00:29<00:02,  1.53it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8843. T_Loss: 3.0700. Mask: 0.9820. :  92%|█████████▏| 46/50 [00:29<00:02,  1.53it/s]Train Iter: 947/1000. LR: 0.0082. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8843. T_Loss: 3.0700. Mask: 0.9820. :  94%|█████████▍| 47/50 [00:29<00:01,  1.94it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.39s. Batch: 0.63s. S_Loss: 0.8840. T_Loss: 3.0644. Mask: 0.9822. :  94%|█████████▍| 47/50 [00:30<00:01,  1.94it/s]Train Iter: 948/1000. LR: 0.0079. Data: 0.39s. Batch: 0.63s. S_Loss: 0.8840. T_Loss: 3.0644. Mask: 0.9822. :  96%|█████████▌| 48/50 [00:30<00:00,  2.12it/s]total : 1000  current step :  946
total : 1000  current step :  947
total : 1000  current step :  948
Train Iter: 949/1000. LR: 0.0076. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8843. T_Loss: 3.0727. Mask: 0.9823. :  96%|█████████▌| 48/50 [00:31<00:00,  2.12it/s]Train Iter: 949/1000. LR: 0.0076. Data: 0.40s. Batch: 0.64s. S_Loss: 0.8843. T_Loss: 3.0727. Mask: 0.9823. :  98%|█████████▊| 49/50 [00:31<00:00,  1.59it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8847. T_Loss: 3.0717. Mask: 0.9823. :  98%|█████████▊| 49/50 [00:31<00:00,  1.59it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8847. T_Loss: 3.0717. Mask: 0.9823. : 100%|██████████| 50/50 [00:31<00:00,  1.89it/s]Train Iter: 950/1000. LR: 0.0073. Data: 0.40s. Batch: 0.63s. S_Loss: 0.8847. T_Loss: 3.0717. Mask: 0.9823. : 100%|██████████| 50/50 [00:31<00:00,  1.58it/s]
total : 1000  current step :  949
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8464. top1: 96.88. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8464. top1: 96.88. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8466. top1: 96.68. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.74it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8466. top1: 96.68. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.79it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8516. top1: 96.61. top5: 99.87. :  25%|██▌       | 2/8 [00:00<00:02,  2.79it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8516. top1: 96.61. top5: 99.87. :  38%|███▊      | 3/8 [00:00<00:01,  3.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8526. top1: 96.58. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  3.41it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8526. top1: 96.58. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8737. top1: 95.00. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.65it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8737. top1: 95.00. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8894. top1: 93.82. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8894. top1: 93.82. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9033. top1: 92.86. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.21it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.9033. top1: 92.86. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9144. top1: 92.20. top5: 99.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9144. top1: 92.20. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.80it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.9144. top1: 92.20. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]
total : 1000  current step :  950
  0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.26s. S_Loss: 0.8890. T_Loss: 3.4989. Mask: 0.9844. :   0%|          | 0/50 [00:00<?, ?it/s]Train Iter: 951/1000. LR: 0.0070. Data: 0.01s. Batch: 0.26s. S_Loss: 0.8890. T_Loss: 3.4989. Mask: 0.9844. :   2%|▏         | 1/50 [00:00<00:12,  3.88it/s]total : 1000  current step :  951
Train Iter: 952/1000. LR: 0.0068. Data: 0.42s. Batch: 0.66s. S_Loss: 0.8948. T_Loss: 3.3718. Mask: 0.9844. :   2%|▏         | 1/50 [00:01<00:12,  3.88it/s]Train Iter: 952/1000. LR: 0.0068. Data: 0.42s. Batch: 0.66s. S_Loss: 0.8948. T_Loss: 3.3718. Mask: 0.9844. :   4%|▍         | 2/50 [00:01<00:35,  1.37it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.31s. Batch: 0.55s. S_Loss: 0.8861. T_Loss: 3.2922. Mask: 0.9870. :   4%|▍         | 2/50 [00:01<00:35,  1.37it/s]Train Iter: 953/1000. LR: 0.0065. Data: 0.31s. Batch: 0.55s. S_Loss: 0.8861. T_Loss: 3.2922. Mask: 0.9870. :   6%|▌         | 3/50 [00:01<00:25,  1.81it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.27s. Batch: 0.52s. S_Loss: 0.8854. T_Loss: 3.1391. Mask: 0.9834. :   6%|▌         | 3/50 [00:02<00:25,  1.81it/s]Train Iter: 954/1000. LR: 0.0062. Data: 0.27s. Batch: 0.52s. S_Loss: 0.8854. T_Loss: 3.1391. Mask: 0.9834. :   8%|▊         | 4/50 [00:02<00:22,  2.00it/s]total : 1000  current step :  952
total : 1000  current step :  953
total : 1000  current step :  954
Train Iter: 955/1000. LR: 0.0060. Data: 0.40s. Batch: 0.65s. S_Loss: 0.8771. T_Loss: 3.0532. Mask: 0.9820. :   8%|▊         | 4/50 [00:03<00:22,  2.00it/s]Train Iter: 955/1000. LR: 0.0060. Data: 0.40s. Batch: 0.65s. S_Loss: 0.8771. T_Loss: 3.0532. Mask: 0.9820. :  10%|█         | 5/50 [00:03<00:33,  1.34it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.34s. Batch: 0.59s. S_Loss: 0.8701. T_Loss: 3.0420. Mask: 0.9831. :  10%|█         | 5/50 [00:03<00:33,  1.34it/s]Train Iter: 956/1000. LR: 0.0057. Data: 0.34s. Batch: 0.59s. S_Loss: 0.8701. T_Loss: 3.0420. Mask: 0.9831. :  12%|█▏        | 6/50 [00:03<00:25,  1.70it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.30s. Batch: 0.54s. S_Loss: 0.8721. T_Loss: 3.0708. Mask: 0.9849. :  12%|█▏        | 6/50 [00:03<00:25,  1.70it/s]Train Iter: 957/1000. LR: 0.0055. Data: 0.30s. Batch: 0.54s. S_Loss: 0.8721. T_Loss: 3.0708. Mask: 0.9849. :  14%|█▍        | 7/50 [00:03<00:20,  2.12it/s]total : 1000  current step :  955
total : 1000  current step :  956
total : 1000  current step :  957
Train Iter: 958/1000. LR: 0.0052. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8739. T_Loss: 3.1050. Mask: 0.9844. :  14%|█▍        | 7/50 [00:04<00:20,  2.12it/s]Train Iter: 958/1000. LR: 0.0052. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8739. T_Loss: 3.1050. Mask: 0.9844. :  16%|█▌        | 8/50 [00:04<00:27,  1.51it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8730. T_Loss: 3.0595. Mask: 0.9835. :  16%|█▌        | 8/50 [00:05<00:27,  1.51it/s]Train Iter: 959/1000. LR: 0.0050. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8730. T_Loss: 3.0595. Mask: 0.9835. :  18%|█▊        | 9/50 [00:05<00:23,  1.75it/s]total : 1000  current step :  958
total : 1000  current step :  959
Train Iter: 960/1000. LR: 0.0048. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8724. T_Loss: 3.0253. Mask: 0.9836. :  18%|█▊        | 9/50 [00:05<00:23,  1.75it/s]Train Iter: 960/1000. LR: 0.0048. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8724. T_Loss: 3.0253. Mask: 0.9836. :  20%|██        | 10/50 [00:05<00:22,  1.77it/s]total : 1000  current step :  960
Train Iter: 961/1000. LR: 0.0045. Data: 0.39s. Batch: 0.63s. S_Loss: 0.8700. T_Loss: 3.0149. Mask: 0.9847. :  20%|██        | 10/50 [00:06<00:22,  1.77it/s]Train Iter: 961/1000. LR: 0.0045. Data: 0.39s. Batch: 0.63s. S_Loss: 0.8700. T_Loss: 3.0149. Mask: 0.9847. :  22%|██▏       | 11/50 [00:06<00:29,  1.34it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.36s. Batch: 0.60s. S_Loss: 0.8698. T_Loss: 3.0224. Mask: 0.9840. :  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]Train Iter: 962/1000. LR: 0.0043. Data: 0.36s. Batch: 0.60s. S_Loss: 0.8698. T_Loss: 3.0224. Mask: 0.9840. :  24%|██▍       | 12/50 [00:07<00:22,  1.66it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.33s. Batch: 0.57s. S_Loss: 0.8697. T_Loss: 3.0437. Mask: 0.9844. :  24%|██▍       | 12/50 [00:07<00:22,  1.66it/s]Train Iter: 963/1000. LR: 0.0041. Data: 0.33s. Batch: 0.57s. S_Loss: 0.8697. T_Loss: 3.0437. Mask: 0.9844. :  26%|██▌       | 13/50 [00:07<00:17,  2.12it/s]total : 1000  current step :  961
total : 1000  current step :  962
total : 1000  current step :  963
Train Iter: 964/1000. LR: 0.0039. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8703. T_Loss: 3.0394. Mask: 0.9838. :  26%|██▌       | 13/50 [00:08<00:17,  2.12it/s]Train Iter: 964/1000. LR: 0.0039. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8703. T_Loss: 3.0394. Mask: 0.9838. :  28%|██▊       | 14/50 [00:08<00:23,  1.51it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8748. T_Loss: 3.0565. Mask: 0.9833. :  28%|██▊       | 14/50 [00:08<00:23,  1.51it/s]Train Iter: 965/1000. LR: 0.0037. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8748. T_Loss: 3.0565. Mask: 0.9833. :  30%|███       | 15/50 [00:08<00:19,  1.77it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8750. T_Loss: 3.0541. Mask: 0.9839. :  30%|███       | 15/50 [00:09<00:19,  1.77it/s]Train Iter: 966/1000. LR: 0.0035. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8750. T_Loss: 3.0541. Mask: 0.9839. :  32%|███▏      | 16/50 [00:09<00:16,  2.12it/s]total : 1000  current step :  964
total : 1000  current step :  965
total : 1000  current step :  966
Train Iter: 967/1000. LR: 0.0033. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8743. T_Loss: 3.0700. Mask: 0.9844. :  32%|███▏      | 16/50 [00:10<00:16,  2.12it/s]Train Iter: 967/1000. LR: 0.0033. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8743. T_Loss: 3.0700. Mask: 0.9844. :  34%|███▍      | 17/50 [00:10<00:21,  1.57it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8746. T_Loss: 3.0785. Mask: 0.9848. :  34%|███▍      | 17/50 [00:10<00:21,  1.57it/s]Train Iter: 968/1000. LR: 0.0031. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8746. T_Loss: 3.0785. Mask: 0.9848. :  36%|███▌      | 18/50 [00:10<00:16,  1.92it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8753. T_Loss: 3.0926. Mask: 0.9850. :  36%|███▌      | 18/50 [00:10<00:16,  1.92it/s]Train Iter: 969/1000. LR: 0.0029. Data: 0.33s. Batch: 0.56s. S_Loss: 0.8753. T_Loss: 3.0926. Mask: 0.9850. :  38%|███▊      | 19/50 [00:10<00:14,  2.11it/s]total : 1000  current step :  967
total : 1000  current step :  968
total : 1000  current step :  969
Train Iter: 970/1000. LR: 0.0027. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8744. T_Loss: 3.0887. Mask: 0.9852. :  38%|███▊      | 19/50 [00:11<00:14,  2.11it/s]Train Iter: 970/1000. LR: 0.0027. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8744. T_Loss: 3.0887. Mask: 0.9852. :  40%|████      | 20/50 [00:11<00:19,  1.53it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8740. T_Loss: 3.0803. Mask: 0.9853. :  40%|████      | 20/50 [00:12<00:19,  1.53it/s]Train Iter: 971/1000. LR: 0.0025. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8740. T_Loss: 3.0803. Mask: 0.9853. :  42%|████▏     | 21/50 [00:12<00:16,  1.73it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8737. T_Loss: 3.0851. Mask: 0.9854. :  42%|████▏     | 21/50 [00:12<00:16,  1.73it/s]Train Iter: 972/1000. LR: 0.0024. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8737. T_Loss: 3.0851. Mask: 0.9854. :  44%|████▍     | 22/50 [00:12<00:14,  1.96it/s]total : 1000  current step :  970
total : 1000  current step :  971
total : 1000  current step :  972
Train Iter: 973/1000. LR: 0.0022. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8729. T_Loss: 3.0844. Mask: 0.9851. :  44%|████▍     | 22/50 [00:13<00:14,  1.96it/s]Train Iter: 973/1000. LR: 0.0022. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8729. T_Loss: 3.0844. Mask: 0.9851. :  46%|████▌     | 23/50 [00:13<00:17,  1.56it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8715. T_Loss: 3.0743. Mask: 0.9849. :  46%|████▌     | 23/50 [00:13<00:17,  1.56it/s]Train Iter: 974/1000. LR: 0.0021. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8715. T_Loss: 3.0743. Mask: 0.9849. :  48%|████▊     | 24/50 [00:13<00:15,  1.73it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8703. T_Loss: 3.0772. Mask: 0.9847. :  48%|████▊     | 24/50 [00:14<00:15,  1.73it/s]Train Iter: 975/1000. LR: 0.0019. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8703. T_Loss: 3.0772. Mask: 0.9847. :  50%|█████     | 25/50 [00:14<00:12,  2.05it/s]total : 1000  current step :  973
total : 1000  current step :  974
total : 1000  current step :  975
Train Iter: 976/1000. LR: 0.0018. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8710. T_Loss: 3.0702. Mask: 0.9841. :  50%|█████     | 25/50 [00:15<00:12,  2.05it/s]Train Iter: 976/1000. LR: 0.0018. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8710. T_Loss: 3.0702. Mask: 0.9841. :  52%|█████▏    | 26/50 [00:15<00:16,  1.49it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8728. T_Loss: 3.0925. Mask: 0.9844. :  52%|█████▏    | 26/50 [00:15<00:16,  1.49it/s]Train Iter: 977/1000. LR: 0.0016. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8728. T_Loss: 3.0925. Mask: 0.9844. :  54%|█████▍    | 27/50 [00:15<00:12,  1.77it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8748. T_Loss: 3.1176. Mask: 0.9847. :  54%|█████▍    | 27/50 [00:16<00:12,  1.77it/s]Train Iter: 978/1000. LR: 0.0015. Data: 0.34s. Batch: 0.57s. S_Loss: 0.8748. T_Loss: 3.1176. Mask: 0.9847. :  56%|█████▌    | 28/50 [00:16<00:11,  1.88it/s]total : 1000  current step :  976
total : 1000  current step :  977
total : 1000  current step :  978
Train Iter: 979/1000. LR: 0.0013. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8739. T_Loss: 3.1201. Mask: 0.9850. :  56%|█████▌    | 28/50 [00:17<00:11,  1.88it/s]Train Iter: 979/1000. LR: 0.0013. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8739. T_Loss: 3.1201. Mask: 0.9850. :  58%|█████▊    | 29/50 [00:17<00:14,  1.43it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8736. T_Loss: 3.1139. Mask: 0.9849. :  58%|█████▊    | 29/50 [00:17<00:14,  1.43it/s]Train Iter: 980/1000. LR: 0.0012. Data: 0.36s. Batch: 0.58s. S_Loss: 0.8736. T_Loss: 3.1139. Mask: 0.9849. :  60%|██████    | 30/50 [00:17<00:12,  1.59it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8746. T_Loss: 3.1134. Mask: 0.9846. :  60%|██████    | 30/50 [00:17<00:12,  1.59it/s]Train Iter: 981/1000. LR: 0.0011. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8746. T_Loss: 3.1134. Mask: 0.9846. :  62%|██████▏   | 31/50 [00:17<00:10,  1.87it/s]total : 1000  current step :  979
total : 1000  current step :  980
total : 1000  current step :  981
Train Iter: 982/1000. LR: 0.0010. Data: 0.37s. Batch: 0.59s. S_Loss: 0.8737. T_Loss: 3.1113. Mask: 0.9846. :  62%|██████▏   | 31/50 [00:19<00:10,  1.87it/s]Train Iter: 982/1000. LR: 0.0010. Data: 0.37s. Batch: 0.59s. S_Loss: 0.8737. T_Loss: 3.1113. Mask: 0.9846. :  64%|██████▍   | 32/50 [00:19<00:12,  1.41it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8741. T_Loss: 3.1182. Mask: 0.9847. :  64%|██████▍   | 32/50 [00:19<00:12,  1.41it/s]Train Iter: 983/1000. LR: 0.0009. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8741. T_Loss: 3.1182. Mask: 0.9847. :  66%|██████▌   | 33/50 [00:19<00:10,  1.62it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8741. T_Loss: 3.1183. Mask: 0.9848. :  66%|██████▌   | 33/50 [00:19<00:10,  1.62it/s]Train Iter: 984/1000. LR: 0.0008. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8741. T_Loss: 3.1183. Mask: 0.9848. :  68%|██████▊   | 34/50 [00:19<00:08,  1.94it/s]total : 1000  current step :  982
total : 1000  current step :  983
total : 1000  current step :  984
Train Iter: 985/1000. LR: 0.0007. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8740. T_Loss: 3.1154. Mask: 0.9846. :  68%|██████▊   | 34/50 [00:20<00:08,  1.94it/s]Train Iter: 985/1000. LR: 0.0007. Data: 0.37s. Batch: 0.60s. S_Loss: 0.8740. T_Loss: 3.1154. Mask: 0.9846. :  70%|███████   | 35/50 [00:20<00:11,  1.35it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8744. T_Loss: 3.1141. Mask: 0.9847. :  70%|███████   | 35/50 [00:21<00:11,  1.35it/s]Train Iter: 986/1000. LR: 0.0006. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8744. T_Loss: 3.1141. Mask: 0.9847. :  72%|███████▏  | 36/50 [00:21<00:08,  1.63it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8746. T_Loss: 3.1117. Mask: 0.9845. :  72%|███████▏  | 36/50 [00:21<00:08,  1.63it/s]Train Iter: 987/1000. LR: 0.0005. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8746. T_Loss: 3.1117. Mask: 0.9845. :  74%|███████▍  | 37/50 [00:21<00:06,  2.09it/s]total : 1000  current step :  985
total : 1000  current step :  986
total : 1000  current step :  987
Train Iter: 988/1000. LR: 0.0004. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8746. T_Loss: 3.1051. Mask: 0.9846. :  74%|███████▍  | 37/50 [00:22<00:06,  2.09it/s]Train Iter: 988/1000. LR: 0.0004. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8746. T_Loss: 3.1051. Mask: 0.9846. :  76%|███████▌  | 38/50 [00:22<00:07,  1.51it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8746. T_Loss: 3.1019. Mask: 0.9849. :  76%|███████▌  | 38/50 [00:22<00:07,  1.51it/s]Train Iter: 989/1000. LR: 0.0004. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8746. T_Loss: 3.1019. Mask: 0.9849. :  78%|███████▊  | 39/50 [00:22<00:06,  1.81it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8740. T_Loss: 3.0970. Mask: 0.9851. :  78%|███████▊  | 39/50 [00:23<00:06,  1.81it/s]Train Iter: 990/1000. LR: 0.0003. Data: 0.35s. Batch: 0.58s. S_Loss: 0.8740. T_Loss: 3.0970. Mask: 0.9851. :  80%|████████  | 40/50 [00:23<00:04,  2.14it/s]total : 1000  current step :  988
total : 1000  current step :  989
total : 1000  current step :  990
Train Iter: 991/1000. LR: 0.0002. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8742. T_Loss: 3.1041. Mask: 0.9850. :  80%|████████  | 40/50 [00:24<00:04,  2.14it/s]Train Iter: 991/1000. LR: 0.0002. Data: 0.36s. Batch: 0.59s. S_Loss: 0.8742. T_Loss: 3.1041. Mask: 0.9850. :  82%|████████▏ | 41/50 [00:24<00:06,  1.43it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8749. T_Loss: 3.1058. Mask: 0.9851. :  82%|████████▏ | 41/50 [00:24<00:06,  1.43it/s]Train Iter: 992/1000. LR: 0.0002. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8749. T_Loss: 3.1058. Mask: 0.9851. :  84%|████████▍ | 42/50 [00:24<00:04,  1.67it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8749. T_Loss: 3.1010. Mask: 0.9847. :  84%|████████▍ | 42/50 [00:24<00:04,  1.67it/s]Train Iter: 993/1000. LR: 0.0002. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8749. T_Loss: 3.1010. Mask: 0.9847. :  86%|████████▌ | 43/50 [00:24<00:03,  2.03it/s]total : 1000  current step :  991
total : 1000  current step :  992
total : 1000  current step :  993
Train Iter: 994/1000. LR: 0.0001. Data: 0.36s. Batch: 0.60s. S_Loss: 0.8751. T_Loss: 3.1075. Mask: 0.9849. :  86%|████████▌ | 43/50 [00:26<00:03,  2.03it/s]Train Iter: 994/1000. LR: 0.0001. Data: 0.36s. Batch: 0.60s. S_Loss: 0.8751. T_Loss: 3.1075. Mask: 0.9849. :  88%|████████▊ | 44/50 [00:26<00:04,  1.35it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8755. T_Loss: 3.1116. Mask: 0.9848. :  88%|████████▊ | 44/50 [00:26<00:04,  1.35it/s]Train Iter: 995/1000. LR: 0.0001. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8755. T_Loss: 3.1116. Mask: 0.9848. :  90%|█████████ | 45/50 [00:26<00:03,  1.60it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8752. T_Loss: 3.1095. Mask: 0.9845. :  90%|█████████ | 45/50 [00:26<00:03,  1.60it/s]Train Iter: 996/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8752. T_Loss: 3.1095. Mask: 0.9845. :  92%|█████████▏| 46/50 [00:26<00:02,  1.99it/s]total : 1000  current step :  994
total : 1000  current step :  995
total : 1000  current step :  996
Train Iter: 997/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8762. T_Loss: 3.1185. Mask: 0.9847. :  92%|█████████▏| 46/50 [00:27<00:02,  1.99it/s]Train Iter: 997/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8762. T_Loss: 3.1185. Mask: 0.9847. :  94%|█████████▍| 47/50 [00:27<00:01,  1.52it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8761. T_Loss: 3.1192. Mask: 0.9849. :  94%|█████████▍| 47/50 [00:28<00:01,  1.52it/s]Train Iter: 998/1000. LR: 0.0000. Data: 0.35s. Batch: 0.59s. S_Loss: 0.8761. T_Loss: 3.1192. Mask: 0.9849. :  96%|█████████▌| 48/50 [00:28<00:01,  1.70it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8764. T_Loss: 3.1284. Mask: 0.9849. :  96%|█████████▌| 48/50 [00:28<00:01,  1.70it/s]Train Iter: 999/1000. LR: 0.0000. Data: 0.34s. Batch: 0.58s. S_Loss: 0.8764. T_Loss: 3.1284. Mask: 0.9849. :  98%|█████████▊| 49/50 [00:28<00:00,  2.03it/s]total : 1000  current step :  997
total : 1000  current step :  998
total : 1000  current step :  999
Train Iter: 1000/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 0.8768. T_Loss: 3.1349. Mask: 0.9848. :  98%|█████████▊| 49/50 [00:30<00:00,  2.03it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 0.8768. T_Loss: 3.1349. Mask: 0.9848. : 100%|██████████| 50/50 [00:30<00:00,  1.21it/s]Train Iter: 1000/1000. LR: 0.0000. Data: 0.36s. Batch: 0.60s. S_Loss: 0.8768. T_Loss: 3.1349. Mask: 0.9848. : 100%|██████████| 50/50 [00:30<00:00,  1.66it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8526. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8526. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.47it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8540. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.47it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8540. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8592. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8592. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8596. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8596. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8730. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8730. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8831. top1: 94.21. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8831. top1: 94.21. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8926. top1: 93.25. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8926. top1: 93.25. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9010. top1: 92.85. top5: 99.55. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9010. top1: 92.85. top5: 99.55. : 100%|██████████| 8/8 [00:02<00:00,  3.79it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9010. top1: 92.85. top5: 99.55. : 100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.62s. Batch: 0.69s. Loss: 0.8506. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  1/70. Data: 0.62s. Batch: 0.69s. Loss: 0.8506. :  33%|███▎      | 1/3 [00:00<00:01,  1.46it/s]Finetune Epoch:  1/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8240. :  33%|███▎      | 1/3 [00:01<00:01,  1.46it/s]Finetune Epoch:  1/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8240. :  67%|██████▋   | 2/3 [00:01<00:00,  2.13it/s]Finetune Epoch:  1/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8139. :  67%|██████▋   | 2/3 [00:01<00:00,  2.13it/s]Finetune Epoch:  1/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8139. : 100%|██████████| 3/3 [00:01<00:00,  2.44it/s]Finetune Epoch:  1/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8139. : 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8521. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8521. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8535. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8535. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8588. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.48it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8588. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8591. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.60it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8591. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8725. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8725. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8825. top1: 94.21. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8825. top1: 94.21. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8920. top1: 93.25. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8920. top1: 93.25. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9004. top1: 92.85. top5: 99.55. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9004. top1: 92.85. top5: 99.55. : 100%|██████████| 8/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.9004. top1: 92.85. top5: 99.55. : 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.69s. Batch: 0.79s. Loss: 0.8300. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  2/70. Data: 0.69s. Batch: 0.79s. Loss: 0.8300. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch:  2/70. Data: 0.84s. Batch: 0.93s. Loss: 0.8140. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch:  2/70. Data: 0.84s. Batch: 0.93s. Loss: 0.8140. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch:  2/70. Data: 1.00s. Batch: 1.08s. Loss: 0.8154. :  67%|██████▋   | 2/3 [00:01<00:00,  2.01it/s]Finetune Epoch:  2/70. Data: 1.00s. Batch: 1.08s. Loss: 0.8154. : 100%|██████████| 3/3 [00:01<00:00,  2.49it/s]Finetune Epoch:  2/70. Data: 1.00s. Batch: 1.08s. Loss: 0.8154. : 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8516. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8516. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8531. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.54it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8531. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.10it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8584. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.10it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8584. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8587. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8587. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.74it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8720. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.74it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8720. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8820. top1: 94.21. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8820. top1: 94.21. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8914. top1: 93.25. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8914. top1: 93.25. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8998. top1: 92.85. top5: 99.55. :  88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8998. top1: 92.85. top5: 99.55. : 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8998. top1: 92.85. top5: 99.55. : 100%|██████████| 8/8 [00:03<00:00,  2.45it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.84s. Batch: 0.94s. Loss: 0.8362. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  3/70. Data: 0.84s. Batch: 0.94s. Loss: 0.8362. :  33%|███▎      | 1/3 [00:00<00:01,  1.06it/s]Finetune Epoch:  3/70. Data: 1.01s. Batch: 1.10s. Loss: 0.8189. :  33%|███▎      | 1/3 [00:01<00:01,  1.06it/s]Finetune Epoch:  3/70. Data: 1.01s. Batch: 1.10s. Loss: 0.8189. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch:  3/70. Data: 1.20s. Batch: 1.28s. Loss: 0.8193. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch:  3/70. Data: 1.20s. Batch: 1.28s. Loss: 0.8193. : 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]Finetune Epoch:  3/70. Data: 1.20s. Batch: 1.28s. Loss: 0.8193. : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8512. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8512. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8527. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.46it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8527. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8580. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8580. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8583. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8583. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.92it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8715. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.92it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8715. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8814. top1: 94.21. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:00,  3.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8814. top1: 94.21. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.70it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8908. top1: 93.25. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.70it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8908. top1: 93.25. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8991. top1: 92.85. top5: 99.55. :  88%|████████▊ | 7/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8991. top1: 92.85. top5: 99.55. : 100%|██████████| 8/8 [00:02<00:00,  4.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8991. top1: 92.85. top5: 99.55. : 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.72s. Batch: 0.80s. Loss: 0.8190. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  4/70. Data: 0.72s. Batch: 0.80s. Loss: 0.8190. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch:  4/70. Data: 0.91s. Batch: 0.98s. Loss: 0.8134. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch:  4/70. Data: 0.91s. Batch: 0.98s. Loss: 0.8134. :  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s]Finetune Epoch:  4/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8184. :  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s]Finetune Epoch:  4/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8184. : 100%|██████████| 3/3 [00:01<00:00,  2.28it/s]Finetune Epoch:  4/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8184. : 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8508. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8508. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.47it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8524. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.47it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8524. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8577. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.24it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8577. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8580. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.77it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8580. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8711. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.14it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8711. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.15it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8809. top1: 94.21. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:00,  3.15it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8809. top1: 94.21. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8902. top1: 93.25. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.24it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8902. top1: 93.25. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8985. top1: 92.90. top5: 99.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8985. top1: 92.90. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.62it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8985. top1: 92.90. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.55s. Batch: 0.60s. Loss: 0.8213. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  5/70. Data: 0.55s. Batch: 0.60s. Loss: 0.8213. :  33%|███▎      | 1/3 [00:00<00:01,  1.66it/s]Finetune Epoch:  5/70. Data: 0.72s. Batch: 0.77s. Loss: 0.8099. :  33%|███▎      | 1/3 [00:00<00:01,  1.66it/s]Finetune Epoch:  5/70. Data: 0.72s. Batch: 0.77s. Loss: 0.8099. :  67%|██████▋   | 2/3 [00:00<00:00,  2.21it/s]Finetune Epoch:  5/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8119. :  67%|██████▋   | 2/3 [00:01<00:00,  2.21it/s]Finetune Epoch:  5/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8119. : 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]Finetune Epoch:  5/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8119. : 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8503. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8503. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.43it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8520. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.43it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8520. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8573. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.12it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8573. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8576. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8576. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8706. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8706. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8803. top1: 94.21. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:00,  3.36it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8803. top1: 94.21. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.42it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8896. top1: 93.25. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.42it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8896. top1: 93.25. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8978. top1: 92.90. top5: 99.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.40it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8978. top1: 92.90. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8978. top1: 92.90. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  2.97it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.57s. Batch: 0.63s. Loss: 0.7949. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  6/70. Data: 0.57s. Batch: 0.63s. Loss: 0.7949. :  33%|███▎      | 1/3 [00:00<00:01,  1.59it/s]Finetune Epoch:  6/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8130. :  33%|███▎      | 1/3 [00:00<00:01,  1.59it/s]Finetune Epoch:  6/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8130. :  67%|██████▋   | 2/3 [00:00<00:00,  2.13it/s]Finetune Epoch:  6/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8181. :  67%|██████▋   | 2/3 [00:01<00:00,  2.13it/s]Finetune Epoch:  6/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8181. : 100%|██████████| 3/3 [00:01<00:00,  2.51it/s]Finetune Epoch:  6/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8181. : 100%|██████████| 3/3 [00:01<00:00,  2.01it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8498. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8498. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8515. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8515. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8569. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.42it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8569. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.79it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8572. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.79it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8572. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8700. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.99it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8700. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8797. top1: 94.21. top5: 99.61. :  62%|██████▎   | 5/8 [00:02<00:00,  3.08it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8797. top1: 94.21. top5: 99.61. :  75%|███████▌  | 6/8 [00:02<00:00,  3.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8890. top1: 93.30. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.16it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8890. top1: 93.30. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.23it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8972. top1: 92.95. top5: 99.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.23it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8972. top1: 92.95. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8972. top1: 92.95. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.57s. Batch: 0.66s. Loss: 0.8113. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  7/70. Data: 0.57s. Batch: 0.66s. Loss: 0.8113. :  33%|███▎      | 1/3 [00:00<00:01,  1.52it/s]Finetune Epoch:  7/70. Data: 0.76s. Batch: 0.84s. Loss: 0.8016. :  33%|███▎      | 1/3 [00:01<00:01,  1.52it/s]Finetune Epoch:  7/70. Data: 0.76s. Batch: 0.84s. Loss: 0.8016. :  67%|██████▋   | 2/3 [00:01<00:00,  2.08it/s]Finetune Epoch:  7/70. Data: 0.93s. Batch: 1.00s. Loss: 0.8066. :  67%|██████▋   | 2/3 [00:01<00:00,  2.08it/s]Finetune Epoch:  7/70. Data: 0.93s. Batch: 1.00s. Loss: 0.8066. : 100%|██████████| 3/3 [00:01<00:00,  2.43it/s]Finetune Epoch:  7/70. Data: 0.93s. Batch: 1.00s. Loss: 0.8066. : 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8494. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8494. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8511. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8511. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.53it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8565. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.53it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8565. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8568. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8568. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8695. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.09it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8695. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8792. top1: 94.21. top5: 99.61. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8792. top1: 94.21. top5: 99.61. :  75%|███████▌  | 6/8 [00:01<00:00,  3.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8884. top1: 93.30. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.76it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8884. top1: 93.30. top5: 99.67. :  88%|████████▊ | 7/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8965. top1: 92.95. top5: 99.60. :  88%|████████▊ | 7/8 [00:02<00:00,  3.78it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8965. top1: 92.95. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8965. top1: 92.95. top5: 99.60. : 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.65s. Batch: 0.71s. Loss: 0.8440. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  8/70. Data: 0.65s. Batch: 0.71s. Loss: 0.8440. :  33%|███▎      | 1/3 [00:00<00:01,  1.41it/s]Finetune Epoch:  8/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8359. :  33%|███▎      | 1/3 [00:01<00:01,  1.41it/s]Finetune Epoch:  8/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8359. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch:  8/70. Data: 0.97s. Batch: 1.04s. Loss: 0.8232. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch:  8/70. Data: 0.97s. Batch: 1.04s. Loss: 0.8232. : 100%|██████████| 3/3 [00:01<00:00,  2.41it/s]Finetune Epoch:  8/70. Data: 0.97s. Batch: 1.04s. Loss: 0.8232. : 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8489. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8489. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8507. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8507. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8561. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8561. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8563. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8563. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.40it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8690. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.40it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8690. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8786. top1: 94.21. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.43it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8786. top1: 94.21. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.61it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8878. top1: 93.30. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.61it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8878. top1: 93.30. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.64it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8959. top1: 92.95. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.64it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8959. top1: 92.95. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  4.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8959. top1: 92.95. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.66s. Batch: 0.75s. Loss: 0.7965. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch:  9/70. Data: 0.66s. Batch: 0.75s. Loss: 0.7965. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch:  9/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8240. :  33%|███▎      | 1/3 [00:01<00:01,  1.33it/s]Finetune Epoch:  9/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8240. :  67%|██████▋   | 2/3 [00:01<00:00,  2.17it/s]Finetune Epoch:  9/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8173. :  67%|██████▋   | 2/3 [00:01<00:00,  2.17it/s]Finetune Epoch:  9/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8173. : 100%|██████████| 3/3 [00:01<00:00,  2.65it/s]Finetune Epoch:  9/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8173. : 100%|██████████| 3/3 [00:01<00:00,  1.93it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8485. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8485. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8504. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8504. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8558. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8558. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8560. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.85it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8560. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.24it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8686. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.24it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8686. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8781. top1: 94.21. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8781. top1: 94.21. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.50it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8872. top1: 93.30. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.50it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8872. top1: 93.30. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.81it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8952. top1: 93.00. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.81it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8952. top1: 93.00. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.99it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8952. top1: 93.00. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.51s. Batch: 0.56s. Loss: 0.8062. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 10/70. Data: 0.51s. Batch: 0.56s. Loss: 0.8062. :  33%|███▎      | 1/3 [00:00<00:01,  1.77it/s]Finetune Epoch: 10/70. Data: 0.64s. Batch: 0.69s. Loss: 0.8170. :  33%|███▎      | 1/3 [00:00<00:01,  1.77it/s]Finetune Epoch: 10/70. Data: 0.64s. Batch: 0.69s. Loss: 0.8170. :  67%|██████▋   | 2/3 [00:00<00:00,  2.64it/s]Finetune Epoch: 10/70. Data: 0.78s. Batch: 0.83s. Loss: 0.8166. :  67%|██████▋   | 2/3 [00:01<00:00,  2.64it/s]Finetune Epoch: 10/70. Data: 0.78s. Batch: 0.83s. Loss: 0.8166. : 100%|██████████| 3/3 [00:01<00:00,  2.93it/s]Finetune Epoch: 10/70. Data: 0.78s. Batch: 0.83s. Loss: 0.8166. : 100%|██████████| 3/3 [00:01<00:00,  2.39it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8481. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8481. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  2.08it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8500. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  2.08it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8500. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:01,  3.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8554. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:00<00:01,  3.04it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8554. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:00<00:01,  3.29it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8556. top1: 96.00. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.29it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8556. top1: 96.00. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.55it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8681. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.55it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8681. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.37it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8775. top1: 94.21. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.37it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8775. top1: 94.21. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8866. top1: 93.30. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8866. top1: 93.30. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8946. top1: 93.00. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.44it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8946. top1: 93.00. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8946. top1: 93.00. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.61s. Batch: 0.69s. Loss: 0.7937. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 11/70. Data: 0.61s. Batch: 0.69s. Loss: 0.7937. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 11/70. Data: 0.74s. Batch: 0.82s. Loss: 0.8199. :  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]Finetune Epoch: 11/70. Data: 0.74s. Batch: 0.82s. Loss: 0.8199. :  67%|██████▋   | 2/3 [00:00<00:00,  2.27it/s]Finetune Epoch: 11/70. Data: 0.89s. Batch: 0.97s. Loss: 0.8140. :  67%|██████▋   | 2/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 11/70. Data: 0.89s. Batch: 0.97s. Loss: 0.8140. : 100%|██████████| 3/3 [00:01<00:00,  2.65it/s]Finetune Epoch: 11/70. Data: 0.89s. Batch: 0.97s. Loss: 0.8140. : 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8476. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8476. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8496. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8496. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8549. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8549. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8552. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8552. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.20it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8676. top1: 94.92. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.20it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8676. top1: 94.92. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8770. top1: 94.14. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8770. top1: 94.14. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.48it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8860. top1: 93.25. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.48it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8860. top1: 93.25. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8940. top1: 93.00. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8940. top1: 93.00. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.51it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8940. top1: 93.00. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.74s. Batch: 0.79s. Loss: 0.8066. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 12/70. Data: 0.74s. Batch: 0.79s. Loss: 0.8066. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 12/70. Data: 0.89s. Batch: 0.95s. Loss: 0.8046. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 12/70. Data: 0.89s. Batch: 0.95s. Loss: 0.8046. :  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s]Finetune Epoch: 12/70. Data: 1.07s. Batch: 1.14s. Loss: 0.8105. :  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s]Finetune Epoch: 12/70. Data: 1.07s. Batch: 1.14s. Loss: 0.8105. : 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]Finetune Epoch: 12/70. Data: 1.07s. Batch: 1.14s. Loss: 0.8105. : 100%|██████████| 3/3 [00:01<00:00,  1.72it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8471. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8471. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8492. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8492. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.29it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8546. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.29it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8546. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8548. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8548. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.93it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8671. top1: 94.92. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.93it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8671. top1: 94.92. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8764. top1: 94.14. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8764. top1: 94.14. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.19it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8854. top1: 93.25. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.19it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8854. top1: 93.25. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8934. top1: 93.00. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8934. top1: 93.00. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8934. top1: 93.00. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.84it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.72s. Batch: 0.78s. Loss: 0.8360. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 13/70. Data: 0.72s. Batch: 0.78s. Loss: 0.8360. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 13/70. Data: 0.93s. Batch: 0.98s. Loss: 0.8174. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 13/70. Data: 0.93s. Batch: 0.98s. Loss: 0.8174. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 13/70. Data: 1.09s. Batch: 1.14s. Loss: 0.8119. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 13/70. Data: 1.09s. Batch: 1.14s. Loss: 0.8119. : 100%|██████████| 3/3 [00:01<00:00,  2.33it/s]Finetune Epoch: 13/70. Data: 1.09s. Batch: 1.14s. Loss: 0.8119. : 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.81s. Loss: 0.8467. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.81s. Loss: 0.8467. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.23it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8488. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.23it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8488. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.08it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8542. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.08it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8542. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.52it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8544. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.52it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8544. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8666. top1: 94.92. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.83it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8666. top1: 94.92. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8759. top1: 94.14. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.98it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8759. top1: 94.14. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8848. top1: 93.25. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8848. top1: 93.25. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8928. top1: 93.05. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.17it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8928. top1: 93.05. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8928. top1: 93.05. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.71s. Batch: 0.79s. Loss: 0.8187. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 14/70. Data: 0.71s. Batch: 0.79s. Loss: 0.8187. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 14/70. Data: 0.88s. Batch: 0.95s. Loss: 0.8323. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 14/70. Data: 0.88s. Batch: 0.95s. Loss: 0.8323. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 14/70. Data: 1.02s. Batch: 1.09s. Loss: 0.8124. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 14/70. Data: 1.02s. Batch: 1.09s. Loss: 0.8124. : 100%|██████████| 3/3 [00:01<00:00,  2.53it/s]Finetune Epoch: 14/70. Data: 1.02s. Batch: 1.09s. Loss: 0.8124. : 100%|██████████| 3/3 [00:01<00:00,  1.95it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8462. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8462. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8484. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.82it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8484. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8538. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.27it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8538. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.79it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8540. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.79it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8540. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8661. top1: 94.92. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8661. top1: 94.92. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8754. top1: 94.14. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.19it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8754. top1: 94.14. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8843. top1: 93.30. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8843. top1: 93.30. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8922. top1: 93.15. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8922. top1: 93.15. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8922. top1: 93.15. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.91it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.77s. Batch: 0.84s. Loss: 0.8090. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 15/70. Data: 0.77s. Batch: 0.84s. Loss: 0.8090. :  33%|███▎      | 1/3 [00:00<00:01,  1.20it/s]Finetune Epoch: 15/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8076. :  33%|███▎      | 1/3 [00:01<00:01,  1.20it/s]Finetune Epoch: 15/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8076. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 15/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8152. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 15/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8152. : 100%|██████████| 3/3 [00:01<00:00,  2.17it/s]Finetune Epoch: 15/70. Data: 1.14s. Batch: 1.20s. Loss: 0.8152. : 100%|██████████| 3/3 [00:01<00:00,  1.72it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8458. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8458. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.49it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8480. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.49it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8480. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8534. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8534. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8536. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.68it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8536. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8656. top1: 94.92. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8656. top1: 94.92. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8748. top1: 94.14. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8748. top1: 94.14. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8837. top1: 93.30. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8837. top1: 93.30. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  2.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8916. top1: 93.15. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  2.86it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8916. top1: 93.15. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.88it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8916. top1: 93.15. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.61it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.77s. Batch: 0.85s. Loss: 0.7839. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 16/70. Data: 0.77s. Batch: 0.85s. Loss: 0.7839. :  33%|███▎      | 1/3 [00:00<00:01,  1.17it/s]Finetune Epoch: 16/70. Data: 0.96s. Batch: 1.04s. Loss: 0.7902. :  33%|███▎      | 1/3 [00:01<00:01,  1.17it/s]Finetune Epoch: 16/70. Data: 0.96s. Batch: 1.04s. Loss: 0.7902. :  67%|██████▋   | 2/3 [00:01<00:00,  1.73it/s]Finetune Epoch: 16/70. Data: 1.14s. Batch: 1.22s. Loss: 0.8094. :  67%|██████▋   | 2/3 [00:01<00:00,  1.73it/s]Finetune Epoch: 16/70. Data: 1.14s. Batch: 1.22s. Loss: 0.8094. : 100%|██████████| 3/3 [00:01<00:00,  2.14it/s]Finetune Epoch: 16/70. Data: 1.14s. Batch: 1.22s. Loss: 0.8094. : 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8453. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8453. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8475. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.44it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8475. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8529. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8529. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8531. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8531. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8651. top1: 94.92. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8651. top1: 94.92. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8743. top1: 94.14. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.05it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8743. top1: 94.14. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8831. top1: 93.30. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.14it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8831. top1: 93.30. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.19it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8909. top1: 93.15. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.19it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8909. top1: 93.15. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8909. top1: 93.15. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.73s. Batch: 0.79s. Loss: 0.8320. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 17/70. Data: 0.73s. Batch: 0.79s. Loss: 0.8320. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 17/70. Data: 0.87s. Batch: 0.95s. Loss: 0.8370. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 17/70. Data: 0.87s. Batch: 0.95s. Loss: 0.8370. :  67%|██████▋   | 2/3 [00:01<00:00,  1.96it/s]Finetune Epoch: 17/70. Data: 1.05s. Batch: 1.12s. Loss: 0.8195. :  67%|██████▋   | 2/3 [00:01<00:00,  1.96it/s]Finetune Epoch: 17/70. Data: 1.05s. Batch: 1.12s. Loss: 0.8195. : 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]Finetune Epoch: 17/70. Data: 1.05s. Batch: 1.12s. Loss: 0.8195. : 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8448. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8448. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8472. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8472. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8526. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.38it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8526. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8527. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.63it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8527. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8647. top1: 94.92. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.78it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8647. top1: 94.92. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8737. top1: 94.14. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.78it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8737. top1: 94.14. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8825. top1: 93.36. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8825. top1: 93.36. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8904. top1: 93.20. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8904. top1: 93.20. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.63it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8904. top1: 93.20. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.74s. Batch: 0.81s. Loss: 0.8053. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 18/70. Data: 0.74s. Batch: 0.81s. Loss: 0.8053. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch: 18/70. Data: 0.95s. Batch: 1.02s. Loss: 0.8263. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch: 18/70. Data: 0.95s. Batch: 1.02s. Loss: 0.8263. :  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]Finetune Epoch: 18/70. Data: 1.15s. Batch: 1.22s. Loss: 0.8170. :  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]Finetune Epoch: 18/70. Data: 1.15s. Batch: 1.22s. Loss: 0.8170. : 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 18/70. Data: 1.15s. Batch: 1.22s. Loss: 0.8170. : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8444. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8444. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8468. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8468. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8522. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.26it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8522. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8524. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8524. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8642. top1: 94.92. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8642. top1: 94.92. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8732. top1: 94.21. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8732. top1: 94.21. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8820. top1: 93.42. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8820. top1: 93.42. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8898. top1: 93.25. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8898. top1: 93.25. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8898. top1: 93.25. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.86s. Batch: 0.98s. Loss: 0.8153. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 19/70. Data: 0.86s. Batch: 0.98s. Loss: 0.8153. :  33%|███▎      | 1/3 [00:00<00:01,  1.02it/s]Finetune Epoch: 19/70. Data: 1.04s. Batch: 1.14s. Loss: 0.8164. :  33%|███▎      | 1/3 [00:01<00:01,  1.02it/s]Finetune Epoch: 19/70. Data: 1.04s. Batch: 1.14s. Loss: 0.8164. :  67%|██████▋   | 2/3 [00:01<00:00,  1.70it/s]Finetune Epoch: 19/70. Data: 1.25s. Batch: 1.32s. Loss: 0.8185. :  67%|██████▋   | 2/3 [00:01<00:00,  1.70it/s]Finetune Epoch: 19/70. Data: 1.25s. Batch: 1.32s. Loss: 0.8185. : 100%|██████████| 3/3 [00:01<00:00,  1.98it/s]Finetune Epoch: 19/70. Data: 1.25s. Batch: 1.32s. Loss: 0.8185. : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8441. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8441. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8465. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8465. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8519. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8519. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8521. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.53it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8521. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8638. top1: 94.92. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8638. top1: 94.92. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8727. top1: 94.21. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8727. top1: 94.21. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8814. top1: 93.42. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.06it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8814. top1: 93.42. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8892. top1: 93.25. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8892. top1: 93.25. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8892. top1: 93.25. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.64it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8061. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 20/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8061. :  33%|███▎      | 1/3 [00:00<00:01,  1.22it/s]Finetune Epoch: 20/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8037. :  33%|███▎      | 1/3 [00:01<00:01,  1.22it/s]Finetune Epoch: 20/70. Data: 0.92s. Batch: 0.97s. Loss: 0.8037. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 20/70. Data: 1.08s. Batch: 1.12s. Loss: 0.8113. :  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s]Finetune Epoch: 20/70. Data: 1.08s. Batch: 1.12s. Loss: 0.8113. : 100%|██████████| 3/3 [00:01<00:00,  2.36it/s]Finetune Epoch: 20/70. Data: 1.08s. Batch: 1.12s. Loss: 0.8113. : 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 0.8437. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.82s. Loss: 0.8437. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.22it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8462. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.22it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8462. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8516. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8516. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8517. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.54it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8517. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8634. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8634. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.92it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8722. top1: 94.27. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.92it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8722. top1: 94.27. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8809. top1: 93.47. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8809. top1: 93.47. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8886. top1: 93.35. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8886. top1: 93.35. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8886. top1: 93.35. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.55it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.81s. Batch: 0.90s. Loss: 0.8527. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 21/70. Data: 0.81s. Batch: 0.90s. Loss: 0.8527. :  33%|███▎      | 1/3 [00:00<00:01,  1.11it/s]Finetune Epoch: 21/70. Data: 0.97s. Batch: 1.05s. Loss: 0.8155. :  33%|███▎      | 1/3 [00:01<00:01,  1.11it/s]Finetune Epoch: 21/70. Data: 0.97s. Batch: 1.05s. Loss: 0.8155. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 21/70. Data: 1.13s. Batch: 1.20s. Loss: 0.8234. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 21/70. Data: 1.13s. Batch: 1.20s. Loss: 0.8234. : 100%|██████████| 3/3 [00:01<00:00,  2.31it/s]Finetune Epoch: 21/70. Data: 1.13s. Batch: 1.20s. Loss: 0.8234. : 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8433. top1: 96.48. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8433. top1: 96.48. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8458. top1: 96.48. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8458. top1: 96.48. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8513. top1: 96.09. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8513. top1: 96.09. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8514. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8514. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8629. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.07it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8629. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.26it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8717. top1: 94.34. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.26it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8717. top1: 94.34. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8803. top1: 93.53. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.11it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8803. top1: 93.53. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8880. top1: 93.40. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8880. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8880. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8267. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 22/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8267. :  33%|███▎      | 1/3 [00:00<00:01,  1.18it/s]Finetune Epoch: 22/70. Data: 0.96s. Batch: 1.04s. Loss: 0.8171. :  33%|███▎      | 1/3 [00:01<00:01,  1.18it/s]Finetune Epoch: 22/70. Data: 0.96s. Batch: 1.04s. Loss: 0.8171. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 22/70. Data: 1.14s. Batch: 1.22s. Loss: 0.8151. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 22/70. Data: 1.14s. Batch: 1.22s. Loss: 0.8151. : 100%|██████████| 3/3 [00:01<00:00,  2.11it/s]Finetune Epoch: 22/70. Data: 1.14s. Batch: 1.22s. Loss: 0.8151. : 100%|██████████| 3/3 [00:01<00:00,  1.67it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8429. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.77s. Loss: 0.8429. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8455. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.29it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8455. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8510. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.14it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8510. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8511. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8511. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.62it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8625. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.62it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8625. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.83it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8712. top1: 94.34. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.83it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8712. top1: 94.34. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8798. top1: 93.53. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8798. top1: 93.53. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8874. top1: 93.40. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8874. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8874. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.54it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.63s. Batch: 0.70s. Loss: 0.8177. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 23/70. Data: 0.63s. Batch: 0.70s. Loss: 0.8177. :  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]Finetune Epoch: 23/70. Data: 0.79s. Batch: 0.87s. Loss: 0.8054. :  33%|███▎      | 1/3 [00:01<00:01,  1.43it/s]Finetune Epoch: 23/70. Data: 0.79s. Batch: 0.87s. Loss: 0.8054. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 23/70. Data: 0.95s. Batch: 1.02s. Loss: 0.8081. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 23/70. Data: 0.95s. Batch: 1.02s. Loss: 0.8081. : 100%|██████████| 3/3 [00:01<00:00,  2.52it/s]Finetune Epoch: 23/70. Data: 0.95s. Batch: 1.02s. Loss: 0.8081. : 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8426. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8426. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8452. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8452. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8507. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8507. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8508. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8508. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8621. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8621. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8708. top1: 94.34. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.06it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8708. top1: 94.34. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8793. top1: 93.53. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.23it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8793. top1: 93.53. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8868. top1: 93.40. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8868. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8868. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.73s. Batch: 0.79s. Loss: 0.7879. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 24/70. Data: 0.73s. Batch: 0.79s. Loss: 0.7879. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 24/70. Data: 0.86s. Batch: 0.92s. Loss: 0.8051. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 24/70. Data: 0.86s. Batch: 0.92s. Loss: 0.8051. :  67%|██████▋   | 2/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 24/70. Data: 1.00s. Batch: 1.06s. Loss: 0.8136. :  67%|██████▋   | 2/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 24/70. Data: 1.00s. Batch: 1.06s. Loss: 0.8136. : 100%|██████████| 3/3 [00:01<00:00,  2.56it/s]Finetune Epoch: 24/70. Data: 1.00s. Batch: 1.06s. Loss: 0.8136. : 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 0.8422. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 0.8422. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.32it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8449. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.32it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.54s. Loss: 0.8449. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  2.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8504. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  2.00it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8504. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.36it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8504. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.36it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8504. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8617. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.80it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8617. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8703. top1: 94.34. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.97it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8703. top1: 94.34. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8787. top1: 93.53. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8787. top1: 93.53. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8863. top1: 93.40. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.31it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8863. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.81it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8863. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.71it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.66s. Batch: 0.72s. Loss: 0.8240. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 25/70. Data: 0.66s. Batch: 0.72s. Loss: 0.8240. :  33%|███▎      | 1/3 [00:00<00:01,  1.39it/s]Finetune Epoch: 25/70. Data: 0.83s. Batch: 0.90s. Loss: 0.8045. :  33%|███▎      | 1/3 [00:01<00:01,  1.39it/s]Finetune Epoch: 25/70. Data: 0.83s. Batch: 0.90s. Loss: 0.8045. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch: 25/70. Data: 1.01s. Batch: 1.08s. Loss: 0.8046. :  67%|██████▋   | 2/3 [00:01<00:00,  1.97it/s]Finetune Epoch: 25/70. Data: 1.01s. Batch: 1.08s. Loss: 0.8046. : 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]Finetune Epoch: 25/70. Data: 1.01s. Batch: 1.08s. Loss: 0.8046. : 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8418. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8418. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8445. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8445. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.74it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8500. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.74it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8500. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  3.22it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8501. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.22it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8501. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.47it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8613. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.47it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8613. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8698. top1: 94.34. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8698. top1: 94.34. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8782. top1: 93.53. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.22it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8782. top1: 93.53. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8857. top1: 93.40. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8857. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8857. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.60s. Batch: 0.67s. Loss: 0.8470. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 26/70. Data: 0.60s. Batch: 0.67s. Loss: 0.8470. :  33%|███▎      | 1/3 [00:00<00:01,  1.48it/s]Finetune Epoch: 26/70. Data: 0.75s. Batch: 0.83s. Loss: 0.8313. :  33%|███▎      | 1/3 [00:00<00:01,  1.48it/s]Finetune Epoch: 26/70. Data: 0.75s. Batch: 0.83s. Loss: 0.8313. :  67%|██████▋   | 2/3 [00:00<00:00,  2.20it/s]Finetune Epoch: 26/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8206. :  67%|██████▋   | 2/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 26/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8206. : 100%|██████████| 3/3 [00:01<00:00,  2.56it/s]Finetune Epoch: 26/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8206. : 100%|██████████| 3/3 [00:01<00:00,  1.93it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8415. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8415. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.59it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8443. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.59it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8443. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8498. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8498. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8498. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.84it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8498. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.10it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8609. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.10it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8609. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8693. top1: 94.34. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8693. top1: 94.34. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8777. top1: 93.53. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8777. top1: 93.53. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8852. top1: 93.40. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8852. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.51it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8852. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.69s. Batch: 0.74s. Loss: 0.7900. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 27/70. Data: 0.69s. Batch: 0.74s. Loss: 0.7900. :  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Finetune Epoch: 27/70. Data: 0.85s. Batch: 0.91s. Loss: 0.8152. :  33%|███▎      | 1/3 [00:01<00:01,  1.35it/s]Finetune Epoch: 27/70. Data: 0.85s. Batch: 0.91s. Loss: 0.8152. :  67%|██████▋   | 2/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 27/70. Data: 1.04s. Batch: 1.10s. Loss: 0.8165. :  67%|██████▋   | 2/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 27/70. Data: 1.04s. Batch: 1.10s. Loss: 0.8165. : 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]Finetune Epoch: 27/70. Data: 1.04s. Batch: 1.10s. Loss: 0.8165. : 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8411. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8411. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8440. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8440. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8495. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.51it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8495. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8495. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8495. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.05it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8605. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.05it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8605. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8689. top1: 94.34. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.25it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8689. top1: 94.34. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.38it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8772. top1: 93.53. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.38it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8772. top1: 93.53. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8846. top1: 93.40. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8846. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.82it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8846. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.62s. Batch: 0.68s. Loss: 0.7954. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 28/70. Data: 0.62s. Batch: 0.68s. Loss: 0.7954. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 28/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8074. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 28/70. Data: 0.76s. Batch: 0.82s. Loss: 0.8074. :  67%|██████▋   | 2/3 [00:00<00:00,  2.27it/s]Finetune Epoch: 28/70. Data: 0.90s. Batch: 0.95s. Loss: 0.8130. :  67%|██████▋   | 2/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 28/70. Data: 0.90s. Batch: 0.95s. Loss: 0.8130. : 100%|██████████| 3/3 [00:01<00:00,  2.79it/s]Finetune Epoch: 28/70. Data: 0.90s. Batch: 0.95s. Loss: 0.8130. : 100%|██████████| 3/3 [00:01<00:00,  2.11it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8408. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8408. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8437. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8437. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.43it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8492. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.43it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8492. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8492. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.11it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8492. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.38it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8601. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.38it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8601. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.29it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8684. top1: 94.34. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.29it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8684. top1: 94.34. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.46it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8767. top1: 93.53. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.46it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8767. top1: 93.53. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8840. top1: 93.40. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8840. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.92it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8840. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.64s. Batch: 0.70s. Loss: 0.8425. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 29/70. Data: 0.64s. Batch: 0.70s. Loss: 0.8425. :  33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]Finetune Epoch: 29/70. Data: 0.79s. Batch: 0.86s. Loss: 0.8177. :  33%|███▎      | 1/3 [00:01<00:01,  1.42it/s]Finetune Epoch: 29/70. Data: 0.79s. Batch: 0.86s. Loss: 0.8177. :  67%|██████▋   | 2/3 [00:01<00:00,  2.13it/s]Finetune Epoch: 29/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8182. :  67%|██████▋   | 2/3 [00:01<00:00,  2.13it/s]Finetune Epoch: 29/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8182. : 100%|██████████| 3/3 [00:01<00:00,  2.46it/s]Finetune Epoch: 29/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8182. : 100%|██████████| 3/3 [00:01<00:00,  1.89it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8404. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8404. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8434. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8434. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8489. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.73it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8489. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  3.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8489. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.18it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8489. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8597. top1: 95.00. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.41it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8597. top1: 95.00. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8680. top1: 94.34. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.65it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8680. top1: 94.34. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.44it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8762. top1: 93.53. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.44it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8762. top1: 93.53. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8835. top1: 93.40. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8835. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.77it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8835. top1: 93.40. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8288. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 30/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8288. :  33%|███▎      | 1/3 [00:00<00:01,  1.23it/s]Finetune Epoch: 30/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8151. :  33%|███▎      | 1/3 [00:01<00:01,  1.23it/s]Finetune Epoch: 30/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8151. :  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s]Finetune Epoch: 30/70. Data: 1.09s. Batch: 1.15s. Loss: 0.8142. :  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s]Finetune Epoch: 30/70. Data: 1.09s. Batch: 1.15s. Loss: 0.8142. : 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]Finetune Epoch: 30/70. Data: 1.09s. Batch: 1.15s. Loss: 0.8142. : 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8401. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8401. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.59it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8431. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.59it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8431. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8487. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.06it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8487. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8486. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.46it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8486. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8593. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8593. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8675. top1: 94.40. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8675. top1: 94.40. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8756. top1: 93.58. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8756. top1: 93.58. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8829. top1: 93.45. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.04it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8829. top1: 93.45. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8829. top1: 93.45. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.66it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8080. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 31/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8080. :  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Finetune Epoch: 31/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8123. :  33%|███▎      | 1/3 [00:01<00:01,  1.13it/s]Finetune Epoch: 31/70. Data: 1.02s. Batch: 1.07s. Loss: 0.8123. :  67%|██████▋   | 2/3 [00:01<00:00,  1.70it/s]Finetune Epoch: 31/70. Data: 1.20s. Batch: 1.25s. Loss: 0.8107. :  67%|██████▋   | 2/3 [00:01<00:00,  1.70it/s]Finetune Epoch: 31/70. Data: 1.20s. Batch: 1.25s. Loss: 0.8107. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 31/70. Data: 1.20s. Batch: 1.25s. Loss: 0.8107. : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8398. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8398. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8428. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.45it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8428. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.10it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8484. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.10it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8484. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8484. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.50it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8484. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8589. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8589. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8670. top1: 94.40. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8670. top1: 94.40. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.18it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8751. top1: 93.64. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.18it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8751. top1: 93.64. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8824. top1: 93.50. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.20it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8824. top1: 93.50. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8824. top1: 93.50. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.65it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.81s. Batch: 0.86s. Loss: 0.8068. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 32/70. Data: 0.81s. Batch: 0.86s. Loss: 0.8068. :  33%|███▎      | 1/3 [00:00<00:01,  1.16it/s]Finetune Epoch: 32/70. Data: 0.99s. Batch: 1.04s. Loss: 0.7971. :  33%|███▎      | 1/3 [00:01<00:01,  1.16it/s]Finetune Epoch: 32/70. Data: 0.99s. Batch: 1.04s. Loss: 0.7971. :  67%|██████▋   | 2/3 [00:01<00:00,  1.77it/s]Finetune Epoch: 32/70. Data: 1.16s. Batch: 1.21s. Loss: 0.8047. :  67%|██████▋   | 2/3 [00:01<00:00,  1.77it/s]Finetune Epoch: 32/70. Data: 1.16s. Batch: 1.21s. Loss: 0.8047. : 100%|██████████| 3/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 32/70. Data: 1.16s. Batch: 1.21s. Loss: 0.8047. : 100%|██████████| 3/3 [00:01<00:00,  1.69it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8395. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8395. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8426. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8426. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8482. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.36it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8482. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8481. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.58it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8481. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8586. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8586. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8666. top1: 94.40. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8666. top1: 94.40. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.27it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8747. top1: 93.64. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.27it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8747. top1: 93.64. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8819. top1: 93.50. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8819. top1: 93.50. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8819. top1: 93.50. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.61it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8164. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 33/70. Data: 0.82s. Batch: 0.88s. Loss: 0.8164. :  33%|███▎      | 1/3 [00:00<00:01,  1.14it/s]Finetune Epoch: 33/70. Data: 1.00s. Batch: 1.07s. Loss: 0.8126. :  33%|███▎      | 1/3 [00:01<00:01,  1.14it/s]Finetune Epoch: 33/70. Data: 1.00s. Batch: 1.07s. Loss: 0.8126. :  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]Finetune Epoch: 33/70. Data: 1.18s. Batch: 1.25s. Loss: 0.8034. :  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]Finetune Epoch: 33/70. Data: 1.18s. Batch: 1.25s. Loss: 0.8034. : 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 33/70. Data: 1.18s. Batch: 1.25s. Loss: 0.8034. : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8392. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8392. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8423. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8423. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8479. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8479. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8479. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8479. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8582. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.75it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8582. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8662. top1: 94.47. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8662. top1: 94.47. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8742. top1: 93.69. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8742. top1: 93.69. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8813. top1: 93.55. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8813. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8813. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.70s. Batch: 0.75s. Loss: 0.8158. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 34/70. Data: 0.70s. Batch: 0.75s. Loss: 0.8158. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch: 34/70. Data: 0.90s. Batch: 0.98s. Loss: 0.8115. :  33%|███▎      | 1/3 [00:01<00:01,  1.33it/s]Finetune Epoch: 34/70. Data: 0.90s. Batch: 0.98s. Loss: 0.8115. :  67%|██████▋   | 2/3 [00:01<00:00,  1.74it/s]Finetune Epoch: 34/70. Data: 1.12s. Batch: 1.18s. Loss: 0.8133. :  67%|██████▋   | 2/3 [00:01<00:00,  1.74it/s]Finetune Epoch: 34/70. Data: 1.12s. Batch: 1.18s. Loss: 0.8133. : 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 34/70. Data: 1.12s. Batch: 1.18s. Loss: 0.8133. : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.87s. Loss: 0.8387. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.87s. Loss: 0.8387. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:06,  1.15it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8419. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:06,  1.15it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.58s. Loss: 0.8419. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.90it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8475. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.90it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8475. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.44it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8475. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.44it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8475. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8578. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8578. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8657. top1: 94.47. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8657. top1: 94.47. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  2.95it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8737. top1: 93.69. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  2.95it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8737. top1: 93.69. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8808. top1: 93.55. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8808. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8808. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.68s. Batch: 0.76s. Loss: 0.8011. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 35/70. Data: 0.68s. Batch: 0.76s. Loss: 0.8011. :  33%|███▎      | 1/3 [00:00<00:01,  1.32it/s]Finetune Epoch: 35/70. Data: 0.85s. Batch: 0.91s. Loss: 0.8137. :  33%|███▎      | 1/3 [00:01<00:01,  1.32it/s]Finetune Epoch: 35/70. Data: 0.85s. Batch: 0.91s. Loss: 0.8137. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 35/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8077. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 35/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8077. : 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]Finetune Epoch: 35/70. Data: 1.01s. Batch: 1.07s. Loss: 0.8077. : 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8384. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.73s. Loss: 0.8384. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8417. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:05,  1.37it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8417. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.28it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8473. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.28it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8473. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8472. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8472. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8574. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8574. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8653. top1: 94.47. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8653. top1: 94.47. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8732. top1: 93.69. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8732. top1: 93.69. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8803. top1: 93.55. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8803. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.32it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8803. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.76it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.73s. Batch: 0.82s. Loss: 0.8135. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 36/70. Data: 0.73s. Batch: 0.82s. Loss: 0.8135. :  33%|███▎      | 1/3 [00:00<00:01,  1.22it/s]Finetune Epoch: 36/70. Data: 0.93s. Batch: 1.01s. Loss: 0.8145. :  33%|███▎      | 1/3 [00:01<00:01,  1.22it/s]Finetune Epoch: 36/70. Data: 0.93s. Batch: 1.01s. Loss: 0.8145. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 36/70. Data: 1.10s. Batch: 1.17s. Loss: 0.8163. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 36/70. Data: 1.10s. Batch: 1.17s. Loss: 0.8163. : 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 36/70. Data: 1.10s. Batch: 1.17s. Loss: 0.8163. : 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8381. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.71s. Loss: 0.8381. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8414. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.41it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8414. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8470. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.17it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8470. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8469. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8469. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.26it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8570. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.26it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8570. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8649. top1: 94.47. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8649. top1: 94.47. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8727. top1: 93.69. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.12it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8727. top1: 93.69. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8798. top1: 93.55. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8798. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8798. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.75s. Batch: 0.82s. Loss: 0.8164. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 37/70. Data: 0.75s. Batch: 0.82s. Loss: 0.8164. :  33%|███▎      | 1/3 [00:00<00:01,  1.22it/s]Finetune Epoch: 37/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8160. :  33%|███▎      | 1/3 [00:01<00:01,  1.22it/s]Finetune Epoch: 37/70. Data: 0.94s. Batch: 1.00s. Loss: 0.8160. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 37/70. Data: 1.12s. Batch: 1.18s. Loss: 0.8097. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 37/70. Data: 1.12s. Batch: 1.18s. Loss: 0.8097. : 100%|██████████| 3/3 [00:01<00:00,  2.18it/s]Finetune Epoch: 37/70. Data: 1.12s. Batch: 1.18s. Loss: 0.8097. : 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8377. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8377. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8411. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8411. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.62it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8467. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.62it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8467. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8466. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.17it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8466. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.49it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8566. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.49it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8566. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.74it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8644. top1: 94.47. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.74it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8644. top1: 94.47. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.58it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8723. top1: 93.69. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.58it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8723. top1: 93.69. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8793. top1: 93.55. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8793. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8793. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.80s. Batch: 0.90s. Loss: 0.7819. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 38/70. Data: 0.80s. Batch: 0.90s. Loss: 0.7819. :  33%|███▎      | 1/3 [00:00<00:01,  1.11it/s]Finetune Epoch: 38/70. Data: 0.99s. Batch: 1.07s. Loss: 0.8064. :  33%|███▎      | 1/3 [00:01<00:01,  1.11it/s]Finetune Epoch: 38/70. Data: 0.99s. Batch: 1.07s. Loss: 0.8064. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 38/70. Data: 1.16s. Batch: 1.23s. Loss: 0.8074. :  67%|██████▋   | 2/3 [00:01<00:00,  1.75it/s]Finetune Epoch: 38/70. Data: 1.16s. Batch: 1.23s. Loss: 0.8074. : 100%|██████████| 3/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 38/70. Data: 1.16s. Batch: 1.23s. Loss: 0.8074. : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8374. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8374. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8408. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.29it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.53s. Loss: 0.8408. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8464. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.07it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8464. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8463. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8463. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8563. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.94it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8563. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8640. top1: 94.47. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.10it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8640. top1: 94.47. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.27it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8718. top1: 93.69. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.27it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8718. top1: 93.69. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8788. top1: 93.55. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.35it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8788. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.56it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8788. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.70s. Batch: 0.74s. Loss: 0.7902. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 39/70. Data: 0.70s. Batch: 0.74s. Loss: 0.7902. :  33%|███▎      | 1/3 [00:00<00:01,  1.34it/s]Finetune Epoch: 39/70. Data: 0.89s. Batch: 0.95s. Loss: 0.8006. :  33%|███▎      | 1/3 [00:01<00:01,  1.34it/s]Finetune Epoch: 39/70. Data: 0.89s. Batch: 0.95s. Loss: 0.8006. :  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s]Finetune Epoch: 39/70. Data: 1.08s. Batch: 1.13s. Loss: 0.8088. :  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s]Finetune Epoch: 39/70. Data: 1.08s. Batch: 1.13s. Loss: 0.8088. : 100%|██████████| 3/3 [00:01<00:00,  2.17it/s]Finetune Epoch: 39/70. Data: 1.08s. Batch: 1.13s. Loss: 0.8088. : 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8370. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8370. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8405. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.92it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8405. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.83it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8461. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.83it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8461. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  3.12it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8460. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.12it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8460. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.34it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8559. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.34it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8559. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8636. top1: 94.47. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.24it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8636. top1: 94.47. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8713. top1: 93.69. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.13it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8713. top1: 93.69. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.19it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8783. top1: 93.55. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.19it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8783. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8783. top1: 93.55. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 40/70. Data: 0.67s. Batch: 0.72s. Loss: 0.7990. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 40/70. Data: 0.67s. Batch: 0.72s. Loss: 0.7990. :  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Finetune Epoch: 40/70. Data: 0.83s. Batch: 0.90s. Loss: 0.8203. :  33%|███▎      | 1/3 [00:01<00:01,  1.38it/s]Finetune Epoch: 40/70. Data: 0.83s. Batch: 0.90s. Loss: 0.8203. :  67%|██████▋   | 2/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 40/70. Data: 0.99s. Batch: 1.06s. Loss: 0.8157. :  67%|██████▋   | 2/3 [00:01<00:00,  1.99it/s]Finetune Epoch: 40/70. Data: 0.99s. Batch: 1.06s. Loss: 0.8157. : 100%|██████████| 3/3 [00:01<00:00,  2.42it/s]Finetune Epoch: 40/70. Data: 0.99s. Batch: 1.06s. Loss: 0.8157. : 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8367. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8367. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.42it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8402. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.42it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.51s. Loss: 0.8402. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.11it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8459. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.11it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8459. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8458. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.62it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8458. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8555. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.84it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8555. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8631. top1: 94.53. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8631. top1: 94.53. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8709. top1: 93.75. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.07it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8709. top1: 93.75. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8778. top1: 93.60. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8778. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.69it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8778. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.67s. Batch: 0.73s. Loss: 0.8264. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 41/70. Data: 0.67s. Batch: 0.73s. Loss: 0.8264. :  33%|███▎      | 1/3 [00:00<00:01,  1.37it/s]Finetune Epoch: 41/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8223. :  33%|███▎      | 1/3 [00:01<00:01,  1.37it/s]Finetune Epoch: 41/70. Data: 0.83s. Batch: 0.89s. Loss: 0.8223. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 41/70. Data: 1.00s. Batch: 1.07s. Loss: 0.8182. :  67%|██████▋   | 2/3 [00:01<00:00,  2.05it/s]Finetune Epoch: 41/70. Data: 1.00s. Batch: 1.07s. Loss: 0.8182. : 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]Finetune Epoch: 41/70. Data: 1.00s. Batch: 1.07s. Loss: 0.8182. : 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8365. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8365. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8400. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8400. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8457. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.19it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8457. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8456. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.48it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8456. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.67it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8552. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.67it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8552. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8628. top1: 94.53. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.94it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8628. top1: 94.53. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8704. top1: 93.75. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8704. top1: 93.75. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8773. top1: 93.60. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8773. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.70it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8773. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.70s. Batch: 0.79s. Loss: 0.8001. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 42/70. Data: 0.70s. Batch: 0.79s. Loss: 0.8001. :  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Finetune Epoch: 42/70. Data: 0.88s. Batch: 0.96s. Loss: 0.8030. :  33%|███▎      | 1/3 [00:01<00:01,  1.27it/s]Finetune Epoch: 42/70. Data: 0.88s. Batch: 0.96s. Loss: 0.8030. :  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s]Finetune Epoch: 42/70. Data: 1.06s. Batch: 1.13s. Loss: 0.8131. :  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s]Finetune Epoch: 42/70. Data: 1.06s. Batch: 1.13s. Loss: 0.8131. : 100%|██████████| 3/3 [00:01<00:00,  2.26it/s]Finetune Epoch: 42/70. Data: 1.06s. Batch: 1.13s. Loss: 0.8131. : 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8362. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.63s. Loss: 0.8362. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8398. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.58it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8398. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8455. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8455. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8453. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.74it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8453. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8549. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8549. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8624. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.20it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8624. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.33it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8700. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.33it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8700. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8768. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.39it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8768. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8768. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.64s. Batch: 0.70s. Loss: 0.8268. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 43/70. Data: 0.64s. Batch: 0.70s. Loss: 0.8268. :  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]Finetune Epoch: 43/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8363. :  33%|███▎      | 1/3 [00:01<00:01,  1.44it/s]Finetune Epoch: 43/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8363. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 43/70. Data: 1.01s. Batch: 1.06s. Loss: 0.8174. :  67%|██████▋   | 2/3 [00:01<00:00,  2.02it/s]Finetune Epoch: 43/70. Data: 1.01s. Batch: 1.06s. Loss: 0.8174. : 100%|██████████| 3/3 [00:01<00:00,  2.23it/s]Finetune Epoch: 43/70. Data: 1.01s. Batch: 1.06s. Loss: 0.8174. : 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 0.8358. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.72s. Loss: 0.8358. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8395. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:05,  1.39it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8395. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8452. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.20it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8452. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.83it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8450. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.83it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8450. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8545. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.25it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8545. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.46it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8619. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.46it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8619. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.58it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8695. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.58it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8695. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8763. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.54it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8763. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.74it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8763. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.69s. Batch: 0.78s. Loss: 0.7833. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 44/70. Data: 0.69s. Batch: 0.78s. Loss: 0.7833. :  33%|███▎      | 1/3 [00:00<00:01,  1.28it/s]Finetune Epoch: 44/70. Data: 0.85s. Batch: 0.92s. Loss: 0.8085. :  33%|███▎      | 1/3 [00:01<00:01,  1.28it/s]Finetune Epoch: 44/70. Data: 0.85s. Batch: 0.92s. Loss: 0.8085. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 44/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8095. :  67%|██████▋   | 2/3 [00:01<00:00,  2.07it/s]Finetune Epoch: 44/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8095. : 100%|██████████| 3/3 [00:01<00:00,  2.33it/s]Finetune Epoch: 44/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8095. : 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8355. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8355. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8392. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8392. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8449. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8449. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8447. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.71it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8447. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8541. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.02it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8541. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8615. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8615. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8691. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8691. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8759. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.33it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8759. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8759. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.59s. Batch: 0.67s. Loss: 0.8101. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 45/70. Data: 0.59s. Batch: 0.67s. Loss: 0.8101. :  33%|███▎      | 1/3 [00:00<00:01,  1.50it/s]Finetune Epoch: 45/70. Data: 0.78s. Batch: 0.85s. Loss: 0.8135. :  33%|███▎      | 1/3 [00:01<00:01,  1.50it/s]Finetune Epoch: 45/70. Data: 0.78s. Batch: 0.85s. Loss: 0.8135. :  67%|██████▋   | 2/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 45/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8124. :  67%|██████▋   | 2/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 45/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8124. : 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]Finetune Epoch: 45/70. Data: 0.96s. Batch: 1.02s. Loss: 0.8124. : 100%|██████████| 3/3 [00:01<00:00,  1.89it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8352. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8352. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8389. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.47s. Loss: 0.8389. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8446. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.31it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8446. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8444. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8444. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8538. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.98it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8538. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8611. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.11it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8611. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8686. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.37it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8686. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8754. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.22it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8754. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.34it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8754. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.76it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.59s. Batch: 0.64s. Loss: 0.8121. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 46/70. Data: 0.59s. Batch: 0.64s. Loss: 0.8121. :  33%|███▎      | 1/3 [00:00<00:01,  1.57it/s]Finetune Epoch: 46/70. Data: 0.75s. Batch: 0.83s. Loss: 0.8202. :  33%|███▎      | 1/3 [00:01<00:01,  1.57it/s]Finetune Epoch: 46/70. Data: 0.75s. Batch: 0.83s. Loss: 0.8202. :  67%|██████▋   | 2/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 46/70. Data: 0.92s. Batch: 0.99s. Loss: 0.8115. :  67%|██████▋   | 2/3 [00:01<00:00,  2.06it/s]Finetune Epoch: 46/70. Data: 0.92s. Batch: 0.99s. Loss: 0.8115. : 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]Finetune Epoch: 46/70. Data: 0.92s. Batch: 0.99s. Loss: 0.8115. : 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8349. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8349. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8387. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8387. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.59it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8444. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.59it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8444. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8443. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.87it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8443. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8535. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8535. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8607. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.32it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8607. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.36it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8682. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.36it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8682. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8749. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.36it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8749. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.81it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8749. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.99it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.65s. Batch: 0.74s. Loss: 0.8222. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 47/70. Data: 0.65s. Batch: 0.74s. Loss: 0.8222. :  33%|███▎      | 1/3 [00:00<00:01,  1.36it/s]Finetune Epoch: 47/70. Data: 0.80s. Batch: 0.87s. Loss: 0.8160. :  33%|███▎      | 1/3 [00:01<00:01,  1.36it/s]Finetune Epoch: 47/70. Data: 0.80s. Batch: 0.87s. Loss: 0.8160. :  67%|██████▋   | 2/3 [00:01<00:00,  2.14it/s]Finetune Epoch: 47/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8127. :  67%|██████▋   | 2/3 [00:01<00:00,  2.14it/s]Finetune Epoch: 47/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8127. : 100%|██████████| 3/3 [00:01<00:00,  2.67it/s]Finetune Epoch: 47/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8127. : 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8346. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8346. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8384. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8384. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8442. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8442. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  3.05it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8440. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.05it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8440. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.31it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8531. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.31it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8531. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.55it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8603. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.55it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8603. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.54it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8677. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.54it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8677. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8744. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8744. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.66it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8744. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.59s. Batch: 0.64s. Loss: 0.7979. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 48/70. Data: 0.59s. Batch: 0.64s. Loss: 0.7979. :  33%|███▎      | 1/3 [00:00<00:01,  1.55it/s]Finetune Epoch: 48/70. Data: 0.77s. Batch: 0.82s. Loss: 0.7961. :  33%|███▎      | 1/3 [00:01<00:01,  1.55it/s]Finetune Epoch: 48/70. Data: 0.77s. Batch: 0.82s. Loss: 0.7961. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 48/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8078. :  67%|██████▋   | 2/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 48/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8078. : 100%|██████████| 3/3 [00:01<00:00,  2.34it/s]Finetune Epoch: 48/70. Data: 0.95s. Batch: 1.01s. Loss: 0.8078. : 100%|██████████| 3/3 [00:01<00:00,  1.93it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8342. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8342. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8381. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8381. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8439. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.33it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8439. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8437. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.82it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8437. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.03it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8528. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.03it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8528. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8599. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.13it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8599. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8673. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.40it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8673. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8739. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.43it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8739. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.77it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8739. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.98it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.67s. Batch: 0.75s. Loss: 0.8123. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 49/70. Data: 0.67s. Batch: 0.75s. Loss: 0.8123. :  33%|███▎      | 1/3 [00:00<00:01,  1.34it/s]Finetune Epoch: 49/70. Data: 0.86s. Batch: 0.93s. Loss: 0.8134. :  33%|███▎      | 1/3 [00:01<00:01,  1.34it/s]Finetune Epoch: 49/70. Data: 0.86s. Batch: 0.93s. Loss: 0.8134. :  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s]Finetune Epoch: 49/70. Data: 1.04s. Batch: 1.11s. Loss: 0.8101. :  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s]Finetune Epoch: 49/70. Data: 1.04s. Batch: 1.11s. Loss: 0.8101. : 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 49/70. Data: 1.04s. Batch: 1.11s. Loss: 0.8101. : 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8340. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8340. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8379. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8379. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.28it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8437. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.28it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8437. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8435. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8435. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8524. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.88it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8524. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8595. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.01it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8595. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8669. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8669. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8734. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.26it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8734. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.64it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8734. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.90it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.53s. Batch: 0.62s. Loss: 0.8463. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 50/70. Data: 0.53s. Batch: 0.62s. Loss: 0.8463. :  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s]Finetune Epoch: 50/70. Data: 0.67s. Batch: 0.74s. Loss: 0.8204. :  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s]Finetune Epoch: 50/70. Data: 0.67s. Batch: 0.74s. Loss: 0.8204. :  67%|██████▋   | 2/3 [00:00<00:00,  2.55it/s]Finetune Epoch: 50/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8124. :  67%|██████▋   | 2/3 [00:01<00:00,  2.55it/s]Finetune Epoch: 50/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8124. : 100%|██████████| 3/3 [00:01<00:00,  3.08it/s]Finetune Epoch: 50/70. Data: 0.80s. Batch: 0.86s. Loss: 0.8124. : 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8337. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.59s. Loss: 0.8337. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8377. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8377. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.72it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8434. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.72it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8434. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  3.37it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8432. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.37it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8432. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8521. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.61it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8521. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8592. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.70it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8592. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8664. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.67it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8664. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.90it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8730. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.90it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8730. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  4.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.28s. Loss: 0.8730. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.35it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8284. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 51/70. Data: 0.75s. Batch: 0.81s. Loss: 0.8284. :  33%|███▎      | 1/3 [00:00<00:01,  1.24it/s]Finetune Epoch: 51/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8104. :  33%|███▎      | 1/3 [00:01<00:01,  1.24it/s]Finetune Epoch: 51/70. Data: 0.93s. Batch: 0.99s. Loss: 0.8104. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 51/70. Data: 1.12s. Batch: 1.18s. Loss: 0.8110. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 51/70. Data: 1.12s. Batch: 1.18s. Loss: 0.8110. : 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]Finetune Epoch: 51/70. Data: 1.12s. Batch: 1.18s. Loss: 0.8110. : 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8334. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.62s. Loss: 0.8334. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.62it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8374. top1: 96.29. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.62it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8374. top1: 96.29. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8432. top1: 95.96. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.32it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8432. top1: 95.96. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8430. top1: 95.90. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.72it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8430. top1: 95.90. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8518. top1: 95.16. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.76it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8518. top1: 95.16. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8588. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.73it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8588. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  2.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8660. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  2.99it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8660. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8725. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.24it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8725. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8725. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.83s. Batch: 0.92s. Loss: 0.8204. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 52/70. Data: 0.83s. Batch: 0.92s. Loss: 0.8204. :  33%|███▎      | 1/3 [00:00<00:01,  1.08it/s]Finetune Epoch: 52/70. Data: 0.98s. Batch: 1.06s. Loss: 0.8079. :  33%|███▎      | 1/3 [00:01<00:01,  1.08it/s]Finetune Epoch: 52/70. Data: 0.98s. Batch: 1.06s. Loss: 0.8079. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 52/70. Data: 1.15s. Batch: 1.22s. Loss: 0.8149. :  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s]Finetune Epoch: 52/70. Data: 1.15s. Batch: 1.22s. Loss: 0.8149. : 100%|██████████| 3/3 [00:01<00:00,  2.21it/s]Finetune Epoch: 52/70. Data: 1.15s. Batch: 1.22s. Loss: 0.8149. : 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8331. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.70s. Loss: 0.8331. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.42it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8371. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:04,  1.42it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.52s. Loss: 0.8371. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:02,  2.05it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8429. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.05it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8429. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.43it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8427. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.43it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8427. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8515. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.00it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8515. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8584. top1: 94.53. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.38it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8584. top1: 94.53. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8656. top1: 93.75. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8656. top1: 93.75. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8721. top1: 93.60. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8721. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.75it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8721. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.91it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.64s. Batch: 0.74s. Loss: 0.8074. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 53/70. Data: 0.64s. Batch: 0.74s. Loss: 0.8074. :  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Finetune Epoch: 53/70. Data: 0.79s. Batch: 0.89s. Loss: 0.8100. :  33%|███▎      | 1/3 [00:01<00:01,  1.35it/s]Finetune Epoch: 53/70. Data: 0.79s. Batch: 0.89s. Loss: 0.8100. :  67%|██████▋   | 2/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 53/70. Data: 0.94s. Batch: 1.02s. Loss: 0.8121. :  67%|██████▋   | 2/3 [00:01<00:00,  2.09it/s]Finetune Epoch: 53/70. Data: 0.94s. Batch: 1.02s. Loss: 0.8121. : 100%|██████████| 3/3 [00:01<00:00,  2.72it/s]Finetune Epoch: 53/70. Data: 0.94s. Batch: 1.02s. Loss: 0.8121. : 100%|██████████| 3/3 [00:01<00:00,  2.08it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8328. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.64s. Loss: 0.8328. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.56it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8369. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.56it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8369. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8427. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.30it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8427. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8424. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.59it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8424. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8511. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8511. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8580. top1: 94.53. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.02it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8580. top1: 94.53. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8652. top1: 93.75. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.31it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8652. top1: 93.75. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8717. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8717. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.37it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8717. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.71it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.61s. Batch: 0.68s. Loss: 0.7964. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 54/70. Data: 0.61s. Batch: 0.68s. Loss: 0.7964. :  33%|███▎      | 1/3 [00:00<00:01,  1.48it/s]Finetune Epoch: 54/70. Data: 0.77s. Batch: 0.82s. Loss: 0.8115. :  33%|███▎      | 1/3 [00:00<00:01,  1.48it/s]Finetune Epoch: 54/70. Data: 0.77s. Batch: 0.82s. Loss: 0.8115. :  67%|██████▋   | 2/3 [00:00<00:00,  2.21it/s]Finetune Epoch: 54/70. Data: 0.95s. Batch: 1.00s. Loss: 0.8137. :  67%|██████▋   | 2/3 [00:01<00:00,  2.21it/s]Finetune Epoch: 54/70. Data: 0.95s. Batch: 1.00s. Loss: 0.8137. : 100%|██████████| 3/3 [00:01<00:00,  2.38it/s]Finetune Epoch: 54/70. Data: 0.95s. Batch: 1.00s. Loss: 0.8137. : 100%|██████████| 3/3 [00:01<00:00,  2.01it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8324. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8324. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8365. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.48it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8365. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8424. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.22it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8424. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8421. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.64it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8421. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8508. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.90it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8508. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8576. top1: 94.53. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.87it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8576. top1: 94.53. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8648. top1: 93.75. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.15it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8648. top1: 93.75. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.19it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8712. top1: 93.60. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.19it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8712. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.46it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8712. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.76it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.61s. Batch: 0.68s. Loss: 0.8069. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 55/70. Data: 0.61s. Batch: 0.68s. Loss: 0.8069. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 55/70. Data: 0.76s. Batch: 0.83s. Loss: 0.8118. :  33%|███▎      | 1/3 [00:00<00:01,  1.47it/s]Finetune Epoch: 55/70. Data: 0.76s. Batch: 0.83s. Loss: 0.8118. :  67%|██████▋   | 2/3 [00:00<00:00,  2.18it/s]Finetune Epoch: 55/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8123. :  67%|██████▋   | 2/3 [00:01<00:00,  2.18it/s]Finetune Epoch: 55/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8123. : 100%|██████████| 3/3 [00:01<00:00,  2.60it/s]Finetune Epoch: 55/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8123. : 100%|██████████| 3/3 [00:01<00:00,  2.02it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8321. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8321. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8362. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:03,  1.98it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8362. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.69it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8421. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:00<00:02,  2.69it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8421. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:00<00:01,  3.34it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8418. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  3.34it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8418. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.52it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8504. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.52it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8504. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8573. top1: 94.53. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8573. top1: 94.53. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.66it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8644. top1: 93.75. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.66it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8644. top1: 93.75. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8708. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.65it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8708. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.87it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.29s. Loss: 0.8708. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.12it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.73s. Batch: 0.80s. Loss: 0.7770. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 56/70. Data: 0.73s. Batch: 0.80s. Loss: 0.7770. :  33%|███▎      | 1/3 [00:00<00:01,  1.25it/s]Finetune Epoch: 56/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8069. :  33%|███▎      | 1/3 [00:01<00:01,  1.25it/s]Finetune Epoch: 56/70. Data: 0.94s. Batch: 1.01s. Loss: 0.8069. :  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]Finetune Epoch: 56/70. Data: 1.13s. Batch: 1.20s. Loss: 0.8122. :  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]Finetune Epoch: 56/70. Data: 1.13s. Batch: 1.20s. Loss: 0.8122. : 100%|██████████| 3/3 [00:01<00:00,  2.13it/s]Finetune Epoch: 56/70. Data: 1.13s. Batch: 1.20s. Loss: 0.8122. : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 0.8317. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 0.8317. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.32it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8359. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:05,  1.32it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.50s. Loss: 0.8359. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8418. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.21it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8418. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8415. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8415. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8501. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.95it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8501. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8569. top1: 94.53. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.17it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8569. top1: 94.53. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8640. top1: 93.75. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.30it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8640. top1: 93.75. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8704. top1: 93.60. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8704. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.91it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8704. top1: 93.60. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.65s. Batch: 0.70s. Loss: 0.8369. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 57/70. Data: 0.65s. Batch: 0.70s. Loss: 0.8369. :  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]Finetune Epoch: 57/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8040. :  33%|███▎      | 1/3 [00:01<00:01,  1.43it/s]Finetune Epoch: 57/70. Data: 0.82s. Batch: 0.87s. Loss: 0.8040. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 57/70. Data: 0.98s. Batch: 1.03s. Loss: 0.8115. :  67%|██████▋   | 2/3 [00:01<00:00,  2.04it/s]Finetune Epoch: 57/70. Data: 0.98s. Batch: 1.03s. Loss: 0.8115. : 100%|██████████| 3/3 [00:01<00:00,  2.45it/s]Finetune Epoch: 57/70. Data: 0.98s. Batch: 1.03s. Loss: 0.8115. : 100%|██████████| 3/3 [00:01<00:00,  1.94it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8314. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8314. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8357. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.46it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8357. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8415. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.41it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8415. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8413. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8413. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.19it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8498. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.19it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8498. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8565. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:01<00:00,  3.45it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8565. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:01<00:00,  3.39it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8636. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.39it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8636. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8699. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8699. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8699. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  2.73it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.79s. Batch: 0.85s. Loss: 0.8150. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 58/70. Data: 0.79s. Batch: 0.85s. Loss: 0.8150. :  33%|███▎      | 1/3 [00:00<00:01,  1.18it/s]Finetune Epoch: 58/70. Data: 0.97s. Batch: 1.03s. Loss: 0.8031. :  33%|███▎      | 1/3 [00:01<00:01,  1.18it/s]Finetune Epoch: 58/70. Data: 0.97s. Batch: 1.03s. Loss: 0.8031. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 58/70. Data: 1.16s. Batch: 1.21s. Loss: 0.8061. :  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]Finetune Epoch: 58/70. Data: 1.16s. Batch: 1.21s. Loss: 0.8061. : 100%|██████████| 3/3 [00:01<00:00,  2.14it/s]Finetune Epoch: 58/70. Data: 1.16s. Batch: 1.21s. Loss: 0.8061. : 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 0.8310. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.76s. Loss: 0.8310. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.32it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8353. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.32it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.56s. Loss: 0.8353. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8412. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.91it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8412. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.44it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8409. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.44it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8409. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.63it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8494. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:02<00:01,  2.63it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8494. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:02<00:01,  2.61it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8561. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.61it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8561. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  2.74it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8632. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  2.74it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8632. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8695. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8695. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.47it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8695. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.60it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.70s. Batch: 0.75s. Loss: 0.8195. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 59/70. Data: 0.70s. Batch: 0.75s. Loss: 0.8195. :  33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]Finetune Epoch: 59/70. Data: 0.90s. Batch: 0.98s. Loss: 0.7957. :  33%|███▎      | 1/3 [00:01<00:01,  1.33it/s]Finetune Epoch: 59/70. Data: 0.90s. Batch: 0.98s. Loss: 0.7957. :  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]Finetune Epoch: 59/70. Data: 1.15s. Batch: 1.23s. Loss: 0.8066. :  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]Finetune Epoch: 59/70. Data: 1.15s. Batch: 1.23s. Loss: 0.8066. : 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]Finetune Epoch: 59/70. Data: 1.15s. Batch: 1.23s. Loss: 0.8066. : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8308. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8308. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8351. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8351. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8410. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.18it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8410. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8407. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:01,  2.65it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8407. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8491. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  3.01it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8491. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:00,  3.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8558. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:00,  3.14it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8558. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8628. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  3.09it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8628. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8691. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8691. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.30it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8691. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.61it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8380. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 60/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8380. :  33%|███▎      | 1/3 [00:00<00:01,  1.14it/s]Finetune Epoch: 60/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8335. :  33%|███▎      | 1/3 [00:01<00:01,  1.14it/s]Finetune Epoch: 60/70. Data: 1.02s. Batch: 1.08s. Loss: 0.8335. :  67%|██████▋   | 2/3 [00:01<00:00,  1.68it/s]Finetune Epoch: 60/70. Data: 1.21s. Batch: 1.27s. Loss: 0.8142. :  67%|██████▋   | 2/3 [00:01<00:00,  1.68it/s]Finetune Epoch: 60/70. Data: 1.21s. Batch: 1.27s. Loss: 0.8142. : 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]Finetune Epoch: 60/70. Data: 1.21s. Batch: 1.27s. Loss: 0.8142. : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8304. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.78s. Loss: 0.8304. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.28it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8348. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.28it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.55s. Loss: 0.8348. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8407. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.94it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8407. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.36it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8404. top1: 95.80. top5: 99.71. :  38%|███▊      | 3/8 [00:01<00:02,  2.36it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8404. top1: 95.80. top5: 99.71. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8488. top1: 95.08. top5: 99.69. :  50%|█████     | 4/8 [00:01<00:01,  2.79it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8488. top1: 95.08. top5: 99.69. :  62%|██████▎   | 5/8 [00:01<00:01,  2.93it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8554. top1: 94.60. top5: 99.67. :  62%|██████▎   | 5/8 [00:02<00:01,  2.93it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8554. top1: 94.60. top5: 99.67. :  75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8624. top1: 93.81. top5: 99.72. :  75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8624. top1: 93.81. top5: 99.72. :  88%|████████▊ | 7/8 [00:02<00:00,  3.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8687. top1: 93.65. top5: 99.65. :  88%|████████▊ | 7/8 [00:02<00:00,  3.11it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8687. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:02<00:00,  3.18it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8687. top1: 93.65. top5: 99.65. : 100%|██████████| 8/8 [00:03<00:00,  2.60it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.82s. Batch: 0.89s. Loss: 0.8178. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 61/70. Data: 0.82s. Batch: 0.89s. Loss: 0.8178. :  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Finetune Epoch: 61/70. Data: 0.98s. Batch: 1.05s. Loss: 0.8075. :  33%|███▎      | 1/3 [00:01<00:01,  1.12it/s]Finetune Epoch: 61/70. Data: 0.98s. Batch: 1.05s. Loss: 0.8075. :  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s]Finetune Epoch: 61/70. Data: 1.14s. Batch: 1.21s. Loss: 0.8108. :  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s]Finetune Epoch: 61/70. Data: 1.14s. Batch: 1.21s. Loss: 0.8108. : 100%|██████████| 3/3 [00:01<00:00,  2.28it/s]Finetune Epoch: 61/70. Data: 1.14s. Batch: 1.21s. Loss: 0.8108. : 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8301. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.80s. Loss: 0.8301. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.26it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8346. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:01<00:05,  1.26it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.57s. Loss: 0.8346. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:01<00:03,  1.90it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8405. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:03,  1.90it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8405. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:02,  2.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8402. top1: 95.80. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:02,  2.31it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8402. top1: 95.80. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  2.58it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8484. top1: 95.16. top5: 99.77. :  50%|█████     | 4/8 [00:02<00:01,  2.58it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.43s. Loss: 0.8484. top1: 95.16. top5: 99.77. :  62%|██████▎   | 5/8 [00:02<00:01,  2.61it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8550. top1: 94.60. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.61it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8550. top1: 94.60. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.75it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8620. top1: 93.81. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.75it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8620. top1: 93.81. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  2.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8682. top1: 93.70. top5: 99.70. :  88%|████████▊ | 7/8 [00:03<00:00,  2.83it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8682. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:03<00:00,  3.21it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8682. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:03<00:00,  2.44it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.69s. Batch: 0.79s. Loss: 0.8132. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 62/70. Data: 0.69s. Batch: 0.79s. Loss: 0.8132. :  33%|███▎      | 1/3 [00:00<00:01,  1.26it/s]Finetune Epoch: 62/70. Data: 0.84s. Batch: 0.92s. Loss: 0.8219. :  33%|███▎      | 1/3 [00:01<00:01,  1.26it/s]Finetune Epoch: 62/70. Data: 0.84s. Batch: 0.92s. Loss: 0.8219. :  67%|██████▋   | 2/3 [00:01<00:00,  2.08it/s]Finetune Epoch: 62/70. Data: 0.99s. Batch: 1.07s. Loss: 0.8126. :  67%|██████▋   | 2/3 [00:01<00:00,  2.08it/s]Finetune Epoch: 62/70. Data: 0.99s. Batch: 1.07s. Loss: 0.8126. : 100%|██████████| 3/3 [00:01<00:00,  2.54it/s]Finetune Epoch: 62/70. Data: 0.99s. Batch: 1.07s. Loss: 0.8126. : 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.8298. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.74s. Loss: 0.8298. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:05,  1.35it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8343. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:05,  1.35it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.49s. Loss: 0.8343. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8402. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.25it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8402. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8399. top1: 95.80. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  2.73it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8399. top1: 95.80. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  3.22it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8481. top1: 95.16. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.22it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8481. top1: 95.16. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.53it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8547. top1: 94.60. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.53it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8547. top1: 94.60. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8616. top1: 93.81. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.60it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8616. top1: 93.81. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8678. top1: 93.70. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.38it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8678. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8678. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.67s. Batch: 0.76s. Loss: 0.8241. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 63/70. Data: 0.67s. Batch: 0.76s. Loss: 0.8241. :  33%|███▎      | 1/3 [00:00<00:01,  1.32it/s]Finetune Epoch: 63/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8126. :  33%|███▎      | 1/3 [00:00<00:01,  1.32it/s]Finetune Epoch: 63/70. Data: 0.81s. Batch: 0.88s. Loss: 0.8126. :  67%|██████▋   | 2/3 [00:00<00:00,  2.20it/s]Finetune Epoch: 63/70. Data: 0.98s. Batch: 1.05s. Loss: 0.8076. :  67%|██████▋   | 2/3 [00:01<00:00,  2.20it/s]Finetune Epoch: 63/70. Data: 0.98s. Batch: 1.05s. Loss: 0.8076. : 100%|██████████| 3/3 [00:01<00:00,  2.33it/s]Finetune Epoch: 63/70. Data: 0.98s. Batch: 1.05s. Loss: 0.8076. : 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8295. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8295. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8341. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8341. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.49it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8400. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.49it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8400. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8397. top1: 95.80. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  2.90it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8397. top1: 95.80. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8478. top1: 95.16. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.11it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8478. top1: 95.16. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8543. top1: 94.60. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:00,  3.04it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8543. top1: 94.60. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8612. top1: 93.81. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.26it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8612. top1: 93.81. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8674. top1: 93.70. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.50it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8674. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.97it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8674. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.58s. Batch: 0.65s. Loss: 0.8253. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 64/70. Data: 0.58s. Batch: 0.65s. Loss: 0.8253. :  33%|███▎      | 1/3 [00:00<00:01,  1.53it/s]Finetune Epoch: 64/70. Data: 0.74s. Batch: 0.81s. Loss: 0.8212. :  33%|███▎      | 1/3 [00:00<00:01,  1.53it/s]Finetune Epoch: 64/70. Data: 0.74s. Batch: 0.81s. Loss: 0.8212. :  67%|██████▋   | 2/3 [00:00<00:00,  2.23it/s]Finetune Epoch: 64/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8116. :  67%|██████▋   | 2/3 [00:01<00:00,  2.23it/s]Finetune Epoch: 64/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8116. : 100%|██████████| 3/3 [00:01<00:00,  2.62it/s]Finetune Epoch: 64/70. Data: 0.90s. Batch: 0.96s. Loss: 0.8116. : 100%|██████████| 3/3 [00:01<00:00,  2.01it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8292. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.69s. Loss: 0.8292. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8338. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8338. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8397. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.41s. Loss: 0.8397. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.70it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8394. top1: 95.80. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  2.70it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8394. top1: 95.80. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  2.86it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8475. top1: 95.16. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.86it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8475. top1: 95.16. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8540. top1: 94.60. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.99it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8540. top1: 94.60. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8608. top1: 93.81. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8608. top1: 93.81. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8670. top1: 93.70. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8670. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.28it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8670. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.76s. Batch: 0.84s. Loss: 0.8184. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 65/70. Data: 0.76s. Batch: 0.84s. Loss: 0.8184. :  33%|███▎      | 1/3 [00:00<00:01,  1.19it/s]Finetune Epoch: 65/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8134. :  33%|███▎      | 1/3 [00:01<00:01,  1.19it/s]Finetune Epoch: 65/70. Data: 0.92s. Batch: 0.98s. Loss: 0.8134. :  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s]Finetune Epoch: 65/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8091. :  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s]Finetune Epoch: 65/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8091. : 100%|██████████| 3/3 [00:01<00:00,  2.36it/s]Finetune Epoch: 65/70. Data: 1.08s. Batch: 1.14s. Loss: 0.8091. : 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8290. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.66s. Loss: 0.8290. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8336. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8336. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8395. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.39it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8395. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8392. top1: 95.80. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  2.78it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8392. top1: 95.80. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  3.22it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8472. top1: 95.16. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.22it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8472. top1: 95.16. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.40it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8536. top1: 94.60. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.40it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8536. top1: 94.60. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8605. top1: 93.81. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.45it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8605. top1: 93.81. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.47it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8666. top1: 93.70. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.47it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8666. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.59it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8666. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  2.90it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.80s. Batch: 0.87s. Loss: 0.8083. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 66/70. Data: 0.80s. Batch: 0.87s. Loss: 0.8083. :  33%|███▎      | 1/3 [00:00<00:01,  1.15it/s]Finetune Epoch: 66/70. Data: 0.95s. Batch: 1.01s. Loss: 0.7985. :  33%|███▎      | 1/3 [00:01<00:01,  1.15it/s]Finetune Epoch: 66/70. Data: 0.95s. Batch: 1.01s. Loss: 0.7985. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 66/70. Data: 1.10s. Batch: 1.16s. Loss: 0.8104. :  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Finetune Epoch: 66/70. Data: 1.10s. Batch: 1.16s. Loss: 0.8104. : 100%|██████████| 3/3 [00:01<00:00,  2.34it/s]Finetune Epoch: 66/70. Data: 1.10s. Batch: 1.16s. Loss: 0.8104. : 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8287. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.60s. Loss: 0.8287. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8333. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8333. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8393. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.60it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8393. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.79it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8390. top1: 95.80. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  2.79it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.36s. Loss: 0.8390. top1: 95.80. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  3.03it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8469. top1: 95.16. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.03it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8469. top1: 95.16. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8533. top1: 94.60. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8533. top1: 94.60. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.47it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8601. top1: 93.81. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.47it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8601. top1: 93.81. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8662. top1: 93.70. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.53it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8662. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.90it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8662. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.77s. Batch: 0.81s. Loss: 0.8188. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 67/70. Data: 0.77s. Batch: 0.81s. Loss: 0.8188. :  33%|███▎      | 1/3 [00:00<00:01,  1.23it/s]Finetune Epoch: 67/70. Data: 0.94s. Batch: 1.00s. Loss: 0.7925. :  33%|███▎      | 1/3 [00:01<00:01,  1.23it/s]Finetune Epoch: 67/70. Data: 0.94s. Batch: 1.00s. Loss: 0.7925. :  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s]Finetune Epoch: 67/70. Data: 1.10s. Batch: 1.16s. Loss: 0.8062. :  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s]Finetune Epoch: 67/70. Data: 1.10s. Batch: 1.16s. Loss: 0.8062. : 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 67/70. Data: 1.10s. Batch: 1.16s. Loss: 0.8062. : 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8285. top1: 96.09. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.65s. Loss: 0.8285. top1: 96.09. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8331. top1: 96.09. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.46s. Loss: 0.8331. top1: 96.09. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8391. top1: 95.83. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.37it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8391. top1: 95.83. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.80it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8388. top1: 95.80. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  2.80it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8388. top1: 95.80. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  3.37it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8466. top1: 95.16. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.37it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8466. top1: 95.16. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.46it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8530. top1: 94.60. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.46it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8530. top1: 94.60. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.68it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8597. top1: 93.81. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.68it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.31s. Loss: 0.8597. top1: 93.81. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8658. top1: 93.70. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8658. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.90it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.30s. Loss: 0.8658. top1: 93.70. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 68/70. Data: 0.79s. Batch: 0.86s. Loss: 0.8138. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 68/70. Data: 0.79s. Batch: 0.86s. Loss: 0.8138. :  33%|███▎      | 1/3 [00:00<00:01,  1.16it/s]Finetune Epoch: 68/70. Data: 0.97s. Batch: 1.03s. Loss: 0.8200. :  33%|███▎      | 1/3 [00:01<00:01,  1.16it/s]Finetune Epoch: 68/70. Data: 0.97s. Batch: 1.03s. Loss: 0.8200. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 68/70. Data: 1.13s. Batch: 1.19s. Loss: 0.8026. :  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s]Finetune Epoch: 68/70. Data: 1.13s. Batch: 1.19s. Loss: 0.8026. : 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]Finetune Epoch: 68/70. Data: 1.13s. Batch: 1.19s. Loss: 0.8026. : 100%|██████████| 3/3 [00:01<00:00,  1.75it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8282. top1: 95.70. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.61s. Loss: 0.8282. top1: 95.70. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.63it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8330. top1: 95.90. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.63it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.44s. Loss: 0.8330. top1: 95.90. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8389. top1: 95.70. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.40s. Loss: 0.8389. top1: 95.70. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.66it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8386. top1: 95.70. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  2.66it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8386. top1: 95.70. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  2.87it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8464. top1: 95.08. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  2.87it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.38s. Loss: 0.8464. top1: 95.08. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:01,  2.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8527. top1: 94.53. top5: 99.74. :  62%|██████▎   | 5/8 [00:02<00:01,  2.82it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8527. top1: 94.53. top5: 99.74. :  75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8593. top1: 93.75. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8593. top1: 93.75. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8654. top1: 93.65. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.14it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8654. top1: 93.65. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.49it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8654. top1: 93.65. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.75s. Batch: 0.82s. Loss: 0.8187. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 69/70. Data: 0.75s. Batch: 0.82s. Loss: 0.8187. :  33%|███▎      | 1/3 [00:00<00:01,  1.21it/s]Finetune Epoch: 69/70. Data: 0.93s. Batch: 1.01s. Loss: 0.8055. :  33%|███▎      | 1/3 [00:01<00:01,  1.21it/s]Finetune Epoch: 69/70. Data: 0.93s. Batch: 1.01s. Loss: 0.8055. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 69/70. Data: 1.12s. Batch: 1.19s. Loss: 0.8098. :  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s]Finetune Epoch: 69/70. Data: 1.12s. Batch: 1.19s. Loss: 0.8098. : 100%|██████████| 3/3 [00:01<00:00,  2.17it/s]Finetune Epoch: 69/70. Data: 1.12s. Batch: 1.19s. Loss: 0.8098. : 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8280. top1: 95.70. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.67s. Loss: 0.8280. top1: 95.70. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8327. top1: 95.90. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.50it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.48s. Loss: 0.8327. top1: 95.90. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8387. top1: 95.70. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.23it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.42s. Loss: 0.8387. top1: 95.70. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8384. top1: 95.70. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  2.67it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8384. top1: 95.70. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  3.12it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8461. top1: 95.08. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.12it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.34s. Loss: 0.8461. top1: 95.08. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8523. top1: 94.53. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.51it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8523. top1: 94.53. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.75it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8590. top1: 93.75. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.75it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8590. top1: 93.75. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.41it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8650. top1: 93.65. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.41it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8650. top1: 93.65. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.58it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8650. top1: 93.65. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8213. :   0%|          | 0/3 [00:00<?, ?it/s]Finetune Epoch: 70/70. Data: 0.77s. Batch: 0.85s. Loss: 0.8213. :  33%|███▎      | 1/3 [00:00<00:01,  1.18it/s]Finetune Epoch: 70/70. Data: 0.95s. Batch: 1.02s. Loss: 0.8039. :  33%|███▎      | 1/3 [00:01<00:01,  1.18it/s]Finetune Epoch: 70/70. Data: 0.95s. Batch: 1.02s. Loss: 0.8039. :  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s]Finetune Epoch: 70/70. Data: 1.13s. Batch: 1.19s. Loss: 0.8103. :  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s]Finetune Epoch: 70/70. Data: 1.13s. Batch: 1.19s. Loss: 0.8103. : 100%|██████████| 3/3 [00:01<00:00,  2.18it/s]Finetune Epoch: 70/70. Data: 1.13s. Batch: 1.19s. Loss: 0.8103. : 100%|██████████| 3/3 [00:01<00:00,  1.68it/s]
  0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8278. top1: 95.70. top5: 100.00. :   0%|          | 0/8 [00:00<?, ?it/s]Test Iter:   1/  8. Data: 0.00s. Batch: 0.68s. Loss: 0.8278. top1: 95.70. top5: 100.00. :  12%|█▎        | 1/8 [00:00<00:04,  1.47it/s]Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8326. top1: 95.90. top5: 99.80. :  12%|█▎        | 1/8 [00:00<00:04,  1.47it/s] Test Iter:   2/  8. Data: 0.00s. Batch: 0.45s. Loss: 0.8326. top1: 95.90. top5: 99.80. :  25%|██▌       | 2/8 [00:00<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8386. top1: 95.70. top5: 99.74. :  25%|██▌       | 2/8 [00:01<00:02,  2.47it/s]Test Iter:   3/  8. Data: 0.00s. Batch: 0.39s. Loss: 0.8386. top1: 95.70. top5: 99.74. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8382. top1: 95.70. top5: 99.80. :  38%|███▊      | 3/8 [00:01<00:01,  2.88it/s]Test Iter:   4/  8. Data: 0.00s. Batch: 0.37s. Loss: 0.8382. top1: 95.70. top5: 99.80. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8458. top1: 95.08. top5: 99.77. :  50%|█████     | 4/8 [00:01<00:01,  3.04it/s]Test Iter:   5/  8. Data: 0.00s. Batch: 0.35s. Loss: 0.8458. top1: 95.08. top5: 99.77. :  62%|██████▎   | 5/8 [00:01<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8520. top1: 94.53. top5: 99.74. :  62%|██████▎   | 5/8 [00:01<00:00,  3.30it/s]Test Iter:   6/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8520. top1: 94.53. top5: 99.74. :  75%|███████▌  | 6/8 [00:01<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8586. top1: 93.75. top5: 99.78. :  75%|███████▌  | 6/8 [00:02<00:00,  3.49it/s]Test Iter:   7/  8. Data: 0.00s. Batch: 0.33s. Loss: 0.8586. top1: 93.75. top5: 99.78. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8646. top1: 93.65. top5: 99.70. :  88%|████████▊ | 7/8 [00:02<00:00,  3.42it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8646. top1: 93.65. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]Test Iter:   8/  8. Data: 0.00s. Batch: 0.32s. Loss: 0.8646. top1: 93.65. top5: 99.70. : 100%|██████████| 8/8 [00:02<00:00,  2.94it/s]
total 9984 correct 8129 accuracy 81.42027243589743
[INFO] main.py:349 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 10 -> 10
[INFO] main.py:357 > [2-3] Start to train under online
[INFO] main.py:372 > Train over streamed data once
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42 pseudo_stream_size 42
num_stuff 237
[INFO] rainbow_memory.py:120 > Streamed samples: 800
[INFO] rainbow_memory.py:121 > In-memory samples: 500
[INFO] rainbow_memory.py:122 > Pseudo samples: 9984
[INFO] rainbow_memory.py:128 > Train samples: 11284
[INFO] rainbow_memory.py:129 > Test samples: 10000
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([38, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
stream torch.Size([44, 3, 32, 32]) mem torch.Size([42, 3, 32, 32]) pseudo torch.Size([42, 3, 32, 32])
last_idx 17
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
pseudo shape torch.Size([42, 3, 32, 32])
final_idx 237
task4/train/loss 0.5338673469524423 0
task4/test/loss 6.84974807683836 0
task4/test/acc 0.1891 0
task4/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 1/1 | train_loss 0.5339 | train_acc 0.7326 | test_loss 6.8497 | test_acc 0.1891 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[INFO] finetune.py:679 > Compute uncertainty by vr_randaug!
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
dog           50
deer          50
automobile    50
bird          50
truck         50
frog          50
horse         50
airplane      50
cat           50
ship          50
Name: klass, dtype: int64
[INFO] main.py:388 > Train over memory
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42 pseudo_stream_size 42
num_stuff 0
[INFO] rainbow_memory.py:120 > Streamed samples: 0
[INFO] rainbow_memory.py:121 > In-memory samples: 500
[INFO] rainbow_memory.py:122 > Pseudo samples: 0
[INFO] rainbow_memory.py:128 > Train samples: 500
[INFO] rainbow_memory.py:129 > Test samples: 10000
last_idx 11
final_idx 0
task4/train/loss 4.6478743354479475 0
task4/test/loss 2.701119757012317 0
task4/test/acc 0.1856 0
task4/train/lr 0.005000000000000001 0
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 1/256 | train_loss 4.6479 | train_acc 0.1760 | test_loss 2.7011 | test_acc 0.1856 | lr 0.0050
last_idx 11
final_idx 0
task4/train/loss 2.2717200020949044 1
task4/test/loss 2.9639787998115805 1
task4/test/acc 0.3742 1
task4/train/lr 0.05 1
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 2/256 | train_loss 2.2717 | train_acc 0.4900 | test_loss 2.9640 | test_acc 0.3742 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 2.190876523653666 2
task4/test/loss 2.0558547777564904 2
task4/test/acc 0.4562 2
task4/train/lr 0.05 2
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 3/256 | train_loss 2.1909 | train_acc 0.5920 | test_loss 2.0559 | test_acc 0.4562 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 1.7981736660003662 3
task4/test/loss 1.7846283976707542 3
task4/test/acc 0.5135 3
task4/train/lr 0.02525 3
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 4/256 | train_loss 1.7982 | train_acc 0.6300 | test_loss 1.7846 | test_acc 0.5135 | lr 0.0253
last_idx 11
final_idx 0
task4/train/loss 1.5026229321956635 4
task4/test/loss 1.5953398424020984 4
task4/test/acc 0.4944 4
task4/train/lr 0.05 4
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 5/256 | train_loss 1.5026 | train_acc 0.6140 | test_loss 1.5953 | test_acc 0.4944 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 1.276161606113116 5
task4/test/loss 1.4133988771783679 5
task4/test/acc 0.542 5
task4/train/lr 0.04275089283436705 5
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 6/256 | train_loss 1.2762 | train_acc 0.6460 | test_loss 1.4134 | test_acc 0.5420 | lr 0.0428
last_idx 11
final_idx 0
task4/train/loss 1.1501354724168777 6
task4/test/loss 1.5321757568601977 6
task4/test/acc 0.5149 6
task4/train/lr 0.02525 6
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 7/256 | train_loss 1.1501 | train_acc 0.6880 | test_loss 1.5322 | test_acc 0.5149 | lr 0.0253
last_idx 11
final_idx 0
task4/train/loss 1.1187740166982014 7
task4/test/loss 1.3602751762720577 7
task4/test/acc 0.5568 7
task4/train/lr 0.00774910716563295 7
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 8/256 | train_loss 1.1188 | train_acc 0.7060 | test_loss 1.3603 | test_acc 0.5568 | lr 0.0077
last_idx 11
final_idx 0
task4/train/loss 1.2892888933420181 8
task4/test/loss 1.4952667284952967 8
task4/test/acc 0.508 8
task4/train/lr 0.05 8
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 9/256 | train_loss 1.2893 | train_acc 0.6380 | test_loss 1.4953 | test_acc 0.5080 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 1.3482126295566559 9
task4/test/loss 1.5948373726324032 9
task4/test/acc 0.4607 9
task4/train/lr 0.04811601842965435 9
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 10/256 | train_loss 1.3482 | train_acc 0.5700 | test_loss 1.5948 | test_acc 0.4607 | lr 0.0481
last_idx 11
final_idx 0
task4/train/loss 0.9914418458938599 10
task4/test/loss 1.662706056184936 10
task4/test/acc 0.48 10
task4/train/lr 0.04275089283436705 10
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 11/256 | train_loss 0.9914 | train_acc 0.7100 | test_loss 1.6627 | test_acc 0.4800 | lr 0.0428
last_idx 11
final_idx 0
task4/train/loss 1.0472588539123535 11
task4/test/loss 1.6554766251591213 11
task4/test/acc 0.4641 11
task4/train/lr 0.03472141495103598 11
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 12/256 | train_loss 1.0473 | train_acc 0.6700 | test_loss 1.6555 | test_acc 0.4641 | lr 0.0347
last_idx 11
final_idx 0
task4/train/loss 1.0511063486337662 12
task4/test/loss 1.4231725353420825 12
task4/test/acc 0.5306 12
task4/train/lr 0.02525 12
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 13/256 | train_loss 1.0511 | train_acc 0.6520 | test_loss 1.4232 | test_acc 0.5306 | lr 0.0253
last_idx 11
final_idx 0
task4/train/loss 0.8377124418814977 13
task4/test/loss 1.3048740852820246 13
task4/test/acc 0.5609 13
task4/train/lr 0.01577858504896403 13
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 14/256 | train_loss 0.8377 | train_acc 0.7700 | test_loss 1.3049 | test_acc 0.5609 | lr 0.0158
last_idx 11
final_idx 0
task4/train/loss 0.8593121791879336 14
task4/test/loss 1.2759960149986702 14
task4/test/acc 0.5637 14
task4/train/lr 0.00774910716563295 14
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 15/256 | train_loss 0.8593 | train_acc 0.7100 | test_loss 1.2760 | test_acc 0.5637 | lr 0.0077
last_idx 11
final_idx 0
task4/train/loss 0.894796813527743 15
task4/test/loss 1.2590605078036325 15
task4/test/acc 0.5721 15
task4/train/lr 0.0023839815703456534 15
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 16/256 | train_loss 0.8948 | train_acc 0.7700 | test_loss 1.2591 | test_acc 0.5721 | lr 0.0024
last_idx 11
final_idx 0
task4/train/loss 1.2754740466674168 16
task4/test/loss 1.5973662396794872 16
task4/test/acc 0.472 16
task4/train/lr 0.05 16
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 17/256 | train_loss 1.2755 | train_acc 0.5920 | test_loss 1.5974 | test_acc 0.4720 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 1.0499801486730576 17
task4/test/loss 1.538119082126701 17
task4/test/acc 0.4935 17
task4/train/lr 0.049524435689979954 17
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 18/256 | train_loss 1.0500 | train_acc 0.6820 | test_loss 1.5381 | test_acc 0.4935 | lr 0.0495
last_idx 11
final_idx 0
task4/train/loss 1.182057186961174 18
task4/test/loss 1.3192699230030964 18
task4/test/acc 0.5511 18
task4/train/lr 0.04811601842965435 18
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 19/256 | train_loss 1.1821 | train_acc 0.6500 | test_loss 1.3193 | test_acc 0.5511 | lr 0.0481
last_idx 11
final_idx 0
task4/train/loss 1.2259339193503063 19
task4/test/loss 1.4711118826740666 19
task4/test/acc 0.5032 19
task4/train/lr 0.045828872904488 19
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 20/256 | train_loss 1.2259 | train_acc 0.5960 | test_loss 1.4711 | test_acc 0.5032 | lr 0.0458
last_idx 11
final_idx 0
task4/train/loss 1.143275278309981 20
task4/test/loss 1.4246278138537156 20
task4/test/acc 0.5178 20
task4/train/lr 0.04275089283436705 20
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 21/256 | train_loss 1.1433 | train_acc 0.6620 | test_loss 1.4246 | test_acc 0.5178 | lr 0.0428
last_idx 11
final_idx 0
task4/train/loss 1.0034076099594433 21
task4/test/loss 1.3351187878533413 21
task4/test/acc 0.5525 21
task4/train/lr 0.039000363267235154 21
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 22/256 | train_loss 1.0034 | train_acc 0.7000 | test_loss 1.3351 | test_acc 0.5525 | lr 0.0390
last_idx 11
final_idx 0
task4/train/loss 0.8641831005613009 22
task4/test/loss 1.4039773576353725 22
task4/test/acc 0.529 22
task4/train/lr 0.03472141495103598 22
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 23/256 | train_loss 0.8642 | train_acc 0.7360 | test_loss 1.4040 | test_acc 0.5290 | lr 0.0347
last_idx 11
final_idx 0
task4/train/loss 1.111911155283451 23
task4/test/loss 1.3821571435321842 23
task4/test/acc 0.5293 23
task4/train/lr 0.03007848546989918 23
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 24/256 | train_loss 1.1119 | train_acc 0.7460 | test_loss 1.3822 | test_acc 0.5293 | lr 0.0301
last_idx 11
final_idx 0
task4/train/loss 0.8218796625733376 24
task4/test/loss 1.3912355217494463 24
task4/test/acc 0.5371 24
task4/train/lr 0.02525 24
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 25/256 | train_loss 0.8219 | train_acc 0.7660 | test_loss 1.3912 | test_acc 0.5371 | lr 0.0253
last_idx 11
final_idx 0
task4/train/loss 1.1464721336960793 25
task4/test/loss 1.2779171898176795 25
task4/test/acc 0.5696 25
task4/train/lr 0.02042151453010083 25
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 26/256 | train_loss 1.1465 | train_acc 0.7360 | test_loss 1.2779 | test_acc 0.5696 | lr 0.0204
last_idx 11
final_idx 0
task4/train/loss 0.9767026677727699 26
task4/test/loss 1.2620326502804171 26
task4/test/acc 0.571 26
task4/train/lr 0.01577858504896403 26
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 27/256 | train_loss 0.9767 | train_acc 0.6880 | test_loss 1.2620 | test_acc 0.5710 | lr 0.0158
last_idx 11
final_idx 0
task4/train/loss 1.1506550908088684 27
task4/test/loss 1.2031601383759265 27
task4/test/acc 0.584 27
task4/train/lr 0.011499636732764853 27
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 28/256 | train_loss 1.1507 | train_acc 0.6160 | test_loss 1.2032 | test_acc 0.5840 | lr 0.0115
last_idx 11
final_idx 0
task4/train/loss 0.6045513624946276 28
task4/test/loss 1.1962012486499654 28
task4/test/acc 0.5914 28
task4/train/lr 0.00774910716563295 28
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 29/256 | train_loss 0.6046 | train_acc 0.8380 | test_loss 1.1962 | test_acc 0.5914 | lr 0.0077
last_idx 11
final_idx 0
task4/train/loss 0.6828132991989454 29
task4/test/loss 1.1879323748381514 29
task4/test/acc 0.5939 29
task4/train/lr 0.004671127095512003 29
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 30/256 | train_loss 0.6828 | train_acc 0.8260 | test_loss 1.1879 | test_acc 0.5939 | lr 0.0047
last_idx 11
final_idx 0
task4/train/loss 0.7595305517315865 30
task4/test/loss 1.1788846446495307 30
task4/test/acc 0.5991 30
task4/train/lr 0.0023839815703456534 30
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 31/256 | train_loss 0.7595 | train_acc 0.7380 | test_loss 1.1789 | test_acc 0.5991 | lr 0.0024
last_idx 11
final_idx 0
task4/train/loss 0.9056147659818331 31
task4/test/loss 1.1820973556554109 31
task4/test/acc 0.5951 31
task4/train/lr 0.0009755643100200469 31
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 32/256 | train_loss 0.9056 | train_acc 0.7200 | test_loss 1.1821 | test_acc 0.5951 | lr 0.0010
last_idx 11
final_idx 0
task4/train/loss 1.1202492862939835 32
task4/test/loss 1.4509493713839012 32
task4/test/acc 0.5272 32
task4/train/lr 0.05 32
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 33/256 | train_loss 1.1202 | train_acc 0.6880 | test_loss 1.4509 | test_acc 0.5272 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 1.0477169478933017 33
task4/test/loss 1.5496491086587572 33
task4/test/acc 0.4925 33
task4/train/lr 0.049880821985136874 33
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 34/256 | train_loss 1.0477 | train_acc 0.7080 | test_loss 1.5496 | test_acc 0.4925 | lr 0.0499
last_idx 11
final_idx 0
task4/train/loss 1.0228820617000263 34
task4/test/loss 1.6482776749813766 34
task4/test/acc 0.4895 34
task4/train/lr 0.049524435689979954 34
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 35/256 | train_loss 1.0229 | train_acc 0.7560 | test_loss 1.6483 | test_acc 0.4895 | lr 0.0495
last_idx 11
final_idx 0
task4/train/loss 1.0272297039628029 35
task4/test/loss 1.6701202435713065 35
task4/test/acc 0.4846 35
task4/train/lr 0.048934273309372174 35
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 36/256 | train_loss 1.0272 | train_acc 0.6980 | test_loss 1.6701 | test_acc 0.4846 | lr 0.0489
last_idx 11
final_idx 0
task4/train/loss 0.9741148898998896 36
task4/test/loss 1.653647673038537 36
task4/test/acc 0.4822 36
task4/train/lr 0.04811601842965435 36
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 37/256 | train_loss 0.9741 | train_acc 0.7280 | test_loss 1.6536 | test_acc 0.4822 | lr 0.0481
last_idx 11
final_idx 0
task4/train/loss 1.1006161173184712 37
task4/test/loss 1.6053490451814836 37
task4/test/acc 0.4988 37
task4/train/lr 0.04707755129262179 37
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 38/256 | train_loss 1.1006 | train_acc 0.6500 | test_loss 1.6053 | test_acc 0.4988 | lr 0.0471
last_idx 11
final_idx 0
task4/train/loss 0.7501812974611918 38
task4/test/loss 1.367470672648204 38
task4/test/acc 0.5528 38
task4/train/lr 0.045828872904488 38
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 39/256 | train_loss 0.7502 | train_acc 0.7960 | test_loss 1.3675 | test_acc 0.5528 | lr 0.0458
last_idx 11
final_idx 0
task4/train/loss 0.8760042215387026 39
task4/test/loss 1.5626241769968419 39
task4/test/acc 0.512 39
task4/train/lr 0.04438200872072774 39
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 40/256 | train_loss 0.8760 | train_acc 0.7340 | test_loss 1.5626 | test_acc 0.5120 | lr 0.0444
last_idx 11
final_idx 0
task4/train/loss 0.7750709988176823 40
task4/test/loss 1.3946590687622105 40
task4/test/acc 0.5439 40
task4/train/lr 0.04275089283436705 40
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 41/256 | train_loss 0.7751 | train_acc 0.8040 | test_loss 1.3947 | test_acc 0.5439 | lr 0.0428
last_idx 11
final_idx 0
task4/train/loss 0.96286491304636 41
task4/test/loss 1.9066843059763574 41
task4/test/acc 0.4529 41
task4/train/lr 0.040951233783050225 41
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 42/256 | train_loss 0.9629 | train_acc 0.6960 | test_loss 1.9067 | test_acc 0.4529 | lr 0.0410
last_idx 11
final_idx 0
task4/train/loss 0.7962126781543096 42
task4/test/loss 1.3607134098807971 42
task4/test/acc 0.5521 42
task4/train/lr 0.039000363267235154 42
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 43/256 | train_loss 0.7962 | train_acc 0.8000 | test_loss 1.3607 | test_acc 0.5521 | lr 0.0390
last_idx 11
final_idx 0
task4/train/loss 0.9338622465729713 43
task4/test/loss 1.4475318305521159 43
task4/test/acc 0.5417 43
task4/train/lr 0.03691706923644345 43
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 44/256 | train_loss 0.9339 | train_acc 0.7440 | test_loss 1.4475 | test_acc 0.5417 | lr 0.0369
last_idx 11
final_idx 0
task4/train/loss 0.7788635889689127 44
task4/test/loss 1.3973510737220447 44
task4/test/acc 0.5591 44
task4/train/lr 0.03472141495103598 44
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 45/256 | train_loss 0.7789 | train_acc 0.7940 | test_loss 1.3974 | test_acc 0.5591 | lr 0.0347
last_idx 11
final_idx 0
task4/train/loss 0.8304692879319191 45
task4/test/loss 1.3476057753228305 45
task4/test/acc 0.5658 45
task4/train/lr 0.03243454576204794 45
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 46/256 | train_loss 0.8305 | train_acc 0.7480 | test_loss 1.3476 | test_acc 0.5658 | lr 0.0324
last_idx 11
final_idx 0
task4/train/loss 0.7610139958560467 46
task4/test/loss 1.3308800925549709 46
task4/test/acc 0.5728 46
task4/train/lr 0.03007848546989918 46
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 47/256 | train_loss 0.7610 | train_acc 0.8500 | test_loss 1.3309 | test_acc 0.5728 | lr 0.0301
last_idx 11
final_idx 0
task4/train/loss 0.6020347128311793 47
task4/test/loss 1.3440628642575783 47
task4/test/acc 0.5757 47
task4/train/lr 0.027675924223156633 47
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 48/256 | train_loss 0.6020 | train_acc 0.8100 | test_loss 1.3441 | test_acc 0.5757 | lr 0.0277
last_idx 11
final_idx 0
task4/train/loss 0.7158036070565382 48
task4/test/loss 1.3734220399668342 48
task4/test/acc 0.5723 48
task4/train/lr 0.02525 48
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 49/256 | train_loss 0.7158 | train_acc 0.7780 | test_loss 1.3734 | test_acc 0.5723 | lr 0.0253
last_idx 11
final_idx 0
task4/train/loss 1.191346898674965 49
task4/test/loss 1.3765968204590313 49
task4/test/acc 0.5516 49
task4/train/lr 0.022824075776843374 49
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 50/256 | train_loss 1.1913 | train_acc 0.6020 | test_loss 1.3766 | test_acc 0.5516 | lr 0.0228
last_idx 11
final_idx 0
task4/train/loss 1.0034867996970813 50
task4/test/loss 1.2247312347355641 50
task4/test/acc 0.593 50
task4/train/lr 0.02042151453010083 50
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 51/256 | train_loss 1.0035 | train_acc 0.6960 | test_loss 1.2247 | test_acc 0.5930 | lr 0.0204
last_idx 11
final_idx 0
task4/train/loss 0.9884526158372561 51
task4/test/loss 1.2393438495826303 51
task4/test/acc 0.5821 51
task4/train/lr 0.018065454237952062 51
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 52/256 | train_loss 0.9885 | train_acc 0.7520 | test_loss 1.2393 | test_acc 0.5821 | lr 0.0181
last_idx 11
final_idx 0
task4/train/loss 1.1537507722775142 52
task4/test/loss 1.2357444437710863 52
task4/test/acc 0.5886 52
task4/train/lr 0.01577858504896403 52
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 53/256 | train_loss 1.1538 | train_acc 0.6360 | test_loss 1.2357 | test_acc 0.5886 | lr 0.0158
last_idx 11
final_idx 0
task4/train/loss 0.5964104359348615 53
task4/test/loss 1.20982354186606 53
task4/test/acc 0.6011 53
task4/train/lr 0.013582930763556558 53
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 54/256 | train_loss 0.5964 | train_acc 0.8640 | test_loss 1.2098 | test_acc 0.6011 | lr 0.0136
last_idx 11
final_idx 0
task4/train/loss 0.9141702751318613 54
task4/test/loss 1.2610268702632503 54
task4/test/acc 0.5884 54
task4/train/lr 0.011499636732764853 54
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 55/256 | train_loss 0.9142 | train_acc 0.7720 | test_loss 1.2610 | test_acc 0.5884 | lr 0.0115
last_idx 11
final_idx 0
task4/train/loss 0.7825786309937636 55
task4/test/loss 1.250005517089576 55
task4/test/acc 0.5902 55
task4/train/lr 0.009548766216949778 55
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 56/256 | train_loss 0.7826 | train_acc 0.8100 | test_loss 1.2500 | test_acc 0.5902 | lr 0.0095
last_idx 11
final_idx 0
task4/train/loss 0.5457609295845032 56
task4/test/loss 1.224888366173234 56
task4/test/acc 0.5995 56
task4/train/lr 0.00774910716563295 56
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 57/256 | train_loss 0.5458 | train_acc 0.8840 | test_loss 1.2249 | test_acc 0.5995 | lr 0.0077
last_idx 11
final_idx 0
task4/train/loss 0.7139395363628864 57
task4/test/loss 1.2285800503011335 57
task4/test/acc 0.6011 57
task4/train/lr 0.0061179912792722595 57
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 58/256 | train_loss 0.7139 | train_acc 0.8160 | test_loss 1.2286 | test_acc 0.6011 | lr 0.0061
last_idx 11
final_idx 0
task4/train/loss 0.8713851074377695 58
task4/test/loss 1.2372236230917144 58
task4/test/acc 0.6005 58
task4/train/lr 0.004671127095512003 58
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 59/256 | train_loss 0.8714 | train_acc 0.7360 | test_loss 1.2372 | test_acc 0.6005 | lr 0.0047
last_idx 11
final_idx 0
task4/train/loss 0.8550630075236162 59
task4/test/loss 1.2349880310265642 59
task4/test/acc 0.5979 59
task4/train/lr 0.0034224487073782153 59
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 60/256 | train_loss 0.8551 | train_acc 0.7700 | test_loss 1.2350 | test_acc 0.5979 | lr 0.0034
last_idx 11
final_idx 0
task4/train/loss 0.7462189545234045 60
task4/test/loss 1.2325896714863025 60
task4/test/acc 0.6013 60
task4/train/lr 0.0023839815703456534 60
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 61/256 | train_loss 0.7462 | train_acc 0.7420 | test_loss 1.2326 | test_acc 0.6013 | lr 0.0024
last_idx 11
final_idx 0
task4/train/loss 0.6870772515734037 61
task4/test/loss 1.215868106275274 61
task4/test/acc 0.6044 61
task4/train/lr 0.0015657266906278318 61
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 62/256 | train_loss 0.6871 | train_acc 0.8540 | test_loss 1.2159 | test_acc 0.6044 | lr 0.0016
last_idx 11
final_idx 0
task4/train/loss 0.5817345275233189 62
task4/test/loss 1.2203674350391354 62
task4/test/acc 0.6038 62
task4/train/lr 0.0009755643100200469 62
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 63/256 | train_loss 0.5817 | train_acc 0.8720 | test_loss 1.2204 | test_acc 0.6038 | lr 0.0010
last_idx 11
final_idx 0
task4/train/loss 0.6311882250010967 63
task4/test/loss 1.213365461219821 63
task4/test/acc 0.6046 63
task4/train/lr 0.0006191780148631288 63
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 64/256 | train_loss 0.6312 | train_acc 0.8580 | test_loss 1.2134 | test_acc 0.6046 | lr 0.0006
last_idx 11
final_idx 0
task4/train/loss 0.6878110331793627 64
task4/test/loss 1.541124412234415 64
task4/test/acc 0.5245 64
task4/train/lr 0.05 64
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 65/256 | train_loss 0.6878 | train_acc 0.7740 | test_loss 1.5411 | test_acc 0.5245 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 1.316188079615434 65
task4/test/loss 1.5513322471003783 65
task4/test/acc 0.502 65
task4/train/lr 0.04997018754107802 65
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 66/256 | train_loss 1.3162 | train_acc 0.6300 | test_loss 1.5513 | test_acc 0.5020 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 1.0087444260716438 66
task4/test/loss 1.4187276569897669 66
task4/test/acc 0.5523 66
task4/train/lr 0.049880821985136874 66
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 67/256 | train_loss 1.0087 | train_acc 0.6760 | test_loss 1.4187 | test_acc 0.5523 | lr 0.0499
last_idx 11
final_idx 0
task4/train/loss 0.8127244388063749 67
task4/test/loss 1.8556760396612317 67
task4/test/acc 0.4683 67
task4/train/lr 0.04973211862162834 67
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 68/256 | train_loss 0.8127 | train_acc 0.7980 | test_loss 1.8557 | test_acc 0.4683 | lr 0.0497
last_idx 11
final_idx 0
task4/train/loss 0.7496487709383169 68
task4/test/loss 1.8664440955443864 68
task4/test/acc 0.4866 68
task4/train/lr 0.049524435689979954 68
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 69/256 | train_loss 0.7496 | train_acc 0.7820 | test_loss 1.8664 | test_acc 0.4866 | lr 0.0495
last_idx 11
final_idx 0
task4/train/loss 0.741126095255216 69
task4/test/loss 1.4338964749323695 69
task4/test/acc 0.5442 69
task4/train/lr 0.04925827351656497 69
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 70/256 | train_loss 0.7411 | train_acc 0.8420 | test_loss 1.4339 | test_acc 0.5442 | lr 0.0493
last_idx 11
final_idx 0
task4/train/loss 1.115191323061784 70
task4/test/loss 1.5294026180840374 70
task4/test/acc 0.5166 70
task4/train/lr 0.048934273309372174 70
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 71/256 | train_loss 1.1152 | train_acc 0.6760 | test_loss 1.5294 | test_acc 0.5166 | lr 0.0489
last_idx 11
final_idx 0
task4/train/loss 0.9785405819614729 71
task4/test/loss 1.4313720625482107 71
task4/test/acc 0.5475 71
task4/train/lr 0.048553215613279764 71
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 72/256 | train_loss 0.9785 | train_acc 0.7660 | test_loss 1.4314 | test_acc 0.5475 | lr 0.0486
last_idx 11
final_idx 0
task4/train/loss 0.9936630936960379 72
task4/test/loss 1.3974616190320568 72
task4/test/acc 0.5477 72
task4/train/lr 0.04811601842965435 72
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 73/256 | train_loss 0.9937 | train_acc 0.6800 | test_loss 1.3975 | test_acc 0.5477 | lr 0.0481
last_idx 11
final_idx 0
task4/train/loss 0.8123159296810627 73
task4/test/loss 1.5385264405294468 73
task4/test/acc 0.5098 73
task4/train/lr 0.047623735004805226 73
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 74/256 | train_loss 0.8123 | train_acc 0.8000 | test_loss 1.5385 | test_acc 0.5098 | lr 0.0476
last_idx 11
final_idx 0
task4/train/loss 0.9792253884176413 74
task4/test/loss 1.4709223456550062 74
task4/test/acc 0.532 74
task4/train/lr 0.04707755129262179 74
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 75/256 | train_loss 0.9792 | train_acc 0.7260 | test_loss 1.4709 | test_acc 0.5320 | lr 0.0471
last_idx 11
final_idx 0
task4/train/loss 0.9230681533614794 75
task4/test/loss 1.4167054513828796 75
task4/test/acc 0.5447 75
task4/train/lr 0.046478783097506735 75
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 76/256 | train_loss 0.9231 | train_acc 0.7840 | test_loss 1.4167 | test_acc 0.5447 | lr 0.0465
last_idx 11
final_idx 0
task4/train/loss 0.8635422314206759 76
task4/test/loss 1.709021987883668 76
task4/test/acc 0.4974 76
task4/train/lr 0.045828872904488 76
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 77/256 | train_loss 0.8635 | train_acc 0.7840 | test_loss 1.7090 | test_acc 0.4974 | lr 0.0458
last_idx 11
final_idx 0
task4/train/loss 0.99831972271204 77
task4/test/loss 1.8060366393704164 77
task4/test/acc 0.4935 77
task4/train/lr 0.04512938640414596 77
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 78/256 | train_loss 0.9983 | train_acc 0.6960 | test_loss 1.8060 | test_acc 0.4935 | lr 0.0451
last_idx 11
final_idx 0
task4/train/loss 0.9601549506187439 78
task4/test/loss 1.4258228404480113 78
task4/test/acc 0.5522 78
task4/train/lr 0.04438200872072774 78
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 79/256 | train_loss 0.9602 | train_acc 0.7320 | test_loss 1.4258 | test_acc 0.5522 | lr 0.0444
last_idx 11
final_idx 0
task4/train/loss 0.890018696586291 79
task4/test/loss 1.6628764518781711 79
task4/test/acc 0.4996 79
task4/train/lr 0.043588540352535246 79
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 80/256 | train_loss 0.8900 | train_acc 0.7860 | test_loss 1.6629 | test_acc 0.4996 | lr 0.0436
last_idx 11
final_idx 0
task4/train/loss 0.7427063013116518 80
task4/test/loss 1.3975850939750671 80
task4/test/acc 0.5455 80
task4/train/lr 0.04275089283436705 80
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 81/256 | train_loss 0.7427 | train_acc 0.8000 | test_loss 1.3976 | test_acc 0.5455 | lr 0.0428
last_idx 11
final_idx 0
task4/train/loss 0.9029933102428913 81
task4/test/loss 1.4018155223967737 81
task4/test/acc 0.5547 81
task4/train/lr 0.04187108413246371 81
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 82/256 | train_loss 0.9030 | train_acc 0.7720 | test_loss 1.4018 | test_acc 0.5547 | lr 0.0419
last_idx 11
final_idx 0
task4/train/loss 0.6106073334813118 82
task4/test/loss 1.5396105888250626 82
task4/test/acc 0.5373 82
task4/train/lr 0.040951233783050225 82
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 83/256 | train_loss 0.6106 | train_acc 0.8560 | test_loss 1.5396 | test_acc 0.5373 | lr 0.0410
last_idx 11
final_idx 0
task4/train/loss 0.8634443692862988 83
task4/test/loss 1.4905893498084002 83
task4/test/acc 0.5382 83
task4/train/lr 0.03999355778618773 83
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 84/256 | train_loss 0.8634 | train_acc 0.8080 | test_loss 1.4906 | test_acc 0.5382 | lr 0.0400
last_idx 11
final_idx 0
task4/train/loss 0.7082186775902907 84
task4/test/loss 1.4314112694639909 84
task4/test/acc 0.5595 84
task4/train/lr 0.039000363267235154 84
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 85/256 | train_loss 0.7082 | train_acc 0.8460 | test_loss 1.4314 | test_acc 0.5595 | lr 0.0390
last_idx 11
final_idx 0
task4/train/loss 0.7621262514342865 85
task4/test/loss 1.5689282136266691 85
task4/test/acc 0.5427 85
task4/train/lr 0.03797404291878224 85
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 86/256 | train_loss 0.7621 | train_acc 0.8240 | test_loss 1.5689 | test_acc 0.5427 | lr 0.0380
last_idx 11
final_idx 0
task4/train/loss 0.75771414488554 86
task4/test/loss 1.44549873601972 86
task4/test/acc 0.5417 86
task4/train/lr 0.03691706923644345 86
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 87/256 | train_loss 0.7577 | train_acc 0.7600 | test_loss 1.4455 | test_acc 0.5417 | lr 0.0369
last_idx 11
final_idx 0
task4/train/loss 0.5601446305712064 87
task4/test/loss 1.5545758215481775 87
task4/test/acc 0.5684 87
task4/train/lr 0.03583198856239948 87
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 88/256 | train_loss 0.5601 | train_acc 0.8380 | test_loss 1.5546 | test_acc 0.5684 | lr 0.0358
last_idx 11
final_idx 0
task4/train/loss 0.8843714855611324 88
task4/test/loss 1.4337090181938388 88
task4/test/acc 0.5704 88
task4/train/lr 0.03472141495103598 88
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 89/256 | train_loss 0.8844 | train_acc 0.7480 | test_loss 1.4337 | test_acc 0.5704 | lr 0.0347
last_idx 11
final_idx 0
task4/train/loss 0.8153428907195727 89
task4/test/loss 1.3260812373799191 89
task4/test/acc 0.5746 89
task4/train/lr 0.03358802387145745 89
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 90/256 | train_loss 0.8153 | train_acc 0.7860 | test_loss 1.3261 | test_acc 0.5746 | lr 0.0336
last_idx 11
final_idx 0
task4/train/loss 0.7710760335127512 90
task4/test/loss 1.6043019387544246 90
task4/test/acc 0.5274 90
task4/train/lr 0.03243454576204794 90
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 91/256 | train_loss 0.7711 | train_acc 0.8160 | test_loss 1.6043 | test_acc 0.5274 | lr 0.0324
last_idx 11
final_idx 0
task4/train/loss 0.9191804813841978 91
task4/test/loss 1.3405109671385664 91
task4/test/acc 0.5676 91
task4/train/lr 0.03126375945260579 91
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 92/256 | train_loss 0.9192 | train_acc 0.6640 | test_loss 1.3405 | test_acc 0.5676 | lr 0.0313
last_idx 11
final_idx 0
task4/train/loss 0.8913339339196682 92
task4/test/loss 1.3417021626312482 92
task4/test/acc 0.5671 92
task4/train/lr 0.03007848546989918 92
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 93/256 | train_loss 0.8913 | train_acc 0.6860 | test_loss 1.3417 | test_acc 0.5671 | lr 0.0301
last_idx 11
final_idx 0
task4/train/loss 0.6844804404924313 93
task4/test/loss 1.407394681583371 93
task4/test/acc 0.5726 93
task4/train/lr 0.028881579242770204 93
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 94/256 | train_loss 0.6845 | train_acc 0.8500 | test_loss 1.4074 | test_acc 0.5726 | lr 0.0289
last_idx 11
final_idx 0
task4/train/loss 0.904886016001304 94
task4/test/loss 1.3206162101082635 94
task4/test/acc 0.5745 94
task4/train/lr 0.027675924223156633 94
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 95/256 | train_loss 0.9049 | train_acc 0.6760 | test_loss 1.3206 | test_acc 0.5745 | lr 0.0277
last_idx 11
final_idx 0
task4/train/loss 1.0875367547074954 95
task4/test/loss 1.4085258062471424 95
task4/test/acc 0.5482 95
task4/train/lr 0.0264644249396036 95
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 96/256 | train_loss 1.0875 | train_acc 0.6560 | test_loss 1.4085 | test_acc 0.5482 | lr 0.0265
last_idx 11
final_idx 0
task4/train/loss 0.8568000830709934 96
task4/test/loss 1.2840939232131892 96
task4/test/acc 0.577 96
task4/train/lr 0.02525 96
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 97/256 | train_loss 0.8568 | train_acc 0.7980 | test_loss 1.2841 | test_acc 0.5770 | lr 0.0253
last_idx 11
final_idx 0
task4/train/loss 0.688318595290184 97
task4/test/loss 1.3048277886813147 97
task4/test/acc 0.5833 97
task4/train/lr 0.024035575060396407 97
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 98/256 | train_loss 0.6883 | train_acc 0.8380 | test_loss 1.3048 | test_acc 0.5833 | lr 0.0240
last_idx 11
final_idx 0
task4/train/loss 0.742099137355884 98
task4/test/loss 1.3723152536049223 98
task4/test/acc 0.5879 98
task4/train/lr 0.022824075776843374 98
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 99/256 | train_loss 0.7421 | train_acc 0.7760 | test_loss 1.3723 | test_acc 0.5879 | lr 0.0228
last_idx 11
final_idx 0
task4/train/loss 0.6420337458451589 99
task4/test/loss 1.3946729223979146 99
task4/test/acc 0.572 99
task4/train/lr 0.0216184207572298 99
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 100/256 | train_loss 0.6420 | train_acc 0.8740 | test_loss 1.3947 | test_acc 0.5720 | lr 0.0216
last_idx 11
final_idx 0
task4/train/loss 0.8974188479284445 100
task4/test/loss 1.295496955775378 100
task4/test/acc 0.5818 100
task4/train/lr 0.02042151453010083 100
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 101/256 | train_loss 0.8974 | train_acc 0.6860 | test_loss 1.2955 | test_acc 0.5818 | lr 0.0204
last_idx 11
final_idx 0
task4/train/loss 0.6147830945750078 101
task4/test/loss 1.251593219083652 101
task4/test/acc 0.5946 101
task4/train/lr 0.019236240547394222 101
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 102/256 | train_loss 0.6148 | train_acc 0.8600 | test_loss 1.2516 | test_acc 0.5946 | lr 0.0192
last_idx 11
final_idx 0
task4/train/loss 0.5940659455955029 102
task4/test/loss 1.2375145225148452 102
task4/test/acc 0.6042 102
task4/train/lr 0.018065454237952062 102
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 103/256 | train_loss 0.5941 | train_acc 0.8740 | test_loss 1.2375 | test_acc 0.6042 | lr 0.0181
last_idx 11
final_idx 0
task4/train/loss 0.4794502928853035 103
task4/test/loss 1.2159735723807101 103
task4/test/acc 0.6107 103
task4/train/lr 0.016911976128542557 103
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 104/256 | train_loss 0.4795 | train_acc 0.9020 | test_loss 1.2160 | test_acc 0.6107 | lr 0.0169
last_idx 11
final_idx 0
task4/train/loss 1.015520801146825 104
task4/test/loss 1.2453149150599514 104
task4/test/acc 0.6015 104
task4/train/lr 0.01577858504896403 104
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 105/256 | train_loss 1.0155 | train_acc 0.7060 | test_loss 1.2453 | test_acc 0.6015 | lr 0.0158
last_idx 11
final_idx 0
task4/train/loss 0.777606655533115 105
task4/test/loss 1.2476125669976075 105
task4/test/acc 0.6004 105
task4/train/lr 0.014668011437600525 105
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 106/256 | train_loss 0.7776 | train_acc 0.7960 | test_loss 1.2476 | test_acc 0.6004 | lr 0.0147
last_idx 11
final_idx 0
task4/train/loss 0.9353543122609457 106
task4/test/loss 1.197154057065123 106
task4/test/acc 0.6127 106
task4/train/lr 0.013582930763556558 106
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 107/256 | train_loss 0.9354 | train_acc 0.6460 | test_loss 1.1972 | test_acc 0.6127 | lr 0.0136
last_idx 11
final_idx 0
task4/train/loss 0.5572855851302544 107
task4/test/loss 1.2070937384116023 107
task4/test/acc 0.616 107
task4/train/lr 0.012525957081217764 107
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 108/256 | train_loss 0.5573 | train_acc 0.8800 | test_loss 1.2071 | test_acc 0.6160 | lr 0.0125
last_idx 11
final_idx 0
task4/train/loss 0.5573258666942517 108
task4/test/loss 1.2073390231582157 108
task4/test/acc 0.617 108
task4/train/lr 0.011499636732764853 108
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 109/256 | train_loss 0.5573 | train_acc 0.8500 | test_loss 1.2073 | test_acc 0.6170 | lr 0.0115
last_idx 11
final_idx 0
task4/train/loss 0.71657237286369 109
task4/test/loss 1.281479611992836 109
task4/test/acc 0.598 109
task4/train/lr 0.010506442213812275 109
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 110/256 | train_loss 0.7166 | train_acc 0.8160 | test_loss 1.2815 | test_acc 0.5980 | lr 0.0105
last_idx 11
final_idx 0
task4/train/loss 0.6307292828957239 110
task4/test/loss 1.2468058234244062 110
task4/test/acc 0.6028 110
task4/train/lr 0.009548766216949778 110
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 111/256 | train_loss 0.6307 | train_acc 0.8200 | test_loss 1.2468 | test_acc 0.6028 | lr 0.0095
last_idx 11
final_idx 0
task4/train/loss 0.5994674693793058 111
task4/test/loss 1.2367262101748533 111
task4/test/acc 0.607 111
task4/train/lr 0.008628915867536294 111
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 112/256 | train_loss 0.5995 | train_acc 0.8280 | test_loss 1.2367 | test_acc 0.6070 | lr 0.0086
last_idx 11
final_idx 0
task4/train/loss 0.8757168954859177 112
task4/test/loss 1.1969243175627893 112
task4/test/acc 0.6138 112
task4/train/lr 0.00774910716563295 112
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 113/256 | train_loss 0.8757 | train_acc 0.8600 | test_loss 1.1969 | test_acc 0.6138 | lr 0.0077
last_idx 11
final_idx 0
task4/train/loss 0.7791771497577429 113
task4/test/loss 1.2322573423908467 113
task4/test/acc 0.6019 113
task4/train/lr 0.006911459647464768 113
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 114/256 | train_loss 0.7792 | train_acc 0.8360 | test_loss 1.2323 | test_acc 0.6019 | lr 0.0069
last_idx 11
final_idx 0
task4/train/loss 0.6359508782625198 114
task4/test/loss 1.2199729573831224 114
task4/test/acc 0.6066 114
task4/train/lr 0.0061179912792722595 114
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 115/256 | train_loss 0.6360 | train_acc 0.8700 | test_loss 1.2200 | test_acc 0.6066 | lr 0.0061
last_idx 11
final_idx 0
task4/train/loss 0.4193401349087556 115
task4/test/loss 1.2071019083000065 115
task4/test/acc 0.6142 115
task4/train/lr 0.005370613595854041 115
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 116/256 | train_loss 0.4193 | train_acc 0.8980 | test_loss 1.2071 | test_acc 0.6142 | lr 0.0054
last_idx 11
final_idx 0
task4/train/loss 0.7933273985981941 116
task4/test/loss 1.1947662583307217 116
task4/test/acc 0.6183 116
task4/train/lr 0.004671127095512003 116
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 117/256 | train_loss 0.7933 | train_acc 0.7780 | test_loss 1.1948 | test_acc 0.6183 | lr 0.0047
last_idx 11
final_idx 0
task4/train/loss 0.5515812505036592 117
task4/test/loss 1.2042204420033253 117
task4/test/acc 0.619 117
task4/train/lr 0.004021216902493268 117
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 118/256 | train_loss 0.5516 | train_acc 0.8520 | test_loss 1.2042 | test_acc 0.6190 | lr 0.0040
last_idx 11
final_idx 0
task4/train/loss 0.5750876249124607 118
task4/test/loss 1.2081319615244865 118
task4/test/acc 0.6143 118
task4/train/lr 0.0034224487073782153 118
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 119/256 | train_loss 0.5751 | train_acc 0.8500 | test_loss 1.2081 | test_acc 0.6143 | lr 0.0034
last_idx 11
final_idx 0
task4/train/loss 0.43340462011595565 119
task4/test/loss 1.2079756172341214 119
task4/test/acc 0.6201 119
task4/train/lr 0.0028762649951947776 119
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 120/256 | train_loss 0.4334 | train_acc 0.8920 | test_loss 1.2080 | test_acc 0.6201 | lr 0.0029
last_idx 11
final_idx 0
task4/train/loss 0.7262940397486091 120
task4/test/loss 1.205962256773522 120
task4/test/acc 0.6183 120
task4/train/lr 0.0023839815703456534 120
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 121/256 | train_loss 0.7263 | train_acc 0.8020 | test_loss 1.2060 | test_acc 0.6183 | lr 0.0024
last_idx 11
final_idx 0
task4/train/loss 0.6525951555619637 121
task4/test/loss 1.1976257380947732 121
task4/test/acc 0.6165 121
task4/train/lr 0.0019467843867202379 121
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 122/256 | train_loss 0.6526 | train_acc 0.8940 | test_loss 1.1976 | test_acc 0.6165 | lr 0.0019
last_idx 11
final_idx 0
task4/train/loss 0.7966114661345879 122
task4/test/loss 1.1907265586288351 122
task4/test/acc 0.6216 122
task4/train/lr 0.0015657266906278318 122
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 123/256 | train_loss 0.7966 | train_acc 0.7820 | test_loss 1.1907 | test_acc 0.6216 | lr 0.0016
last_idx 11
final_idx 0
task4/train/loss 0.8519051683445772 123
task4/test/loss 1.1912393760785722 123
task4/test/acc 0.6186 123
task4/train/lr 0.0012417264834350366 123
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 124/256 | train_loss 0.8519 | train_acc 0.6840 | test_loss 1.1912 | test_acc 0.6186 | lr 0.0012
last_idx 11
final_idx 0
task4/train/loss 0.8042025056978067 124
task4/test/loss 1.2012238829282291 124
task4/test/acc 0.6148 124
task4/train/lr 0.0009755643100200469 124
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 125/256 | train_loss 0.8042 | train_acc 0.7720 | test_loss 1.2012 | test_acc 0.6148 | lr 0.0010
last_idx 11
final_idx 0
task4/train/loss 0.5467141196131706 125
task4/test/loss 1.199868538923431 125
task4/test/acc 0.619 125
task4/train/lr 0.0007678813783716699 125
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 126/256 | train_loss 0.5467 | train_acc 0.9020 | test_loss 1.1999 | test_acc 0.6190 | lr 0.0008
last_idx 11
final_idx 0
task4/train/loss 0.8058354159196218 126
task4/test/loss 1.1840872824714894 126
task4/test/acc 0.6189 126
task4/train/lr 0.0006191780148631288 126
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 127/256 | train_loss 0.8058 | train_acc 0.8040 | test_loss 1.1841 | test_acc 0.6189 | lr 0.0006
last_idx 11
final_idx 0
task4/train/loss 0.7219177111983299 127
task4/test/loss 1.1937691577171023 127
task4/test/acc 0.6182 127
task4/train/lr 0.0005298124589219829 127
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 128/256 | train_loss 0.7219 | train_acc 0.7920 | test_loss 1.1938 | test_acc 0.6182 | lr 0.0005
last_idx 11
final_idx 0
task4/train/loss 0.7183855020751556 128
task4/test/loss 1.5129098981096034 128
task4/test/acc 0.5231 128
task4/train/lr 0.05 128
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 129/256 | train_loss 0.7184 | train_acc 0.8220 | test_loss 1.5129 | test_acc 0.5231 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 0.6111386939883232 129
task4/test/loss 1.3713219595844286 129
task4/test/acc 0.5635 129
task4/train/lr 0.04999254576273106 129
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 130/256 | train_loss 0.6111 | train_acc 0.8840 | test_loss 1.3713 | test_acc 0.5635 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 1.1415824790795643 130
task4/test/loss 1.345006825500413 130
task4/test/acc 0.5551 130
task4/train/lr 0.04997018754107802 130
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 131/256 | train_loss 1.1416 | train_acc 0.7100 | test_loss 1.3450 | test_acc 0.5551 | lr 0.0500
last_idx 11
final_idx 0
task4/train/loss 0.8108216884235541 131
task4/test/loss 1.3133063783640402 131
task4/test/acc 0.5798 131
task4/train/lr 0.04993293880279759 131
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 132/256 | train_loss 0.8108 | train_acc 0.8080 | test_loss 1.3133 | test_acc 0.5798 | lr 0.0499
last_idx 11
final_idx 0
task4/train/loss 0.6765030560394129 132
task4/test/loss 1.6317532444209384 132
task4/test/acc 0.5197 132
task4/train/lr 0.049880821985136874 132
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 133/256 | train_loss 0.6765 | train_acc 0.7580 | test_loss 1.6318 | test_acc 0.5197 | lr 0.0499
last_idx 11
final_idx 0
task4/train/loss 1.0184155677755673 133
task4/test/loss 1.6206813714744752 133
task4/test/acc 0.495 133
task4/train/lr 0.04981386848131808 133
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 134/256 | train_loss 1.0184 | train_acc 0.6260 | test_loss 1.6207 | test_acc 0.4950 | lr 0.0498
last_idx 11
final_idx 0
task4/train/loss 0.9277771400908629 134
task4/test/loss 1.6260926060770686 134
task4/test/acc 0.5072 134
task4/train/lr 0.04973211862162834 134
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 135/256 | train_loss 0.9278 | train_acc 0.7300 | test_loss 1.6261 | test_acc 0.5072 | lr 0.0497
last_idx 11
final_idx 0
task4/train/loss 0.6504622312883536 135
task4/test/loss 1.9377217116978085 135
task4/test/acc 0.5054 135
task4/train/lr 0.04963562164912629 135
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 136/256 | train_loss 0.6505 | train_acc 0.8960 | test_loss 1.9377 | test_acc 0.5054 | lr 0.0496
last_idx 11
final_idx 0
task4/train/loss 0.9071777338782946 136
task4/test/loss 1.567589547942605 136
task4/test/acc 0.5094 136
task4/train/lr 0.049524435689979954 136
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 137/256 | train_loss 0.9072 | train_acc 0.7300 | test_loss 1.5676 | test_acc 0.5094 | lr 0.0495
last_idx 11
final_idx 0
task4/train/loss 0.8725234617789587 137
task4/test/loss 1.3856790868336695 137
task4/test/acc 0.5679 137
task4/train/lr 0.04939862771845358 137
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 138/256 | train_loss 0.8725 | train_acc 0.6880 | test_loss 1.3857 | test_acc 0.5679 | lr 0.0494
last_idx 11
final_idx 0
task4/train/loss 0.6875297700365385 138
task4/test/loss 1.7017169369940173 138
task4/test/acc 0.537 138
task4/train/lr 0.04925827351656497 138
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 139/256 | train_loss 0.6875 | train_acc 0.8420 | test_loss 1.7017 | test_acc 0.5370 | lr 0.0493
last_idx 11
final_idx 0
task4/train/loss 0.7616409050921599 139
task4/test/loss 1.3760773777438884 139
task4/test/acc 0.5674 139
task4/train/lr 0.04910345762843714 139
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 140/256 | train_loss 0.7616 | train_acc 0.7880 | test_loss 1.3761 | test_acc 0.5674 | lr 0.0491
last_idx 11
final_idx 0
task4/train/loss 0.9972785090406736 140
task4/test/loss 21.47112688892766 140
task4/test/acc 0.3079 140
task4/train/lr 0.048934273309372174 140
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 141/256 | train_loss 0.9973 | train_acc 0.7240 | test_loss 21.4711 | test_acc 0.3079 | lr 0.0489
last_idx 11
final_idx 0
task4/train/loss 1.454969048500061 141
task4/test/loss 1.7092346256239372 141
task4/test/acc 0.4501 141
task4/train/lr 0.04875082246967766 141
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 142/256 | train_loss 1.4550 | train_acc 0.5980 | test_loss 1.7092 | test_acc 0.4501 | lr 0.0488
last_idx 11
final_idx 0
task4/train/loss 1.0342194239298503 142
task4/test/loss 1.5124851077664316 142
task4/test/acc 0.5397 142
task4/train/lr 0.048553215613279764 142
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 143/256 | train_loss 1.0342 | train_acc 0.7340 | test_loss 1.5125 | test_acc 0.5397 | lr 0.0486
last_idx 11
final_idx 0
task4/train/loss 1.0830027982592583 143
task4/test/loss 1.8169745970190616 143
task4/test/acc 0.4995 143
task4/train/lr 0.04834157177115979 143
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 144/256 | train_loss 1.0830 | train_acc 0.7060 | test_loss 1.8170 | test_acc 0.4995 | lr 0.0483
last_idx 11
final_idx 0
task4/train/loss 0.9489164873957634 144
task4/test/loss 1.578301493470606 144
task4/test/acc 0.5164 144
task4/train/lr 0.04811601842965435 144
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 145/256 | train_loss 0.9489 | train_acc 0.7240 | test_loss 1.5783 | test_acc 0.5164 | lr 0.0481
last_idx 11
final_idx 0
task4/train/loss 0.9471005375186602 145
task4/test/loss 1.539565936025036 145
task4/test/acc 0.5179 145
task4/train/lr 0.047876691453662384 145
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 146/256 | train_loss 0.9471 | train_acc 0.7560 | test_loss 1.5396 | test_acc 0.5179 | lr 0.0479
last_idx 11
final_idx 0
task4/train/loss 1.0690606931845348 146
task4/test/loss 1.5356170609593391 146
task4/test/acc 0.5328 146
task4/train/lr 0.047623735004805226 146
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 147/256 | train_loss 1.0691 | train_acc 0.6920 | test_loss 1.5356 | test_acc 0.5328 | lr 0.0476
last_idx 11
final_idx 0
task4/train/loss 0.8933565306166807 147
task4/test/loss 1.301880152079097 147
task4/test/acc 0.5809 147
task4/train/lr 0.047357301454589 147
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 148/256 | train_loss 0.8934 | train_acc 0.7240 | test_loss 1.3019 | test_acc 0.5809 | lr 0.0474
last_idx 11
final_idx 0
task4/train/loss 1.157897378007571 148
task4/test/loss 1.755985970952009 148
task4/test/acc 0.5153 148
task4/train/lr 0.04707755129262179 148
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 149/256 | train_loss 1.1579 | train_acc 0.7560 | test_loss 1.7560 | test_acc 0.5153 | lr 0.0471
last_idx 11
final_idx 0
task4/train/loss 0.8937337224682173 149
task4/test/loss 1.2932489433309489 149
task4/test/acc 0.5805 149
task4/train/lr 0.04678465302994061 149
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 150/256 | train_loss 0.8937 | train_acc 0.7520 | test_loss 1.2932 | test_acc 0.5805 | lr 0.0468
last_idx 11
final_idx 0
task4/train/loss 0.9940976823369662 150
task4/test/loss 1.494479164611875 150
task4/test/acc 0.5317 150
task4/train/lr 0.046478783097506735 150
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 151/256 | train_loss 0.9941 | train_acc 0.6640 | test_loss 1.4945 | test_acc 0.5317 | lr 0.0465
last_idx 11
final_idx 0
task4/train/loss 0.7850461974740028 151
task4/test/loss 1.3382939113336696 151
task4/test/acc 0.5728 151
task4/train/lr 0.04616012573993025 151
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 152/256 | train_loss 0.7850 | train_acc 0.7560 | test_loss 1.3383 | test_acc 0.5728 | lr 0.0462
last_idx 11
final_idx 0
task4/train/loss 0.5747518539428711 152
task4/test/loss 1.7323019246391036 152
task4/test/acc 0.5189 152
task4/train/lr 0.045828872904488 152
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 153/256 | train_loss 0.5748 | train_acc 0.8700 | test_loss 1.7323 | test_acc 0.5189 | lr 0.0458
last_idx 11
final_idx 0
task4/train/loss 0.7403367621203264 153
task4/test/loss 1.5458378376145112 153
task4/test/acc 0.55 153
task4/train/lr 0.0454852241255017 153
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 154/256 | train_loss 0.7403 | train_acc 0.7860 | test_loss 1.5458 | test_acc 0.5500 | lr 0.0455
last_idx 11
final_idx 0
task4/train/loss 1.0602577924728394 154
task4/test/loss 1.5122113604294627 154
task4/test/acc 0.5303 154
task4/train/lr 0.04512938640414596 154
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 155/256 | train_loss 1.0603 | train_acc 0.6380 | test_loss 1.5122 | test_acc 0.5303 | lr 0.0451
last_idx 11
final_idx 0
task4/train/loss 0.7241036960234245 155
task4/test/loss 1.3123263248748946 155
task4/test/acc 0.5797 155
task4/train/lr 0.04476157408375851 155
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 156/256 | train_loss 0.7241 | train_acc 0.7920 | test_loss 1.3123 | test_acc 0.5797 | lr 0.0448
last_idx 11
final_idx 0
task4/train/loss 0.9794739447534084 156
task4/test/loss 1.6467867883151037 156
task4/test/acc 0.498 156
task4/train/lr 0.04438200872072774 156
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 157/256 | train_loss 0.9795 | train_acc 0.6860 | test_loss 1.6468 | test_acc 0.4980 | lr 0.0444
last_idx 11
final_idx 0
task4/train/loss 1.101237201442321 157
task4/test/loss 1.4026540899485873 157
task4/test/acc 0.5463 157
task4/train/lr 0.0439909189510355 157
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 158/256 | train_loss 1.1012 | train_acc 0.6200 | test_loss 1.4027 | test_acc 0.5463 | lr 0.0440
last_idx 11
final_idx 0
task4/train/loss 0.7999502209325632 158
task4/test/loss 1.3763300168671107 158
task4/test/acc 0.5563 158
task4/train/lr 0.043588540352535246 158
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 159/256 | train_loss 0.8000 | train_acc 0.7340 | test_loss 1.3763 | test_acc 0.5563 | lr 0.0436
last_idx 11
final_idx 0
task4/train/loss 0.558245132987698 159
task4/test/loss 1.8105155088399585 159
task4/test/acc 0.5132 159
task4/train/lr 0.043175115303048815 159
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 160/256 | train_loss 0.5582 | train_acc 0.8880 | test_loss 1.8105 | test_acc 0.5132 | lr 0.0432
last_idx 11
final_idx 0
task4/train/loss 0.7296009336908659 160
task4/test/loss 1.489590944754973 160
task4/test/acc 0.5545 160
task4/train/lr 0.04275089283436705 160
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 161/256 | train_loss 0.7296 | train_acc 0.8220 | test_loss 1.4896 | test_acc 0.5545 | lr 0.0428
last_idx 11
final_idx 0
task4/train/loss 0.9649103867510954 161
task4/test/loss 1.4856656681288753 161
task4/test/acc 0.5415 161
task4/train/lr 0.042316128482242414 161
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 162/256 | train_loss 0.9649 | train_acc 0.6860 | test_loss 1.4857 | test_acc 0.5415 | lr 0.0423
last_idx 11
final_idx 0
task4/train/loss 0.7030189322928587 162
task4/test/loss 1.4625695448434144 162
task4/test/acc 0.5539 162
task4/train/lr 0.04187108413246371 162
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 163/256 | train_loss 0.7030 | train_acc 0.8040 | test_loss 1.4626 | test_acc 0.5539 | lr 0.0419
last_idx 11
final_idx 0
task4/train/loss 0.4890182887514432 163
task4/test/loss 1.4727519879999913 163
task4/test/acc 0.5778 163
task4/train/lr 0.04141602786310598 163
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 164/256 | train_loss 0.4890 | train_acc 0.9060 | test_loss 1.4728 | test_acc 0.5778 | lr 0.0414
last_idx 11
final_idx 0
task4/train/loss 0.858599436779817 164
task4/test/loss 1.3483568786790496 164
task4/test/acc 0.5949 164
task4/train/lr 0.040951233783050225 164
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 165/256 | train_loss 0.8586 | train_acc 0.7800 | test_loss 1.3484 | test_acc 0.5949 | lr 0.0410
last_idx 11
final_idx 0
task4/train/loss 0.9011263934274515 165
task4/test/loss 1.4111384821957664 165
task4/test/acc 0.5758 165
task4/train/lr 0.040476981866870515 165
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 166/256 | train_loss 0.9011 | train_acc 0.7480 | test_loss 1.4111 | test_acc 0.5758 | lr 0.0405
last_idx 11
final_idx 0
task4/train/loss 0.808566921701034 166
task4/test/loss 1.4030365500795214 166
task4/test/acc 0.5693 166
task4/train/lr 0.03999355778618773 166
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 167/256 | train_loss 0.8086 | train_acc 0.6860 | test_loss 1.4030 | test_acc 0.5693 | lr 0.0400
last_idx 11
final_idx 0
task4/train/loss 0.8966926746070385 167
task4/test/loss 1.3112434506939168 167
task4/test/acc 0.5859 167
task4/train/lr 0.039501252737591676 167
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 168/256 | train_loss 0.8967 | train_acc 0.7340 | test_loss 1.3112 | test_acc 0.5859 | lr 0.0395
last_idx 11
final_idx 0
task4/train/loss 0.9065021947026253 168
task4/test/loss 1.2914705502620913 168
task4/test/acc 0.5898 168
task4/train/lr 0.039000363267235154 168
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 169/256 | train_loss 0.9065 | train_acc 0.7020 | test_loss 1.2915 | test_acc 0.5898 | lr 0.0390
last_idx 11
final_idx 0
task4/train/loss 0.7526368151108423 169
task4/test/loss 1.425741740747502 169
task4/test/acc 0.5686 169
task4/train/lr 0.03849119109220566 169
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 170/256 | train_loss 0.7526 | train_acc 0.7940 | test_loss 1.4257 | test_acc 0.5686 | lr 0.0385
last_idx 11
final_idx 0
task4/train/loss 0.6253826624403397 170
task4/test/loss 1.275289082474876 170
task4/test/acc 0.6113 170
task4/train/lr 0.03797404291878224 170
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 171/256 | train_loss 0.6254 | train_acc 0.8160 | test_loss 1.2753 | test_acc 0.6113 | lr 0.0380
last_idx 11
final_idx 0
task4/train/loss 0.7153058666735888 171
task4/test/loss 1.3702199741413719 171
task4/test/acc 0.5914 171
task4/train/lr 0.03744923025768716 171
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 172/256 | train_loss 0.7153 | train_acc 0.7780 | test_loss 1.3702 | test_acc 0.5914 | lr 0.0374
last_idx 11
final_idx 0
task4/train/loss 0.9076700409253439 172
task4/test/loss 1.2924432793730183 172
task4/test/acc 0.5959 172
task4/train/lr 0.03691706923644345 172
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 173/256 | train_loss 0.9077 | train_acc 0.6780 | test_loss 1.2924 | test_acc 0.5959 | lr 0.0369
last_idx 11
final_idx 0
task4/train/loss 0.7770743860552708 173
task4/test/loss 1.5312697942319669 173
task4/test/acc 0.5396 173
task4/train/lr 0.03637788040895152 173
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 174/256 | train_loss 0.7771 | train_acc 0.7400 | test_loss 1.5313 | test_acc 0.5396 | lr 0.0364
last_idx 11
final_idx 0
task4/train/loss 0.31780399568378925 174
task4/test/loss 1.5854207282526451 174
task4/test/acc 0.5514 174
task4/train/lr 0.03583198856239948 174
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 175/256 | train_loss 0.3178 | train_acc 0.9320 | test_loss 1.5854 | test_acc 0.5514 | lr 0.0358
last_idx 11
final_idx 0
task4/train/loss 0.6425694810847441 175
task4/test/loss 1.536437233093015 175
task4/test/acc 0.5687 175
task4/train/lr 0.0352797225216235 175
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 176/256 | train_loss 0.6426 | train_acc 0.8320 | test_loss 1.5364 | test_acc 0.5687 | lr 0.0353
last_idx 11
final_idx 0
task4/train/loss 0.5387913528829813 176
task4/test/loss 1.365892844503386 176
task4/test/acc 0.5857 176
task4/train/lr 0.03472141495103598 176
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 177/256 | train_loss 0.5388 | train_acc 0.8520 | test_loss 1.3659 | test_acc 0.5857 | lr 0.0347
last_idx 11
final_idx 0
task4/train/loss 0.571098351230224 177
task4/test/loss 1.4534999388351775 177
task4/test/acc 0.5553 177
task4/train/lr 0.034157402154240964 177
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 178/256 | train_loss 0.5711 | train_acc 0.8900 | test_loss 1.4535 | test_acc 0.5553 | lr 0.0342
last_idx 11
final_idx 0
task4/train/loss 0.7373086102306843 178
task4/test/loss 1.8026804405738388 178
task4/test/acc 0.5029 178
task4/train/lr 0.03358802387145745 178
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 179/256 | train_loss 0.7373 | train_acc 0.7740 | test_loss 1.8027 | test_acc 0.5029 | lr 0.0336
last_idx 11
final_idx 0
task4/train/loss 0.4500879130015771 179
task4/test/loss 1.4111318190892537 179
task4/test/acc 0.5785 179
task4/train/lr 0.03301362307487257 179
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 180/256 | train_loss 0.4501 | train_acc 0.8320 | test_loss 1.4111 | test_acc 0.5785 | lr 0.0330
last_idx 11
final_idx 0
task4/train/loss 0.7360260759790739 180
task4/test/loss 1.3913434166134448 180
task4/test/acc 0.5785 180
task4/train/lr 0.03243454576204794 180
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 181/256 | train_loss 0.7360 | train_acc 0.7520 | test_loss 1.3913 | test_acc 0.5785 | lr 0.0324
last_idx 11
final_idx 0
task4/train/loss 0.7313731461763382 181
task4/test/loss 1.4863092060151852 181
task4/test/acc 0.5531 181
task4/train/lr 0.03185114074750374 181
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 182/256 | train_loss 0.7314 | train_acc 0.7920 | test_loss 1.4863 | test_acc 0.5531 | lr 0.0319
last_idx 11
final_idx 0
task4/train/loss 0.7973859769602617 182
task4/test/loss 1.3294808055486595 182
task4/test/acc 0.5826 182
task4/train/lr 0.03126375945260579 182
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 183/256 | train_loss 0.7974 | train_acc 0.7540 | test_loss 1.3295 | test_acc 0.5826 | lr 0.0313
last_idx 11
final_idx 0
task4/train/loss 0.9027789272367954 183
task4/test/loss 1.3294834539032818 183
task4/test/acc 0.5711 183
task4/train/lr 0.030672755693882527 183
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 184/256 | train_loss 0.9028 | train_acc 0.7280 | test_loss 1.3295 | test_acc 0.5711 | lr 0.0307
last_idx 11
final_idx 0
task4/train/loss 0.7356960227092108 184
task4/test/loss 1.3702902647486903 184
task4/test/acc 0.5745 184
task4/train/lr 0.03007848546989918 184
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 185/256 | train_loss 0.7357 | train_acc 0.7660 | test_loss 1.3703 | test_acc 0.5745 | lr 0.0301
last_idx 11
final_idx 0
task4/train/loss 0.6289520375430584 185
task4/test/loss 1.434195494769435 185
task4/test/acc 0.5779 185
task4/train/lr 0.029481306746817457 185
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 186/256 | train_loss 0.6290 | train_acc 0.8480 | test_loss 1.4342 | test_acc 0.5779 | lr 0.0295
last_idx 11
final_idx 0
task4/train/loss 0.6880949282397827 186
task4/test/loss 1.2957196178143484 186
task4/test/acc 0.5942 186
task4/train/lr 0.028881579242770204 186
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 187/256 | train_loss 0.6881 | train_acc 0.8800 | test_loss 1.2957 | test_acc 0.5942 | lr 0.0289
last_idx 11
final_idx 0
task4/train/loss 0.7571035400032997 187
task4/test/loss 1.2832773823225707 187
task4/test/acc 0.5967 187
task4/train/lr 0.028279664211180604 187
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 188/256 | train_loss 0.7571 | train_acc 0.8300 | test_loss 1.2833 | test_acc 0.5967 | lr 0.0283
last_idx 11
final_idx 0
task4/train/loss 0.9300936522583166 188
task4/test/loss 1.411704331505717 188
task4/test/acc 0.5763 188
task4/train/lr 0.027675924223156633 188
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 189/256 | train_loss 0.9301 | train_acc 0.6800 | test_loss 1.4117 | test_acc 0.5763 | lr 0.0277
last_idx 11
final_idx 0
task4/train/loss 0.4710046748320262 189
task4/test/loss 1.4488725621710743 189
task4/test/acc 0.5745 189
task4/train/lr 0.027070722949091772 189
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 190/256 | train_loss 0.4710 | train_acc 0.9100 | test_loss 1.4489 | test_acc 0.5745 | lr 0.0271
last_idx 11
final_idx 0
task4/train/loss 0.5656890744964281 190
task4/test/loss 1.3356849910658704 190
task4/test/acc 0.5949 190
task4/train/lr 0.0264644249396036 190
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 191/256 | train_loss 0.5657 | train_acc 0.8680 | test_loss 1.3357 | test_acc 0.5949 | lr 0.0265
last_idx 11
final_idx 0
task4/train/loss 0.9255542885512114 191
task4/test/loss 1.3775816919249402 191
task4/test/acc 0.5845 191
task4/train/lr 0.02585739540594208 191
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 192/256 | train_loss 0.9256 | train_acc 0.6440 | test_loss 1.3776 | test_acc 0.5845 | lr 0.0259
last_idx 11
final_idx 0
task4/train/loss 0.6425707880407572 192
task4/test/loss 1.2286079996510555 192
task4/test/acc 0.6111 192
task4/train/lr 0.02525 192
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 193/256 | train_loss 0.6426 | train_acc 0.8640 | test_loss 1.2286 | test_acc 0.6111 | lr 0.0253
last_idx 11
final_idx 0
task4/train/loss 0.7435263295968374 193
task4/test/loss 1.3168814422791464 193
task4/test/acc 0.5889 193
task4/train/lr 0.024642604594057926 193
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 194/256 | train_loss 0.7435 | train_acc 0.8380 | test_loss 1.3169 | test_acc 0.5889 | lr 0.0246
last_idx 11
final_idx 0
task4/train/loss 0.896754385282596 194
task4/test/loss 1.2742029939565742 194
task4/test/acc 0.5999 194
task4/train/lr 0.024035575060396407 194
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 195/256 | train_loss 0.8968 | train_acc 0.7460 | test_loss 1.2742 | test_acc 0.5999 | lr 0.0240
last_idx 11
final_idx 0
task4/train/loss 0.5830060665806135 195
task4/test/loss 1.2942781402615078 195
task4/test/acc 0.605 195
task4/train/lr 0.023429277050908234 195
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 196/256 | train_loss 0.5830 | train_acc 0.8580 | test_loss 1.2943 | test_acc 0.6050 | lr 0.0234
last_idx 11
final_idx 0
task4/train/loss 0.7842613154401382 196
task4/test/loss 1.2442690259531926 196
task4/test/acc 0.6048 196
task4/train/lr 0.022824075776843374 196
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 197/256 | train_loss 0.7843 | train_acc 0.7580 | test_loss 1.2443 | test_acc 0.6048 | lr 0.0228
last_idx 11
final_idx 0
task4/train/loss 1.0639604950944583 197
task4/test/loss 1.3824719284710132 197
task4/test/acc 0.5711 197
task4/train/lr 0.022220335788819403 197
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 198/256 | train_loss 1.0640 | train_acc 0.7380 | test_loss 1.3825 | test_acc 0.5711 | lr 0.0222
last_idx 11
final_idx 0
task4/train/loss 1.0155750562747319 198
task4/test/loss 1.2335009794486196 198
task4/test/acc 0.6016 198
task4/train/lr 0.0216184207572298 198
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 199/256 | train_loss 1.0156 | train_acc 0.6820 | test_loss 1.2335 | test_acc 0.6016 | lr 0.0216
last_idx 11
final_idx 0
task4/train/loss 0.5325074500093857 199
task4/test/loss 1.2642962107794327 199
task4/test/acc 0.6079 199
task4/train/lr 0.021018693253182546 199
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 200/256 | train_loss 0.5325 | train_acc 0.8860 | test_loss 1.2643 | test_acc 0.6079 | lr 0.0210
last_idx 11
final_idx 0
task4/train/loss 1.1689997278153896 200
task4/test/loss 1.235143938216201 200
task4/test/acc 0.596 200
task4/train/lr 0.02042151453010083 200
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 201/256 | train_loss 1.1690 | train_acc 0.6380 | test_loss 1.2351 | test_acc 0.5960 | lr 0.0204
last_idx 11
final_idx 0
task4/train/loss 0.54260821826756 201
task4/test/loss 1.225054424975002 201
task4/test/acc 0.6114 201
task4/train/lr 0.019827244306117476 201
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 202/256 | train_loss 0.5426 | train_acc 0.8140 | test_loss 1.2251 | test_acc 0.6114 | lr 0.0198
last_idx 11
final_idx 0
task4/train/loss 0.7975538556153575 202
task4/test/loss 1.2621944853872584 202
task4/test/acc 0.6007 202
task4/train/lr 0.019236240547394222 202
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 203/256 | train_loss 0.7976 | train_acc 0.6720 | test_loss 1.2622 | test_acc 0.6007 | lr 0.0192
last_idx 11
final_idx 0
task4/train/loss 0.5906495315333208 203
task4/test/loss 1.2702918498401057 203
task4/test/acc 0.6042 203
task4/train/lr 0.018648859252496267 203
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 204/256 | train_loss 0.5906 | train_acc 0.8440 | test_loss 1.2703 | test_acc 0.6042 | lr 0.0186
last_idx 11
final_idx 0
task4/train/loss 0.47878891602158546 204
task4/test/loss 1.3306001474459965 204
task4/test/acc 0.6005 204
task4/train/lr 0.018065454237952062 204
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 205/256 | train_loss 0.4788 | train_acc 0.9160 | test_loss 1.3306 | test_acc 0.6005 | lr 0.0181
last_idx 11
final_idx 0
task4/train/loss 0.5621986805150906 205
task4/test/loss 1.3221014165564586 205
task4/test/acc 0.5943 205
task4/train/lr 0.017486376925127438 205
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 206/256 | train_loss 0.5622 | train_acc 0.8840 | test_loss 1.3221 | test_acc 0.5943 | lr 0.0175
last_idx 11
final_idx 0
task4/train/loss 0.5822214282428225 206
task4/test/loss 1.3355352336116004 206
task4/test/acc 0.5966 206
task4/train/lr 0.016911976128542557 206
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 207/256 | train_loss 0.5822 | train_acc 0.8880 | test_loss 1.3355 | test_acc 0.5966 | lr 0.0169
last_idx 11
final_idx 0
task4/train/loss 0.5583190595110258 207
task4/test/loss 1.250172769422071 207
task4/test/acc 0.6117 207
task4/train/lr 0.016342597845759043 207
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 208/256 | train_loss 0.5583 | train_acc 0.8080 | test_loss 1.2502 | test_acc 0.6117 | lr 0.0163
last_idx 11
final_idx 0
task4/train/loss 0.5488885523130497 208
task4/test/loss 1.2657225522817226 208
task4/test/acc 0.6107 208
task4/train/lr 0.01577858504896403 208
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 209/256 | train_loss 0.5489 | train_acc 0.8720 | test_loss 1.2657 | test_acc 0.6107 | lr 0.0158
last_idx 11
final_idx 0
task4/train/loss 0.5060683153569698 209
task4/test/loss 1.2119925876327775 209
task4/test/acc 0.6291 209
task4/train/lr 0.015220277478376504 209
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 210/256 | train_loss 0.5061 | train_acc 0.8500 | test_loss 1.2120 | test_acc 0.6291 | lr 0.0152
last_idx 11
final_idx 0
task4/train/loss 0.5845947647467256 210
task4/test/loss 1.221616868256477 210
task4/test/acc 0.6275 210
task4/train/lr 0.014668011437600525 210
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 211/256 | train_loss 0.5846 | train_acc 0.8540 | test_loss 1.2216 | test_acc 0.6275 | lr 0.0147
last_idx 11
final_idx 0
task4/train/loss 0.5775753601143757 211
task4/test/loss 1.2448235965873067 211
task4/test/acc 0.6248 211
task4/train/lr 0.014122119591048485 211
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 212/256 | train_loss 0.5776 | train_acc 0.8540 | test_loss 1.2448 | test_acc 0.6248 | lr 0.0141
last_idx 11
final_idx 0
task4/train/loss 0.39126716429988545 212
task4/test/loss 1.2551569778537541 212
task4/test/acc 0.6258 212
task4/train/lr 0.013582930763556558 212
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 213/256 | train_loss 0.3913 | train_acc 0.9280 | test_loss 1.2552 | test_acc 0.6258 | lr 0.0136
last_idx 11
final_idx 0
task4/train/loss 0.8260662748167912 213
task4/test/loss 1.3164590795834858 213
task4/test/acc 0.5978 213
task4/train/lr 0.013050769742312849 213
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 214/256 | train_loss 0.8261 | train_acc 0.7200 | test_loss 1.3165 | test_acc 0.5978 | lr 0.0131
last_idx 11
final_idx 0
task4/train/loss 0.4149085246026516 214
task4/test/loss 1.2831883961171435 214
task4/test/acc 0.6077 214
task4/train/lr 0.012525957081217764 214
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 215/256 | train_loss 0.4149 | train_acc 0.9340 | test_loss 1.2832 | test_acc 0.6077 | lr 0.0125
last_idx 11
final_idx 0
task4/train/loss 0.6187422561148802 215
task4/test/loss 1.2341304344304822 215
task4/test/acc 0.6192 215
task4/train/lr 0.01200880890779435 215
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 216/256 | train_loss 0.6187 | train_acc 0.8880 | test_loss 1.2341 | test_acc 0.6192 | lr 0.0120
last_idx 11
final_idx 0
task4/train/loss 0.5088938771126171 216
task4/test/loss 1.2397331728747016 216
task4/test/acc 0.6225 216
task4/train/lr 0.011499636732764853 216
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 217/256 | train_loss 0.5089 | train_acc 0.9200 | test_loss 1.2397 | test_acc 0.6225 | lr 0.0115
last_idx 11
final_idx 0
task4/train/loss 0.8396906517446041 217
task4/test/loss 1.2420493668892927 217
task4/test/acc 0.6137 217
task4/train/lr 0.010998747262408329 217
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 218/256 | train_loss 0.8397 | train_acc 0.6840 | test_loss 1.2420 | test_acc 0.6137 | lr 0.0110
last_idx 11
final_idx 0
task4/train/loss 0.8504613979409138 218
task4/test/loss 1.2079842435686212 218
task4/test/acc 0.6154 218
task4/train/lr 0.010506442213812275 218
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 219/256 | train_loss 0.8505 | train_acc 0.8080 | test_loss 1.2080 | test_acc 0.6154 | lr 0.0105
last_idx 11
final_idx 0
task4/train/loss 0.8166354428976774 219
task4/test/loss 1.206048099618209 219
task4/test/acc 0.6234 219
task4/train/lr 0.01002301813312949 219
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 220/256 | train_loss 0.8166 | train_acc 0.7200 | test_loss 1.2060 | test_acc 0.6234 | lr 0.0100
last_idx 11
final_idx 0
task4/train/loss 0.3579200791815917 220
task4/test/loss 1.2070277466585762 220
task4/test/acc 0.6308 220
task4/train/lr 0.009548766216949778 220
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 221/256 | train_loss 0.3579 | train_acc 0.9360 | test_loss 1.2070 | test_acc 0.6308 | lr 0.0095
last_idx 11
final_idx 0
task4/train/loss 0.576871051453054 221
task4/test/loss 1.2118196031242086 221
task4/test/acc 0.6314 221
task4/train/lr 0.009083972136894032 221
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 222/256 | train_loss 0.5769 | train_acc 0.8020 | test_loss 1.2118 | test_acc 0.6314 | lr 0.0091
last_idx 11
final_idx 0
task4/train/loss 0.7189993311961492 222
task4/test/loss 1.2605053162888478 222
task4/test/acc 0.6188 222
task4/train/lr 0.008628915867536294 222
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 223/256 | train_loss 0.7190 | train_acc 0.8300 | test_loss 1.2605 | test_acc 0.6188 | lr 0.0086
last_idx 11
final_idx 0
task4/train/loss 0.8859529259304205 223
task4/test/loss 1.2218284161205877 223
task4/test/acc 0.6206 223
task4/train/lr 0.008183871517757594 223
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 224/256 | train_loss 0.8860 | train_acc 0.7640 | test_loss 1.2218 | test_acc 0.6206 | lr 0.0082
last_idx 11
final_idx 0
task4/train/loss 0.3525292274231712 224
task4/test/loss 1.226699217500394 224
task4/test/acc 0.6215 224
task4/train/lr 0.00774910716563295 224
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 225/256 | train_loss 0.3525 | train_acc 0.9060 | test_loss 1.2267 | test_acc 0.6215 | lr 0.0077
last_idx 11
final_idx 0
task4/train/loss 0.6579789699365696 225
task4/test/loss 1.2141720201624067 225
task4/test/acc 0.6239 225
task4/train/lr 0.007324884696951197 225
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 226/256 | train_loss 0.6580 | train_acc 0.8300 | test_loss 1.2142 | test_acc 0.6239 | lr 0.0073
last_idx 11
final_idx 0
task4/train/loss 0.7330803675577044 226
task4/test/loss 1.1970218473620582 226
task4/test/acc 0.6193 226
task4/train/lr 0.006911459647464768 226
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 227/256 | train_loss 0.7331 | train_acc 0.8040 | test_loss 1.1970 | test_acc 0.6193 | lr 0.0069
last_idx 11
final_idx 0
task4/train/loss 0.6624534223228693 227
task4/test/loss 1.1902540102601051 227
task4/test/acc 0.6218 227
task4/train/lr 0.006509081048964508 227
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 228/256 | train_loss 0.6625 | train_acc 0.7900 | test_loss 1.1903 | test_acc 0.6218 | lr 0.0065
last_idx 11
final_idx 0
task4/train/loss 0.7751677141835293 228
task4/test/loss 1.198216418555954 228
task4/test/acc 0.6217 228
task4/train/lr 0.0061179912792722595 228
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 229/256 | train_loss 0.7752 | train_acc 0.7260 | test_loss 1.1982 | test_acc 0.6217 | lr 0.0061
last_idx 11
final_idx 0
task4/train/loss 0.4867564303179582 229
task4/test/loss 1.2121901620636906 229
task4/test/acc 0.6245 229
task4/train/lr 0.005738425916241496 229
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 230/256 | train_loss 0.4868 | train_acc 0.8860 | test_loss 1.2122 | test_acc 0.6245 | lr 0.0057
last_idx 11
final_idx 0
task4/train/loss 0.6181761755918463 230
task4/test/loss 1.2038811091006847 230
task4/test/acc 0.6293 230
task4/train/lr 0.005370613595854041 230
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 231/256 | train_loss 0.6182 | train_acc 0.8400 | test_loss 1.2039 | test_acc 0.6293 | lr 0.0054
last_idx 11
final_idx 0
task4/train/loss 0.43006653152406216 231
task4/test/loss 1.2337817379126423 231
task4/test/acc 0.6291 231
task4/train/lr 0.005014775874498306 231
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 232/256 | train_loss 0.4301 | train_acc 0.8520 | test_loss 1.2338 | test_acc 0.6291 | lr 0.0050
last_idx 11
final_idx 0
task4/train/loss 0.6281323283910751 232
task4/test/loss 1.1927013623348452 232
task4/test/acc 0.6325 232
task4/train/lr 0.004671127095512003 232
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 233/256 | train_loss 0.6281 | train_acc 0.8720 | test_loss 1.1927 | test_acc 0.6325 | lr 0.0047
last_idx 11
final_idx 0
task4/train/loss 0.2409275071695447 233
task4/test/loss 1.2353697372110266 233
task4/test/acc 0.6304 233
task4/train/lr 0.004339874260069749 233
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 234/256 | train_loss 0.2409 | train_acc 0.9600 | test_loss 1.2354 | test_acc 0.6304 | lr 0.0043
last_idx 11
final_idx 0
task4/train/loss 0.8125102849056324 234
task4/test/loss 1.2138394556547467 234
task4/test/acc 0.625 234
task4/train/lr 0.004021216902493268 234
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 235/256 | train_loss 0.8125 | train_acc 0.7120 | test_loss 1.2138 | test_acc 0.6250 | lr 0.0040
last_idx 11
final_idx 0
task4/train/loss 0.45052102021873 235
task4/test/loss 1.2167887532135897 235
task4/test/acc 0.6287 235
task4/train/lr 0.0037153469700593944 235
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 236/256 | train_loss 0.4505 | train_acc 0.9480 | test_loss 1.2168 | test_acc 0.6287 | lr 0.0037
last_idx 11
final_idx 0
task4/train/loss 0.6217167892803749 236
task4/test/loss 1.2137938705191278 236
task4/test/acc 0.6273 236
task4/train/lr 0.0034224487073782153 236
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 237/256 | train_loss 0.6217 | train_acc 0.7160 | test_loss 1.2138 | test_acc 0.6273 | lr 0.0034
last_idx 11
final_idx 0
task4/train/loss 0.5312406672164798 237
task4/test/loss 1.1948901629238797 237
task4/test/acc 0.6299 237
task4/train/lr 0.0031426985454109987 237
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 238/256 | train_loss 0.5312 | train_acc 0.8640 | test_loss 1.1949 | test_acc 0.6299 | lr 0.0031
last_idx 11
final_idx 0
task4/train/loss 0.3228537840768695 238
task4/test/loss 1.2228873809962941 238
task4/test/acc 0.6282 238
task4/train/lr 0.0028762649951947776 238
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 239/256 | train_loss 0.3229 | train_acc 0.8820 | test_loss 1.2229 | test_acc 0.6282 | lr 0.0029
last_idx 11
final_idx 0
task4/train/loss 0.713019359856844 239
task4/test/loss 1.202328554370947 239
task4/test/acc 0.6278 239
task4/train/lr 0.0026233085463376153 239
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 240/256 | train_loss 0.7130 | train_acc 0.8220 | test_loss 1.2023 | test_acc 0.6278 | lr 0.0026
last_idx 11
final_idx 0
task4/train/loss 0.5960932599070171 240
task4/test/loss 1.217216964222883 240
task4/test/acc 0.6279 240
task4/train/lr 0.0023839815703456534 240
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 241/256 | train_loss 0.5961 | train_acc 0.7980 | test_loss 1.2172 | test_acc 0.6279 | lr 0.0024
last_idx 11
final_idx 0
task4/train/loss 0.5535061007055143 241
task4/test/loss 1.2013288189967473 241
task4/test/acc 0.6275 241
task4/train/lr 0.0021584282288402137 241
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 242/256 | train_loss 0.5535 | train_acc 0.8520 | test_loss 1.2013 | test_acc 0.6275 | lr 0.0022
last_idx 11
final_idx 0
task4/train/loss 0.8943687342107296 242
task4/test/loss 1.1932426514594179 242
task4/test/acc 0.6289 242
task4/train/lr 0.0019467843867202379 242
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 243/256 | train_loss 0.8944 | train_acc 0.7880 | test_loss 1.1932 | test_acc 0.6289 | lr 0.0019
last_idx 11
final_idx 0
task4/train/loss 0.7416602789113919 243
task4/test/loss 1.1853673914284037 243
task4/test/acc 0.6258 243
task4/train/lr 0.0017491775303223424 243
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 244/256 | train_loss 0.7417 | train_acc 0.7940 | test_loss 1.1854 | test_acc 0.6258 | lr 0.0017
last_idx 11
final_idx 0
task4/train/loss 0.4134419610102971 244
task4/test/loss 1.2019250775899804 244
task4/test/acc 0.6284 244
task4/train/lr 0.0015657266906278318 244
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 245/256 | train_loss 0.4134 | train_acc 0.8880 | test_loss 1.2019 | test_acc 0.6284 | lr 0.0016
last_idx 11
final_idx 0
task4/train/loss 0.48985359569390613 245
task4/test/loss 1.2189350380447872 245
task4/test/acc 0.6271 245
task4/train/lr 0.001396542371562864 245
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 246/256 | train_loss 0.4899 | train_acc 0.8540 | test_loss 1.2189 | test_acc 0.6271 | lr 0.0014
last_idx 11
final_idx 0
task4/train/loss 0.36555408624311286 246
task4/test/loss 1.225478194523276 246
task4/test/acc 0.6282 246
task4/train/lr 0.0012417264834350366 246
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 247/256 | train_loss 0.3656 | train_acc 0.8860 | test_loss 1.2255 | test_acc 0.6282 | lr 0.0012
last_idx 11
final_idx 0
task4/train/loss 0.6063161082565784 247
task4/test/loss 1.2229985670562376 247
task4/test/acc 0.6277 247
task4/train/lr 0.0011013722815464207 247
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 248/256 | train_loss 0.6063 | train_acc 0.9200 | test_loss 1.2230 | test_acc 0.6277 | lr 0.0011
last_idx 11
final_idx 0
task4/train/loss 0.5020128873487314 248
task4/test/loss 1.2240050756617595 248
task4/test/acc 0.6283 248
task4/train/lr 0.0009755643100200469 248
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 249/256 | train_loss 0.5020 | train_acc 0.8280 | test_loss 1.2240 | test_acc 0.6283 | lr 0.0010
last_idx 11
final_idx 0
task4/train/loss 0.6796102691441774 249
task4/test/loss 1.207464116053623 249
task4/test/acc 0.6282 249
task4/train/lr 0.0008643783508737047 249
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 250/256 | train_loss 0.6796 | train_acc 0.8460 | test_loss 1.2075 | test_acc 0.6282 | lr 0.0009
last_idx 11
final_idx 0
task4/train/loss 0.5509003984431425 250
task4/test/loss 1.211172061923303 250
task4/test/acc 0.6304 250
task4/train/lr 0.0007678813783716699 250
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 251/256 | train_loss 0.5509 | train_acc 0.8500 | test_loss 1.2112 | test_acc 0.6304 | lr 0.0008
last_idx 11
final_idx 0
task4/train/loss 0.24661085987463593 251
task4/test/loss 1.2182511798383897 251
task4/test/acc 0.6341 251
task4/train/lr 0.0006861315186819283 251
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 252/256 | train_loss 0.2466 | train_acc 0.9540 | test_loss 1.2183 | test_acc 0.6341 | lr 0.0007
last_idx 11
final_idx 0
task4/train/loss 0.5313441629211108 252
task4/test/loss 1.2197050843061061 252
task4/test/acc 0.632 252
task4/train/lr 0.0006191780148631288 252
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 253/256 | train_loss 0.5313 | train_acc 0.8180 | test_loss 1.2197 | test_acc 0.6320 | lr 0.0006
last_idx 11
final_idx 0
task4/train/loss 0.4635817063972354 253
task4/test/loss 1.230902689198653 253
task4/test/acc 0.6276 253
task4/train/lr 0.0005670611972024174 253
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 254/256 | train_loss 0.4636 | train_acc 0.8240 | test_loss 1.2309 | test_acc 0.6276 | lr 0.0006
last_idx 11
final_idx 0
task4/train/loss 0.7757683355982105 254
task4/test/loss 1.2063116399865401 254
task4/test/acc 0.6297 254
task4/train/lr 0.0005298124589219829 254
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 255/256 | train_loss 0.7758 | train_acc 0.7820 | test_loss 1.2063 | test_acc 0.6297 | lr 0.0005
last_idx 11
final_idx 0
task4/train/loss 0.42863599490374327 255
task4/test/loss 1.2192989042061462 255
task4/test/acc 0.6362 255
task4/train/lr 0.0005074542372689448 255
[INFO] rainbow_memory.py:184 > Task 4 | Epoch 256/256 | train_loss 0.4286 | train_acc 0.8700 | test_loss 1.2193 | test_acc 0.6362 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (4)
[INFO] main.py:398 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (4)
[INFO] main.py:405 > [2-5] Report task result
Metrics/TaskAcc 0.6362 4
[INFO] main.py:432 > ======== Summary =======
[INFO] main.py:434 > A_last 0.6362 | A_avg 0.713915 | F_last 0.14912500232458115 | I_last -0.6362
